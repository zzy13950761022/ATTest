"""
Test file for torch.autograd.functional - Group G1 (Core differential functions)
Generated by ATTest
"""
import math
import pytest
import torch
import torch.autograd.functional as autograd_func
import numpy as np
from typing import Callable, Tuple, Union

# Set random seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Helper functions
def simple_scalar_func(x: torch.Tensor) -> torch.Tensor:
    """Simple scalar function: sum of squares"""
    return torch.sum(x ** 2)

def linear_transform_func(x: torch.Tensor) -> torch.Tensor:
    """Linear transformation: y = A @ x + b"""
    A = torch.tensor([[1.0, 0.5, 0.2],
                      [0.5, 1.0, 0.3],
                      [0.2, 0.3, 1.0]], dtype=x.dtype)
    b = torch.tensor([0.1, 0.2, 0.3], dtype=x.dtype)
    return A @ x + b

def independent_output_func(x: torch.Tensor) -> torch.Tensor:
    """Function with output independent of input (for strict mode testing)"""
    return torch.tensor([1.0, 2.0], dtype=x.dtype, device=x.device)

def numerical_gradient(func: Callable, x: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:
    """Compute numerical gradient using finite differences"""
    grad = torch.zeros_like(x)
    x_flat = x.flatten()
    grad_flat = grad.flatten()
    
    for i in range(len(x_flat)):
        x_plus = x_flat.clone()
        x_minus = x_flat.clone()
        x_plus[i] += eps
        x_minus[i] -= eps
        
        x_plus_reshaped = x_plus.reshape(x.shape)
        x_minus_reshaped = x_minus.reshape(x.shape)
        
        f_plus = func(x_plus_reshaped)
        f_minus = func(x_minus_reshaped)
        
        if f_plus.dim() == 0 and f_minus.dim() == 0:
            grad_flat[i] = (f_plus - f_minus) / (2 * eps)
        else:
            # For vector outputs, compute gradient for each output dimension
            pass
    
    return grad

def numerical_jacobian(func: Callable, x: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:
    """Compute numerical Jacobian using finite differences"""
    x_flat = x.flatten()
    y = func(x)
    y_flat = y.flatten()
    
    jac = torch.zeros(len(y_flat), len(x_flat), dtype=x.dtype, device=x.device)
    
    for i in range(len(x_flat)):
        x_plus = x_flat.clone()
        x_minus = x_flat.clone()
        x_plus[i] += eps
        x_minus[i] -= eps
        
        x_plus_reshaped = x_plus.reshape(x.shape)
        x_minus_reshaped = x_minus.reshape(x.shape)
        
        y_plus = func(x_plus_reshaped).flatten()
        y_minus = func(x_minus_reshaped).flatten()
        
        jac[:, i] = (y_plus - y_minus) / (2 * eps)
    
    return jac

# Test class
class TestAutogradFunctionalG1:
    """Test cases for torch.autograd.functional - Group G1"""
    
    # ==== BLOCK:HEADER START ====
    # Fixtures and setup for G1 tests
    
    @pytest.fixture
    def random_tensor_2x2_float32(self):
        """Random 2x2 tensor with float32 dtype"""
        return torch.randn(2, 2, dtype=torch.float32)
    
    @pytest.fixture
    def random_tensor_3_float64(self):
        """Random 3-element vector with float64 dtype"""
        return torch.randn(3, dtype=torch.float64)
    
    @pytest.fixture
    def random_vector_for_vjp(self):
        """Random vector for vjp testing"""
        return torch.randn(1, dtype=torch.float32)
    # ==== BLOCK:HEADER END ====
    
    # ==== BLOCK:CASE_01 START ====
    @pytest.mark.parametrize("func_type,inputs_shape,dtype,device,create_graph,strict", [
        ("simple_scalar", [2, 2], torch.float32, "cpu", False, False),
    ])
    def test_vjp_basic_functionality(self, func_type, inputs_shape, dtype, device, create_graph, strict):
        """TC-01: vjp基本功能验证
        
        Test basic vector-Jacobian product functionality with weak assertions.
        """
        # Setup
        torch.manual_seed(42)
        
        # Create input tensor
        x = torch.randn(*inputs_shape, dtype=dtype, device=device, requires_grad=True)
        
        # Select function based on type
        if func_type == "simple_scalar":
            func = simple_scalar_func
        else:
            pytest.skip(f"Unsupported func_type: {func_type}")
        
        # Create vector v (same shape as output)
        with torch.no_grad():
            y = func(x)
            if y.dim() == 0:  # scalar output
                v = torch.tensor(1.0, dtype=dtype, device=device)
            else:
                v = torch.randn_like(y)
        
        # Compute vjp using autograd.functional
        output, vjp_result = autograd_func.vjp(
            func=func,
            inputs=x,
            v=v,
            create_graph=create_graph,
            strict=strict
        )
        
        # Weak assertion 1: shape_match
        assert output.shape == y.shape, f"Output shape mismatch: {output.shape} != {y.shape}"
        assert vjp_result.shape == x.shape, f"VJP result shape mismatch: {vjp_result.shape} != {x.shape}"
        
        # Weak assertion 2: dtype_match
        assert output.dtype == dtype, f"Output dtype mismatch: {output.dtype} != {dtype}"
        assert vjp_result.dtype == dtype, f"VJP result dtype mismatch: {vjp_result.dtype} != {dtype}"
        
        # Weak assertion 3: finite_values
        assert torch.isfinite(output).all(), "Output contains non-finite values"
        assert torch.isfinite(vjp_result).all(), "VJP result contains non-finite values"
        
        # Weak assertion 4: gradient_check (compare with torch.autograd.grad)
        # Compute gradient using torch.autograd.grad
        y_manual = func(x)
        if y_manual.dim() == 0:
            # For scalar output, vjp should equal gradient
            grad_manual = torch.autograd.grad(y_manual, x, create_graph=create_graph)[0]
            # vjp_result should equal grad_manual when v=1.0
            if torch.allclose(v, torch.tensor(1.0, dtype=dtype, device=device)):
                assert torch.allclose(vjp_result, grad_manual, rtol=1e-5, atol=1e-7), \
                    f"VJP result doesn't match manual gradient: max diff = {(vjp_result - grad_manual).abs().max().item()}"
        
        # Additional check: output should match direct function evaluation
        assert torch.allclose(output, y, rtol=1e-5, atol=1e-7), \
            f"Output doesn't match direct evaluation: max diff = {(output - y).abs().max().item()}"
        
        # Check that create_graph parameter works
        if create_graph:
            # Should be able to compute gradients of vjp_result
            assert vjp_result.requires_grad, "vjp_result should require grad when create_graph=True"
        else:
            # Should not have gradient history
            assert not vjp_result.requires_grad, "vjp_result should not require grad when create_graph=False"
    # ==== BLOCK:CASE_01 END ====
    
    # ==== BLOCK:CASE_02 START ====
    @pytest.mark.parametrize("func_type,inputs_shape,dtype,device,create_graph,strict,strategy", [
        ("linear_transform", [3], torch.float64, "cpu", False, False, "reverse-mode"),
    ])
    def test_jacobian_matrix_correctness(self, func_type, inputs_shape, dtype, device, create_graph, strict, strategy):
        """TC-02: jacobian矩阵正确性
        
        Test Jacobian matrix correctness with weak assertions.
        """
        # Setup
        torch.manual_seed(42)
        
        # Create input tensor
        x = torch.randn(*inputs_shape, dtype=dtype, device=device, requires_grad=True)
        
        # Select function based on type
        if func_type == "linear_transform":
            func = linear_transform_func
        else:
            pytest.skip(f"Unsupported func_type: {func_type}")
        
        # Compute Jacobian using autograd.functional
        jacobian = autograd_func.jacobian(
            func=func,
            inputs=x,
            create_graph=create_graph,
            strict=strict,
            strategy=strategy
        )
        
        # Get output for shape reference
        y = func(x)
        
        # Weak assertion 1: shape_match
        # Jacobian shape should be (output_elements, input_elements)
        expected_shape = (y.numel(), x.numel())
        assert jacobian.shape == expected_shape, \
            f"Jacobian shape mismatch: {jacobian.shape} != {expected_shape}"
        
        # Weak assertion 2: dtype_match
        assert jacobian.dtype == dtype, f"Jacobian dtype mismatch: {jacobian.dtype} != {dtype}"
        
        # Weak assertion 3: finite_values
        assert torch.isfinite(jacobian).all(), "Jacobian contains non-finite values"
        
        # Weak assertion 4: matrix_structure
        # For linear functions, Jacobian should be constant (independent of x)
        # Let's test with a different input point
        x2 = torch.randn_like(x)
        jacobian2 = autograd_func.jacobian(
            func=func,
            inputs=x2,
            create_graph=create_graph,
            strict=strict,
            strategy=strategy
        )
        
        # For linear functions, Jacobian should be the same
        if func_type == "linear_transform":
            assert torch.allclose(jacobian, jacobian2, rtol=1e-5, atol=1e-7), \
                "Jacobian should be constant for linear functions"
        
        # Additional check: compare with analytical Jacobian for linear functions
        if func_type == "linear_transform":
            # For y = A @ x + b, Jacobian should be A
            A = torch.tensor([[1.0, 0.5, 0.2],
                             [0.5, 1.0, 0.3],
                             [0.2, 0.3, 1.0]], dtype=dtype)
            assert torch.allclose(jacobian, A, rtol=1e-5, atol=1e-7), \
                f"Jacobian doesn't match analytical solution: max diff = {(jacobian - A).abs().max().item()}"
        
        # Check strategy parameter
        if strategy == "reverse-mode":
            # Should work for both reverse and forward mode
            pass
        elif strategy == "forward-mode":
            # Forward mode requires vectorize=True, but we're testing without it
            # This should still work for small inputs
            pass
        
        # Check that create_graph parameter works
        if create_graph:
            # Should be able to compute gradients of jacobian
            assert jacobian.requires_grad, "jacobian should require grad when create_graph=True"
        else:
            # Should not have gradient history
            assert not jacobian.requires_grad, "jacobian should not require grad when create_graph=False"
    # ==== BLOCK:CASE_02 END ====
    
    # ==== BLOCK:CASE_05 START ====
    # TC-05: strict模式检测 (Deferred - placeholder only)
    # This test case is deferred and will be implemented in later iterations
    # when strict mode testing is enabled.
    pass
    # ==== BLOCK:CASE_05 END ====
    
    # ==== BLOCK:FOOTER START ====
    # ==== BLOCK:FOOTER END ====