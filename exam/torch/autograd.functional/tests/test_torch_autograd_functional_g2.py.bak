"""
Test file for torch.autograd.functional - Group G2 (Higher-order differentials and vectorization)
Generated by ATTest
"""
import math
import pytest
import torch
import torch.autograd.functional as autograd_func
import numpy as np
from typing import Callable, Tuple, Union

# Set random seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Helper functions for G2
def quadratic_form_func(x: torch.Tensor) -> torch.Tensor:
    """Quadratic form: x^T A x"""
    A = torch.tensor([[2.0, 0.5],
                      [0.5, 1.0]], dtype=x.dtype)
    return x @ A @ x

def simple_scalar_func(x: torch.Tensor) -> torch.Tensor:
    """Simple scalar function: sum of squares"""
    return torch.sum(x ** 2)

def vectorized_func(x: torch.Tensor) -> torch.Tensor:
    """Function suitable for vectorization testing"""
    return torch.stack([x[0]**2, x[1]**3])

def numerical_hessian(func: Callable, x: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:
    """Compute numerical Hessian using finite differences"""
    n = x.numel()
    hess = torch.zeros(n, n, dtype=x.dtype, device=x.device)
    
    for i in range(n):
        # Compute gradient at x + eps*e_i and x - eps*e_i
        x_plus = x.flatten().clone()
        x_minus = x.flatten().clone()
        x_plus[i] += eps
        x_minus[i] -= eps
        
        x_plus_reshaped = x_plus.reshape(x.shape)
        x_minus_reshaped = x_minus.reshape(x.shape)
        
        # Compute gradients using autograd
        x_plus_reshaped.requires_grad_(True)
        x_minus_reshaped.requires_grad_(True)
        
        y_plus = func(x_plus_reshaped)
        y_minus = func(x_minus_reshaped)
        
        if y_plus.dim() == 0 and y_minus.dim() == 0:
            grad_plus = torch.autograd.grad(y_plus, x_plus_reshaped, create_graph=False)[0].flatten()
            grad_minus = torch.autograd.grad(y_minus, x_minus_reshaped, create_graph=False)[0].flatten()
            
            hess[i, :] = (grad_plus - grad_minus) / (2 * eps)
    
    return hess

# ==== BLOCK:HEADER START ====
# Fixtures and setup for G2 tests

@pytest.fixture
def random_tensor_2_float32():
    """Random 2-element vector with float32 dtype"""
    return torch.randn(2, dtype=torch.float32)

@pytest.fixture
def random_tensor_2x2_float32():
    """Random 2x2 tensor with float32 dtype"""
    return torch.randn(2, 2, dtype=torch.float32)

@pytest.fixture
def random_vector_for_vhp():
    """Random vector for vhp testing"""
    return torch.randn(2, dtype=torch.float32)
# ==== BLOCK:HEADER END ====

# Test class
class TestAutogradFunctionalG2:
    """Test cases for torch.autograd.functional - Group G2"""
    
    # ==== BLOCK:CASE_03 START ====
    @pytest.mark.parametrize("func_type,inputs_shape,dtype,device,create_graph,strict", [
        ("quadratic_form", [2], torch.float32, "cpu", False, False),
    ])
    def test_vhp_basic_functionality(self, func_type, inputs_shape, dtype, device, create_graph, strict):
        """TC-03: vhp基本功能验证
        
        Test basic vector-Hessian product functionality with weak assertions.
        """
        # Setup
        torch.manual_seed(42)
        
        # Create input tensor
        x = torch.randn(*inputs_shape, dtype=dtype, device=device, requires_grad=True)
        
        # Select function based on type
        if func_type == "quadratic_form":
            func = quadratic_form_func
        else:
            pytest.skip(f"Unsupported func_type: {func_type}")
        
        # Create vector v (same shape as input for vhp)
        v = torch.randn(*inputs_shape, dtype=dtype, device=device)
        
        # Compute vhp using autograd.functional
        output, vhp_result = autograd_func.vhp(
            func=func,
            inputs=x,
            v=v,
            create_graph=create_graph,
            strict=strict
        )
        
        # Get direct function evaluation for comparison
        y = func(x)
        
        # Weak assertion 1: shape_match
        assert output.shape == y.shape, f"Output shape mismatch: {output.shape} != {y.shape}"
        assert vhp_result.shape == x.shape, f"VHP result shape mismatch: {vhp_result.shape} != {x.shape}"
        
        # Weak assertion 2: dtype_match
        assert output.dtype == dtype, f"Output dtype mismatch: {output.dtype} != {dtype}"
        assert vhp_result.dtype == dtype, f"VHP result dtype mismatch: {vhp_result.dtype} != {dtype}"
        
        # Weak assertion 3: finite_values
        assert torch.isfinite(output).all(), "Output contains non-finite values"
        assert torch.isfinite(vhp_result).all(), "VHP result contains non-finite values"
        
        # Weak assertion 4: hessian_consistency
        # For quadratic forms, vhp should be linear in v
        # Test with scaled v
        v_scaled = 2.0 * v
        _, vhp_result_scaled = autograd_func.vhp(func, x, v_scaled, create_graph=create_graph, strict=strict)
        
        # Check linearity: vhp(2*v) should equal 2*vhp(v)
        assert torch.allclose(vhp_result_scaled, 2.0 * vhp_result, rtol=1e-5, atol=1e-7), \
            "VHP should be linear in v for quadratic forms"
        
        # Additional check: output should match direct function evaluation
        assert torch.allclose(output, y, rtol=1e-5, atol=1e-7), \
            f"Output doesn't match direct evaluation: max diff = {(output - y).abs().max().item()}"
        
        # For quadratic forms, we can compute analytical vhp
        if func_type == "quadratic_form":
            # For f(x) = x^T A x, gradient is 2Ax, Hessian is 2A
            # So vhp(v) = Hessian * v = 2A * v
            A = torch.tensor([[2.0, 0.5],
                             [0.5, 1.0]], dtype=dtype)
            analytical_vhp = 2.0 * (A @ v)
            
            # Compare with computed vhp
            assert torch.allclose(vhp_result, analytical_vhp, rtol=1e-5, atol=1e-7), \
                f"VHP doesn't match analytical solution: max diff = {(vhp_result - analytical_vhp).abs().max().item()}"
        
        # Check that create_graph parameter works
        if create_graph:
            # Should be able to compute gradients of vhp_result
            assert vhp_result.requires_grad, "vhp_result should require grad when create_graph=True"
        else:
            # Should not have gradient history
            assert not vhp_result.requires_grad, "vhp_result should not require grad when create_graph=False"
    # ==== BLOCK:CASE_03 END ====
    
    # ==== BLOCK:CASE_04 START ====
    @pytest.mark.parametrize("func_type,inputs_shape,dtype,device,create_graph,strict", [
        ("simple_scalar", [2, 2], torch.float32, "cpu", True, False),
    ])
    def test_create_graph_parameter(self, func_type, inputs_shape, dtype, device, create_graph, strict):
        """TC-04: create_graph参数测试
        
        Test create_graph parameter functionality with weak assertions.
        """
        # Setup
        torch.manual_seed(42)
        
        # Create input tensor
        x = torch.randn(*inputs_shape, dtype=dtype, device=device, requires_grad=True)
        
        # Select function based on type
        if func_type == "simple_scalar":
            func = simple_scalar_func
        else:
            pytest.skip(f"Unsupported func_type: {func_type}")
        
        # Create vector v (same shape as output)
        with torch.no_grad():
            y = func(x)
            if y.dim() == 0:  # scalar output
                v = torch.tensor(1.0, dtype=dtype, device=device)
            else:
                v = torch.randn_like(y)
        
        # Compute vjp with create_graph=True
        output, vjp_result = autograd_func.vjp(
            func=func,
            inputs=x,
            v=v,
            create_graph=create_graph,
            strict=strict
        )
        
        # Weak assertion 1: shape_match
        assert output.shape == y.shape, f"Output shape mismatch: {output.shape} != {y.shape}"
        assert vjp_result.shape == x.shape, f"VJP result shape mismatch: {vjp_result.shape} != {x.shape}"
        
        # Weak assertion 2: dtype_match
        assert output.dtype == dtype, f"Output dtype mismatch: {output.dtype} != {dtype}"
        assert vjp_result.dtype == dtype, f"VJP result dtype mismatch: {vjp_result.dtype} != {dtype}"
        
        # Weak assertion 3: finite_values
        assert torch.isfinite(output).all(), "Output contains non-finite values"
        assert torch.isfinite(vjp_result).all(), "VJP result contains non-finite values"
        
        # Weak assertion 4: graph_creation
        # When create_graph=True, we should be able to compute higher-order gradients
        if create_graph:
            # vjp_result should have gradient history
            assert vjp_result.requires_grad, "vjp_result should require grad when create_graph=True"
            
            # Test that we can compute gradient of vjp_result
            # Create a simple function of vjp_result
            loss = torch.sum(vjp_result ** 2)
            
            # Compute gradient
            grad_of_vjp = torch.autograd.grad(loss, x, create_graph=False)[0]
            
            # Check that gradient computation succeeds
            assert grad_of_vjp is not None, "Should be able to compute gradient of vjp_result"
            assert grad_of_vjp.shape == x.shape, "Gradient shape should match input shape"
            assert torch.isfinite(grad_of_vjp).all(), "Gradient should contain finite values"
            
            # For create_graph=True, we should also be able to compute second-order gradients
            # by setting create_graph=True in the inner grad call
            if x.requires_grad:
                loss2 = torch.sum(vjp_result ** 2)
                second_order_grad = torch.autograd.grad(loss2, x, create_graph=True)[0]
                assert second_order_grad.requires_grad, "Second order gradient should require grad"
        else:
            # When create_graph=False, vjp_result should not have gradient history
            assert not vjp_result.requires_grad, "vjp_result should not require grad when create_graph=False"
        
        # Additional check: compare with manual gradient computation
        y_manual = func(x)
        grad_manual = torch.autograd.grad(y_manual, x, create_graph=create_graph)[0]
        
        # For scalar output with v=1.0, vjp_result should equal gradient
        if y_manual.dim() == 0 and torch.allclose(v, torch.tensor(1.0, dtype=dtype, device=device)):
            assert torch.allclose(vjp_result, grad_manual, rtol=1e-5, atol=1e-7), \
                f"VJP result doesn't match manual gradient: max diff = {(vjp_result - grad_manual).abs().max().item()}"
            
            # Check gradient properties match
            assert vjp_result.requires_grad == grad_manual.requires_grad, \
                "VJP result and manual gradient should have same requires_grad status"
        
        # Verify output matches direct evaluation
        assert torch.allclose(output, y, rtol=1e-5, atol=1e-7), \
            f"Output doesn't match direct evaluation: max diff = {(output - y).abs().max().item()}"
    # ==== BLOCK:CASE_04 END ====
    
    # ==== BLOCK:CASE_06 START ====
    # TC-06: 向量化功能测试 (Deferred - placeholder only)
    # This test case is deferred and will be implemented in later iterations
    # when vectorization testing is enabled and requires_mock is available.
    pass
    # ==== BLOCK:CASE_06 END ====
    
    # ==== BLOCK:FOOTER START ====
    # ==== BLOCK:FOOTER END ====