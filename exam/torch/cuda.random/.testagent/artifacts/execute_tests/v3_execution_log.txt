=== Run Tests ===
FFFF....FFFF......FF..FFFFF......ssssssssFF..FFFFF.......ssssss.......   [100%]
=================================== FAILURES ===================================
____ TestTorchCudaRandomCudaUnavailable.test_get_rng_state_cuda_unavailable ____

self = <test_torch_cuda_random_cuda_unavailable.TestTorchCudaRandomCudaUnavailable object at 0x126387c70>

    def test_get_rng_state_cuda_unavailable(self):
        """Test get_rng_state when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            with pytest.raises(RuntimeError) as exc_info:
>               torch.cuda.get_rng_state()

tests/test_torch_cuda_random_cuda_unavailable.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/random.py:22: in get_rng_state
    _lazy_init()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, 'is_initializing'):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method")
            if not hasattr(torch._C, '_cuda_getDeviceCount'):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/__init__.py:221: AssertionError
__ TestTorchCudaRandomCudaUnavailable.test_get_rng_state_all_cuda_unavailable __

self = <test_torch_cuda_random_cuda_unavailable.TestTorchCudaRandomCudaUnavailable object at 0x126387f40>

    def test_get_rng_state_all_cuda_unavailable(self):
        """Test get_rng_state_all when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
>           with pytest.raises(RuntimeError) as exc_info:
E           Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_cuda_random_cuda_unavailable.py:28: Failed
____ TestTorchCudaRandomCudaUnavailable.test_set_rng_state_cuda_unavailable ____

self = <test_torch_cuda_random_cuda_unavailable.TestTorchCudaRandomCudaUnavailable object at 0x1263cc250>

    def test_set_rng_state_cuda_unavailable(self):
        """Test set_rng_state when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            test_state = torch.ByteTensor(100).random_(0, 256)
>           with pytest.raises(RuntimeError) as exc_info:
E           Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_cuda_random_cuda_unavailable.py:39: Failed
__ TestTorchCudaRandomCudaUnavailable.test_set_rng_state_all_cuda_unavailable __

self = <test_torch_cuda_random_cuda_unavailable.TestTorchCudaRandomCudaUnavailable object at 0x1263cc520>

    def test_set_rng_state_all_cuda_unavailable(self):
        """Test set_rng_state_all when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            test_states = [torch.ByteTensor(100).random_(0, 256) for _ in range(2)]
>           with pytest.raises(RuntimeError) as exc_info:
E           Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_cuda_random_cuda_unavailable.py:50: Failed
____ TestTorchCudaRandomCudaUnavailable.test_initial_seed_cuda_unavailable _____

self = <test_torch_cuda_random_cuda_unavailable.TestTorchCudaRandomCudaUnavailable object at 0x126387d60>

    def test_initial_seed_cuda_unavailable(self):
        """Test initial_seed when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            with pytest.raises(RuntimeError) as exc_info:
>               torch.cuda.initial_seed()

tests/test_torch_cuda_random_cuda_unavailable.py:107: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/random.py:159: in initial_seed
    _lazy_init()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, 'is_initializing'):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method")
            if not hasattr(torch._C, '_cuda_getDeviceCount'):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/__init__.py:221: AssertionError
__ TestTorchCudaRandomCudaUnavailable.test_empty_state_list_cuda_unavailable ___

self = <test_torch_cuda_random_cuda_unavailable.TestTorchCudaRandomCudaUnavailable object at 0x126386a70>

    def test_empty_state_list_cuda_unavailable(self):
        """Test empty state list handling when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
>           with pytest.raises(RuntimeError) as exc_info:
E           Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_cuda_random_cuda_unavailable.py:116: Failed
___ TestTorchCudaRandomCudaUnavailable.test_invalid_device_cuda_unavailable ____

self = <test_torch_cuda_random_cuda_unavailable.TestTorchCudaRandomCudaUnavailable object at 0x1263cccd0>

    def test_invalid_device_cuda_unavailable(self):
        """Test invalid device index when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            # Test with invalid device index
            with pytest.raises(RuntimeError) as exc_info:
>               torch.cuda.get_rng_state(device=-1)

tests/test_torch_cuda_random_cuda_unavailable.py:128: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/random.py:22: in get_rng_state
    _lazy_init()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, 'is_initializing'):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method")
            if not hasattr(torch._C, '_cuda_getDeviceCount'):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/__init__.py:221: AssertionError
_ TestTorchCudaRandomCudaUnavailable.test_non_byte_tensor_state_cuda_unavailable _

self = <test_torch_cuda_random_cuda_unavailable.TestTorchCudaRandomCudaUnavailable object at 0x1263cc9d0>

    def test_non_byte_tensor_state_cuda_unavailable(self):
        """Test non-ByteTensor state when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            # Create non-ByteTensor state
            float_state = torch.FloatTensor(100).normal_()
    
>           with pytest.raises(RuntimeError) as exc_info:
E           Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_cuda_random_cuda_unavailable.py:148: Failed
___ TestTorchCudaRandomG1CudaUnavailable.test_get_rng_state_cuda_unavailable ___

self = <test_torch_cuda_random_g1_cuda_unavailable.TestTorchCudaRandomG1CudaUnavailable object at 0x1263de980>

    def test_get_rng_state_cuda_unavailable(self):
        """Test get_rng_state when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            with pytest.raises(RuntimeError) as exc_info:
>               torch.cuda.get_rng_state()

tests/test_torch_cuda_random_g1_cuda_unavailable.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/random.py:22: in get_rng_state
    _lazy_init()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, 'is_initializing'):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method")
            if not hasattr(torch._C, '_cuda_getDeviceCount'):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/__init__.py:221: AssertionError
___ TestTorchCudaRandomG1CudaUnavailable.test_set_rng_state_cuda_unavailable ___

self = <test_torch_cuda_random_g1_cuda_unavailable.TestTorchCudaRandomG1CudaUnavailable object at 0x1263de830>

    def test_set_rng_state_cuda_unavailable(self):
        """Test set_rng_state when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            test_state = torch.ByteTensor(100).random_(0, 256)
>           with pytest.raises(RuntimeError) as exc_info:
E           Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_cuda_random_g1_cuda_unavailable.py:29: Failed
___ TestTorchCudaRandomG1CudaUnavailable.test_initial_seed_cuda_unavailable ____

self = <test_torch_cuda_random_g1_cuda_unavailable.TestTorchCudaRandomG1CudaUnavailable object at 0x1263dde70>

    def test_initial_seed_cuda_unavailable(self):
        """Test initial_seed when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            with pytest.raises(RuntimeError) as exc_info:
>               torch.cuda.initial_seed()

tests/test_torch_cuda_random_g1_cuda_unavailable.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/random.py:159: in initial_seed
    _lazy_init()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, 'is_initializing'):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method")
            if not hasattr(torch._C, '_cuda_getDeviceCount'):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/__init__.py:221: AssertionError
_ TestTorchCudaRandomG1CudaUnavailable.test_invalid_device_index_cuda_unavailable _

self = <test_torch_cuda_random_g1_cuda_unavailable.TestTorchCudaRandomG1CudaUnavailable object at 0x1263dddb0>

    def test_invalid_device_index_cuda_unavailable(self):
        """Test invalid device index when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            # Test with invalid device index
            with pytest.raises(RuntimeError) as exc_info:
>               torch.cuda.get_rng_state(device=-1)

tests/test_torch_cuda_random_g1_cuda_unavailable.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/random.py:22: in get_rng_state
    _lazy_init()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, 'is_initializing'):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method")
            if not hasattr(torch._C, '_cuda_getDeviceCount'):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/__init__.py:221: AssertionError
_ TestTorchCudaRandomG1CudaUnavailable.test_non_byte_tensor_state_cuda_unavailable _

self = <test_torch_cuda_random_g1_cuda_unavailable.TestTorchCudaRandomG1CudaUnavailable object at 0x1263dd780>

    def test_non_byte_tensor_state_cuda_unavailable(self):
        """Test non-ByteTensor state when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            # Create non-ByteTensor state
            float_state = torch.FloatTensor(100).normal_()
    
>           with pytest.raises(RuntimeError) as exc_info:
E           Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_cuda_random_g1_cuda_unavailable.py:94: Failed
_ TestTorchCudaRandomG1CudaUnavailable.test_single_device_state_management_cuda_unavailable _

self = <test_torch_cuda_random_g1_cuda_unavailable.TestTorchCudaRandomG1CudaUnavailable object at 0x1263de740>

    def test_single_device_state_management_cuda_unavailable(self):
        """Test single device state management when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            # Test all G1 functions in CUDA unavailable scenario
    
            # get_rng_state should raise RuntimeError
            with pytest.raises(RuntimeError):
>               torch.cuda.get_rng_state()

tests/test_torch_cuda_random_g1_cuda_unavailable.py:109: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/random.py:22: in get_rng_state
    _lazy_init()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, 'is_initializing'):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method")
            if not hasattr(torch._C, '_cuda_getDeviceCount'):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/__init__.py:221: AssertionError
_ TestTorchCudaRandomG1CudaUnavailable.test_device_string_format_cuda_unavailable _

self = <test_torch_cuda_random_g1_cuda_unavailable.TestTorchCudaRandomG1CudaUnavailable object at 0x1263dd2a0>

    def test_device_string_format_cuda_unavailable(self):
        """Test device string format when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            test_state = torch.ByteTensor(100).random_(0, 256)
    
            # Test with string device
            with pytest.raises(RuntimeError) as exc_info:
>               torch.cuda.get_rng_state(device="cuda")

tests/test_torch_cuda_random_g1_cuda_unavailable.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/random.py:22: in get_rng_state
    _lazy_init()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, 'is_initializing'):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method")
            if not hasattr(torch._C, '_cuda_getDeviceCount'):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/__init__.py:221: AssertionError
_ TestTorchCudaRandomG2CudaUnavailable.test_get_rng_state_all_cuda_unavailable _

self = <test_torch_cuda_random_g2_cuda_unavailable.TestTorchCudaRandomG2CudaUnavailable object at 0x1263df070>

    def test_get_rng_state_all_cuda_unavailable(self):
        """Test get_rng_state_all when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
>           with pytest.raises(RuntimeError) as exc_info:
E           Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_cuda_random_g2_cuda_unavailable.py:18: Failed
_ TestTorchCudaRandomG2CudaUnavailable.test_set_rng_state_all_cuda_unavailable _

self = <test_torch_cuda_random_g2_cuda_unavailable.TestTorchCudaRandomG2CudaUnavailable object at 0x1263debf0>

    def test_set_rng_state_all_cuda_unavailable(self):
        """Test set_rng_state_all when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            test_states = [torch.ByteTensor(100).random_(0, 256) for _ in range(2)]
>           with pytest.raises(RuntimeError) as exc_info:
E           Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_cuda_random_g2_cuda_unavailable.py:29: Failed
_ TestTorchCudaRandomG2CudaUnavailable.test_empty_state_list_cuda_unavailable __

self = <test_torch_cuda_random_g2_cuda_unavailable.TestTorchCudaRandomG2CudaUnavailable object at 0x1263dde40>

    def test_empty_state_list_cuda_unavailable(self):
        """Test empty state list handling when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
>           with pytest.raises(RuntimeError) as exc_info:
E           Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_cuda_random_g2_cuda_unavailable.py:62: Failed
_ TestTorchCudaRandomG2CudaUnavailable.test_invalid_device_index_cuda_unavailable _

self = <test_torch_cuda_random_g2_cuda_unavailable.TestTorchCudaRandomG2CudaUnavailable object at 0x1263dd300>

    def test_invalid_device_index_cuda_unavailable(self):
        """Test invalid device index when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            # Test with invalid device index for get_rng_state
            with pytest.raises(RuntimeError) as exc_info:
>               torch.cuda.get_rng_state(device=-1)

tests/test_torch_cuda_random_g2_cuda_unavailable.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/random.py:22: in get_rng_state
    _lazy_init()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, 'is_initializing'):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method")
            if not hasattr(torch._C, '_cuda_getDeviceCount'):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/__init__.py:221: AssertionError
_ TestTorchCudaRandomG2CudaUnavailable.test_multi_device_batch_management_cuda_unavailable _

self = <test_torch_cuda_random_g2_cuda_unavailable.TestTorchCudaRandomG2CudaUnavailable object at 0x1263dc8b0>

    def test_multi_device_batch_management_cuda_unavailable(self):
        """Test multi-device batch management when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            # Test all G2 functions in CUDA unavailable scenario
    
            # get_rng_state_all should raise RuntimeError
>           with pytest.raises(RuntimeError):
E           Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_cuda_random_g2_cuda_unavailable.py:95: Failed
_ TestTorchCudaRandomG2CudaUnavailable.test_state_list_length_mismatch_cuda_unavailable _

self = <test_torch_cuda_random_g2_cuda_unavailable.TestTorchCudaRandomG2CudaUnavailable object at 0x1263dd5d0>

    def test_state_list_length_mismatch_cuda_unavailable(self):
        """Test state list length mismatch when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            # Create state list with wrong length
            wrong_length_states = [
                torch.ByteTensor(100).random_(0, 256),
                torch.ByteTensor(100).random_(0, 256),
                torch.ByteTensor(100).random_(0, 256)  # Three states
            ]
    
>           with pytest.raises(RuntimeError) as exc_info:
E           Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_cuda_random_g2_cuda_unavailable.py:128: Failed
_ TestTorchCudaRandomG2CudaUnavailable.test_zero_length_tensor_state_cuda_unavailable _

self = <test_torch_cuda_random_g2_cuda_unavailable.TestTorchCudaRandomG2CudaUnavailable object at 0x1263de860>

    def test_zero_length_tensor_state_cuda_unavailable(self):
        """Test zero-length tensor state when CUDA is not available."""
        with patch('torch.cuda.is_available', return_value=False):
            zero_tensor_states = [torch.ByteTensor(0), torch.ByteTensor(0)]
    
>           with pytest.raises(RuntimeError) as exc_info:
E           Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_cuda_random_g2_cuda_unavailable.py:140: Failed
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                                  Stmts   Miss Branch BrPart  Cover   Missing
-------------------------------------------------------------------------------------------------
tests/test_torch_cuda_random_cuda_unavailable.py         91     20      2      0    78%   21-22, 31-32, 42-43, 53-54, 109-110, 119-120, 130-139, 151-153
tests/test_torch_cuda_random_g1.py                      178    137     28      0    20%   37-42, 47-48, 53-54, 59-60, 84-144, 169-251, 277, 302, 325-367, 391-438, 469-472, 483, 494-504, 515-523, 530-544, 555-561
tests/test_torch_cuda_random_g1_cuda_unavailable.py      87     31      2      0    65%   21-22, 32-33, 65-66, 76-85, 97-99, 112-131, 142-150
tests/test_torch_cuda_random_g2.py                      239    170     62      8    27%   34, 38, 47-48, 53-60, 96-188, 226-281, 311-313, 317-386, 416-418, 422-493, 528-530, 533-551, 561->exit, 566-567
tests/test_torch_cuda_random_g2_cuda_unavailable.py      89     27      4      0    69%   21-22, 32-33, 65-66, 76-86, 99-116, 131-132, 143-144
tests/test_torch_cuda_random_g2_temp.py                 149    123     40      0    14%   31-36, 45-46, 51-58, 81-177, 202-261, 278, 286-299, 305-323, 333-364
-------------------------------------------------------------------------------------------------
TOTAL                                                   833    508    138      8    35%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_cuda_random_cuda_unavailable.py::TestTorchCudaRandomCudaUnavailable::test_get_rng_state_cuda_unavailable
FAILED tests/test_torch_cuda_random_cuda_unavailable.py::TestTorchCudaRandomCudaUnavailable::test_get_rng_state_all_cuda_unavailable
FAILED tests/test_torch_cuda_random_cuda_unavailable.py::TestTorchCudaRandomCudaUnavailable::test_set_rng_state_cuda_unavailable
FAILED tests/test_torch_cuda_random_cuda_unavailable.py::TestTorchCudaRandomCudaUnavailable::test_set_rng_state_all_cuda_unavailable
FAILED tests/test_torch_cuda_random_cuda_unavailable.py::TestTorchCudaRandomCudaUnavailable::test_initial_seed_cuda_unavailable
FAILED tests/test_torch_cuda_random_cuda_unavailable.py::TestTorchCudaRandomCudaUnavailable::test_empty_state_list_cuda_unavailable
FAILED tests/test_torch_cuda_random_cuda_unavailable.py::TestTorchCudaRandomCudaUnavailable::test_invalid_device_cuda_unavailable
FAILED tests/test_torch_cuda_random_cuda_unavailable.py::TestTorchCudaRandomCudaUnavailable::test_non_byte_tensor_state_cuda_unavailable
FAILED tests/test_torch_cuda_random_g1_cuda_unavailable.py::TestTorchCudaRandomG1CudaUnavailable::test_get_rng_state_cuda_unavailable
FAILED tests/test_torch_cuda_random_g1_cuda_unavailable.py::TestTorchCudaRandomG1CudaUnavailable::test_set_rng_state_cuda_unavailable
FAILED tests/test_torch_cuda_random_g1_cuda_unavailable.py::TestTorchCudaRandomG1CudaUnavailable::test_initial_seed_cuda_unavailable
FAILED tests/test_torch_cuda_random_g1_cuda_unavailable.py::TestTorchCudaRandomG1CudaUnavailable::test_invalid_device_index_cuda_unavailable
FAILED tests/test_torch_cuda_random_g1_cuda_unavailable.py::TestTorchCudaRandomG1CudaUnavailable::test_non_byte_tensor_state_cuda_unavailable
FAILED tests/test_torch_cuda_random_g1_cuda_unavailable.py::TestTorchCudaRandomG1CudaUnavailable::test_single_device_state_management_cuda_unavailable
FAILED tests/test_torch_cuda_random_g1_cuda_unavailable.py::TestTorchCudaRandomG1CudaUnavailable::test_device_string_format_cuda_unavailable
FAILED tests/test_torch_cuda_random_g2_cuda_unavailable.py::TestTorchCudaRandomG2CudaUnavailable::test_get_rng_state_all_cuda_unavailable
FAILED tests/test_torch_cuda_random_g2_cuda_unavailable.py::TestTorchCudaRandomG2CudaUnavailable::test_set_rng_state_all_cuda_unavailable
FAILED tests/test_torch_cuda_random_g2_cuda_unavailable.py::TestTorchCudaRandomG2CudaUnavailable::test_empty_state_list_cuda_unavailable
FAILED tests/test_torch_cuda_random_g2_cuda_unavailable.py::TestTorchCudaRandomG2CudaUnavailable::test_invalid_device_index_cuda_unavailable
FAILED tests/test_torch_cuda_random_g2_cuda_unavailable.py::TestTorchCudaRandomG2CudaUnavailable::test_multi_device_batch_management_cuda_unavailable
FAILED tests/test_torch_cuda_random_g2_cuda_unavailable.py::TestTorchCudaRandomG2CudaUnavailable::test_state_list_length_mismatch_cuda_unavailable
FAILED tests/test_torch_cuda_random_g2_cuda_unavailable.py::TestTorchCudaRandomG2CudaUnavailable::test_zero_length_tensor_state_cuda_unavailable
22 failed, 34 passed, 14 skipped in 1.05s

Error: exit 1