=== Run Tests ===
....F..                                                                  [100%]
=================================== FAILURES ===================================
_________ TestTorchRandom.test_fork_rng_basic_context[None-True-basic] _________

self = <test_torch_random.TestTorchRandom object at 0x1317081f0>, devices = None
enabled = True, context_type = 'basic'
mock_cuda_functions = {'bad_fork': <MagicMock name='_is_in_bad_fork' id='5124923856'>, 'device_count': <MagicMock name='device_count' id='51...Mock name='get_rng_state' id='5124964608'>, 'get_state_all': <MagicMock name='get_rng_state_all' id='5124883648'>, ...}

    @pytest.mark.parametrize("devices,enabled,context_type", [
        (None, True, "basic"),
    ])
    def test_fork_rng_basic_context(self, devices, enabled, context_type, mock_cuda_functions):
        """Test basic fork_rng context management (TC-04)."""
    
        # Setup mock CUDA state - only if CUDA is available in the mock
        mock_cuda_state = torch.ByteTensor([1, 2, 3, 4, 5])
        mock_cuda_functions['get_state_all'].return_value = [mock_cuda_state] * 2  # 2 devices
    
        # Save original CPU RNG state
        original_cpu_state = torch.random.get_rng_state()
    
        # Generate some random data before context
        before_random = torch.randn(3, 3)
    
        # Enter fork_rng context
        with torch.random.fork_rng(devices=devices, enabled=enabled):
            # Inside context, RNG should be forked
            # Generate random data inside context
            inside_random = torch.randn(3, 3)
    
            # Change RNG state inside context
            torch.random.manual_seed(999)
            changed_inside_random = torch.randn(3, 3)
    
            # Verify CUDA functions were called for context entry if enabled
            # Only check if CUDA is mocked as available (device_count > 0)
            if enabled and mock_cuda_functions['device_count'].return_value > 0:
>               mock_cuda_functions['get_state_all'].assert_called_once()

tests/test_torch_random.py:274: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='get_rng_state_all' id='5124883648'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'get_rng_state_all' to have been called once. Called 0 times.

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/unittest/mock.py:908: AssertionError
=============================== warnings summary ===============================
exam/torch_group/random/tests/test_torch_random.py::TestTorchRandom::test_fork_rng_basic_context[None-True-basic]
  /opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/random.py:99: UserWarning: CUDA reports that you have 2 available devices, and you have used fork_rng without explicitly specifying which devices are being used. For safety, we initialize *every* CUDA device by default, which can be quite slow if you have a lot of GPUs.  If you know that you are only making use of a few CUDA devices, set the environment variable CUDA_VISIBLE_DEVICES or the 'devices' keyword argument of fork_rng with the set of devices you are actually using.  For example, if you are using CPU only, set CUDA_VISIBLE_DEVICES= or devices=[]; if you are using GPU 0 only, set CUDA_VISIBLE_DEVICES=0 or devices=[0].  To initialize all devices and suppress this warning, set the 'devices' keyword argument to `range(torch.cuda.device_count())`.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                         Stmts   Miss Branch BrPart  Cover   Missing
------------------------------------------------------------------------
tests/test_torch_random.py     136     33     16      4    72%   87-92, 129->exit, 155, 281-340
------------------------------------------------------------------------
TOTAL                          136     33     16      4    72%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_random.py::TestTorchRandom::test_fork_rng_basic_context[None-True-basic]
1 failed, 6 passed, 1 warning in 0.59s

Error: exit 1