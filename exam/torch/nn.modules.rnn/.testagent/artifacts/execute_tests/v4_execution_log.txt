=== Run Tests ===
....F....X...........F.....x.                                            [100%]
=================================== FAILURES ===================================
_ test_lstm_projection_functionality[LSTM-10-20-15-1-False-False-float32-2-4] __

set_random_seed = None, mode = 'LSTM', input_size = 10, hidden_size = 20
proj_size = 15, num_layers = 1, batch_first = False, bidirectional = False
dtype_str = 'float32', batch_size = 2, seq_len = 4

    @pytest.mark.parametrize(
        "mode,input_size,hidden_size,proj_size,num_layers,batch_first,bidirectional,dtype_str,batch_size,seq_len",
        [
            # Base case from test plan: LSTM with projection
            ("LSTM", 10, 20, 15, 1, False, False, "float32", 2, 4),
        ]
    )
    def test_lstm_projection_functionality(
        set_random_seed,
        mode,
        input_size,
        hidden_size,
        proj_size,
        num_layers,
        batch_first,
        bidirectional,
        dtype_str,
        batch_size,
        seq_len,
    ):
        """
        Test LSTM projection functionality constraint checking.
    
        Weak assertions:
        - output_shape_proj: Check output tensor shape with projection
        - hidden_shape_proj: Check hidden state shape with projection
        - dtype: Check output dtype matches input dtype
        - finite: Check all values are finite
        - proj_size_constraint: Check projection size constraints
        """
        # Convert dtype string to torch dtype
        dtype_map = {
            "float32": torch.float32,
            "float64": torch.float64,
        }
        dtype = dtype_map[dtype_str]
    
        # Create LSTM instance with projection
        # Note: proj_size is only supported for LSTM in PyTorch
        lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            proj_size=proj_size,
            num_layers=num_layers,
            batch_first=batch_first,
            bidirectional=bidirectional,
        )
    
        # Convert LSTM parameters to match input dtype if needed
        if dtype == torch.float64:
            lstm = lstm.double()
    
        # Create test input
        x = create_test_input(batch_size, seq_len, input_size, batch_first, dtype)
    
        # Forward pass
        output, (h_n, c_n) = lstm(x)
    
        # Calculate expected shapes with projection
        num_directions = 2 if bidirectional else 1
    
        # Expected output shape: output uses proj_size instead of hidden_size
        if batch_first:
            expected_output_shape = (batch_size, seq_len, num_directions * proj_size)
        else:
            expected_output_shape = (seq_len, batch_size, num_directions * proj_size)
    
        # Expected hidden state shape: h_n uses proj_size
        expected_hidden_shape = (num_layers * num_directions, batch_size, proj_size)
    
        # Expected cell state shape: c_n still uses hidden_size (not projected)
        expected_cell_shape = (num_layers * num_directions, batch_size, hidden_size)
    
        # Weak assertions
        # 1. Output shape assertion with projection
        assert_shape_equal(output, expected_output_shape,
                          f"Output shape mismatch for LSTM with projection")
    
        # 2. Hidden state shape assertion with projection
        assert_shape_equal(h_n, expected_hidden_shape,
                          f"Hidden state shape mismatch for LSTM with projection")
    
        # 3. Cell state shape assertion (should still use hidden_size)
        assert_shape_equal(c_n, expected_cell_shape,
                          f"Cell state shape mismatch for LSTM with projection")
    
        # 4. Dtype assertion
        assert_dtype_equal(output, dtype,
                          f"Output dtype mismatch for LSTM with projection")
        assert_dtype_equal(h_n, dtype,
                          f"Hidden state dtype mismatch for LSTM with projection")
        assert_dtype_equal(c_n, dtype,
                          f"Cell state dtype mismatch for LSTM with projection")
    
        # 5. Finite values assertion
        assert_finite(output, f"Output contains non-finite values for LSTM with projection")
        assert_finite(h_n, f"Hidden state contains non-finite values for LSTM with projection")
        assert_finite(c_n, f"Cell state contains non-finite values for LSTM with projection")
    
        # 6. No NaN assertion (additional safety check)
        assert_no_nan(output, f"Output contains NaN values for LSTM with projection")
        assert_no_nan(h_n, f"Hidden state contains NaN values for LSTM with projection")
        assert_no_nan(c_n, f"Cell state contains NaN values for LSTM with projection")
    
        # 7. Projection size constraint check
        # proj_size must be < hidden_size (PyTorch enforces this)
        assert proj_size < hidden_size, \
            f"proj_size ({proj_size}) must be smaller than hidden_size ({hidden_size})"
    
        # LSTM-specific projection checks
        # Check that output dimension matches proj_size (not hidden_size)
        assert output.shape[-1] == num_directions * proj_size, \
            f"Output dimension should be {num_directions * proj_size} with projection, got {output.shape[-1]}"
    
        # Check that hidden state dimension matches proj_size
        assert h_n.shape[-1] == proj_size, \
            f"Hidden state dimension should be {proj_size} with projection, got {h_n.shape[-1]}"
    
        # Check that cell state dimension still matches hidden_size (not projected)
        assert c_n.shape[-1] == hidden_size, \
            f"Cell state dimension should be {hidden_size} (not projected), got {c_n.shape[-1]}"
    
        # Check parameter shapes for projection
        # LSTM with projection has different weight/bias parameter shapes
        for name, param in lstm.named_parameters():
            if "weight_ih" in name:
                # weight_ih_l0 shape: (4*hidden_size, input_size)
                # For LSTM with projection, weight_ih_l0 still connects input to hidden
                # So last dimension should be input_size for layer 0
                if "_l0" in name:
                    # First layer: connects input to hidden
                    assert param.shape[-1] == input_size, \
                        f"weight_ih_l0 should have last dimension {input_size}, got {param.shape}"
                else:
                    # Higher layers: connect previous hidden state to current hidden
                    # For projection, previous hidden state has dimension proj_size
                    assert param.shape[-1] == proj_size, \
                        f"weight_ih_l{k} should have last dimension {proj_size}, got {param.shape}"
    
            elif "weight_hh" in name:
                # weight_hh_l0 shape: (4*hidden_size, proj_size) for LSTM with projection
                # This connects previous hidden state (size proj_size) to current hidden
                assert param.shape[-1] == proj_size, \
                    f"weight_hh {name} should have last dimension {proj_size}, got {param.shape}"
    
            elif "weight_hr" in name:
                # weight_hr_l0 shape: (proj_size, hidden_size) - projection weights
                assert param.shape[0] == proj_size and param.shape[1] == hidden_size, \
                    f"weight_hr {name} should have shape ({proj_size}, {hidden_size}), got {param.shape}"
    
        # Test that projection is only supported for LSTM
        # (This is enforced by PyTorch at construction time)
        # We'll verify by checking that non-LSTM RNNs don't accept proj_size parameter
        with pytest.raises(TypeError, match="got an unexpected keyword argument 'proj_size'"):
>           nn.RNN(input_size=input_size, hidden_size=hidden_size, proj_size=proj_size)

tests/test_torch_nn_modules_rnn_g1.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'RNN' object has no attribute 'proj_size'") raised in repr()] RNN object at 0x140223280>
args = (), kwargs = {'hidden_size': 20, 'input_size': 10, 'proj_size': 15}

    def __init__(self, *args, **kwargs):
        if 'proj_size' in kwargs:
>           raise ValueError("proj_size argument is only supported for LSTM, not RNN or GRU")
E           ValueError: proj_size argument is only supported for LSTM, not RNN or GRU

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/rnn.py:417: ValueError
______________________ test_rnn_invalid_input_dimensions _______________________

    def test_rnn_invalid_input_dimensions():
        """Test that invalid input dimensions raise appropriate errors."""
        # Test 2D input when 3D is expected
        rnn = nn.RNN(input_size=10, hidden_size=20)
        x_2d = torch.randn(5, 10)  # Should be (seq_len, batch, input_size) or (batch, seq_len, input_size)
    
>       with pytest.raises(RuntimeError, match="input must have 3 dimensions"):
E       Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_nn_modules_rnn_g2.py:996: Failed
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                    Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------
tests/test_torch_nn_modules_rnn_g1.py     163     14     32      6    90%   148->exit, 265-266, 325, 338, 411, 431-444, 516
tests/test_torch_nn_modules_rnn_g2.py     424     48    164     36    84%   51, 102-121, 125, 134, 172->exit, 245-274, 278, 288, 297, 311-313, 323-325, 334-335, 342, 353-355, 366, 372, 381-383, 394, 451, 474, 594, 597->617, 603, 628, 640, 651, 741, 745, 868->874, 878->881, 896->899, 907->929, 916, 943->953, 953->exit, 973, 1000-1004, 1173
-----------------------------------------------------------------------------------
TOTAL                                     587     62    196     42    85%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_rnn_g1.py::test_lstm_projection_functionality[LSTM-10-20-15-1-False-False-float32-2-4]
FAILED tests/test_torch_nn_modules_rnn_g2.py::test_rnn_invalid_input_dimensions
2 failed, 25 passed, 1 xfailed, 1 xpassed in 0.93s

Error: exit 1