=== Run Tests ===
.F..F...F                                                                [100%]
=================================== FAILURES ===================================
____ test_basic_rnn_forward_shape[RNN_TANH-20-40-2-True-False-float64-4-8] _____

set_random_seed = None, mode = 'RNN_TANH', input_size = 20, hidden_size = 40
num_layers = 2, batch_first = True, bidirectional = False, dtype_str = 'float64'
batch_size = 4, seq_len = 8

    @pytest.mark.parametrize(
        "mode,input_size,hidden_size,num_layers,batch_first,bidirectional,dtype_str,batch_size,seq_len",
        [
            # Base case from test plan
            ("RNN_TANH", 10, 20, 1, False, False, "float32", 3, 5),
            # Parameter extension: double precision, larger size, batch_first format
            ("RNN_TANH", 20, 40, 2, True, False, "float64", 4, 8),
        ]
    )
    def test_basic_rnn_forward_shape(
        set_random_seed,
        mode,
        input_size,
        hidden_size,
        num_layers,
        batch_first,
        bidirectional,
        dtype_str,
        batch_size,
        seq_len,
    ):
        """
        Test basic RNN forward propagation shape validation.
    
        Weak assertions:
        - output_shape: Check output tensor shape matches expected
        - hidden_shape: Check hidden state shape matches expected
        - dtype: Check output dtype matches input dtype
        - finite: Check all values are finite
        - no_nan: Check no NaN values in output
        """
        # Convert dtype string to torch dtype
        dtype_map = {
            "float32": torch.float32,
            "float64": torch.float64,
        }
        dtype = dtype_map[dtype_str]
    
        # Create RNN instance
        rnn = nn.RNN(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=batch_first,
            bidirectional=bidirectional,
            nonlinearity=mode.lower().replace("rnn_", ""),  # "tanh" or "relu"
        )
    
        # Create test input
        x = create_test_input(batch_size, seq_len, input_size, batch_first, dtype)
    
        # Forward pass
>       output, h_n = rnn(x)

tests/test_torch_nn_modules_rnn_g1.py:103: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = RNN(20, 40, num_layers=2, batch_first=True)
input = tensor([[[-1.2962e-01,  6.2434e-02, -2.0408e-01, -1.5949e+00, -5.3327e-02,
          -2.5598e-01, -6.0302e-01,  4.0889...  1.2416e-01,
          -2.6614e-01,  1.0565e+00, -1.6753e+00,  1.8691e-01, -7.2721e-01]]],
       dtype=torch.float64)
hx = tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0....., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],
       dtype=torch.float64)

    def forward(self, input, hx=None):  # noqa: F811
        orig_input = input
        if isinstance(orig_input, PackedSequence):
            input, batch_sizes, sorted_indices, unsorted_indices = input
            max_batch_size = int(batch_sizes[0])
        else:
            batch_sizes = None
            is_batched = input.dim() == 3
            batch_dim = 0 if self.batch_first else 1
            if not is_batched:
                input = input.unsqueeze(batch_dim)
                if hx is not None:
                    if hx.dim() != 2:
                        raise RuntimeError(
                            f"For unbatched 2-D input, hx should also be 2-D but got {hx.dim()}-D tensor")
                    hx = hx.unsqueeze(1)
            else:
                if hx is not None and hx.dim() != 3:
                    raise RuntimeError(
                        f"For batched 3-D input, hx should also be 3-D but got {hx.dim()}-D tensor")
            max_batch_size = input.size(0) if self.batch_first else input.size(1)
            sorted_indices = None
            unsorted_indices = None
    
        if hx is None:
            num_directions = 2 if self.bidirectional else 1
            hx = torch.zeros(self.num_layers * num_directions,
                             max_batch_size, self.hidden_size,
                             dtype=input.dtype, device=input.device)
        else:
            # Each batch of the hidden state should match the input sequence that
            # the user believes he/she is passing in.
            hx = self.permute_hidden(hx, sorted_indices)
    
        assert hx is not None
        self.check_forward_args(input, hx, batch_sizes)
        assert self.mode == 'RNN_TANH' or self.mode == 'RNN_RELU'
        if batch_sizes is None:
            if self.mode == 'RNN_TANH':
>               result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
                                      self.dropout, self.training, self.bidirectional,
                                      self.batch_first)
E               RuntimeError: expected scalar type Double but found Float

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/rnn.py:476: RuntimeError
____________________________ test_rnn_invalid_mode _____________________________

    def test_rnn_invalid_mode():
        """Test that invalid RNN mode raises ValueError."""
        with pytest.raises(ValueError, match="Unrecognized RNN mode"):
>           nn.RNN(input_size=10, hidden_size=20, nonlinearity="invalid_mode")

tests/test_torch_nn_modules_rnn_g1.py:282: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'RNN' object has no attribute 'proj_size'") raised in repr()] RNN object at 0x12c29b850>
args = (), kwargs = {'hidden_size': 20, 'input_size': 10}

    def __init__(self, *args, **kwargs):
        if 'proj_size' in kwargs:
            raise ValueError("proj_size argument is only supported for LSTM, not RNN or GRU")
        self.nonlinearity = kwargs.pop('nonlinearity', 'tanh')
        if self.nonlinearity == 'tanh':
            mode = 'RNN_TANH'
        elif self.nonlinearity == 'relu':
            mode = 'RNN_RELU'
        else:
>           raise ValueError("Unknown nonlinearity '{}'".format(self.nonlinearity))
E           ValueError: Unknown nonlinearity 'invalid_mode'

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/rnn.py:424: ValueError

During handling of the above exception, another exception occurred:

    def test_rnn_invalid_mode():
        """Test that invalid RNN mode raises ValueError."""
>       with pytest.raises(ValueError, match="Unrecognized RNN mode"):
E       AssertionError: Regex pattern did not match.
E         Expected regex: 'Unrecognized RNN mode'
E         Actual message: "Unknown nonlinearity 'invalid_mode'"

tests/test_torch_nn_modules_rnn_g1.py:281: AssertionError
________________________ test_rnn_zero_sequence_length _________________________

    def test_rnn_zero_sequence_length():
        """Test RNN with zero sequence length (edge case)."""
        rnn = nn.RNN(input_size=10, hidden_size=20)
    
        # Create input with seq_len=0
        batch_size, seq_len, input_size = 3, 0, 10
        x = torch.randn(seq_len, batch_size, input_size)
    
        # This should work and return empty output
>       output, h_n = rnn(x)

tests/test_torch_nn_modules_rnn_g1.py:335: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = RNN(10, 20), input = tensor([], size=(0, 3, 10))
hx = tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0..., 0., 0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])

    def forward(self, input, hx=None):  # noqa: F811
        orig_input = input
        if isinstance(orig_input, PackedSequence):
            input, batch_sizes, sorted_indices, unsorted_indices = input
            max_batch_size = int(batch_sizes[0])
        else:
            batch_sizes = None
            is_batched = input.dim() == 3
            batch_dim = 0 if self.batch_first else 1
            if not is_batched:
                input = input.unsqueeze(batch_dim)
                if hx is not None:
                    if hx.dim() != 2:
                        raise RuntimeError(
                            f"For unbatched 2-D input, hx should also be 2-D but got {hx.dim()}-D tensor")
                    hx = hx.unsqueeze(1)
            else:
                if hx is not None and hx.dim() != 3:
                    raise RuntimeError(
                        f"For batched 3-D input, hx should also be 3-D but got {hx.dim()}-D tensor")
            max_batch_size = input.size(0) if self.batch_first else input.size(1)
            sorted_indices = None
            unsorted_indices = None
    
        if hx is None:
            num_directions = 2 if self.bidirectional else 1
            hx = torch.zeros(self.num_layers * num_directions,
                             max_batch_size, self.hidden_size,
                             dtype=input.dtype, device=input.device)
        else:
            # Each batch of the hidden state should match the input sequence that
            # the user believes he/she is passing in.
            hx = self.permute_hidden(hx, sorted_indices)
    
        assert hx is not None
        self.check_forward_args(input, hx, batch_sizes)
        assert self.mode == 'RNN_TANH' or self.mode == 'RNN_RELU'
        if batch_sizes is None:
            if self.mode == 'RNN_TANH':
>               result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
                                      self.dropout, self.training, self.bidirectional,
                                      self.batch_first)
E               RuntimeError: Expected sequence length to be larger than 0 in RNN

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/rnn.py:476: RuntimeError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                    Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------
tests/test_torch_nn_modules_rnn_g1.py     111      6     16      4    92%   110, 142->exit, 259-260, 338-342, 347
-----------------------------------------------------------------------------------
TOTAL                                     111      6     16      4    92%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_rnn_g1.py::test_basic_rnn_forward_shape[RNN_TANH-20-40-2-True-False-float64-4-8]
FAILED tests/test_torch_nn_modules_rnn_g1.py::test_rnn_invalid_mode - Asserti...
FAILED tests/test_torch_nn_modules_rnn_g1.py::test_rnn_zero_sequence_length
3 failed, 6 passed in 0.83s

Error: exit 1