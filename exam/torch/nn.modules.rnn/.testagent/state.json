{
  "workflow_id": "cdc7ab14",
  "created_at": "2026-01-19T23:24:55.229118",
  "op": "torch_nn_modules_rnn",
  "arch": "python",
  "soc": "python",
  "vendor": "python",
  "project_root": "/Users/zzf1sh/Documents/Project/TestAgent-CLI-main/exam/torch_group/nn.modules.rnn",
  "target": "torch.nn.modules.rnn",
  "target_slug": "torch_nn_modules_rnn",
  "current_stage": "complete",
  "stage_index": 6,
  "mode": "full-auto",
  "epoch_total": 5,
  "epoch_current": 5,
  "last_failure_signature": "da77d55619472edd356f7cb31210d4faf09935b7",
  "last_error_signature": "e3292c12a856a02722a46d87471ff4e6faefa104",
  "last_block_errors": {
    "CASE_05": [
      "AssertionError"
    ],
    "FOOTER": [
      "AssertionError"
    ]
  },
  "auto_stop_reason": "",
  "artifacts": {
    "function_doc.md": "# torch.nn.modules.rnn - 函数说明\n\n## 1. 基本信息\n- **FQN**: torch.nn.modules.rnn\n- **模块文件**: `/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/rnn.py`\n- **签名**: 模块（包含多个类）\n- **对象类型**: Python 模块\n\n## 2. 功能概述\n该模块实现了循环神经网络（RNN）的核心组件。提供 RNN、LSTM、GRU 及其对应的单元版本。支持多层、双向、dropout 和投影功能。处理批量和打包序列输入。\n\n## 3. 参数说明\n模块包含多个类，主要参数包括：\n- **RNNBase 基类参数**:\n  - mode (str): RNN 类型 ('LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU')\n  - input_size (int): 输入特征维度\n  - hidden_size (int): 隐藏状态维度\n  - num_layers (int=1): 层数\n  - bias (bool=True): 是否使用偏置\n  - batch_first (bool=False): 输入是否为 (batch, seq, feature) 格式\n  - dropout (float=0.0): dropout 概率 (0-1)\n  - bidirectional (bool=False): 是否双向\n  - proj_size (int=0): LSTM 投影维度（仅 LSTM 支持）\n\n## 4. 返回值\n各类的 forward 方法返回：\n- **RNN/GRU**: (output, h_n) 元组\n- **LSTM**: (output, (h_n, c_n)) 元组\n- **Cell 版本**: 单个隐藏状态或 (h, c) 元组\n\n## 5. 文档要点\n- 输入张量形状：支持 2D/3D 和 PackedSequence\n- dropout 仅在 num_layers > 1 时有效\n- proj_size 仅 LSTM 支持，且必须 < hidden_size\n- 双向 RNN 输出维度为 2 * hidden_size\n- 权重初始化：均匀分布 U(-√k, √k)，k = 1/hidden_size\n\n## 6. 源码摘要\n- 关键路径：RNNBase → 具体 RNN/LSTM/GRU 类\n- 依赖外部 API：_VF (C++ 后端实现)\n- 支持 CUDA/cuDNN 加速路径\n- 副作用：无 I/O，有随机性（dropout）\n- 参数管理：_flat_weights 用于优化内存访问\n\n## 7. 示例与用法（如有）\n```python\n# RNN 示例\nrnn = nn.RNN(10, 20, 2)\ninput = torch.randn(5, 3, 10)\nh0 = torch.randn(2, 3, 20)\noutput, hn = rnn(input, h0)\n\n# LSTM 示例\nlstm = nn.LSTM(10, 20, 2)\noutput, (hn, cn) = lstm(input, (h0, c0))\n```\n\n## 8. 风险与空白\n- **多实体情况**：模块包含 8 个主要类（RNNBase, RNN, LSTM, GRU, RNNCellBase, RNNCell, LSTMCell, GRUCell）\n- **类型信息不完整**：部分参数类型注解缺失（如 device/dtype）\n- **边界条件**：需要测试 dropout=0 且 num_layers=1 的警告\n- **形状验证**：输入/隐藏状态形状匹配的详细约束\n- **设备兼容性**：CUDA/CPU 路径差异\n- **打包序列**：PackedSequence 处理的特殊逻辑\n- **投影功能**：仅 LSTM 支持 proj_size 的约束验证",
    "requirements.md": "# torch.nn.modules.rnn 测试需求\n\n## 1. 目标与范围\n- 主要功能与期望行为：验证 RNN/LSTM/GRU 及其单元版本的正向传播、参数初始化、形状变换、序列处理功能\n- 不在范围内的内容：反向传播梯度计算、优化器集成、训练收敛性、自定义激活函数\n\n## 2. 输入与约束\n- 参数列表（名称、类型/shape、默认值）：\n  - mode: str ('LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU')\n  - input_size: int (>0)\n  - hidden_size: int (>0)\n  - num_layers: int (≥1, 默认1)\n  - bias: bool (默认True)\n  - batch_first: bool (默认False)\n  - dropout: float (0-1, 默认0.0)\n  - bidirectional: bool (默认False)\n  - proj_size: int (≥0, 默认0, 仅LSTM支持)\n\n- 有效取值范围/维度/设备要求：\n  - dropout 仅在 num_layers > 1 时有效\n  - proj_size 必须 < hidden_size (仅LSTM)\n  - 输入形状：2D (seq_len, batch, input_size) 或 3D (batch, seq_len, input_size) 当 batch_first=True\n  - 隐藏状态形状：多层时 (num_layers * num_directions, batch, hidden_size)\n  - 支持 CPU/CUDA 设备\n\n- 必需与可选组合：\n  - 必需：input_size, hidden_size\n  - 可选：h_0/c_0 初始状态（默认全零）\n  - 组合约束：bidirectional 时输出维度为 2 * hidden_size\n\n- 随机性/全局状态要求：\n  - dropout 引入随机性\n  - 权重初始化：均匀分布 U(-√k, √k), k = 1/hidden_size\n  - 需要设置随机种子保证可重复性\n\n## 3. 输出与判定\n- 期望返回结构及关键字段：\n  - RNN/GRU: (output, h_n) 元组\n  - LSTM: (output, (h_n, c_n)) 元组\n  - Cell 版本：单个隐藏状态或 (h, c) 元组\n  - output 形状：(seq_len, batch, num_directions * hidden_size) 或 batch_first 格式\n\n- 容差/误差界（如浮点）：\n  - 浮点比较容差：1e-5 (单精度), 1e-7 (双精度)\n  - 随机 dropout 允许统计性差异\n  - 设备间（CPU/CUDA）结果一致性容差\n\n- 状态变化或副作用检查点：\n  - 权重参数初始化正确性\n  - dropout 掩码生成一致性\n  - 无外部 I/O 操作\n  - 内存占用在预期范围内\n\n## 4. 错误与异常场景\n- 非法输入/维度/类型触发的异常或警告：\n  - 输入张量维度错误（非2D/3D）\n  - 输入/隐藏状态形状不匹配\n  - proj_size 用于非 LSTM 类型\n  - dropout 在 num_layers=1 时无效（警告）\n  - 无效 mode 字符串\n  - 负值或零值参数\n\n- 边界值（空、None、0 长度、极端形状/数值）：\n  - batch_size=0 或 seq_len=0\n  - 极大 hidden_size (内存边界)\n  - dropout=0.0 和 dropout=1.0 边界\n  - proj_size=hidden_size (应失败)\n  - 单层双向 RNN 输出形状验证\n\n## 5. 依赖与环境\n- 外部资源/设备/网络/文件依赖：\n  - PyTorch C++ 后端 (_VF)\n  - CUDA/cuDNN 库（可选）\n  - 足够 GPU 内存（CUDA 测试）\n\n- 需要 mock/monkeypatch 的部分：\n  - 随机数生成器（dropout 测试）\n  - CUDA 可用性检测\n  - _VF 模块调用（隔离测试）\n  - 内存分配失败场景\n\n## 6. 覆盖与优先级\n- 必测路径（高优先级，最多 5 条，短句）：\n  1. 基础 RNN/LSTM/GRU 正向传播形状正确性\n  2. batch_first 格式输入输出转换\n  3. 双向 RNN 输出维度验证\n  4. dropout 在多层网络中的随机性\n  5. LSTM 投影功能 (proj_size) 约束检查\n\n- 可选路径（中/低优先级合并为一组列表）：\n  - PackedSequence 输入处理\n  - 不同设备（CPU/CUDA）结果一致性\n  - 极端参数值（极大/极小 hidden_size）\n  - 权重初始化分布验证\n  - 单元版本 (RNNCell/LSTMCell/GRUCell) 独立测试\n  - 梯度计算正确性（如包含反向传播）\n\n- 已知风险/缺失信息（仅列条目，不展开）：\n  - 多实体情况（8个主要类）\n  - 类型注解不完整\n  - CUDA/cuDNN 特定优化路径\n  - 内存碎片化影响\n  - 并行计算线程安全性",
    "test_plan.json": "{\n  \"plan_version\": 2,\n  \"target\": \"torch.nn.modules.rnn\",\n  \"block_rules\": {\n    \"header_block\": \"HEADER\",\n    \"footer_block\": \"FOOTER\",\n    \"case_prefix\": \"CASE_\",\n    \"case_format\": \"CASE_01\"\n  },\n  \"iteration_strategy\": {\n    \"round1\": {\n      \"include\": \"SMOKE_SET\",\n      \"assert_level\": \"weak\",\n      \"max_blocks\": 5\n    },\n    \"roundN\": {\n      \"only_fix_failed_blocks\": true,\n      \"block_limit\": 3,\n      \"promote_deferred\": true\n    },\n    \"final\": {\n      \"enable_strong_asserts\": true,\n      \"coverage_optional\": true\n    }\n  },\n  \"test_files\": {\n    \"default\": \"tests/test_torch_nn_modules_rnn.py\",\n    \"all_pattern\": \"tests/test_torch_nn_modules_rnn_*.py\",\n    \"groups\": {\n      \"G1\": \"tests/test_torch_nn_modules_rnn_g1.py\",\n      \"G2\": \"tests/test_torch_nn_modules_rnn_g2.py\"\n    }\n  },\n  \"active_group_order\": [\"G1\", \"G2\"],\n  \"groups\": [\n    {\n      \"group_id\": \"G1\",\n      \"title\": \"核心RNN/LSTM/GRU正向传播\",\n      \"entrypoints\": [\"RNN\", \"LSTM\", \"GRU\"],\n      \"smoke_set\": [\"CASE_01\", \"CASE_02\"],\n      \"deferred_set\": [\"CASE_05\", \"CASE_06\"],\n      \"note\": \"测试基础RNN类型正向传播形状正确性\"\n    },\n    {\n      \"group_id\": \"G2\",\n      \"title\": \"高级功能与边界条件\",\n      \"entrypoints\": [\"bidirectional\", \"dropout\", \"batch_first\", \"proj_size\"],\n      \"smoke_set\": [\"CASE_03\", \"CASE_04\"],\n      \"deferred_set\": [\"CASE_07\", \"CASE_08\"],\n      \"note\": \"测试双向、dropout、batch_first和投影功能\"\n    }\n  ],\n  \"cases\": [\n    {\n      \"tc_id\": \"TC-01\",\n      \"block_id\": \"CASE_01\",\n      \"group_id\": \"G1\",\n      \"name\": \"基础RNN正向传播形状验证\",\n      \"priority\": \"High\",\n      \"param_matrix\": [\n        {\n          \"mode\": \"RNN_TANH\",\n          \"input_size\": 10,\n          \"hidden_size\": 20,\n          \"num_layers\": 1,\n          \"batch_first\": false,\n          \"bidirectional\": false,\n          \"dtype\": \"float32\",\n          \"device\": \"cpu\",\n          \"batch_size\": 3,\n          \"seq_len\": 5\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"output_shape\", \"hidden_shape\", \"dtype\", \"finite\", \"no_nan\"],\n        \"strong\": [\"approx_equal_ref\", \"gradient_check\", \"memory_leak\"]\n      },\n      \"oracle\": \"torch.nn.RNN\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"S\",\n      \"max_lines\": 70,\n      \"max_params\": 8,\n      \"is_parametrized\": true,\n      \"requires_mock\": false\n    },\n    {\n      \"tc_id\": \"TC-02\",\n      \"block_id\": \"CASE_02\",\n      \"group_id\": \"G1\",\n      \"name\": \"LSTM基础功能验证\",\n      \"priority\": \"High\",\n      \"param_matrix\": [\n        {\n          \"mode\": \"LSTM\",\n          \"input_size\": 8,\n          \"hidden_size\": 16,\n          \"num_layers\": 2,\n          \"batch_first\": true,\n          \"bidirectional\": false,\n          \"dtype\": \"float32\",\n          \"device\": \"cpu\",\n          \"batch_size\": 2,\n          \"seq_len\": 4\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"output_shape\", \"hidden_shape\", \"cell_shape\", \"dtype\", \"finite\"],\n        \"strong\": [\"lstm_gate_consistency\", \"forget_bias_check\", \"gradient_flow\"]\n      },\n      \"oracle\": \"torch.nn.LSTM\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"S\",\n      \"max_lines\": 75,\n      \"max_params\": 8,\n      \"is_parametrized\": true,\n      \"requires_mock\": false\n    },\n    {\n      \"tc_id\": \"TC-03\",\n      \"block_id\": \"CASE_03\",\n      \"group_id\": \"G2\",\n      \"name\": \"双向RNN输出维度验证\",\n      \"priority\": \"High\",\n      \"param_matrix\": [\n        {\n          \"mode\": \"GRU\",\n          \"input_size\": 12,\n          \"hidden_size\": 24,\n          \"num_layers\": 1,\n          \"batch_first\": false,\n          \"bidirectional\": true,\n          \"dtype\": \"float32\",\n          \"device\": \"cpu\",\n          \"batch_size\": 3,\n          \"seq_len\": 6\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"output_shape_bidirectional\", \"hidden_shape_bidirectional\", \"dtype\", \"finite\"],\n        \"strong\": [\"forward_backward_consistency\", \"parameter_sharing_check\"]\n      },\n      \"oracle\": \"torch.nn.GRU\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"S\",\n      \"max_lines\": 65,\n      \"max_params\": 8,\n      \"is_parametrized\": true,\n      \"requires_mock\": false\n    },\n    {\n      \"tc_id\": \"TC-04\",\n      \"block_id\": \"CASE_04\",\n      \"group_id\": \"G2\",\n      \"name\": \"多层dropout随机性测试\",\n      \"priority\": \"High\",\n      \"param_matrix\": [\n        {\n          \"mode\": \"RNN_RELU\",\n          \"input_size\": 6,\n          \"hidden_size\": 12,\n          \"num_layers\": 3,\n          \"batch_first\": false,\n          \"bidirectional\": false,\n          \"dropout\": 0.5,\n          \"dtype\": \"float32\",\n          \"device\": \"cpu\",\n          \"batch_size\": 2,\n          \"seq_len\": 3\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"output_shape\", \"hidden_shape\", \"dtype\", \"finite\", \"dropout_mask_present\"],\n        \"strong\": [\"dropout_statistics\", \"seed_consistency\", \"no_dropout_single_layer\"]\n      },\n      \"oracle\": \"torch.nn.RNN\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"M\",\n      \"max_lines\": 80,\n      \"max_params\": 9,\n      \"is_parametrized\": true,\n      \"requires_mock\": true\n    },\n    {\n      \"tc_id\": \"TC-05\",\n      \"block_id\": \"CASE_05\",\n      \"group_id\": \"G1\",\n      \"name\": \"LSTM投影功能约束检查\",\n      \"priority\": \"Medium\",\n      \"param_matrix\": [\n        {\n          \"mode\": \"LSTM\",\n          \"input_size\": 10,\n          \"hidden_size\": 20,\n          \"proj_size\": 15,\n          \"num_layers\": 1,\n          \"batch_first\": false,\n          \"bidirectional\": false,\n          \"dtype\": \"float32\",\n          \"device\": \"cpu\",\n          \"batch_size\": 2,\n          \"seq_len\": 4\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"output_shape_proj\", \"hidden_shape_proj\", \"dtype\", \"finite\", \"proj_size_constraint\"],\n        \"strong\": [\"projection_weight_check\", \"gradient_proj\", \"memory_footprint\"]\n      },\n      \"oracle\": \"torch.nn.LSTM\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"M\",\n      \"max_lines\": 85,\n      \"max_params\": 9,\n      \"is_parametrized\": true,\n      \"requires_mock\": false\n    }\n  ],\n  \"param_extensions\": [\n    {\n      \"base_block_id\": \"CASE_01\",\n      \"priority\": \"Medium\",\n      \"params\": {\n        \"mode\": \"RNN_TANH\",\n        \"input_size\": 20,\n        \"hidden_size\": 40,\n        \"num_layers\": 2,\n        \"batch_first\": true,\n        \"bidirectional\": false,\n        \"dtype\": \"float64\",\n        \"device\": \"cpu\",\n        \"batch_size\": 4,\n        \"seq_len\": 8\n      },\n      \"note\": \"扩展参数：双精度、更大尺寸、batch_first格式\"\n    },\n    {\n      \"base_block_id\": \"CASE_02\",\n      \"priority\": \"Medium\",\n      \"params\": {\n        \"mode\": \"LSTM\",\n        \"input_size\": 16,\n        \"hidden_size\": 32,\n        \"num_layers\": 1,\n        \"batch_first\": false,\n        \"bidirectional\": true,\n        \"dtype\": \"float32\",\n        \"device\": \"cpu\",\n        \"batch_size\": 1,\n        \"seq_len\": 10\n      },\n      \"note\": \"扩展参数：单层双向LSTM、单批次、长序列\"\n    },\n    {\n      \"base_block_id\": \"CASE_03\",\n      \"priority\": \"Low\",\n      \"params\": {\n        \"mode\": \"GRU\",\n        \"input_size\": 4,\n        \"hidden_size\": 8,\n        \"num_layers\": 2,\n        \"batch_first\": true,\n        \"bidirectional\": true,\n        \"dtype\": \"float32\",\n        \"device\": \"cpu\",\n        \"batch_size\": 1,\n        \"seq_len\": 1\n      },\n      \"note\": \"扩展参数：最小参数组合、单元素输入\"\n    }\n  ],\n  \"smoke_set\": [\"CASE_01\", \"CASE_02\", \"CASE_03\", \"CASE_04\"],\n  \"deferred_set\": [\"CASE_05\", \"CASE_06\", \"CASE_07\", \"CASE_08\"]\n}",
    "test_plan.md": "# torch.nn.modules.rnn 测试计划\n\n## 1. 测试策略\n- 单元测试框架：pytest\n- 隔离策略：使用 fixtures 管理 RNN 实例，mock 随机数生成器用于 dropout 测试\n- 随机性处理：固定随机种子保证可重复性，控制 RNG 状态\n- 设备管理：优先 CPU 测试，CUDA 作为可选扩展\n\n## 2. 生成规格摘要（来自 test_plan.json）\n- **SMOKE_SET**: CASE_01（基础RNN）、CASE_02（LSTM）、CASE_03（双向GRU）、CASE_04（多层dropout）\n- **DEFERRED_SET**: CASE_05（LSTM投影）、CASE_06-CASE_08（待定义）\n- **group 列表**:\n  - G1: 核心RNN/LSTM/GRU正向传播（CASE_01,02,05,06）\n  - G2: 高级功能与边界条件（CASE_03,04,07,08）\n- **active_group_order**: [\"G1\", \"G2\"]\n- **断言分级策略**: 首轮仅使用 weak 断言（形状、类型、有限性检查），最终轮启用 strong 断言（近似相等、梯度检查）\n- **预算策略**: \n  - S 尺寸：max_lines=70-85, max_params=8-9\n  - M 尺寸：max_lines=80-85, max_params=9\n  - 所有用例均参数化，仅 CASE_04 需要 mock\n\n## 3. 数据与边界\n- **正常数据集**: 随机生成小规模张量（batch_size≤4, seq_len≤10, hidden_size≤40）\n- **边界值测试**:\n  - 单层/多层网络边界\n  - dropout=0.0/1.0 边界\n  - batch_first=True/False 格式转换\n  - 双向RNN输出维度验证\n- **极端形状**:\n  - batch_size=1, seq_len=1 最小输入\n  - 单层双向RNN特殊处理\n  - 投影尺寸约束（proj_size < hidden_size）\n- **空输入与异常**:\n  - 无效mode字符串异常\n  - 形状不匹配异常\n  - 非LSTM使用proj_size异常\n  - dropout在单层网络警告\n\n## 4. 覆盖映射\n| TC ID | 对应需求 | 覆盖约束 | 风险点 |\n|-------|----------|----------|--------|\n| TC-01 | 基础正向传播 | 形状正确性、类型检查 | 浮点精度差异 |\n| TC-02 | LSTM功能 | 隐藏/细胞状态管理 | 门控单元复杂性 |\n| TC-03 | 双向RNN | 输出维度2×hidden_size | 前后向参数共享 |\n| TC-04 | dropout随机性 | 多层网络dropout有效性 | 随机种子管理 |\n| TC-05 | LSTM投影 | proj_size约束检查 | 投影权重初始化 |\n\n**尚未覆盖的关键风险点**:\n- PackedSequence输入处理\n- CUDA/cuDNN特定优化路径\n- 内存碎片化影响\n- 并行计算线程安全性\n- 单元版本（RNNCell/LSTMCell/GRUCell）独立测试\n\n## 5. 迭代策略\n1. **首轮（Round1）**: 仅生成 SMOKE_SET 中的4个核心用例，使用weak断言\n2. **中间轮（RoundN）**: 修复失败用例，每次最多处理3个block，提升deferred用例\n3. **最终轮（Final）**: 启用strong断言，可选覆盖率检查，补齐所有deferred用例",
    "tests/test_torch_nn_modules_rnn_g1.py": "\"\"\"\nTest module for torch.nn.modules.rnn (Group G1: Core RNN/LSTM/GRU forward propagation)\n\"\"\"\nimport math\nimport pytest\nimport torch\nimport torch.nn as nn\nfrom unittest.mock import patch, MagicMock\nimport numpy as np\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# ==== BLOCK:HEADER START ====\n# Test fixtures and helper functions\n@pytest.fixture\ndef set_random_seed():\n    \"\"\"Fixture to set random seeds before each test.\"\"\"\n    torch.manual_seed(42)\n    np.random.seed(42)\n    yield\n    # Cleanup if needed\n\ndef create_test_input(batch_size, seq_len, input_size, batch_first=False, dtype=torch.float32):\n    \"\"\"Create test input tensor with given parameters.\"\"\"\n    if batch_first:\n        shape = (batch_size, seq_len, input_size)\n    else:\n        shape = (seq_len, batch_size, input_size)\n    return torch.randn(*shape, dtype=dtype)\n\ndef assert_shape_equal(actual, expected, msg=\"\"):\n    \"\"\"Assert that tensor shape matches expected.\"\"\"\n    assert actual.shape == expected, f\"{msg}: expected {expected}, got {actual.shape}\"\n\ndef assert_dtype_equal(actual, expected_dtype, msg=\"\"):\n    \"\"\"Assert that tensor dtype matches expected.\"\"\"\n    assert actual.dtype == expected_dtype, f\"{msg}: expected {expected_dtype}, got {actual.dtype}\"\n\ndef assert_finite(tensor, msg=\"\"):\n    \"\"\"Assert that tensor contains only finite values.\"\"\"\n    assert torch.isfinite(tensor).all(), f\"{msg}: tensor contains non-finite values\"\n\ndef assert_no_nan(tensor, msg=\"\"):\n    \"\"\"Assert that tensor contains no NaN values.\"\"\"\n    assert not torch.isnan(tensor).any(), f\"{msg}: tensor contains NaN values\"\n# ==== BLOCK:HEADER END ====\n\n# ==== BLOCK:CASE_01 START ====\n# Placeholder for CASE_01: 基础RNN正向传播形状验证\n# ==== BLOCK:CASE_01 END ====\n\n# ==== BLOCK:CASE_02 START ====\n# Placeholder for CASE_02: LSTM基础功能验证\n# ==== BLOCK:CASE_02 END ====\n\n# ==== BLOCK:CASE_05 START ====\n# Placeholder for CASE_05: LSTM投影功能约束检查 (DEFERRED)\n# ==== BLOCK:CASE_05 END ====\n\n# ==== BLOCK:CASE_06 START ====\n# Placeholder for CASE_06: (DEFERRED)\n# ==== BLOCK:CASE_06 END ====\n\n# ==== BLOCK:FOOTER START ====\n# Additional test cases and cleanup\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n# ==== BLOCK:FOOTER END ====",
    "tests/test_torch_nn_modules_rnn_g2.py": "\"\"\"\nTest module for torch.nn.modules.rnn (Group G2: Advanced features and boundary conditions)\nThis file will be populated in later iterations.\n\"\"\"\nimport pytest\nimport torch\nimport torch.nn as nn\n\n# ==== BLOCK:HEADER START ====\n# Placeholder for G2 header\n# ==== BLOCK:HEADER END ====\n\n# ==== BLOCK:CASE_03 START ====\n# Placeholder for CASE_03: 双向RNN输出维度验证 (SMOKE_SET)\n# ==== BLOCK:CASE_03 END ====\n\n# ==== BLOCK:CASE_04 START ====\n# Placeholder for CASE_04: 多层dropout随机性测试 (SMOKE_SET)\n# ==== BLOCK:CASE_04 END ====\n\n# ==== BLOCK:CASE_07 START ====\n# Placeholder for CASE_07: (DEFERRED)\n# ==== BLOCK:CASE_07 END ====\n\n# ==== BLOCK:CASE_08 START ====\n# Placeholder for CASE_08: (DEFERRED)\n# ==== BLOCK:CASE_08 END ====\n\n# ==== BLOCK:FOOTER START ====\n# Placeholder for G2 footer\n# ==== BLOCK:FOOTER END ====",
    "execution_log.txt": "=== Run Tests ===\n....F....X...........F.....x.                                            [100%]\n=================================== FAILURES ===================================\n_ test_lstm_projection_functionality[LSTM-10-20-15-1-False-False-float32-2-4] __\n\nset_random_seed = None, mode = 'LSTM', input_size = 10, hidden_size = 20\nproj_size = 15, num_layers = 1, batch_first = False, bidirectional = False\ndtype_str = 'float32', batch_size = 2, seq_len = 4\n\n    @pytest.mark.parametrize(\n        \"mode,input_size,hidden_size,proj_size,num_layers,batch_first,bidirectional,dtype_str,batch_size,seq_len\",\n        [\n            # Base case from test plan: LSTM with projection\n            (\"LSTM\", 10, 20, 15, 1, False, False, \"float32\", 2, 4),\n        ]\n    )\n    def test_lstm_projection_functionality(\n        set_random_seed,\n        mode,\n        input_size,\n        hidden_size,\n        proj_size,\n        num_layers,\n        batch_first,\n        bidirectional,\n        dtype_str,\n        batch_size,\n        seq_len,\n    ):\n        \"\"\"\n        Test LSTM projection functionality constraint checking.\n    \n        Weak assertions:\n        - output_shape_proj: Check output tensor shape with projection\n        - hidden_shape_proj: Check hidden state shape with projection\n        - dtype: Check output dtype matches input dtype\n        - finite: Check all values are finite\n        - proj_size_constraint: Check projection size constraints\n        \"\"\"\n        # Convert dtype string to torch dtype\n        dtype_map = {\n            \"float32\": torch.float32,\n            \"float64\": torch.float64,\n        }\n        dtype = dtype_map[dtype_str]\n    \n        # Create LSTM instance with projection\n        # Note: proj_size is only supported for LSTM in PyTorch\n        lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            proj_size=proj_size,\n            num_layers=num_layers,\n            batch_first=batch_first,\n            bidirectional=bidirectional,\n        )\n    \n        # Convert LSTM parameters to match input dtype if needed\n        if dtype == torch.float64:\n            lstm = lstm.double()\n    \n        # Create test input\n        x = create_test_input(batch_size, seq_len, input_size, batch_first, dtype)\n    \n        # Forward pass\n        output, (h_n, c_n) = lstm(x)\n    \n        # Calculate expected shapes with projection\n        num_directions = 2 if bidirectional else 1\n    \n        # Expected output shape: output uses proj_size instead of hidden_size\n        if batch_first:\n            expected_output_shape = (batch_size, seq_len, num_directions * proj_size)\n        else:\n            expected_output_shape = (seq_len, batch_size, num_directions * proj_size)\n    \n        # Expected hidden state shape: h_n uses proj_size\n        expected_hidden_shape = (num_layers * num_directions, batch_size, proj_size)\n    \n        # Expected cell state shape: c_n still uses hidden_size (not projected)\n        expected_cell_shape = (num_layers * num_directions, batch_size, hidden_size)\n    \n        # Weak assertions\n        # 1. Output shape assertion with projection\n        assert_shape_equal(output, expected_output_shape,\n                          f\"Output shape mismatch for LSTM with projection\")\n    \n        # 2. Hidden state shape assertion with projection\n        assert_shape_equal(h_n, expected_hidden_shape,\n                          f\"Hidden state shape mismatch for LSTM with projection\")\n    \n        # 3. Cell state shape assertion (should still use hidden_size)\n        assert_shape_equal(c_n, expected_cell_shape,\n                          f\"Cell state shape mismatch for LSTM with projection\")\n    \n        # 4. Dtype assertion\n        assert_dtype_equal(output, dtype,\n                          f\"Output dtype mismatch for LSTM with projection\")\n        assert_dtype_equal(h_n, dtype,\n                          f\"Hidden state dtype mismatch for LSTM with projection\")\n        assert_dtype_equal(c_n, dtype,\n                          f\"Cell state dtype mismatch for LSTM with projection\")\n    \n        # 5. Finite values assertion\n        assert_finite(output, f\"Output contains non-finite values for LSTM with projection\")\n        assert_finite(h_n, f\"Hidden state contains non-finite values for LSTM with projection\")\n        assert_finite(c_n, f\"Cell state contains non-finite values for LSTM with projection\")\n    \n        # 6. No NaN assertion (additional safety check)\n        assert_no_nan(output, f\"Output contains NaN values for LSTM with projection\")\n        assert_no_nan(h_n, f\"Hidden state contains NaN values for LSTM with projection\")\n        assert_no_nan(c_n, f\"Cell state contains NaN values for LSTM with projection\")\n    \n        # 7. Projection size constraint check\n        # proj_size must be < hidden_size (PyTorch enforces this)\n        assert proj_size < hidden_size, \\\n            f\"proj_size ({proj_size}) must be smaller than hidden_size ({hidden_size})\"\n    \n        # LSTM-specific projection checks\n        # Check that output dimension matches proj_size (not hidden_size)\n        assert output.shape[-1] == num_directions * proj_size, \\\n            f\"Output dimension should be {num_directions * proj_size} with projection, got {output.shape[-1]}\"\n    \n        # Check that hidden state dimension matches proj_size\n        assert h_n.shape[-1] == proj_size, \\\n            f\"Hidden state dimension should be {proj_size} with projection, got {h_n.shape[-1]}\"\n    \n        # Check that cell state dimension still matches hidden_size (not projected)\n        assert c_n.shape[-1] == hidden_size, \\\n            f\"Cell state dimension should be {hidden_size} (not projected), got {c_n.shape[-1]}\"\n    \n        # Check parameter shapes for projection\n        # LSTM with projection has different weight/bias parameter shapes\n        for name, param in lstm.named_parameters():\n            if \"weight_ih\" in name:\n                # weight_ih_l0 shape: (4*hidden_size, input_size)\n                # For LSTM with projection, weight_ih_l0 still connects input to hidden\n                # So last dimension should be input_size for layer 0\n                if \"_l0\" in name:\n                    # First layer: connects input to hidden\n                    assert param.shape[-1] == input_size, \\\n                        f\"weight_ih_l0 should have last dimension {input_size}, got {param.shape}\"\n                else:\n                    # Higher layers: connect previous hidden state to current hidden\n                    # For projection, previous hidden state has dimension proj_size\n                    # Extract layer number from name\n                    layer_num = name.split('_')[-1][1:]  # e.g., \"l0\" from \"weight_ih_l0\"\n                    assert param.shape[-1] == proj_size, \\\n                        f\"weight_ih_l{layer_num} should have last dimension {proj_size}, got {param.shape}\"\n    \n            elif \"weight_hh\" in name:\n                # weight_hh_l0 shape: (4*hidden_size, proj_size) for LSTM with projection\n                # This connects previous hidden state (size proj_size) to current hidden\n                assert param.shape[-1] == proj_size, \\\n                    f\"weight_hh {name} should have last dimension {proj_size}, got {param.shape}\"\n    \n            elif \"weight_hr\" in name:\n                # weight_hr_l0 shape: (proj_size, hidden_size) - projection weights\n                assert param.shape[0] == proj_size and param.shape[1] == hidden_size, \\\n                    f\"weight_hr {name} should have shape ({proj_size}, {hidden_size}), got {param.shape}\"\n    \n        # Test that projection is only supported for LSTM\n        # (This is enforced by PyTorch at construction time)\n        # We'll verify by checking that non-LSTM RNNs don't accept proj_size parameter\n        # PyTorch raises ValueError, not TypeError, for proj_size in non-LSTM RNNs\n        with pytest.raises(ValueError, match=\"proj_size argument is only supported for LSTM\"):\n            nn.RNN(input_size=input_size, hidden_size=hidden_size, proj_size=proj_size)\n    \n        with pytest.raises(ValueError, match=\"proj_size argument is only supported for LSTM\"):\n            nn.GRU(input_size=input_size, hidden_size=hidden_size, proj_size=proj_size)\n    \n        # Test invalid proj_size values\n        # proj_size must be >= 0\n        with pytest.raises(ValueError, match=\"proj_size has to be a positive integer\"):\n>           nn.LSTM(input_size=input_size, hidden_size=hidden_size, proj_size=-1)\n\ntests/test_torch_nn_modules_rnn_g1.py:440: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/rnn.py:678: in __init__\n    super(LSTM, self).__init__('LSTM', *args, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = LSTM(10, 20, proj_size=-1), mode = 'LSTM', input_size = 10\nhidden_size = 20, num_layers = 1, bias = True, batch_first = False\ndropout = 0.0, bidirectional = False, proj_size = -1, device = None\ndtype = None\n\n    def __init__(self, mode: str, input_size: int, hidden_size: int,\n                 num_layers: int = 1, bias: bool = True, batch_first: bool = False,\n                 dropout: float = 0., bidirectional: bool = False, proj_size: int = 0,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super(RNNBase, self).__init__()\n        self.mode = mode\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        self.dropout = float(dropout)\n        self.bidirectional = bidirectional\n        self.proj_size = proj_size\n        num_directions = 2 if bidirectional else 1\n    \n        if not isinstance(dropout, numbers.Number) or not 0 <= dropout <= 1 or \\\n                isinstance(dropout, bool):\n            raise ValueError(\"dropout should be a number in range [0, 1] \"\n                             \"representing the probability of an element being \"\n                             \"zeroed\")\n        if dropout > 0 and num_layers == 1:\n            warnings.warn(\"dropout option adds dropout after all but last \"\n                          \"recurrent layer, so non-zero dropout expects \"\n                          \"num_layers greater than 1, but got dropout={} and \"\n                          \"num_layers={}\".format(dropout, num_layers))\n        if proj_size < 0:\n>           raise ValueError(\"proj_size should be a positive integer or zero to disable projections\")\nE           ValueError: proj_size should be a positive integer or zero to disable projections\n\n/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/rnn.py:72: ValueError\n\nDuring handling of the above exception, another exception occurred:\n\nset_random_seed = None, mode = 'LSTM', input_size = 10, hidden_size = 20\nproj_size = 15, num_layers = 1, batch_first = False, bidirectional = False\ndtype_str = 'float32', batch_size = 2, seq_len = 4\n\n    @pytest.mark.parametrize(\n        \"mode,input_size,hidden_size,proj_size,num_layers,batch_first,bidirectional,dtype_str,batch_size,seq_len\",\n        [\n            # Base case from test plan: LSTM with projection\n            (\"LSTM\", 10, 20, 15, 1, False, False, \"float32\", 2, 4),\n        ]\n    )\n    def test_lstm_projection_functionality(\n        set_random_seed,\n        mode,\n        input_size,\n        hidden_size,\n        proj_size,\n        num_layers,\n        batch_first,\n        bidirectional,\n        dtype_str,\n        batch_size,\n        seq_len,\n    ):\n        \"\"\"\n        Test LSTM projection functionality constraint checking.\n    \n        Weak assertions:\n        - output_shape_proj: Check output tensor shape with projection\n        - hidden_shape_proj: Check hidden state shape with projection\n        - dtype: Check output dtype matches input dtype\n        - finite: Check all values are finite\n        - proj_size_constraint: Check projection size constraints\n        \"\"\"\n        # Convert dtype string to torch dtype\n        dtype_map = {\n            \"float32\": torch.float32,\n            \"float64\": torch.float64,\n        }\n        dtype = dtype_map[dtype_str]\n    \n        # Create LSTM instance with projection\n        # Note: proj_size is only supported for LSTM in PyTorch\n        lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            proj_size=proj_size,\n            num_layers=num_layers,\n            batch_first=batch_first,\n            bidirectional=bidirectional,\n        )\n    \n        # Convert LSTM parameters to match input dtype if needed\n        if dtype == torch.float64:\n            lstm = lstm.double()\n    \n        # Create test input\n        x = create_test_input(batch_size, seq_len, input_size, batch_first, dtype)\n    \n        # Forward pass\n        output, (h_n, c_n) = lstm(x)\n    \n        # Calculate expected shapes with projection\n        num_directions = 2 if bidirectional else 1\n    \n        # Expected output shape: output uses proj_size instead of hidden_size\n        if batch_first:\n            expected_output_shape = (batch_size, seq_len, num_directions * proj_size)\n        else:\n            expected_output_shape = (seq_len, batch_size, num_directions * proj_size)\n    \n        # Expected hidden state shape: h_n uses proj_size\n        expected_hidden_shape = (num_layers * num_directions, batch_size, proj_size)\n    \n        # Expected cell state shape: c_n still uses hidden_size (not projected)\n        expected_cell_shape = (num_layers * num_directions, batch_size, hidden_size)\n    \n        # Weak assertions\n        # 1. Output shape assertion with projection\n        assert_shape_equal(output, expected_output_shape,\n                          f\"Output shape mismatch for LSTM with projection\")\n    \n        # 2. Hidden state shape assertion with projection\n        assert_shape_equal(h_n, expected_hidden_shape,\n                          f\"Hidden state shape mismatch for LSTM with projection\")\n    \n        # 3. Cell state shape assertion (should still use hidden_size)\n        assert_shape_equal(c_n, expected_cell_shape,\n                          f\"Cell state shape mismatch for LSTM with projection\")\n    \n        # 4. Dtype assertion\n        assert_dtype_equal(output, dtype,\n                          f\"Output dtype mismatch for LSTM with projection\")\n        assert_dtype_equal(h_n, dtype,\n                          f\"Hidden state dtype mismatch for LSTM with projection\")\n        assert_dtype_equal(c_n, dtype,\n                          f\"Cell state dtype mismatch for LSTM with projection\")\n    \n        # 5. Finite values assertion\n        assert_finite(output, f\"Output contains non-finite values for LSTM with projection\")\n        assert_finite(h_n, f\"Hidden state contains non-finite values for LSTM with projection\")\n        assert_finite(c_n, f\"Cell state contains non-finite values for LSTM with projection\")\n    \n        # 6. No NaN assertion (additional safety check)\n        assert_no_nan(output, f\"Output contains NaN values for LSTM with projection\")\n        assert_no_nan(h_n, f\"Hidden state contains NaN values for LSTM with projection\")\n        assert_no_nan(c_n, f\"Cell state contains NaN values for LSTM with projection\")\n    \n        # 7. Projection size constraint check\n        # proj_size must be < hidden_size (PyTorch enforces this)\n        assert proj_size < hidden_size, \\\n            f\"proj_size ({proj_size}) must be smaller than hidden_size ({hidden_size})\"\n    \n        # LSTM-specific projection checks\n        # Check that output dimension matches proj_size (not hidden_size)\n        assert output.shape[-1] == num_directions * proj_size, \\\n            f\"Output dimension should be {num_directions * proj_size} with projection, got {output.shape[-1]}\"\n    \n        # Check that hidden state dimension matches proj_size\n        assert h_n.shape[-1] == proj_size, \\\n            f\"Hidden state dimension should be {proj_size} with projection, got {h_n.shape[-1]}\"\n    \n        # Check that cell state dimension still matches hidden_size (not projected)\n        assert c_n.shape[-1] == hidden_size, \\\n            f\"Cell state dimension should be {hidden_size} (not projected), got {c_n.shape[-1]}\"\n    \n        # Check parameter shapes for projection\n        # LSTM with projection has different weight/bias parameter shapes\n        for name, param in lstm.named_parameters():\n            if \"weight_ih\" in name:\n                # weight_ih_l0 shape: (4*hidden_size, input_size)\n                # For LSTM with projection, weight_ih_l0 still connects input to hidden\n                # So last dimension should be input_size for layer 0\n                if \"_l0\" in name:\n                    # First layer: connects input to hidden\n                    assert param.shape[-1] == input_size, \\\n                        f\"weight_ih_l0 should have last dimension {input_size}, got {param.shape}\"\n                else:\n                    # Higher layers: connect previous hidden state to current hidden\n                    # For projection, previous hidden state has dimension proj_size\n                    # Extract layer number from name\n                    layer_num = name.split('_')[-1][1:]  # e.g., \"l0\" from \"weight_ih_l0\"\n                    assert param.shape[-1] == proj_size, \\\n                        f\"weight_ih_l{layer_num} should have last dimension {proj_size}, got {param.shape}\"\n    \n            elif \"weight_hh\" in name:\n                # weight_hh_l0 shape: (4*hidden_size, proj_size) for LSTM with projection\n                # This connects previous hidden state (size proj_size) to current hidden\n                assert param.shape[-1] == proj_size, \\\n                    f\"weight_hh {name} should have last dimension {proj_size}, got {param.shape}\"\n    \n            elif \"weight_hr\" in name:\n                # weight_hr_l0 shape: (proj_size, hidden_size) - projection weights\n                assert param.shape[0] == proj_size and param.shape[1] == hidden_size, \\\n                    f\"weight_hr {name} should have shape ({proj_size}, {hidden_size}), got {param.shape}\"\n    \n        # Test that projection is only supported for LSTM\n        # (This is enforced by PyTorch at construction time)\n        # We'll verify by checking that non-LSTM RNNs don't accept proj_size parameter\n        # PyTorch raises ValueError, not TypeError, for proj_size in non-LSTM RNNs\n        with pytest.raises(ValueError, match=\"proj_size argument is only supported for LSTM\"):\n            nn.RNN(input_size=input_size, hidden_size=hidden_size, proj_size=proj_size)\n    \n        with pytest.raises(ValueError, match=\"proj_size argument is only supported for LSTM\"):\n            nn.GRU(input_size=input_size, hidden_size=hidden_size, proj_size=proj_size)\n    \n        # Test invalid proj_size values\n        # proj_size must be >= 0\n>       with pytest.raises(ValueError, match=\"proj_size has to be a positive integer\"):\nE       AssertionError: Regex pattern did not match.\nE         Expected regex: 'proj_size has to be a positive integer'\nE         Actual message: 'proj_size should be a positive integer or zero to disable projections'\n\ntests/test_torch_nn_modules_rnn_g1.py:439: AssertionError\n______________________ test_rnn_invalid_input_dimensions _______________________\n\n    def test_rnn_invalid_input_dimensions():\n        \"\"\"Test that invalid input dimensions raise appropriate errors.\"\"\"\n        # Test 1D input (invalid - should be 2D for unbatched or 3D for batched)\n        rnn = nn.RNN(input_size=10, hidden_size=20)\n        x_1d = torch.randn(10)  # 1D input - should fail\n    \n        with pytest.raises(RuntimeError, match=\"input must have 2 or 3 dimensions\"):\n>           rnn(x_1d)\n\ntests/test_torch_nn_modules_rnn_g2.py:997: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl\n    return forward_call(*input, **kwargs)\n/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/rnn.py:472: in forward\n    self.check_forward_args(input, hx, batch_sizes)\n/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/rnn.py:234: in check_forward_args\n    self.check_input(input, batch_sizes)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = RNN(10, 20)\ninput = tensor([[-1.3864],\n        [ 0.3249],\n        [-1.3803],\n        [-0.1548],\n        [ 0.1531],\n        [-0.5718],\n        [ 2.1046],\n        [ 0.8042],\n        [-3.1167],\n        [-2.2687]])\nbatch_sizes = None\n\n    def check_input(self, input: Tensor, batch_sizes: Optional[Tensor]) -> None:\n        expected_input_dim = 2 if batch_sizes is not None else 3\n        if input.dim() != expected_input_dim:\n>           raise RuntimeError(\n                'input must have {} dimensions, got {}'.format(\n                    expected_input_dim, input.dim()))\nE           RuntimeError: input must have 3 dimensions, got 2\n\n/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/rnn.py:206: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\n    def test_rnn_invalid_input_dimensions():\n        \"\"\"Test that invalid input dimensions raise appropriate errors.\"\"\"\n        # Test 1D input (invalid - should be 2D for unbatched or 3D for batched)\n        rnn = nn.RNN(input_size=10, hidden_size=20)\n        x_1d = torch.randn(10)  # 1D input - should fail\n    \n>       with pytest.raises(RuntimeError, match=\"input must have 2 or 3 dimensions\"):\nE       AssertionError: Regex pattern did not match.\nE         Expected regex: 'input must have 2 or 3 dimensions'\nE         Actual message: 'input must have 3 dimensions, got 2'\n\ntests/test_torch_nn_modules_rnn_g2.py:996: AssertionError\n================================ tests coverage ================================\n______________ coverage: platform darwin, python 3.10.19-final-0 _______________\n\nName                                    Stmts   Miss Branch BrPart  Cover   Missing\n-----------------------------------------------------------------------------------\ntest_error_check.py                         0      0      0      0   100%\ntests/test_torch_nn_modules_rnn_g1.py     164     11     32      6    91%   148->exit, 265-266, 325, 338, 412-413, 443-447, 519\ntests/test_torch_nn_modules_rnn_g2.py     432     56    164     36    83%   51, 102-121, 125, 134, 172->exit, 245-274, 278, 288, 297, 311-313, 323-325, 334-335, 342, 353-355, 366, 372, 381-383, 394, 451, 474, 594, 597->617, 603, 628, 640, 651, 741, 745, 868->874, 878->881, 896->899, 907->929, 916, 943->953, 953->exit, 973, 1000-1018, 1187\n-----------------------------------------------------------------------------------\nTOTAL                                     596     67    196     42    85%\nCoverage XML written to file coverage.xml\n=========================== short test summary info ============================\nFAILED tests/test_torch_nn_modules_rnn_g1.py::test_lstm_projection_functionality[LSTM-10-20-15-1-False-False-float32-2-4]\nFAILED tests/test_torch_nn_modules_rnn_g2.py::test_rnn_invalid_input_dimensions\n2 failed, 25 passed, 1 xfailed, 1 xpassed in 0.99s\n\nError: exit 1",
    "exit_code.txt": "1",
    "analysis_plan.json": "{\n  \"status\": \"未完全通过\",\n  \"passed\": 25,\n  \"failed\": 2,\n  \"errors\": 0,\n  \"collection_errors\": false,\n  \"block_limit\": 3,\n  \"failures\": [\n    {\n      \"test\": \"test_lstm_projection_functionality[LSTM-10-20-15-1-False-False-float32-2-4]\",\n      \"block_id\": \"CASE_05\",\n      \"error_type\": \"AssertionError\",\n      \"action\": \"adjust_assertion\",\n      \"note\": \"正则表达式不匹配：预期'proj_size has to be a positive integer'，实际'proj_size should be a positive integer or zero to disable projections'\"\n    },\n    {\n      \"test\": \"test_rnn_invalid_input_dimensions\",\n      \"block_id\": \"FOOTER\",\n      \"error_type\": \"AssertionError\",\n      \"action\": \"adjust_assertion\",\n      \"note\": \"正则表达式不匹配：预期'input must have 2 or 3 dimensions'，实际'input must have 3 dimensions, got 2'\"\n    }\n  ],\n  \"deferred\": [],\n  \"stop_recommended\": false,\n  \"stop_reason\": \"\"\n}",
    "analysis.md": "## 测试结果分析\n\n### 状态与统计\n- **状态**: 未完全通过\n- **通过**: 25个测试\n- **失败**: 2个测试\n- **错误**: 0个\n- **跳过/预期失败**: 2个 (1 xfailed, 1 xpassed)\n\n### 待修复 BLOCK 列表 (2个)\n\n1. **BLOCK_ID**: CASE_05\n   - **测试**: test_lstm_projection_functionality[LSTM-10-20-15-1-False-False-float32-2-4]\n   - **错误类型**: AssertionError\n   - **修复动作**: adjust_assertion\n   - **原因**: 正则表达式不匹配 - 预期错误消息与实际PyTorch错误消息不一致\n\n2. **BLOCK_ID**: FOOTER\n   - **测试**: test_rnn_invalid_input_dimensions\n   - **错误类型**: AssertionError\n   - **修复动作**: adjust_assertion\n   - **原因**: 正则表达式不匹配 - 预期错误消息与实际PyTorch错误消息不一致\n\n### 停止建议\n- **stop_recommended**: false\n- **stop_reason**: 无",
    "test_error_check.py": "import torch\nimport torch.nn as nn\n\n# Test 1: 2D input when 3D is expected\nprint(\"Test 1: 2D input when 3D is expected\")\ntry:\n    rnn = nn.RNN(input_size=10, hidden_size=20)\n    x_2d = torch.randn(5, 10)  # Should be (seq_len, batch, input_size) or (batch, seq_len, input_size)\n    output, h_n = rnn(x_2d)\n    print(\"No error raised - unexpected!\")\nexcept Exception as e:\n    print(f\"Error type: {type(e).__name__}\")\n    print(f\"Error message: {str(e)}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Test 2: Wrong input size\nprint(\"Test 2: Wrong input size\")\ntry:\n    rnn = nn.RNN(input_size=10, hidden_size=20)\n    x_wrong_size = torch.randn(5, 3, 15)  # input_size=15, expected 10\n    output, h_n = rnn(x_wrong_size)\n    print(\"No error raised - unexpected!\")\nexcept Exception as e:\n    print(f\"Error type: {type(e).__name__}\")\n    print(f\"Error message: {str(e)}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Test 3: Check if PyTorch supports unbatched 2D input\nprint(\"Test 3: Check unbatched 2D input (L, H_in)\")\ntry:\n    rnn = nn.RNN(input_size=10, hidden_size=20)\n    x_unbatched = torch.randn(5, 10)  # (L, H_in) - unbatched input\n    output, h_n = rnn(x_unbatched)\n    print(\"Success! PyTorch supports unbatched 2D input\")\n    print(f\"Output shape: {output.shape}\")\n    print(f\"Hidden shape: {h_n.shape}\")\nexcept Exception as e:\n    print(f\"Error type: {type(e).__name__}\")\n    print(f\"Error message: {str(e)}\")",
    "final_report.md": "# torch.nn.modules.rnn 测试报告\n\n## 1. 执行摘要\n测试基本通过，核心功能验证成功，但存在2个断言错误需要修复；关键发现为LSTM投影功能和非LSTM使用proj_size的异常消息格式需要调整。\n\n## 2. 测试范围\n- **目标FQN**: torch.nn.modules.rnn\n- **测试环境**: pytest + PyTorch，CPU优先测试\n- **覆盖场景**:\n  - 基础RNN/LSTM/GRU正向传播形状验证\n  - batch_first格式转换\n  - 双向RNN输出维度验证\n  - 多层网络dropout随机性\n  - LSTM投影功能约束检查\n  - 输入维度异常处理\n- **未覆盖项**:\n  - PackedSequence输入处理\n  - CUDA/cuDNN特定优化路径\n  - 单元版本（RNNCell/LSTMCell/GRUCell）独立测试\n  - 权重初始化分布验证\n  - 设备间（CPU/CUDA）结果一致性\n\n## 3. 结果概览\n- **用例总数**: 29个（25通过 + 2失败 + 2跳过/预期失败）\n- **通过率**: 86.2%（25/29）\n- **主要失败点**:\n  1. LSTM投影功能测试 - 正则表达式匹配失败\n  2. 无效输入维度测试 - 错误消息格式不匹配\n\n## 4. 详细发现\n\n### 高优先级问题\n1. **CASE_05 - LSTM投影功能断言错误**\n   - **严重级别**: 中\n   - **根因**: 预期错误消息与实际PyTorch错误消息格式不一致\n   - **修复动作**: 调整正则表达式以匹配实际PyTorch异常消息格式\n   - **影响**: 影响LSTM投影功能约束验证的准确性\n\n2. **FOOTER - 无效输入维度测试断言错误**\n   - **严重级别**: 中\n   - **根因**: 预期错误消息与实际PyTorch错误消息格式不匹配\n   - **修复动作**: 更新断言中的正则表达式模式\n   - **影响**: 影响输入维度验证测试的可靠性\n\n### 已通过的核心功能验证\n- 基础RNN/LSTM/GRU正向传播形状正确性\n- batch_first格式输入输出转换\n- 双向RNN输出维度验证（2 × hidden_size）\n- dropout在多层网络中的随机性（使用mock控制）\n- 参数初始化正确性\n\n## 5. 覆盖与风险\n\n### 需求覆盖情况\n- ✅ 基础正向传播形状正确性\n- ✅ batch_first格式转换\n- ✅ 双向RNN输出维度验证\n- ✅ dropout随机性（多层网络）\n- ✅ LSTM投影功能约束检查（部分失败）\n- ✅ 输入维度异常处理（部分失败）\n\n### 尚未覆盖的边界/缺失信息\n1. **PackedSequence输入处理**: 未测试打包序列的特殊逻辑\n2. **CUDA/cuDNN优化路径**: 仅测试CPU路径，缺少GPU特定优化验证\n3. **单元版本独立测试**: RNNCell/LSTMCell/GRUCell未单独测试\n4. **极端参数值**: 极大/极小hidden_size边界测试缺失\n5. **内存边界**: 大尺寸张量内存占用验证\n6. **并行计算**: 多线程环境下的线程安全性\n\n### 已知风险\n- 多实体情况（8个主要类）测试覆盖不完整\n- 类型注解不完整可能影响静态分析\n- CUDA/cuDNN特定优化路径未验证\n- 内存碎片化影响未评估\n\n## 6. 后续动作\n\n### 优先级排序的TODO\n\n**P0 - 立即修复（当前迭代）**\n1. 修复CASE_05测试中的正则表达式匹配问题\n   - 更新预期错误消息格式以匹配实际PyTorch异常\n   - 验证LSTM投影功能约束检查的正确性\n\n2. 修复FOOTER测试中的断言错误\n   - 调整无效输入维度测试的错误消息验证\n   - 确保异常处理测试的可靠性\n\n**P1 - 下一轮迭代**\n3. 补充单元版本独立测试\n   - 添加RNNCell、LSTMCell、GRUCell的单独测试用例\n   - 验证单元版本与完整RNN的一致性\n\n4. 扩展边界条件测试\n   - 添加batch_size=0或seq_len=0的边界测试\n   - 验证dropout=1.0的极端情况\n   - 测试proj_size=hidden_size的约束违反场景\n\n**P2 - 后续增强**\n5. 添加PackedSequence输入处理测试\n   - 验证打包序列与普通序列的等价性\n   - 测试不同填充长度的序列处理\n\n6. 设备兼容性测试扩展\n   - 添加CUDA可用性检测和测试\n   - 验证CPU/CUDA结果一致性\n\n7. 性能与内存测试\n   - 添加大尺寸张量内存占用验证\n   - 测试多层网络的性能基准\n\n**P3 - 长期改进**\n8. 覆盖率提升\n   - 添加权重初始化分布验证\n   - 扩展参数组合测试\n   - 增加反向传播梯度检查（如包含）\n\n9. 集成测试\n   - 与其他nn.Module的集成测试\n   - 训练循环中的稳定性验证\n\n### 环境调整建议\n1. 考虑添加CUDA测试环境配置\n2. 增加内存监控工具用于边界测试\n3. 配置覆盖率报告生成工具\n4. 建立测试数据生成工具库\n\n---\n\n**报告生成时间**: 当前时间  \n**测试状态**: 基本通过，需要小范围修复  \n**建议**: 优先修复2个断言错误，然后扩展单元版本和边界条件测试"
  },
  "stage_history": [
    {
      "stage": "understand_function",
      "status": "completed",
      "timestamp": "2026-01-19T23:25:50.143950",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "generate_requirements",
      "status": "completed",
      "timestamp": "2026-01-19T23:26:42.871136",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "design_test_plan",
      "status": "completed",
      "timestamp": "2026-01-19T23:28:43.808562",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-19T23:32:22.448414",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-19T23:32:23.738810",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-19T23:33:24.506817",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-19T23:42:09.279857",
      "attempts": 2,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-19T23:42:10.740882",
      "attempts": 2,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-19T23:43:38.187107",
      "attempts": 2,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-19T23:50:07.727048",
      "attempts": 3,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-19T23:50:09.024371",
      "attempts": 3,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-19T23:51:30.260704",
      "attempts": 3,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-19T23:57:00.616106",
      "attempts": 4,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-19T23:57:02.008192",
      "attempts": 4,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-19T23:58:07.268159",
      "attempts": 4,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-20T00:04:41.706299",
      "attempts": 5,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-20T00:04:43.175838",
      "attempts": 5,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-20T00:05:53.011930",
      "attempts": 5,
      "error": null
    },
    {
      "stage": "generate_report",
      "status": "completed",
      "timestamp": "2026-01-20T00:06:55.979554",
      "attempts": 1,
      "error": null
    }
  ],
  "user_feedback": []
}