=== Run Tests ===
..........F...F..                                                        [100%]
=================================== FAILURES ===================================
_____________________ test_instance_norm_channel_mismatch ______________________

    def test_instance_norm_channel_mismatch():
        """Test channel dimension mismatch error."""
        norm = InstanceNorm2d(num_features=3)
    
        # Input with wrong channel dimension
        wrong_input = torch.randn(2, 4, 5, 5)  # 4 channels, expected 3
    
        # According to PyTorch's actual behavior, InstanceNorm doesn't validate
        # channel dimension mismatch in forward pass. It will compute statistics
        # per channel dimension, which may produce unexpected results but won't raise
        # an error. The weight and bias parameters (if affine=True) will have shape
        # (num_features,) which may cause issues, but with affine=False, it may work.
    
        # With affine=False (default), forward pass should succeed
        # The layer will compute instance statistics for each of the 4 channels
        # This is allowed even though num_features=3
        output = norm(wrong_input)
        assert output.shape == (2, 4, 5, 5), f"Expected shape (2, 4, 5, 5), got {output.shape}"
    
        # Test with affine=True - this should also work
        norm_affine = InstanceNorm2d(num_features=3, affine=True)
        # With affine=True, weight and bias have shape (3,), but input has 4 channels
        # PyTorch will broadcast the parameters or may raise an error
        # Let's test the actual behavior
        try:
>           output_affine = norm_affine(wrong_input)

tests/test_torch_nn_modules_instancenorm_g1.py:644: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:74: in forward
    return self._apply_instance_norm(input)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:34: in _apply_instance_norm
    return F.instance_norm(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[ 1.9269e+00,  1.4873e+00,  9.0072e-01, -2.1055e+00,  6.7842e-01],
          [-1.2345e+00, -4.3067e-02, -1.6... 2.6178e-01, -7.5993e-01, -2.0461e+00],
          [-1.5295e+00,  4.0487e-01,  6.3188e-01,  3.1253e-01, -3.3502e-02]]]])
running_mean = None, running_var = None
weight = Parameter containing:
tensor([1., 1., 1.], requires_grad=True)
bias = Parameter containing:
tensor([0., 0., 0.], requires_grad=True)
use_input_stats = True, momentum = 0.1, eps = 1e-05

    def instance_norm(
        input: Tensor,
        running_mean: Optional[Tensor] = None,
        running_var: Optional[Tensor] = None,
        weight: Optional[Tensor] = None,
        bias: Optional[Tensor] = None,
        use_input_stats: bool = True,
        momentum: float = 0.1,
        eps: float = 1e-5,
    ) -> Tensor:
        r"""Applies Instance Normalization for each channel in each data sample in a
        batch.
    
        See :class:`~torch.nn.InstanceNorm1d`, :class:`~torch.nn.InstanceNorm2d`,
        :class:`~torch.nn.InstanceNorm3d` for details.
        """
        if has_torch_function_variadic(input, running_mean, running_var, weight, bias):
            return handle_torch_function(
                instance_norm,
                (input, running_mean, running_var, weight, bias),
                input,
                running_mean=running_mean,
                running_var=running_var,
                weight=weight,
                bias=bias,
                use_input_stats=use_input_stats,
                momentum=momentum,
                eps=eps,
            )
        if use_input_stats:
            _verify_spatial_size(input.size())
>       return torch.instance_norm(
            input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, torch.backends.cudnn.enabled
        )
E       RuntimeError: weight should contain 8 elements not 6

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/functional.py:2495: RuntimeError

During handling of the above exception, another exception occurred:

    def test_instance_norm_channel_mismatch():
        """Test channel dimension mismatch error."""
        norm = InstanceNorm2d(num_features=3)
    
        # Input with wrong channel dimension
        wrong_input = torch.randn(2, 4, 5, 5)  # 4 channels, expected 3
    
        # According to PyTorch's actual behavior, InstanceNorm doesn't validate
        # channel dimension mismatch in forward pass. It will compute statistics
        # per channel dimension, which may produce unexpected results but won't raise
        # an error. The weight and bias parameters (if affine=True) will have shape
        # (num_features,) which may cause issues, but with affine=False, it may work.
    
        # With affine=False (default), forward pass should succeed
        # The layer will compute instance statistics for each of the 4 channels
        # This is allowed even though num_features=3
        output = norm(wrong_input)
        assert output.shape == (2, 4, 5, 5), f"Expected shape (2, 4, 5, 5), got {output.shape}"
    
        # Test with affine=True - this should also work
        norm_affine = InstanceNorm2d(num_features=3, affine=True)
        # With affine=True, weight and bias have shape (3,), but input has 4 channels
        # PyTorch will broadcast the parameters or may raise an error
        # Let's test the actual behavior
        try:
            output_affine = norm_affine(wrong_input)
            # If it succeeds, check shape
            assert output_affine.shape == (2, 4, 5, 5), f"Expected shape (2, 4, 5, 5), got {output_affine.shape}"
        except RuntimeError as e:
            # It may raise a RuntimeError about shape mismatch
            # This is acceptable behavior
>           assert "shape" in str(e).lower() or "size" in str(e).lower() or "dimension" in str(e).lower(), \
                f"Expected shape-related error, got: {e}"
E           AssertionError: Expected shape-related error, got: weight should contain 8 elements not 6
E           assert ('shape' in 'weight should contain 8 elements not 6' or 'size' in 'weight should contain 8 elements not 6' or 'dimension' in 'weight should contain 8 elements not 6')
E            +  where 'weight should contain 8 elements not 6' = <built-in method lower of str object at 0x10e906430>()
E            +    where <built-in method lower of str object at 0x10e906430> = 'weight should contain 8 elements not 6'.lower
E            +      where 'weight should contain 8 elements not 6' = str(RuntimeError('weight should contain 8 elements not 6'))
E            +  and   'weight should contain 8 elements not 6' = <built-in method lower of str object at 0x10e906430>()
E            +    where <built-in method lower of str object at 0x10e906430> = 'weight should contain 8 elements not 6'.lower
E            +      where 'weight should contain 8 elements not 6' = str(RuntimeError('weight should contain 8 elements not 6'))
E            +  and   'weight should contain 8 elements not 6' = <built-in method lower of str object at 0x10e906430>()
E            +    where <built-in method lower of str object at 0x10e906430> = 'weight should contain 8 elements not 6'.lower
E            +      where 'weight should contain 8 elements not 6' = str(RuntimeError('weight should contain 8 elements not 6'))

tests/test_torch_nn_modules_instancenorm_g1.py:650: AssertionError
_________________ test_lazy_instance_norm_parameter_validation _________________

    def test_lazy_instance_norm_parameter_validation():
        """Test parameter validation for lazy instance norm."""
        # Test that lazy instance norm doesn't require num_features parameter
        # This is the main difference from regular instance norm
        lazy_norm = LazyInstanceNorm2d(affine=False, track_running_stats=False)
        assert lazy_norm.num_features == 0, \
            "LazyInstanceNorm2d should have num_features=0 initially"
    
        # Note: LazyInstanceNorm2d may not validate eps in constructor
        # but in forward pass. We'll test that invalid eps causes issues during forward.
        # For now, we'll test that constructor accepts valid eps values.
    
        # Test valid eps values are accepted
        lazy_norm_eps_small = LazyInstanceNorm2d(eps=1e-8, affine=False, track_running_stats=False)
        assert lazy_norm_eps_small.eps == 1e-8, f"eps should be 1e-8, got {lazy_norm_eps_small.eps}"
    
        lazy_norm_eps_large = LazyInstanceNorm2d(eps=1e-3, affine=False, track_running_stats=False)
        assert lazy_norm_eps_large.eps == 1e-3, f"eps should be 1e-3, got {lazy_norm_eps_large.eps}"
    
        # Test invalid momentum
>       with pytest.raises(ValueError, match="momentum"):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_torch_nn_modules_instancenorm_g2.py:376: Failed
=============================== warnings summary ===============================
exam/torch_group/nn.modules.instancenorm/tests/test_torch_nn_modules_instancenorm_g1.py::test_lazy_instance_norm_inference[test_params0]
exam/torch_group/nn.modules.instancenorm/tests/test_torch_nn_modules_instancenorm_g2.py::test_lazy_instance_norm_inference[test_params0]
exam/torch_group/nn.modules.instancenorm/tests/test_torch_nn_modules_instancenorm_g2.py::test_lazy_instance_norm_parameter_validation
exam/torch_group/nn.modules.instancenorm/tests/test_torch_nn_modules_instancenorm_g2.py::test_lazy_instance_norm_dimension_validation
exam/torch_group/nn.modules.instancenorm/tests/test_torch_nn_modules_instancenorm_g2.py::test_lazy_instance_norm_affine_parameters
  /opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
    warnings.warn('Lazy modules are a new feature under heavy development '

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                             Stmts   Miss Branch BrPart  Cover   Missing
--------------------------------------------------------------------------------------------
test_param_validation.py                             0      0      0      0   100%
tests/test_torch_nn_modules_instancenorm_g1.py     213     16     46     18    85%   21-33, 94, 118->125, 125->exit, 181, 232->exit, 269, 314->328, 317->322, 320, 322->328, 324, 328->341, 330->335, 333, 335->341, 337, 413, 511->exit, 646
tests/test_torch_nn_modules_instancenorm_g2.py     162     28     38     13    74%   21-33, 37-56, 89, 133->147, 136->141, 139, 141->147, 143, 147->160, 149->154, 152, 154->160, 156, 232, 330->exit, 379-390
--------------------------------------------------------------------------------------------
TOTAL                                              375     44     84     31    80%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_instancenorm_g1.py::test_instance_norm_channel_mismatch
FAILED tests/test_torch_nn_modules_instancenorm_g2.py::test_lazy_instance_norm_parameter_validation
2 failed, 15 passed, 5 warnings in 0.64s

Error: exit 1