=== Run Tests ===
F..F..                                                                   [100%]
=================================== FAILURES ===================================
_______________ test_lazy_instance_norm_inference[test_params0] ________________

test_params = {'affine': False, 'device': 'cpu', 'dtype': torch.float32, 'input_shape': (2, 3, 4, 4), ...}

    @pytest.mark.parametrize("test_params", [
        {
            "norm_class": "LazyInstanceNorm2d",
            "affine": False,
            "track_running_stats": False,
            "input_shape": (2, 3, 4, 4),
            "dtype": torch.float32,
            "device": "cpu"
        }
    ])
    def test_lazy_instance_norm_inference(test_params):
        """Test lazy instance norm automatic feature inference."""
        # Unpack parameters
        norm_class = test_params["norm_class"]
        affine = test_params["affine"]
        track_running_stats = test_params["track_running_stats"]
        input_shape = test_params["input_shape"]
        dtype = test_params["dtype"]
        device = test_params["device"]
    
        # Create lazy instance normalization layer
        if norm_class == "LazyInstanceNorm2d":
            norm_layer = LazyInstanceNorm2d(
                affine=affine,
                track_running_stats=track_running_stats,
                dtype=dtype
            )
        else:
            raise ValueError(f"Unsupported norm_class: {norm_class}")
    
        norm_layer.to(device)
    
        # Check initial state - lazy layers have num_features=0 initially
        # This is expected behavior for lazy modules
        assert hasattr(norm_layer, 'num_features'), \
            "Lazy layer should have num_features attribute"
        assert norm_layer.num_features == 0, \
            f"Lazy layer should have num_features=0 initially, got {norm_layer.num_features}"
    
        # Create random input
        input_tensor = torch.randn(*input_shape, dtype=dtype, device=device)
    
        # Forward pass - this should trigger initialization
        output = norm_layer(input_tensor)
    
        # Weak assertions
        # 1. Output shape matches input shape
        assert output.shape == input_shape, \
            f"Output shape {output.shape} doesn't match input shape {input_shape}"
    
        # 2. num_features should be inferred from input after forward pass
        expected_num_features = input_shape[1]  # channel dimension
>       assert norm_layer.num_features == expected_num_features, \
            f"num_features {norm_layer.num_features} should be inferred as {expected_num_features}"
E       AssertionError: num_features 0 should be inferred as 3
E       assert 0 == 3
E        +  where 0 = InstanceNorm2d(0, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False).num_features

tests/test_torch_nn_modules_instancenorm_g2.py:113: AssertionError
_________________ test_lazy_instance_norm_parameter_validation _________________

    def test_lazy_instance_norm_parameter_validation():
        """Test parameter validation for lazy instance norm."""
        # Test that lazy instance norm doesn't require num_features parameter
        # This is the main difference from regular instance norm
        lazy_norm = LazyInstanceNorm2d(affine=False, track_running_stats=False)
        assert lazy_norm.num_features == 0, \
            "LazyInstanceNorm2d should have num_features=0 initially"
    
        # Test invalid eps
>       with pytest.raises(ValueError, match="eps"):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_torch_nn_modules_instancenorm_g2.py:332: Failed
=============================== warnings summary ===============================
exam/torch_group/nn.modules.instancenorm/tests/test_torch_nn_modules_instancenorm_g2.py::test_lazy_instance_norm_inference[test_params0]
exam/torch_group/nn.modules.instancenorm/tests/test_torch_nn_modules_instancenorm_g2.py::test_lazy_instance_norm_parameter_validation
exam/torch_group/nn.modules.instancenorm/tests/test_torch_nn_modules_instancenorm_g2.py::test_lazy_instance_norm_dimension_validation
exam/torch_group/nn.modules.instancenorm/tests/test_torch_nn_modules_instancenorm_g2.py::test_lazy_instance_norm_affine_parameters
  /opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
    warnings.warn('Lazy modules are a new feature under heavy development '

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                             Stmts   Miss Branch BrPart  Cover   Missing
--------------------------------------------------------------------------------------------
tests/test_torch_nn_modules_instancenorm_g2.py     145     35     22      3    68%   21-33, 37-56, 89, 117-150, 199, 297->exit, 335-343
--------------------------------------------------------------------------------------------
TOTAL                                              145     35     22      3    68%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_instancenorm_g2.py::test_lazy_instance_norm_inference[test_params0]
FAILED tests/test_torch_nn_modules_instancenorm_g2.py::test_lazy_instance_norm_parameter_validation
2 failed, 4 passed, 4 warnings in 0.76s

Error: exit 1