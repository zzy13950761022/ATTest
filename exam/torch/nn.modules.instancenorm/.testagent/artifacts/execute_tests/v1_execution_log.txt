=== Run Tests ===
.....FFFFFF                                                              [100%]
=================================== FAILURES ===================================
_______________ test_lazy_instance_norm_inference[test_params0] ________________

test_params = {'affine': False, 'device': 'cpu', 'dtype': torch.float32, 'input_shape': (2, 3, 4, 4), ...}

    @pytest.mark.parametrize("test_params", [
        {
            "norm_class": "LazyInstanceNorm2d",
            "affine": False,
            "track_running_stats": False,
            "input_shape": (2, 3, 4, 4),
            "dtype": torch.float32,
            "device": "cpu"
        }
    ])
    def test_lazy_instance_norm_inference(test_params):
        """Test lazy instance norm automatic feature inference."""
        # Unpack parameters
        norm_class = test_params["norm_class"]
        affine = test_params["affine"]
        track_running_stats = test_params["track_running_stats"]
        input_shape = test_params["input_shape"]
        dtype = test_params["dtype"]
        device = test_params["device"]
    
        # Create lazy instance normalization layer
        if norm_class == "LazyInstanceNorm2d":
            norm_layer = LazyInstanceNorm2d(
                affine=affine,
                track_running_stats=track_running_stats,
                dtype=dtype
            )
        else:
            raise ValueError(f"Unsupported norm_class: {norm_class}")
    
        norm_layer.to(device)
    
        # Check initial state - num_features should not be set
        # Lazy layers don't have num_features until first forward pass
        assert not hasattr(norm_layer, 'num_features') or norm_layer.num_features == 0, \
            "Lazy layer should not have num_features initialized before first forward"
    
        # Create random input
        input_tensor = torch.randn(*input_shape, dtype=dtype, device=device)
    
        # Forward pass - this should trigger initialization
        output = norm_layer(input_tensor)
    
        # Weak assertions
        # 1. Output shape matches input shape
        assert output.shape == input_shape, \
            f"Output shape {output.shape} doesn't match input shape {input_shape}"
    
        # 2. num_features should be inferred from input
        expected_num_features = input_shape[1]  # channel dimension
        assert hasattr(norm_layer, 'num_features'), \
            "num_features attribute should exist after forward pass"
>       assert norm_layer.num_features == expected_num_features, \
            f"num_features {norm_layer.num_features} should be inferred as {expected_num_features}"
E       AssertionError: num_features 0 should be inferred as 3
E       assert 0 == 3
E        +  where 0 = InstanceNorm2d(0, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False).num_features

tests/test_torch_nn_modules_instancenorm_g1.py:293: AssertionError
_____________ test_instance_norm_track_running_stats[test_params0] _____________

test_params = {'affine': False, 'device': 'cpu', 'dtype': torch.float32, 'input_shape': (2, 2, 3, 3, 3), ...}

    @pytest.mark.parametrize("test_params", [
        {
            "norm_class": "InstanceNorm3d",
            "num_features": 2,
            "affine": False,
            "track_running_stats": True,
            "input_shape": (2, 2, 3, 3, 3),
            "dtype": torch.float32,
            "device": "cpu"
        },
        {
            "norm_class": "InstanceNorm3d",
            "num_features": 2,
            "affine": False,
            "track_running_stats": True,
            "input_shape": (2, 2, 3, 3, 3),
            "dtype": torch.float32,
            "device": "cpu",
            "momentum": 0.5
        }
    ])
    def test_instance_norm_track_running_stats(test_params):
        """Test track_running_stats functionality."""
        # Unpack parameters
        norm_class = test_params["norm_class"]
        num_features = test_params["num_features"]
        affine = test_params["affine"]
        track_running_stats = test_params["track_running_stats"]
        input_shape = test_params["input_shape"]
        dtype = test_params["dtype"]
        device = test_params["device"]
        momentum = test_params.get("momentum", 0.1)  # default momentum
    
        # Create instance normalization layer
        if norm_class == "InstanceNorm3d":
            norm_layer = InstanceNorm3d(
                num_features=num_features,
                affine=affine,
                track_running_stats=track_running_stats,
                momentum=momentum,
                dtype=dtype
            )
        else:
            raise ValueError(f"Unsupported norm_class: {norm_class}")
    
        norm_layer.to(device)
    
        # Weak assertions - check running stats exist
        # 1. Running mean and var should exist when track_running_stats=True
        assert hasattr(norm_layer, 'running_mean'), \
            "Should have running_mean when track_running_stats=True"
        assert hasattr(norm_layer, 'running_var'), \
            "Should have running_var when track_running_stats=True"
    
        # 2. Running stats should be initialized
        assert norm_layer.running_mean is not None, \
            "running_mean should be initialized"
        assert norm_layer.running_var is not None, \
            "running_var should be initialized"
    
        # 3. Running stats shapes should match num_features
        assert norm_layer.running_mean.shape == (num_features,), \
            f"running_mean shape {norm_layer.running_mean.shape} should be ({num_features},)"
        assert norm_layer.running_var.shape == (num_features,), \
            f"running_var shape {norm_layer.running_var.shape} should be ({num_features},)"
    
        # 4. Running stats should be initialized to zeros (mean) and ones (var)
        assert torch.allclose(norm_layer.running_mean,
                             torch.zeros(num_features, dtype=dtype)), \
            "running_mean should be initialized to 0"
        assert torch.allclose(norm_layer.running_var,
                             torch.ones(num_features, dtype=dtype)), \
            "running_var should be initialized to 1"
    
        # 5. Running stats should not require gradients
        assert not norm_layer.running_mean.requires_grad, \
            "running_mean should not require gradients"
        assert not norm_layer.running_var.requires_grad, \
            "running_var should not require gradients"
    
        # Create random input
        input_tensor = torch.randn(*input_shape, dtype=dtype, device=device)
    
        # Set layer to training mode (default)
        norm_layer.train()
    
        # Save initial running stats
        initial_mean = norm_layer.running_mean.clone()
        initial_var = norm_layer.running_var.clone()
    
        # Forward pass in training mode
        output = norm_layer(input_tensor)
    
        # 6. Output shape matches input shape
        assert output.shape == input_shape, \
            f"Output shape {output.shape} doesn't match input shape {input_shape}"
    
        # 7. Finite values (no NaN or Inf)
        assert torch.isfinite(output).all(), \
            "Output contains NaN or Inf values"
    
        # 8. Running stats should be updated in training mode
        # Note: Instance norm doesn't update running stats in training mode
        # when track_running_stats=True, it uses batch statistics
        # So running stats should remain unchanged
>       assert torch.allclose(norm_layer.running_mean, initial_mean), \
            "running_mean should not change in training mode"
E       AssertionError: running_mean should not change in training mode
E       assert False
E        +  where False = <built-in method allclose of type object at 0x104ad2320>(tensor([0.0150, 0.0031]), tensor([0., 0.]))
E        +    where <built-in method allclose of type object at 0x104ad2320> = torch.allclose
E        +    and   tensor([0.0150, 0.0031]) = InstanceNorm3d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True).running_mean

tests/test_torch_nn_modules_instancenorm_g1.py:441: AssertionError
_____________ test_instance_norm_track_running_stats[test_params1] _____________

test_params = {'affine': False, 'device': 'cpu', 'dtype': torch.float32, 'input_shape': (2, 2, 3, 3, 3), ...}

    @pytest.mark.parametrize("test_params", [
        {
            "norm_class": "InstanceNorm3d",
            "num_features": 2,
            "affine": False,
            "track_running_stats": True,
            "input_shape": (2, 2, 3, 3, 3),
            "dtype": torch.float32,
            "device": "cpu"
        },
        {
            "norm_class": "InstanceNorm3d",
            "num_features": 2,
            "affine": False,
            "track_running_stats": True,
            "input_shape": (2, 2, 3, 3, 3),
            "dtype": torch.float32,
            "device": "cpu",
            "momentum": 0.5
        }
    ])
    def test_instance_norm_track_running_stats(test_params):
        """Test track_running_stats functionality."""
        # Unpack parameters
        norm_class = test_params["norm_class"]
        num_features = test_params["num_features"]
        affine = test_params["affine"]
        track_running_stats = test_params["track_running_stats"]
        input_shape = test_params["input_shape"]
        dtype = test_params["dtype"]
        device = test_params["device"]
        momentum = test_params.get("momentum", 0.1)  # default momentum
    
        # Create instance normalization layer
        if norm_class == "InstanceNorm3d":
            norm_layer = InstanceNorm3d(
                num_features=num_features,
                affine=affine,
                track_running_stats=track_running_stats,
                momentum=momentum,
                dtype=dtype
            )
        else:
            raise ValueError(f"Unsupported norm_class: {norm_class}")
    
        norm_layer.to(device)
    
        # Weak assertions - check running stats exist
        # 1. Running mean and var should exist when track_running_stats=True
        assert hasattr(norm_layer, 'running_mean'), \
            "Should have running_mean when track_running_stats=True"
        assert hasattr(norm_layer, 'running_var'), \
            "Should have running_var when track_running_stats=True"
    
        # 2. Running stats should be initialized
        assert norm_layer.running_mean is not None, \
            "running_mean should be initialized"
        assert norm_layer.running_var is not None, \
            "running_var should be initialized"
    
        # 3. Running stats shapes should match num_features
        assert norm_layer.running_mean.shape == (num_features,), \
            f"running_mean shape {norm_layer.running_mean.shape} should be ({num_features},)"
        assert norm_layer.running_var.shape == (num_features,), \
            f"running_var shape {norm_layer.running_var.shape} should be ({num_features},)"
    
        # 4. Running stats should be initialized to zeros (mean) and ones (var)
        assert torch.allclose(norm_layer.running_mean,
                             torch.zeros(num_features, dtype=dtype)), \
            "running_mean should be initialized to 0"
        assert torch.allclose(norm_layer.running_var,
                             torch.ones(num_features, dtype=dtype)), \
            "running_var should be initialized to 1"
    
        # 5. Running stats should not require gradients
        assert not norm_layer.running_mean.requires_grad, \
            "running_mean should not require gradients"
        assert not norm_layer.running_var.requires_grad, \
            "running_var should not require gradients"
    
        # Create random input
        input_tensor = torch.randn(*input_shape, dtype=dtype, device=device)
    
        # Set layer to training mode (default)
        norm_layer.train()
    
        # Save initial running stats
        initial_mean = norm_layer.running_mean.clone()
        initial_var = norm_layer.running_var.clone()
    
        # Forward pass in training mode
        output = norm_layer(input_tensor)
    
        # 6. Output shape matches input shape
        assert output.shape == input_shape, \
            f"Output shape {output.shape} doesn't match input shape {input_shape}"
    
        # 7. Finite values (no NaN or Inf)
        assert torch.isfinite(output).all(), \
            "Output contains NaN or Inf values"
    
        # 8. Running stats should be updated in training mode
        # Note: Instance norm doesn't update running stats in training mode
        # when track_running_stats=True, it uses batch statistics
        # So running stats should remain unchanged
>       assert torch.allclose(norm_layer.running_mean, initial_mean), \
            "running_mean should not change in training mode"
E       AssertionError: running_mean should not change in training mode
E       assert False
E        +  where False = <built-in method allclose of type object at 0x104ad2320>(tensor([0.0750, 0.0153]), tensor([0., 0.]))
E        +    where <built-in method allclose of type object at 0x104ad2320> = torch.allclose
E        +    and   tensor([0.0750, 0.0153]) = InstanceNorm3d(2, eps=1e-05, momentum=0.5, affine=False, track_running_stats=True).running_mean

tests/test_torch_nn_modules_instancenorm_g1.py:441: AssertionError
____________________ test_instance_norm_invalid_parameters _____________________

    def test_instance_norm_invalid_parameters():
        """Test invalid parameter validation."""
        # Test invalid num_features
>       with pytest.raises(ValueError, match="num_features"):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_torch_nn_modules_instancenorm_g1.py:488: Failed
________________ test_instance_norm_input_dimension_validation _________________

    def test_instance_norm_input_dimension_validation():
        """Test input dimension validation."""
        # InstanceNorm1d should accept 2D or 3D input
        norm1d = InstanceNorm1d(num_features=3)
    
        # Valid 3D input (batch, channels, length)
        valid_input_3d = torch.randn(2, 3, 10)
        output = norm1d(valid_input_3d)
        assert output.shape == (2, 3, 10)
    
        # Valid 2D input (channels, length) - no batch
        valid_input_2d = torch.randn(3, 10)
        output = norm1d(valid_input_2d)
        assert output.shape == (3, 10)
    
        # Invalid 4D input for InstanceNorm1d
        invalid_input_4d = torch.randn(2, 3, 4, 5)
        with pytest.raises(RuntimeError):
>           norm1d(invalid_input_4d)

tests/test_torch_nn_modules_instancenorm_g1.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:69: in forward
    self._check_input_dim(input)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = InstanceNorm1d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
input = tensor([[[[ 1.0131, -0.3308,  0.5177,  0.3878, -0.5797],
          [-0.1691,  1.9312,  1.0119, -0.4752, -0.4920],
    ...],
          [ 0.1729,  1.0514,  0.8539,  0.5130,  0.5397],
          [ 0.5655,  0.5058,  0.2225, -0.9143,  1.4840]]]])

    def _check_input_dim(self, input):
        if input.dim() not in (2, 3):
>           raise ValueError('expected 2D or 3D input (got {}D input)'
                             .format(input.dim()))
E           ValueError: expected 2D or 3D input (got 4D input)

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:149: ValueError
_____________________ test_instance_norm_channel_mismatch ______________________

    def test_instance_norm_channel_mismatch():
        """Test channel dimension mismatch error."""
        norm = InstanceNorm2d(num_features=3)
    
        # Input with wrong channel dimension
        wrong_input = torch.randn(2, 4, 5, 5)  # 4 channels, expected 3
    
>       with pytest.raises(RuntimeError, match="number of channels"):
E       Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_nn_modules_instancenorm_g1.py:548: Failed
=============================== warnings summary ===============================
exam/torch_group/nn.modules.instancenorm/tests/test_torch_nn_modules_instancenorm_g1.py::test_lazy_instance_norm_inference[test_params0]
  /opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
    warnings.warn('Lazy modules are a new feature under heavy development '

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                             Stmts   Miss Branch BrPart  Cover   Missing
--------------------------------------------------------------------------------------------
tests/test_torch_nn_modules_instancenorm_g1.py     183     49     30      7    68%   21-33, 94, 118->125, 125->exit, 181, 232->exit, 269, 297-330, 379, 443-462, 491-506, 529-539
--------------------------------------------------------------------------------------------
TOTAL                                              183     49     30      7    68%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_instancenorm_g1.py::test_lazy_instance_norm_inference[test_params0]
FAILED tests/test_torch_nn_modules_instancenorm_g1.py::test_instance_norm_track_running_stats[test_params0]
FAILED tests/test_torch_nn_modules_instancenorm_g1.py::test_instance_norm_track_running_stats[test_params1]
FAILED tests/test_torch_nn_modules_instancenorm_g1.py::test_instance_norm_invalid_parameters
FAILED tests/test_torch_nn_modules_instancenorm_g1.py::test_instance_norm_input_dimension_validation
FAILED tests/test_torch_nn_modules_instancenorm_g1.py::test_instance_norm_channel_mismatch
6 failed, 5 passed, 1 warning in 0.82s

Error: exit 1