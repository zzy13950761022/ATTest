=== Run Tests ===
F.FFsssssssssssssssss                                                    [100%]
=================================== FAILURES ===================================
___________________ test_caching_allocator_direct_operations ___________________

    def test_caching_allocator_direct_operations():
        """TC-09: 内存分配器直接操作
    
        测试 caching_allocator_alloc 和 caching_allocator_delete 的直接操作，验证：
        1. 分配器工作正常
        2. 返回的指针有效
        3. 基本属性正确性
        """
        device = 0
        size = 1024
    
        # 使用mock来模拟CUDA环境
        with patch('torch.cuda.is_available', return_value=True):
            with patch('torch.cuda.memory_allocated') as mock_allocated:
                with patch('torch.cuda.memory_reserved') as mock_reserved:
                    with patch('torch.cuda.max_memory_allocated') as mock_max_allocated:
                        with patch('torch.cuda.caching_allocator_alloc') as mock_alloc:
                            with patch('torch.cuda.caching_allocator_delete') as mock_delete:
                                with patch('torch.cuda.empty_cache') as mock_empty_cache:
                                    with patch('torch.cuda.reset_peak_memory_stats') as mock_reset_stats:
    
                                        # 设置mock返回值
                                        mock_allocated.return_value = 0
                                        mock_reserved.return_value = 0
                                        mock_max_allocated.return_value = 0
                                        mock_alloc.return_value = 12345  # 模拟指针
    
                                        # 重置内存统计
                                        if torch.cuda.is_available():
                                            torch.cuda.empty_cache()
                                            torch.cuda.reset_peak_memory_stats(device)
    
                                        # 记录初始内存状态
                                        initial_allocated = torch.cuda.memory_allocated(device)
                                        initial_reserved = torch.cuda.memory_reserved(device)
    
                                        # 使用分配器直接分配内存
                                        mem_ptr = torch.cuda.caching_allocator_alloc(size, device=device)
    
                                        # 验证返回的指针有效
                                        assert mem_ptr is not None, "分配应返回有效指针"
                                        assert isinstance(mem_ptr, int), "指针应为整数类型"
                                        assert mem_ptr > 0, "指针应为正数"
    
                                        # 验证分配器被正确调用
                                        mock_alloc.assert_called_once_with(size, device=device)
    
                                        # 验证分配后内存增加（通过mock控制）
                                        mock_allocated.return_value = size  # 模拟分配后内存
                                        allocated_after = torch.cuda.memory_allocated(device)
                                        assert allocated_after > initial_allocated, "分配后已分配内存应增加"
    
                                        # 验证内存增加量正确
                                        actual_increase = allocated_after - initial_allocated
                                        assert actual_increase == size, f"内存增加量应为请求的大小，实际增加{actual_increase}，期望{size}"
    
                                        # 验证保留内存可能增加
                                        mock_reserved.return_value = size * 2  # 模拟保留内存
                                        reserved_after = torch.cuda.memory_reserved(device)
                                        assert reserved_after >= initial_reserved, "分配后保留内存应增加或不变"
    
                                        # 记录分配后的峰值
                                        mock_max_allocated.return_value = size
                                        peak_after_alloc = torch.cuda.max_memory_allocated(device)
                                        assert peak_after_alloc >= allocated_after, "峰值内存应至少等于当前分配内存"
    
                                        # 使用分配器删除内存
                                        torch.cuda.caching_allocator_delete(mem_ptr)
    
                                        # 验证删除函数被正确调用
                                        mock_delete.assert_called_once_with(mem_ptr)
    
                                        # 验证删除后内存减少
                                        mock_allocated.return_value = 0  # 模拟删除后内存
                                        allocated_after_delete = torch.cuda.memory_allocated(device)
                                        assert allocated_after_delete < allocated_after, "删除后已分配内存应减少"
    
                                        # 调用 empty_cache 确保完全释放
                                        torch.cuda.empty_cache()
>                                       mock_empty_cache.assert_called_once()

tests/test_torch_cuda_memory_advanced.py:137: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='empty_cache' id='4539992336'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'empty_cache' to have been called once. Called 2 times.
E           Calls: [call(), call()].

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/unittest/mock.py:908: AssertionError
_________ test_allocator_function_signatures[caching_allocator_alloc] __________

func_name = 'caching_allocator_alloc'

    @pytest.mark.parametrize("func_name", [
        "caching_allocator_alloc",
        "caching_allocator_delete",
    ])
    def test_allocator_function_signatures(func_name):
        """测试分配器函数的基本签名"""
        # 使用mock来模拟CUDA环境
        with patch('torch.cuda.is_available', return_value=True):
            func = getattr(torch.cuda, func_name)
    
            # 验证函数可调用
            assert callable(func)
    
            # 测试基本调用（使用mock）
            with patch.object(torch.cuda, func_name) as mock_func:
                if func_name == "caching_allocator_alloc":
                    # 模拟分配成功
                    mock_func.return_value = 12345
>                   result = func(1024, device=0)

tests/test_torch_cuda_memory_advanced.py:265: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/memory.py:58: in caching_allocator_alloc
    stream = torch.cuda.current_stream(device)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/__init__.py:591: in current_stream
    _lazy_init()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _lazy_init():
        global _initialized, _queued_calls
        if is_initialized() or hasattr(_tls, 'is_initializing'):
            return
        with _initialization_lock:
            # We be double-checked locking, boys!  This is OK because
            # the above test was GIL protected anyway.  The inner test
            # is for when a thread blocked on some other thread which was
            # doing the initialization; when they get the lock, they will
            # find there is nothing left to do.
            if is_initialized():
                return
            # It is important to prevent other threads from entering _lazy_init
            # immediately, while we are still guaranteed to have the GIL, because some
            # of the C calls we make below will release the GIL
            if _is_in_bad_fork():
                raise RuntimeError(
                    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
                    "multiprocessing, you must use the 'spawn' start method")
            if not hasattr(torch._C, '_cuda_getDeviceCount'):
>               raise AssertionError("Torch not compiled with CUDA enabled")
E               AssertionError: Torch not compiled with CUDA enabled

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/__init__.py:221: AssertionError
_________ test_allocator_function_signatures[caching_allocator_delete] _________

func_name = 'caching_allocator_delete'

    @pytest.mark.parametrize("func_name", [
        "caching_allocator_alloc",
        "caching_allocator_delete",
    ])
    def test_allocator_function_signatures(func_name):
        """测试分配器函数的基本签名"""
        # 使用mock来模拟CUDA环境
        with patch('torch.cuda.is_available', return_value=True):
            func = getattr(torch.cuda, func_name)
    
            # 验证函数可调用
            assert callable(func)
    
            # 测试基本调用（使用mock）
            with patch.object(torch.cuda, func_name) as mock_func:
                if func_name == "caching_allocator_alloc":
                    # 模拟分配成功
                    mock_func.return_value = 12345
                    result = func(1024, device=0)
                    assert isinstance(result, int), "应返回整数指针"
                    assert result >= 0, "指针应为非负数"
                    mock_func.assert_called_once_with(1024, device=0)
                elif func_name == "caching_allocator_delete":
                    # 模拟删除成功
>                   result = func(12345)

tests/test_torch_cuda_memory_advanced.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

mem_ptr = 12345

    def caching_allocator_delete(mem_ptr):
        r"""Deletes memory allocated using the CUDA memory allocator.
    
        Memory allocated with :func:`~torch.cuda.caching_allocator_alloc`.
        is freed here. The associated device and stream are tracked inside
        the allocator.
    
        Args:
            mem_ptr (int): memory address to be freed by the allocator.
    
        .. note::
            See :ref:`cuda-memory-management` for more details about GPU memory
            management.
        """
>       torch._C._cuda_cudaCachingAllocator_raw_delete(mem_ptr)
E       AttributeError: module 'torch._C' has no attribute '_cuda_cudaCachingAllocator_raw_delete'

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/cuda/memory.py:83: AttributeError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                       Stmts   Miss Branch BrPart  Cover   Missing
--------------------------------------------------------------------------------------
test_fix.py                                   13     13      0      0     0%   4-25
tests/test_torch_cuda_memory_advanced.py     147     63     36      4    57%   26-30, 39-45, 50-54, 86->91, 140-206, 266-268, 269->exit, 272-274, 283-288, 292
tests/test_torch_cuda_memory_basic.py        124     94     18      1    22%   26-30, 39-45, 50-54, 73-121, 134-204, 223-240, 252-265, 271-279, 283
tests/test_torch_cuda_memory_stats.py        188    157     76      1    12%   26-30, 39-45, 50-54, 72-171, 189-306, 326-342, 353-372, 378-386, 390
--------------------------------------------------------------------------------------
TOTAL                                        472    327    130      6    28%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_cuda_memory_advanced.py::test_caching_allocator_direct_operations
FAILED tests/test_torch_cuda_memory_advanced.py::test_allocator_function_signatures[caching_allocator_alloc]
FAILED tests/test_torch_cuda_memory_advanced.py::test_allocator_function_signatures[caching_allocator_delete]
3 failed, 1 passed, 17 skipped in 0.90s

Error: exit 1