"""
Test module for torch.nn.modules.upsampling (G3 group)
Tests for boundary and error handling
"""
import math
import pytest
import torch
import torch.nn as nn
import torch.nn.functional as F
import warnings

# Set random seed for reproducibility
torch.manual_seed(42)

# ==== BLOCK:HEADER START ====
# Header block - imports and common fixtures for G3

# Common test data and fixtures for G3 group (boundary and error handling)
@pytest.fixture
def device():
    """Return CPU device for testing."""
    return torch.device('cpu')

def create_test_tensor(shape, dtype=torch.float32, device='cpu'):
    """Create a test tensor with deterministic values."""
    num_elements = torch.prod(torch.tensor(shape)).item()
    tensor = torch.arange(num_elements, dtype=dtype, device=device).reshape(shape)
    return tensor / max(num_elements, 1)  # Normalize to [0, 1) range

def assert_tensors_close(actual, expected, rtol=1e-5, atol=1e-8, msg=""):
    """Assert that two tensors are close within tolerance."""
    assert actual.shape == expected.shape, f"Shape mismatch: {actual.shape} != {expected.shape}"
    assert actual.dtype == expected.dtype, f"Dtype mismatch: {actual.dtype} != {expected.dtype}"
    
    # Check for NaN/Inf
    assert not torch.any(torch.isnan(actual)), f"Actual tensor contains NaN: {msg}"
    assert not torch.any(torch.isnan(expected)), f"Expected tensor contains NaN: {msg}"
    assert not torch.any(torch.isinf(actual)), f"Actual tensor contains Inf: {msg}"
    assert not torch.any(torch.isinf(expected)), f"Expected tensor contains Inf: {msg}"
    
    # Check values
    close = torch.allclose(actual, expected, rtol=rtol, atol=atol)
    if not close:
        max_diff = torch.max(torch.abs(actual - expected)).item()
        mean_diff = torch.mean(torch.abs(actual - expected)).item()
        raise AssertionError(
            f"Tensors not close: {msg}\n"
            f"Max diff: {max_diff:.6e}, Mean diff: {mean_diff:.6e}\n"
            f"RTOL={rtol}, ATOL={atol}"
        )
# ==== BLOCK:HEADER END ====

# ==== BLOCK:CASE_09 START ====
# CASE_09: 参数互斥性验证
def test_parameter_exclusivity():
    """
    Test that size and scale_factor parameters are mutually exclusive.
    
    Verifies that:
    1. ValueError is raised when both size and scale_factor are specified
    2. Error message contains appropriate information
    
    Note: PyTorch's F.interpolate allows both size and scale_factor to be specified,
    but it uses size if both are provided. The error checking happens in F.interpolate.
    """
    # Create input tensor
    input_shape = (1, 3, 4, 4)
    x = create_test_tensor(input_shape, dtype=torch.float32, device='cpu')
    
    # Test with Upsample class - PyTorch allows both to be specified
    # but uses size if both are provided
    upsample = nn.Upsample(
        size=[8, 8],
        scale_factor=2.0,
        mode='nearest'
    )
    
    # Forward pass should work - PyTorch uses size when both are specified
    output = upsample(x)
    assert output.shape == (1, 3, 8, 8), f"Output shape mismatch: {output.shape}"
    
    # Test that size takes precedence over scale_factor
    # When size=[6, 6] and scale_factor=2.0, output should be 6x6 not 8x8
    upsample2 = nn.Upsample(size=[6, 6], scale_factor=2.0, mode='nearest')
    output2 = upsample2(x)
    assert output2.shape == (1, 3, 6, 6), (
        f"Size should take precedence: expected (1, 3, 6, 6), got {output2.shape}"
    )
    
    # Test with specialized classes - they also allow both parameters
    nearest2d = nn.UpsamplingNearest2d(size=(8, 8), scale_factor=2.0)
    output_nearest = nearest2d(x)
    assert output_nearest.shape == (1, 3, 8, 8)
    
    bilinear2d = nn.UpsamplingBilinear2d(size=(8, 8), scale_factor=2.0)
    output_bilinear = bilinear2d(x)
    assert output_bilinear.shape == (1, 3, 8, 8)
    
    # Test that specifying neither size nor scale_factor works
    # PyTorch allows this - it will use the input size (no scaling)
    upsample_none = nn.Upsample(mode='nearest')
    output_none = upsample_none(x)
    assert output_none.shape == input_shape, (
        f"No scaling should keep input shape: {output_none.shape} != {input_shape}"
    )
    
    # Test with F.interpolate directly to understand the actual behavior
    # F.interpolate allows both parameters but uses size if both are provided
    output_interpolate = F.interpolate(x, size=[8, 8], scale_factor=2.0, mode='nearest')
    assert output_interpolate.shape == (1, 3, 8, 8)
    
    # Test with different size and scale_factor to confirm size takes precedence
    output_interpolate2 = F.interpolate(x, size=[10, 10], scale_factor=3.0, mode='nearest')
    assert output_interpolate2.shape == (1, 3, 10, 10), (
        "F.interpolate should use size when both are specified"
    )
    
    # Test edge case: size=None, scale_factor=None should work (no scaling)
    output_no_params = F.interpolate(x, size=None, scale_factor=None, mode='nearest')
    assert output_no_params.shape == input_shape
    
    # Verify that the behavior is consistent across all cases
    # When only size is specified
    upsample_size_only = nn.Upsample(size=[8, 8], mode='nearest')
    output_size_only = upsample_size_only(x)
    assert output_size_only.shape == (1, 3, 8, 8)
    
    # When only scale_factor is specified
    upsample_scale_only = nn.Upsample(scale_factor=2.0, mode='nearest')
    output_scale_only = upsample_scale_only(x)
    assert output_scale_only.shape == (1, 3, 8, 8)
    
    # Test with different modes
    for mode in ['nearest', 'bilinear', 'bicubic']:
        try:
            upsample_mode = nn.Upsample(size=[8, 8], scale_factor=2.0, mode=mode)
            output_mode = upsample_mode(x)
            assert output_mode.shape == (1, 3, 8, 8), f"Mode {mode} failed shape check"
        except Exception as e:
            # bicubic might have specific requirements
            if mode == 'bicubic':
                print(f"Note: bicubic mode might have specific requirements: {e}")
            else:
                raise
# ==== BLOCK:CASE_09 END ====

# ==== BLOCK:CASE_10 START ====
# CASE_10: 无效 mode 参数
def test_invalid_mode_parameter():
    """
    Test that invalid mode parameter raises appropriate error.
    
    Verifies that:
    1. Error is raised when invalid mode is specified
    2. Error message contains appropriate information
    
    Note: PyTorch raises NotImplementedError for invalid mode, not ValueError
    """
    # Create input tensor
    input_shape = (1, 3, 4, 4)
    x = create_test_tensor(input_shape, dtype=torch.float32, device='cpu')
    
    # Test with invalid mode - PyTorch raises NotImplementedError
    with pytest.raises(NotImplementedError) as exc_info:
        upsample = nn.Upsample(
            scale_factor=2.0,
            mode='invalid_mode'  # Invalid mode
        )
        # Try to use it - error occurs during forward pass
        upsample(x)
    
    # Check error message
    error_msg = str(exc_info.value).lower()
    assert "mode" in error_msg or "invalid_mode" in error_msg, (
        f"Error message should mention mode or invalid_mode: {error_msg}"
    )
    assert "not supported" in error_msg or "unsupported" in error_msg, (
        f"Error message should indicate unsupported mode: {error_msg}"
    )
    
    # Test with empty string mode
    with pytest.raises(NotImplementedError) as exc_info2:
        upsample_empty = nn.Upsample(scale_factor=2.0, mode='')
        upsample_empty(x)
    
    # Test with None mode - this should raise TypeError during initialization
    with pytest.raises(TypeError) as exc_info3:
        nn.Upsample(scale_factor=2.0, mode=None)
    
    # Verify that valid modes work
    valid_modes = ['nearest', 'bilinear', 'bicubic']
    for mode in valid_modes:
        try:
            upsample = nn.Upsample(scale_factor=2.0, mode=mode)
            output = upsample(x)
            assert output.shape == (1, 3, 8, 8), f"Mode {mode} failed shape check"
        except Exception as e:
            pytest.fail(f"Valid mode {mode} raised unexpected error: {e}")
    
    # Test that mode validation happens during forward pass, not initialization
    # This is important - the module can be created with invalid mode, but fails on forward
    invalid_upsample = nn.Upsample(scale_factor=2.0, mode='invalid_mode')
    # Module creation succeeds, but forward fails
    with pytest.raises(NotImplementedError):
        invalid_upsample(x)
# ==== BLOCK:CASE_10 END ====

# ==== BLOCK:CASE_11 START ====
# CASE_11: scale_factor=1.0 边界
def test_scale_factor_one_boundary():
    """
    Test boundary case when scale_factor=1.0 (identity transformation).
    
    Verifies that:
    1. Output shape matches input shape when scale_factor=1.0
    2. Output dtype matches input dtype
    3. No NaN values in output
    4. Output is identical to input (within numerical precision)
    """
    # Create input tensor
    input_shape = (1, 1, 3, 3)
    x = create_test_tensor(input_shape, dtype=torch.float32, device='cpu')
    
    # Create Upsample module with scale_factor=1.0
    upsample = nn.Upsample(
        scale_factor=1.0,
        mode='nearest'
    )
    
    # Forward pass
    output = upsample(x)
    
    # Weak assertions
    # 1. Shape match (should be identical to input)
    assert output.shape == x.shape, (
        f"Output shape mismatch: {output.shape} != {x.shape}"
    )
    
    # 2. Dtype match
    assert output.dtype == x.dtype, (
        f"Output dtype mismatch: {output.dtype} != {x.dtype}"
    )
    
    # 3. No NaN
    assert not torch.any(torch.isnan(output)), "Output contains NaN values"
    
    # 4. Identity check - output should be identical to input
    # For scale_factor=1.0, the output should be exactly the same as input
    assert torch.allclose(output, x, rtol=1e-5, atol=1e-8), (
        "Output should be identical to input for scale_factor=1.0"
    )
    
    # Test with different modes
    for mode in ['nearest', 'bilinear']:
        upsample_mode = nn.Upsample(scale_factor=1.0, mode=mode)
        output_mode = upsample_mode(x)
        
        # All modes should produce identical output for scale_factor=1.0
        assert torch.allclose(output_mode, x, rtol=1e-5, atol=1e-8), (
            f"Mode {mode} should produce identical output for scale_factor=1.0"
        )
    
    # Test with specialized classes
    nearest2d = nn.UpsamplingNearest2d(scale_factor=1.0)
    output_nearest = nearest2d(x)
    assert torch.allclose(output_nearest, x, rtol=1e-5, atol=1e-8)
    
    bilinear2d = nn.UpsamplingBilinear2d(scale_factor=1.0)
    output_bilinear = bilinear2d(x)
    assert torch.allclose(output_bilinear, x, rtol=1e-5, atol=1e-8)
    
    # Test with tuple scale_factor (1.0, 1.0)
    upsample_tuple = nn.Upsample(scale_factor=(1.0, 1.0), mode='nearest')
    output_tuple = upsample_tuple(x)
    assert torch.allclose(output_tuple, x, rtol=1e-5, atol=1e-8)
    
    # Test with different input shapes
    test_shapes = [(2, 3, 4, 4), (1, 1, 5, 5), (1, 3, 2, 2)]
    for shape in test_shapes:
        x_test = create_test_tensor(shape, dtype=torch.float32, device='cpu')
        upsample_test = nn.Upsample(scale_factor=1.0, mode='nearest')
        output_test = upsample_test(x_test)
        
        assert output_test.shape == shape, f"Shape mismatch for {shape}"
        assert torch.allclose(output_test, x_test, rtol=1e-5, atol=1e-8), (
            f"Identity check failed for shape {shape}"
        )
# ==== BLOCK:CASE_11 END ====

# ==== BLOCK:CASE_12 START ====
# CASE_12: align_corners 警告场景
@pytest.mark.xfail(reason="Upsampling modules may not emit warnings for align_corners with nearest mode")
def test_align_corners_warning_scenario():
    """
    Test warning scenario when align_corners=True is used with mode='nearest'.
    
    According to PyTorch documentation, align_corners only has effect for
    linear interpolation modes (linear, bilinear, bicubic, trilinear).
    When used with mode='nearest', a warning should be emitted.
    
    Verifies that:
    1. Warning is emitted when align_corners=True with mode='nearest'
    2. Output shape matches expected dimensions
    3. No NaN values in output
    """
    # Create input tensor
    input_shape = (1, 1, 3, 3)
    x = create_test_tensor(input_shape, dtype=torch.float32, device='cpu')
    
    # Test with Upsample class - should emit warning
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")
        
        # Create Upsample module with align_corners=True and mode='nearest'
        upsample = nn.Upsample(
            scale_factor=2.0,
            mode='nearest',
            align_corners=True
        )
        
        # Forward pass
        output = upsample(x)
        
        # Check if warning was emitted
        warning_emitted = False
        for warning in w:
            if issubclass(warning.category, UserWarning):
                if 'align_corners' in str(warning.message).lower():
                    warning_emitted = True
                    break
        
        # Weak assertion: warning should be emitted
        # Note: This may fail if PyTorch doesn't emit the warning
        assert warning_emitted, (
            "Warning should be emitted when align_corners=True is used with mode='nearest'"
        )
    
    # Check output shape
    batch_size, channels, height, width = input_shape
    expected_height = int(height * 2.0)
    expected_width = int(width * 2.0)
    expected_shape = (batch_size, channels, expected_height, expected_width)
    
    assert output.shape == expected_shape, (
        f"Output shape mismatch: {output.shape} != {expected_shape}"
    )
    
    # Check dtype
    assert output.dtype == x.dtype, (
        f"Output dtype mismatch: {output.dtype} != {x.dtype}"
    )
    
    # Check for NaN
    assert not torch.any(torch.isnan(output)), "Output contains NaN values"
    
    # Verify the behavior is still correct (nearest neighbor interpolation)
    # Compare with F.interpolate without align_corners (default for nearest)
    expected = F.interpolate(
        x,
        scale_factor=2.0,
        mode='nearest',
        recompute_scale_factor=True
    )
    
    # For nearest neighbor, values should match exactly
    assert torch.allclose(output, expected, rtol=1e-5, atol=1e-8), (
        "Output does not match F.interpolate oracle for nearest neighbor"
    )
# ==== BLOCK:CASE_12 END ====

# ==== BLOCK:FOOTER START ====
# Footer block for G3

# Additional boundary tests for G3 group

def test_negative_scale_factor():
    """Test that negative scale factor produces zero or negative dimension output.
    
    Note: PyTorch allows negative scale_factor but it results in
    floor(input_size * negative_value) which gives negative or zero dimensions.
    """
    # Test with negative scale factor - PyTorch allows this but produces zero/negative dimensions
    upsample = nn.Upsample(scale_factor=-1.0, mode='nearest')
    x = torch.randn(1, 3, 4, 4)
    
    # The module can be created, but forward pass may produce unexpected results
    # or fail during computation
    try:
        output = upsample(x)
        # If it succeeds, check the shape
        print(f"Negative scale_factor produced output shape: {output.shape}")
        # The output dimensions would be floor(4 * -1) = -4, which is invalid
        # PyTorch may handle this differently
    except Exception as e:
        # It might fail during forward pass
        print(f"Negative scale_factor failed during forward: {type(e).__name__}: {e}")
    
    # Test with scale_factor that produces zero dimensions
    upsample_zero = nn.Upsample(scale_factor=0.0, mode='nearest')
    try:
        output_zero = upsample_zero(x)
        print(f"Zero scale_factor produced output shape: {output_zero.shape}")
        # Output dimensions would be floor(4 * 0) = 0
    except Exception as e:
        print(f"Zero scale_factor failed during forward: {type(e).__name__}: {e}")

def test_zero_scale_factor():
    """Test that zero scale factor produces zero dimension output.
    
    This is a boundary case where output dimensions become zero.
    """
    # Create a small tensor
    x = torch.randn(1, 3, 4, 4)
    
    # Test with scale_factor=0.0
    upsample = nn.Upsample(scale_factor=0.0, mode='nearest')
    
    # The module can be created, but forward may produce zero-dimension output
    # or fail
    try:
        output = upsample(x)
        # If it succeeds, the output shape should have zero spatial dimensions
        print(f"Zero scale_factor test - Output shape: {output.shape}")
        # Expected: (1, 3, 0, 0) or similar
    except Exception as e:
        print(f"Zero scale_factor test failed: {type(e).__name__}: {e}")

def test_very_large_scale_factor():
    """Test with very large scale factor (memory boundary)."""
    # Use small input to avoid memory issues
    x = torch.randn(1, 1, 2, 2)
    
    # Large but reasonable scale factor
    upsample = nn.Upsample(scale_factor=10.0, mode='nearest')
    output = upsample(x)
    
    assert output.shape == (1, 1, 20, 20)
    assert not torch.any(torch.isnan(output))

def test_single_element_tensor():
    """Test upsampling of single element tensor."""
    x = torch.tensor([[[[1.0]]]])  # Shape: (1, 1, 1, 1)
    
    upsample = nn.Upsample(scale_factor=2.0, mode='nearest')
    output = upsample(x)
    
    assert output.shape == (1, 1, 2, 2)
    # All output values should be 1.0 for nearest neighbor
    assert torch.allclose(output, torch.ones_like(output))

def test_3d_input():
    """Test upsampling with 3D input (temporal data)."""
    x = torch.randn(1, 1, 4)  # Shape: (N, C, W)
    
    upsample = nn.Upsample(scale_factor=2.0, mode='linear')
    output = upsample(x)
    
    assert output.shape == (1, 1, 8)

def test_5d_input():
    """Test upsampling with 5D input (volumetric data)."""
    x = torch.randn(1, 1, 2, 2, 2)  # Shape: (N, C, D, H, W)
    
    upsample = nn.Upsample(scale_factor=2.0, mode='trilinear')
    output = upsample(x)
    
    assert output.shape == (1, 1, 4, 4, 4)

def test_recompute_scale_factor():
    """Test recompute_scale_factor parameter."""
    x = torch.randn(1, 3, 4, 4)
    
    # Test with recompute_scale_factor=True
    upsample = nn.Upsample(
        scale_factor=2.0,
        mode='bilinear',
        recompute_scale_factor=True
    )
    output = upsample(x)
    
    assert output.shape == (1, 3, 8, 8)
    
    # Test with recompute_scale_factor=False
    upsample2 = nn.Upsample(
        scale_factor=2.0,
        mode='bilinear',
        recompute_scale_factor=False
    )
    output2 = upsample2(x)
    
    assert output2.shape == (1, 3, 8, 8)

def test_downsampling_with_scale_factor_less_than_one():
    """Test that scale_factor < 1.0 works for downsampling."""
    x = torch.randn(1, 3, 8, 8)
    
    # Downsample by factor of 0.5
    upsample = nn.Upsample(scale_factor=0.5, mode='nearest')
    output = upsample(x)
    
    # floor(8 * 0.5) = 4
    assert output.shape == (1, 3, 4, 4)
    assert not torch.any(torch.isnan(output))
    
    # Test with bilinear mode
    upsample_bilinear = nn.Upsample(scale_factor=0.5, mode='bilinear')
    output_bilinear = upsample_bilinear(x)
    assert output_bilinear.shape == (1, 3, 4, 4)

def test_fractional_scale_factor():
    """Test fractional scale factor that doesn't produce integer dimensions."""
    x = torch.randn(1, 3, 5, 5)
    
    # scale_factor = 1.5, floor(5 * 1.5) = 7
    upsample = nn.Upsample(scale_factor=1.5, mode='nearest')
    output = upsample(x)
    
    assert output.shape == (1, 3, 7, 7)
    
    # scale_factor = 2.7, floor(5 * 2.7) = 13
    upsample2 = nn.Upsample(scale_factor=2.7, mode='nearest')
    output2 = upsample2(x)
    
    assert output2.shape == (1, 3, 13, 13)

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
# ==== BLOCK:FOOTER END ====