=== Run Tests ===
..s....F...                                                              [100%]
=================================== FAILURES ===================================
_ test_checkpoint_exception_handling[invalid_callable-True-True-RuntimeError] __

function_type = 'invalid_callable', use_reentrant = True, has_kwargs = True
expected_exception = 'RuntimeError'

    @pytest.mark.parametrize("function_type,use_reentrant,has_kwargs,expected_exception", [
        # Base case from test plan
        ("invalid_callable", True, True, "RuntimeError"),
    ])
    def test_checkpoint_exception_handling(
        function_type, use_reentrant, has_kwargs, expected_exception
    ):
        """
        Test exception handling in checkpointed functions.
        """
        # Set random seed for reproducibility
        set_random_seed(42)
    
        # Create a simple input tensor
        x = torch.randn(2, 3, dtype=torch.float32)
    
        # Define function based on type
        if function_type == "invalid_callable":
            # Create a callable that raises RuntimeError
            def invalid_func(tensor, **kwargs):
                raise RuntimeError("Invalid function call - test exception")
        else:
            raise ValueError(f"Unsupported function_type: {function_type}")
    
        # Mock targets as specified in test plan
        # We'll mock the internal checkpoint functions to test exception handling
        from unittest.mock import patch, MagicMock
    
        # Test different exception scenarios based on use_reentrant
        if use_reentrant:
            # Test with use_reentrant=True
            if has_kwargs:
                # use_reentrant=True doesn't support kwargs, should raise RuntimeError
                with pytest.raises(RuntimeError) as exc_info:
>                   checkpoint(invalid_func, x, use_reentrant=use_reentrant, extra_kwarg=1)

tests/test_torch_utils_checkpoint_basic.py:380: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

function = <function test_checkpoint_exception_handling.<locals>.invalid_func at 0x12d51c280>
use_reentrant = True
args = (tensor([[ 0.3367,  0.1288,  0.2345],
        [ 0.2303, -1.1229, -0.1863]]),)
kwargs = {'extra_kwarg': 1}, preserve = True

    def checkpoint(function, *args, use_reentrant: bool = True, **kwargs):
        r"""Checkpoint a model or part of the model
    
        Checkpointing works by trading compute for memory. Rather than storing all
        intermediate activations of the entire computation graph for computing
        backward, the checkpointed part does **not** save intermediate activations,
        and instead recomputes them in backward pass. It can be applied on any part
        of a model.
    
        Specifically, in the forward pass, :attr:`function` will run in
        :func:`torch.no_grad` manner, i.e., not storing the intermediate
        activations. Instead, the forward pass saves the inputs tuple and the
        :attr:`function` parameter. In the backwards pass, the saved inputs and
        :attr:`function` is retrieved, and the forward pass is computed on
        :attr:`function` again, now tracking the intermediate activations, and then
        the gradients are calculated using these activation values.
    
        The output of :attr:`function` can contain non-Tensor values and gradient
        recording is only performed for the Tensor values. Note that if the output
        consists of nested structures (ex: custom objects, lists, dicts etc.)
        consisting of Tensors, these Tensors nested in custom structures will not
        be considered as part of autograd.
    
    
        .. warning::
            If :attr:`function` invocation during backward does anything different
            than the one during forward, e.g., due to some global variable, the
            checkpointed version won't be equivalent, and unfortunately it can't be
            detected.
    
        .. warning::
            If ``use_reentrant=True`` is specified, then if the checkpointed segment
            contains tensors detached from the computational graph by `detach()` or
            `torch.no_grad()`, the backward pass will raise an error. This is
            because `checkpoint` makes all the outputs require gradients which
            causes issues when a tensor is defined to have no gradient in the model.
            To circumvent this, detach the tensors outside of the `checkpoint`
            function. Note that the checkpointed segment can contain tensors
            detached from the computational graph if ``use_reentrant=False`` is
            specified.
    
        .. warning::
            If ``use_reentrant=True`` is specified, at least one of the inputs needs
            to have :code:`requires_grad=True` if grads are needed for model inputs,
            otherwise the checkpointed part of the model won't have gradients. At
            least one of the outputs needs to have :code:`requires_grad=True` as
            well. Note that this does not apply if ``use_reentrant=False`` is
            specified.
    
        .. warning::
            If ``use_reentrant=True`` is specified, checkpointing currently only
            supports :func:`torch.autograd.backward` and only if its `inputs`
            argument is not passed. :func:`torch.autograd.grad`
            is not supported. If ``use_reentrant=False`` is specified, checkpointing
            will work with :func:`torch.autograd.grad`.
    
        Args:
            function: describes what to run in the forward pass of the model or
                part of the model. It should also know how to handle the inputs
                passed as the tuple. For example, in LSTM, if user passes
                ``(activation, hidden)``, :attr:`function` should correctly use the
                first input as ``activation`` and the second input as ``hidden``
            preserve_rng_state(bool, optional):  Omit stashing and restoring
                the RNG state during each checkpoint.
                Default: ``True``
            use_reentrant(bool, optional): Use checkpointing
                implementation that requires re-entrant autograd.
                If ``use_reentrant=False`` is specified, ``checkpoint`` will use an
                implementation that does not require re-entrant autograd. This
                allows ``checkpoint`` to support additional functionality, such as
                working as expected with ``torch.autograd.grad`` and support for
                keyword arguments input into the checkpointed function. Note that future
                versions of PyTorch will default to ``use_reentrant=False``.
                Default: ``True``
            args: tuple containing inputs to the :attr:`function`
    
        Returns:
            Output of running :attr:`function` on :attr:`*args`
        """
        # Hack to mix *args with **kwargs in a python 2.7-compliant way
        preserve = kwargs.pop('preserve_rng_state', True)
        if kwargs and use_reentrant:
>           raise ValueError("Unexpected keyword arguments: " + ",".join(arg for arg in kwargs))
E           ValueError: Unexpected keyword arguments: extra_kwarg

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/utils/checkpoint.py:246: ValueError
=============================== warnings summary ===============================
exam/torch_group/utils.checkpoint/tests/test_torch_utils_checkpoint_basic.py::test_checkpoint_basic_functionality[simple_linear-input_shape0-float32-cpu-True-True]
exam/torch_group/utils.checkpoint/tests/test_torch_utils_checkpoint_basic.py::test_checkpoint_basic_functionality[simple_linear-input_shape1-float64-cpu-True-True]
exam/torch_group/utils.checkpoint/tests/test_torch_utils_checkpoint_gradients.py::test_checkpoint_nested_output_structure[nested_output-input_shape0-float32-cpu-True-True]
  /opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
    warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                             Stmts   Miss Branch BrPart  Cover   Missing
--------------------------------------------------------------------------------------------
tests/test_torch_utils_checkpoint_basic.py         186     78     78     28    53%   14, 18->20, 20->22, 26-27, 31, 41, 45-47, 51-63, 68-69, 74-77, 108, 112, 119, 132, 172, 181, 185, 192, 194, 197-200, 208, 213-218, 229, 245->exit, 274, 283, 287, 289->293, 297, 313, 366-368, 384-454, 473-474
tests/test_torch_utils_checkpoint_gradients.py     195     56     80     16    67%   20, 24-28, 32-34, 48-50, 54-66, 71-72, 77-80, 134, 143, 147, 155, 175-186, 200-214, 225->230, 230->247, 242-243, 277, 286, 290, 297, 316->339, 409-410
--------------------------------------------------------------------------------------------
TOTAL                                              381    134    158     44    60%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_utils_checkpoint_basic.py::test_checkpoint_exception_handling[invalid_callable-True-True-RuntimeError]
1 failed, 9 passed, 1 skipped, 3 warnings in 0.76s

Error: exit 1