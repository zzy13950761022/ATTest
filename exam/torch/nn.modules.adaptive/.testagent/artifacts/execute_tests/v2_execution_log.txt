=== Run Tests ===
F....FF                                                                  [100%]
=================================== FAILURES ===================================
_ test_adaptive_softmax_cutoffs_validation[5-30-cutoffs0-4.0-False-valid-dtype0-cpu-False] _

in_features = 5, n_classes = 30, cutoffs = [5, 15], div_value = 4.0
head_bias = False, test_type = 'valid', dtype = torch.float32, device = 'cpu'
should_raise = False

    @pytest.mark.parametrize(
        "in_features,n_classes,cutoffs,div_value,head_bias,test_type,dtype,device,should_raise",
        [
            # Valid cutoffs
            (5, 30, [5, 15], 4.0, False, "valid", torch.float32, "cpu", False),
            # Invalid: non-increasing cutoffs (parameter extension)
            (5, 30, [30, 15], 4.0, False, "invalid_non_increasing", torch.float32, "cpu", True),
            # Invalid: duplicate cutoffs (parameter extension)
            (5, 30, [5, 5, 15], 4.0, False, "invalid_duplicate", torch.float32, "cpu", True),
        ]
    )
    def test_adaptive_softmax_cutoffs_validation(
        in_features: int,
        n_classes: int,
        cutoffs: List[int],
        div_value: float,
        head_bias: bool,
        test_type: str,
        dtype: torch.dtype,
        device: str,
        should_raise: bool
    ):
        """
        Test cutoffs parameter validation.
    
        Weak assertions:
        1. no_exception: Valid cutoffs should not raise exception
        2. module_initialized: Module should be properly initialized for valid cutoffs
        """
        if should_raise:
            # Test that invalid cutoffs raise ValueError
            with pytest.raises(ValueError) as exc_info:
                model = AdaptiveLogSoftmaxWithLoss(
                    in_features=in_features,
                    n_classes=n_classes,
                    cutoffs=cutoffs,
                    div_value=div_value,
                    head_bias=head_bias
                )
    
            # Verify the error message contains relevant information
            error_msg = str(exc_info.value).lower()
            if test_type == "invalid_non_increasing":
                assert "increasing" in error_msg or "sorted" in error_msg, \
                    f"Expected error about non-increasing cutoffs, got: {error_msg}"
            elif test_type == "invalid_duplicate":
                assert "unique" in error_msg or "duplicate" in error_msg, \
                    f"Expected error about duplicate cutoffs, got: {error_msg}"
        else:
            # Test that valid cutoffs work correctly
            try:
                model = AdaptiveLogSoftmaxWithLoss(
                    in_features=in_features,
                    n_classes=n_classes,
                    cutoffs=cutoffs,
                    div_value=div_value,
                    head_bias=head_bias
                )
    
                # Assertion 1: no_exception - module created successfully
                assert model is not None, "Module should be created successfully"
    
                # Assertion 2: module_initialized
                assert hasattr(model, 'head'), "Module should have head attribute"
                assert hasattr(model, 'tail'), "Module should have tail attribute"
                assert isinstance(model.tail, nn.ModuleList), "tail should be a ModuleList"
    
                # Verify cutoffs are stored correctly
>               assert model.cutoffs == cutoffs, f"Cutoffs not stored correctly: expected {cutoffs}, got {model.cutoffs}"
E               AssertionError: Cutoffs not stored correctly: expected [5, 15], got [5, 15, 30]
E               assert [5, 15, 30] == [5, 15]
E                 
E                 Left contains one more item: 30
E                 Use -v to get more diff

tests/test_torch_nn_modules_adaptive_g2.py:157: AssertionError

During handling of the above exception, another exception occurred:

in_features = 5, n_classes = 30, cutoffs = [5, 15], div_value = 4.0
head_bias = False, test_type = 'valid', dtype = torch.float32, device = 'cpu'
should_raise = False

    @pytest.mark.parametrize(
        "in_features,n_classes,cutoffs,div_value,head_bias,test_type,dtype,device,should_raise",
        [
            # Valid cutoffs
            (5, 30, [5, 15], 4.0, False, "valid", torch.float32, "cpu", False),
            # Invalid: non-increasing cutoffs (parameter extension)
            (5, 30, [30, 15], 4.0, False, "invalid_non_increasing", torch.float32, "cpu", True),
            # Invalid: duplicate cutoffs (parameter extension)
            (5, 30, [5, 5, 15], 4.0, False, "invalid_duplicate", torch.float32, "cpu", True),
        ]
    )
    def test_adaptive_softmax_cutoffs_validation(
        in_features: int,
        n_classes: int,
        cutoffs: List[int],
        div_value: float,
        head_bias: bool,
        test_type: str,
        dtype: torch.dtype,
        device: str,
        should_raise: bool
    ):
        """
        Test cutoffs parameter validation.
    
        Weak assertions:
        1. no_exception: Valid cutoffs should not raise exception
        2. module_initialized: Module should be properly initialized for valid cutoffs
        """
        if should_raise:
            # Test that invalid cutoffs raise ValueError
            with pytest.raises(ValueError) as exc_info:
                model = AdaptiveLogSoftmaxWithLoss(
                    in_features=in_features,
                    n_classes=n_classes,
                    cutoffs=cutoffs,
                    div_value=div_value,
                    head_bias=head_bias
                )
    
            # Verify the error message contains relevant information
            error_msg = str(exc_info.value).lower()
            if test_type == "invalid_non_increasing":
                assert "increasing" in error_msg or "sorted" in error_msg, \
                    f"Expected error about non-increasing cutoffs, got: {error_msg}"
            elif test_type == "invalid_duplicate":
                assert "unique" in error_msg or "duplicate" in error_msg, \
                    f"Expected error about duplicate cutoffs, got: {error_msg}"
        else:
            # Test that valid cutoffs work correctly
            try:
                model = AdaptiveLogSoftmaxWithLoss(
                    in_features=in_features,
                    n_classes=n_classes,
                    cutoffs=cutoffs,
                    div_value=div_value,
                    head_bias=head_bias
                )
    
                # Assertion 1: no_exception - module created successfully
                assert model is not None, "Module should be created successfully"
    
                # Assertion 2: module_initialized
                assert hasattr(model, 'head'), "Module should have head attribute"
                assert hasattr(model, 'tail'), "Module should have tail attribute"
                assert isinstance(model.tail, nn.ModuleList), "tail should be a ModuleList"
    
                # Verify cutoffs are stored correctly
                assert model.cutoffs == cutoffs, f"Cutoffs not stored correctly: expected {cutoffs}, got {model.cutoffs}"
    
                # Verify n_classes is stored correctly
                assert model.n_classes == n_classes, f"n_classes not stored correctly: expected {n_classes}, got {model.n_classes}"
    
                # Verify in_features is stored correctly
                assert model.in_features == in_features, f"in_features not stored correctly: expected {in_features}, got {model.in_features}"
    
                # Test forward pass with small batch
                input_tensor = torch.randn(2, in_features, dtype=dtype, device=device)
                target_tensor = torch.randint(0, n_classes, (2,), dtype=torch.long, device=device)
    
                model = model.to(device)
                result = model(input_tensor, target_tensor)
    
                # Verify output shape
                assert result.output.shape == (2,), f"Expected output shape (2,), got {result.output.shape}"
    
                # Verify loss is scalar
                assert result.loss.shape == (), f"Expected scalar loss, got {result.loss.shape}"
    
                # Verify no NaN values
                assert not torch.any(torch.isnan(result.output)), "Output contains NaN values"
                assert not torch.isnan(result.loss), "Loss is NaN"
    
            except Exception as e:
>               pytest.fail(f"Valid cutoffs should not raise exception: {e}")
E               Failed: Valid cutoffs should not raise exception: Cutoffs not stored correctly: expected [5, 15], got [5, 15, 30]
E               assert [5, 15, 30] == [5, 15]
E                 
E                 Left contains one more item: 30
E                 Use -v to get more diff

tests/test_torch_nn_modules_adaptive_g2.py:183: Failed
_________________ test_adaptive_softmax_invalid_parameters_g2 __________________

    def test_adaptive_softmax_invalid_parameters_g2():
        """Test invalid parameter combinations for G2 group."""
        # Test n_classes < 2
        with pytest.raises(ValueError) as exc_info:
            AdaptiveLogSoftmaxWithLoss(
                in_features=10,
                n_classes=1,  # Invalid: n_classes must be >= 2
                cutoffs=[5],
                div_value=4.0,
                head_bias=False
            )
        assert "n_classes" in str(exc_info.value).lower(), \
            f"Expected error about n_classes, got: {exc_info.value}"
    
        # Test in_features <= 0
>       with pytest.raises(ValueError) as exc_info:
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_torch_nn_modules_adaptive_g2.py:454: Failed
__________________ test_adaptive_softmax_log_prob_edge_cases ___________________

    def test_adaptive_softmax_log_prob_edge_cases():
        """Test edge cases for log_prob method."""
        # Test with single cutoff
        model = AdaptiveLogSoftmaxWithLoss(
            in_features=5,
            n_classes=10,
            cutoffs=[5],
            div_value=2.0,
            head_bias=False
        )
    
        # Test non-batched input
        input_tensor = torch.randn(5)
>       log_probs = model.log_prob(input_tensor)

tests/test_torch_nn_modules_adaptive_g2.py:514: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/adaptive.py:280: in log_prob
    return self._get_full_log_prob(input, head_output)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/adaptive.py:249: in _get_full_log_prob
    head_logprob = log_softmax(head_output, dim=1)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([ 1.1481,  0.2769,  1.2215,  0.0142, -0.5685, -0.8605],
       grad_fn=<SqueezeBackward3>)
dim = 1, _stacklevel = 3, dtype = None

    def log_softmax(input: Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[DType] = None) -> Tensor:
        r"""Applies a softmax followed by a logarithm.
    
        While mathematically equivalent to log(softmax(x)), doing these two
        operations separately is slower and numerically unstable. This function
        uses an alternative formulation to compute the output and gradient correctly.
    
        See :class:`~torch.nn.LogSoftmax` for more details.
    
        Args:
            input (Tensor): input
            dim (int): A dimension along which log_softmax will be computed.
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
              If specified, the input tensor is cast to :attr:`dtype` before the operation
              is performed. This is useful for preventing data type overflows. Default: None.
        """
        if has_torch_function_unary(input):
            return handle_torch_function(log_softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype)
        if dim is None:
            dim = _get_softmax_dim("log_softmax", input.dim(), _stacklevel)
        if dtype is None:
>           ret = input.log_softmax(dim)
E           IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/functional.py:1930: IndexError
=============================== warnings summary ===============================
exam/torch_group/nn.modules.adaptive/tests/test_torch_nn_modules_adaptive_g2.py::test_adaptive_softmax_cutoffs_validation[5-30-cutoffs0-4.0-False-valid-dtype0-cpu-False]
exam/torch_group/nn.modules.adaptive/tests/test_torch_nn_modules_adaptive_g2.py::test_adaptive_softmax_log_prob[12-80-cutoffs0-4.0-False-2-dtype0-cpu]
exam/torch_group/nn.modules.adaptive/tests/test_torch_nn_modules_adaptive_g2.py::test_adaptive_softmax_predict_method
exam/torch_group/nn.modules.adaptive/tests/test_torch_nn_modules_adaptive_g2.py::test_adaptive_softmax_invalid_parameters_g2
  /opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op
    warnings.warn("Initializing zero-element tensors is a no-op")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                         Stmts   Miss Branch BrPart  Cover   Missing
----------------------------------------------------------------------------------------
tests/test_torch_nn_modules_adaptive_g2.py     147     58     28      6    59%   45-46, 134->exit, 160-180, 245, 265->284, 286-287, 329-396, 418->exit, 462-498, 516-529
----------------------------------------------------------------------------------------
TOTAL                                          147     58     28      6    59%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_adaptive_g2.py::test_adaptive_softmax_cutoffs_validation[5-30-cutoffs0-4.0-False-valid-dtype0-cpu-False]
FAILED tests/test_torch_nn_modules_adaptive_g2.py::test_adaptive_softmax_invalid_parameters_g2
FAILED tests/test_torch_nn_modules_adaptive_g2.py::test_adaptive_softmax_log_prob_edge_cases
3 failed, 4 passed, 4 warnings in 0.64s

Error: exit 1