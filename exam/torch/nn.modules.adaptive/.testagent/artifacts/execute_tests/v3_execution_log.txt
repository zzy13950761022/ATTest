=== Run Tests ===
.F...F.....F.                                                            [100%]
=================================== FAILURES ===================================
__ test_adaptive_softmax_forward_basic[20-200-cutoffs1-2.0-True-4-dtype1-cpu] __

in_features = 20, n_classes = 200, cutoffs = [20, 100], div_value = 2.0
head_bias = True, batch_size = 4, dtype = torch.float64, device = 'cpu'

    @pytest.mark.parametrize(
        "in_features,n_classes,cutoffs,div_value,head_bias,batch_size,dtype,device",
        [
            # Base case from test plan
            (10, 100, [10, 50], 4.0, False, 2, torch.float32, "cpu"),
            # Parameter extension
            (20, 200, [20, 100], 2.0, True, 4, torch.float64, "cpu"),
        ]
    )
    def test_adaptive_softmax_forward_basic(
        in_features: int,
        n_classes: int,
        cutoffs: List[int],
        div_value: float,
        head_bias: bool,
        batch_size: int,
        dtype: torch.dtype,
        device: str
    ):
        """
        Test basic forward propagation functionality.
    
        Weak assertions:
        1. output_shape: Output tensor has correct shape
        2. loss_scalar: Loss is a scalar value
        3. finite_values: All output values are finite
        4. dtype_match: Output has correct data type
        """
        # Create test data
        input_tensor, target_tensor = create_test_data(
            in_features=in_features,
            n_classes=n_classes,
            batch_size=batch_size,
            dtype=dtype,
            device=device
        )
    
        # Create model
        model = create_adaptive_softmax(
            in_features=in_features,
            n_classes=n_classes,
            cutoffs=cutoffs,
            div_value=div_value,
            head_bias=head_bias,
            device=device
        )
    
        # Forward pass
>       result = model(input_tensor, target_tensor)

tests/test_torch_nn_modules_adaptive_g1.py:137: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/adaptive.py:217: in forward
    cluster_output = self.tail[i - 1](input_subset)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/container.py:204: in forward
    input = module(input)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=20, out_features=10, bias=False)
input = tensor([[-0.0372,  1.0352, -0.0101,  0.0895,  0.0840,  0.3900, -0.5798,  0.8759,
          0.3924,  1.1085,  0.8652,  ....1082, -0.1874,  0.4428, -0.2321, -0.0327, -2.4472,
          1.4654,  0.9461, -1.1149,  1.1146]], dtype=torch.float64)

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: expected scalar type Double but found Float

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/linear.py:114: RuntimeError
_____________________ test_adaptive_softmax_shape_mismatch _____________________

    def test_adaptive_softmax_shape_mismatch():
        """Test shape mismatch error handling."""
        model = AdaptiveLogSoftmaxWithLoss(
            in_features=10,
            n_classes=100,
            cutoffs=[10, 50],
            div_value=4.0,
            head_bias=False
        )
    
        # Test input shape mismatch (wrong in_features)
        input_tensor = torch.randn(3, 8)  # Wrong: 8 instead of 10
        target_tensor = torch.randint(0, 100, (3,), dtype=torch.long)
    
        with pytest.raises(RuntimeError) as exc_info:
            model(input_tensor, target_tensor)
    
        # Test target shape mismatch (wrong batch size)
        input_tensor2 = torch.randn(3, 10)  # Correct shape
        target_tensor2 = torch.randint(0, 100, (4,), dtype=torch.long)  # Wrong: 4 instead of 3
    
        with pytest.raises(RuntimeError) as exc_info2:
            model(input_tensor2, target_tensor2)
    
        # Test target out of range
        input_tensor3 = torch.randn(2, 10)
        target_tensor3 = torch.tensor([-1, 100], dtype=torch.long)  # Invalid: -1 and 100 (n_classes=100)
    
        with pytest.raises(IndexError) as exc_info3:
>           model(input_tensor3, target_tensor3)

tests/test_torch_nn_modules_adaptive_g1.py:332: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = AdaptiveLogSoftmaxWithLoss(
  (head): Linear(in_features=10, out_features=12, bias=False)
  (tail): ModuleList(
    (0...r(in_features=10, out_features=0, bias=False)
      (1): Linear(in_features=0, out_features=50, bias=False)
    )
  )
)
input_ = tensor([[-1.2351,  1.8592,  0.0561,  0.7694, -0.3360,  0.0367,  0.4934,  0.8854,
          0.1824,  0.7864],
        [-0.0579,  0.5667,  0.3088,  1.6617,  0.1751,  0.6084,  1.6309, -0.0847,
          1.0844,  0.9478]])
target_ = tensor([ -1, 100])

    def forward(self, input_: Tensor, target_: Tensor) -> _ASMoutput:
        targ_dim = target_.dim()
    
        if targ_dim == 1:
            if input_.size(0) != target_.size(0):
                raise RuntimeError('Input and target should have the same size '
                                   'in the batch dimension.')
            if input_.dim() != 2:
                raise RuntimeError('1D target tensor expects 2D input tensors, '
                                   'but found inputs with size', input_.size())
        elif targ_dim == 0:
            if input_.dim() != 1:
                raise RuntimeError('0D target tensor expects 1D input tensors, '
                                   'but found inputs with size', input_.size())
        else:
            raise RuntimeError('0D or 1D target tensor expected, '
                               'multi-target not supported')
    
        is_batched = targ_dim > 0
        input = input_ if is_batched else input_.unsqueeze(0)
        target = target_ if is_batched else target_.unsqueeze(0)
    
        used_rows = 0
        batch_size = target.size(0)
    
        output = input.new_zeros(batch_size)
        gather_inds = target.new_empty(batch_size)
    
        cutoff_values = [0] + self.cutoffs
        for i in range(len(cutoff_values) - 1):
    
            low_idx = cutoff_values[i]
            high_idx = cutoff_values[i + 1]
    
            target_mask = (target >= low_idx) & (target < high_idx)
            row_indices = target_mask.nonzero().squeeze()
    
            if row_indices.numel() == 0:
                continue
    
            if i == 0:
                gather_inds.index_copy_(0, row_indices, target[target_mask])
    
            else:
                relative_target = target[target_mask] - low_idx
                input_subset = input.index_select(0, row_indices)
    
                cluster_output = self.tail[i - 1](input_subset)
                cluster_index = self.shortlist_size + i - 1
    
                gather_inds.index_fill_(0, row_indices, cluster_index)
                cluster_logprob = log_softmax(cluster_output, dim=1)
                local_logprob = cluster_logprob.gather(1, relative_target.unsqueeze(1))
                output.index_copy_(0, row_indices, local_logprob.squeeze(1))
    
            used_rows += row_indices.numel()
    
        if used_rows != batch_size:
>           raise RuntimeError("Target values should be in [0, {}], "
                               "but values in range [{}, {}] "
                               "were found. ".format(self.n_classes - 1,
                                                     target.min().item(),
                                                     target.max().item()))
E           RuntimeError: Target values should be in [0, 99], but values in range [-1, 100] were found.

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/adaptive.py:228: RuntimeError
_________________ test_adaptive_softmax_invalid_parameters_g2 __________________

    def test_adaptive_softmax_invalid_parameters_g2():
        """Test invalid parameter combinations for G2 group."""
        # Test n_classes < 2
        with pytest.raises(ValueError) as exc_info:
            AdaptiveLogSoftmaxWithLoss(
                in_features=10,
                n_classes=1,  # Invalid: n_classes must be >= 2
                cutoffs=[5],
                div_value=4.0,
                head_bias=False
            )
        assert "n_classes" in str(exc_info.value).lower(), \
            f"Expected error about n_classes, got: {exc_info.value}"
    
        # Note: in_features <= 0 may not raise an exception in the current implementation
        # as Linear layer might handle it differently. We'll skip this test.
    
        # Test cutoffs value out of range (>= n_classes)
        with pytest.raises(ValueError) as exc_info:
            AdaptiveLogSoftmaxWithLoss(
                in_features=10,
                n_classes=20,
                cutoffs=[25],  # Invalid: 25 >= 20
                div_value=4.0,
                head_bias=False
            )
        assert "cutoff" in str(exc_info.value).lower() or "range" in str(exc_info.value).lower(), \
            f"Expected error about cutoff range, got: {exc_info.value}"
    
        # Test cutoffs value <= 0
        with pytest.raises(ValueError) as exc_info:
            AdaptiveLogSoftmaxWithLoss(
                in_features=10,
                n_classes=20,
                cutoffs=[0],  # Invalid: must be > 0
                div_value=4.0,
                head_bias=False
            )
        assert "cutoff" in str(exc_info.value).lower() or "positive" in str(exc_info.value).lower(), \
            f"Expected error about positive cutoff, got: {exc_info.value}"
    
        # Test div_value <= 0
        with pytest.raises(ValueError) as exc_info:
>           AdaptiveLogSoftmaxWithLoss(
                in_features=10,
                n_classes=20,
                cutoffs=[5],
                div_value=0.0,  # Invalid: must be > 0
                head_bias=False
            )

tests/test_torch_nn_modules_adaptive_g2.py:472: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = AdaptiveLogSoftmaxWithLoss(
  (head): Linear(in_features=10, out_features=6, bias=False)
  (tail): ModuleList()
)
in_features = 10, n_classes = 20, cutoffs = [5], div_value = 0.0
head_bias = False, device = None, dtype = None

    def __init__(
        self,
        in_features: int,
        n_classes: int,
        cutoffs: Sequence[int],
        div_value: float = 4.,
        head_bias: bool = False,
        device=None,
        dtype=None
    ) -> None:
        factory_kwargs = {'device': device, 'dtype': dtype}
        super(AdaptiveLogSoftmaxWithLoss, self).__init__()
    
        cutoffs = list(cutoffs)
    
        if (cutoffs != sorted(cutoffs)) \
                or (min(cutoffs) <= 0) \
                or (max(cutoffs) > (n_classes - 1)) \
                or (len(set(cutoffs)) != len(cutoffs)) \
                or any([int(c) != c for c in cutoffs]):
    
            raise ValueError("cutoffs should be a sequence of unique, positive "
                             "integers sorted in an increasing order, where "
                             "each value is between 1 and n_classes-1")
    
        self.in_features = in_features
        self.n_classes = n_classes
        self.cutoffs = cutoffs + [n_classes]
        self.div_value = div_value
        self.head_bias = head_bias
    
        self.shortlist_size = self.cutoffs[0]
        self.n_clusters = len(self.cutoffs) - 1
        self.head_size = self.shortlist_size + self.n_clusters
    
        self.head = Linear(self.in_features, self.head_size, bias=self.head_bias,
                           **factory_kwargs)
        self.tail = ModuleList()
    
        for i in range(self.n_clusters):
    
>           hsz = int(self.in_features // (self.div_value ** (i + 1)))
E           ZeroDivisionError: float floor division by zero

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/adaptive.py:154: ZeroDivisionError
=============================== warnings summary ===============================
exam/torch_group/nn.modules.adaptive/tests/test_torch_nn_modules_adaptive_g1.py::test_adaptive_softmax_forward_basic[10-100-cutoffs0-4.0-False-2-dtype0-cpu]
exam/torch_group/nn.modules.adaptive/tests/test_torch_nn_modules_adaptive_g1.py::test_adaptive_softmax_edge_cases_g1
exam/torch_group/nn.modules.adaptive/tests/test_torch_nn_modules_adaptive_g1.py::test_adaptive_softmax_shape_mismatch
exam/torch_group/nn.modules.adaptive/tests/test_torch_nn_modules_adaptive_g2.py::test_adaptive_softmax_cutoffs_validation[5-30-cutoffs0-4.0-False-valid-dtype0-cpu-False]
exam/torch_group/nn.modules.adaptive/tests/test_torch_nn_modules_adaptive_g2.py::test_adaptive_softmax_log_prob[12-80-cutoffs0-4.0-False-2-dtype0-cpu]
exam/torch_group/nn.modules.adaptive/tests/test_torch_nn_modules_adaptive_g2.py::test_adaptive_softmax_predict_method
exam/torch_group/nn.modules.adaptive/tests/test_torch_nn_modules_adaptive_g2.py::test_adaptive_softmax_log_prob_edge_cases
  /opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op
    warnings.warn("Initializing zero-element tensors is a no-op")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                         Stmts   Miss Branch BrPart  Cover   Missing
----------------------------------------------------------------------------------------
test_fix.py                                     31     31      0      0     0%   4-63
tests/test_torch_nn_modules_adaptive_g1.py      89      1      6      1    98%   142
tests/test_torch_nn_modules_adaptive_g2.py     137     24     18      3    81%   45-46, 134->exit, 189-190, 327-386, 408->exit, 479
----------------------------------------------------------------------------------------
TOTAL                                          257     56     24      4    78%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_adaptive_g1.py::test_adaptive_softmax_forward_basic[20-200-cutoffs1-2.0-True-4-dtype1-cpu]
FAILED tests/test_torch_nn_modules_adaptive_g1.py::test_adaptive_softmax_shape_mismatch
FAILED tests/test_torch_nn_modules_adaptive_g2.py::test_adaptive_softmax_invalid_parameters_g2
3 failed, 10 passed, 7 warnings in 0.94s

Error: exit 1