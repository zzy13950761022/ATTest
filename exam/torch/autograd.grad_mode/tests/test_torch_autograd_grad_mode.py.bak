# ==== BLOCK:HEADER START ====
import torch
import pytest
import numpy as np
from unittest.mock import patch, MagicMock
import random
import os
import sys

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# Helper functions
def create_tensor(device='cpu', dtype=torch.float32, requires_grad=True):
    """Create a test tensor with specified properties."""
    if dtype == torch.float32:
        data = torch.randn(3, 4, device=device, dtype=dtype)
    elif dtype == torch.float64:
        data = torch.randn(3, 4, device=device, dtype=dtype)
    else:
        raise ValueError(f"Unsupported dtype: {dtype}")
    
    if requires_grad:
        data.requires_grad_(True)
    return data

def check_grad_enabled(expected):
    """Check if gradient computation is enabled as expected."""
    return torch.is_grad_enabled() == expected

def test_create_tensor_error_handling():
    """Test error handling in create_tensor helper function.
    
    This test covers the error handling branch for unsupported dtypes.
    """
    # Test with supported dtypes - should work
    tensor_f32 = create_tensor(dtype=torch.float32)
    assert tensor_f32.dtype == torch.float32
    
    tensor_f64 = create_tensor(dtype=torch.float64)
    assert tensor_f64.dtype == torch.float64
    
    # Test with unsupported dtype - should raise ValueError
    with pytest.raises(ValueError, match="Unsupported dtype"):
        # Try to create tensor with an unsupported dtype
        # We'll use torch.int32 as an example of unsupported dtype
        create_tensor(dtype=torch.int32)
    
    # Test with another unsupported dtype
    with pytest.raises(ValueError, match="Unsupported dtype"):
        create_tensor(dtype=torch.bool)
    
    # Test that the error message contains the dtype information
    try:
        create_tensor(dtype=torch.int64)
    except ValueError as e:
        assert "Unsupported dtype" in str(e)
        assert "torch.int64" in str(e) or "int64" in str(e)

# Group handling logic
def get_current_test_group():
    """Determine which test group this file belongs to based on filename."""
    current_file = os.path.basename(__file__)
    
    if current_file == "test_torch_autograd_grad_mode_g1.py":
        return "G1"
    elif current_file == "test_torch_autograd_grad_mode_g2.py":
        return "G2"
    elif current_file == "test_torch_autograd_grad_mode.py":
        # Main file contains all tests
        return "ALL"
    else:
        return "UNKNOWN"

# Skip tests that don't belong to the current group
def skip_if_not_in_group(group_id):
    """Skip test if it doesn't belong to the specified group."""
    current_group = get_current_test_group()
    
    if current_group == "ALL":
        return  # Main file runs all tests
    elif current_group != group_id:
        pytest.skip(f"Test belongs to group {group_id}, but current file is for group {current_group}")
# ==== BLOCK:HEADER END ====

# ==== BLOCK:CASE_01 START ====
@pytest.mark.parametrize("device,dtype,requires_grad_input", [
    ("cpu", torch.float32, True),
    ("cpu", torch.float64, False),
])
def test_no_grad_basic(device, dtype, requires_grad_input):
    """TC-01: no_grad基础功能
    
    测试no_grad上下文管理器的基本功能：
    1. 在no_grad上下文内创建的张量requires_grad=False
    2. 退出上下文后梯度计算状态恢复
    3. 无异常抛出
    
    Weak asserts:
    - requires_grad_false: 上下文内张量requires_grad=False
    - state_restored: 退出后梯度状态恢复
    - no_exception: 无异常抛出
    """
    # 记录初始状态
    initial_grad_enabled = torch.is_grad_enabled()
    
    # 创建输入张量
    input_tensor = create_tensor(device=device, dtype=dtype, requires_grad=requires_grad_input)
    
    # 测试no_grad上下文
    with torch.no_grad():
        # 断言1: 在no_grad上下文内梯度计算被禁用
        assert not torch.is_grad_enabled(), "Gradient computation should be disabled in no_grad context"
        
        # 创建新张量
        output_tensor = input_tensor * 2
        
        # 断言2: 在no_grad上下文内创建的新张量requires_grad=False
        assert not output_tensor.requires_grad, (
            f"Tensor created in no_grad context should have requires_grad=False, "
            f"but got {output_tensor.requires_grad}"
        )
        
        # 即使输入张量requires_grad=True，输出张量也不应该有梯度
        if requires_grad_input:
            assert input_tensor.requires_grad, "Input tensor should have requires_grad=True"
            assert not output_tensor.requires_grad, "Output tensor should not require grad even if input does"
        
        # 执行一些计算操作
        result = output_tensor.sum()
        
        # 断言3: 可以正常执行计算操作
        assert result is not None, "Computation should succeed in no_grad context"
    
    # 断言4: 退出上下文后梯度计算状态恢复
    assert torch.is_grad_enabled() == initial_grad_enabled, (
        f"Gradient enabled state should be restored to initial value {initial_grad_enabled}, "
        f"but got {torch.is_grad_enabled()}"
    )
    
    # 断言5: 可以正常创建新的张量（状态已恢复）
    new_tensor = torch.tensor([1.0, 2.0, 3.0], device=device, dtype=dtype, requires_grad=True)
    assert new_tensor.requires_grad == True, "New tensor should be able to require grad after exiting no_grad"
    
    # 清理
    del input_tensor, output_tensor, result, new_tensor
# ==== BLOCK:CASE_01 END ====

# ==== BLOCK:CASE_02 START ====
@pytest.mark.parametrize("device,dtype,nesting_order", [
    ("cpu", torch.float32, "no_grad_inside_enable"),
    ("cpu", torch.float32, "enable_inside_no_grad"),
])
def test_enable_grad_interaction(device, dtype, nesting_order):
    """TC-02: enable_grad与no_grad交互
    
    测试enable_grad和no_grad上下文管理器的交互：
    1. 嵌套上下文的状态传播
    2. 正确恢复状态
    3. 无异常抛出
    
    Weak asserts:
    - state_propagation: 嵌套上下文状态正确传播
    - correct_restoration: 状态正确恢复
    - no_exception: 无异常抛出
    """
    # 记录初始状态
    initial_grad_enabled = torch.is_grad_enabled()
    
    # 创建测试张量
    test_tensor = create_tensor(device=device, dtype=dtype, requires_grad=True)
    
    if nesting_order == "no_grad_inside_enable":
        # 测试场景：no_grad嵌套在enable_grad内部
        with torch.enable_grad():
            # 外层：enable_grad应该启用梯度计算
            assert torch.is_grad_enabled(), "enable_grad should enable gradient computation"
            
            # 在外层创建张量
            outer_tensor = test_tensor * 2
            assert outer_tensor.requires_grad, "Tensor created in enable_grad context should require grad"
            
            with torch.no_grad():
                # 内层：no_grad应该禁用梯度计算
                assert not torch.is_grad_enabled(), "no_grad should disable gradient computation inside enable_grad"
                
                # 在内层创建张量
                inner_tensor = test_tensor * 3
                assert not inner_tensor.requires_grad, "Tensor created in no_grad context should not require grad"
                
                # 可以访问外层张量
                combined = outer_tensor + inner_tensor
                assert not combined.requires_grad, "Operation with no_grad tensor should not require grad"
            
            # 退出内层后：应该恢复到enable_grad状态
            assert torch.is_grad_enabled(), "Should return to enable_grad state after exiting no_grad"
            
            # 在外层创建新张量
            new_outer_tensor = test_tensor * 4
            assert new_outer_tensor.requires_grad, "New tensor in enable_grad context should require grad"
        
        # 退出外层后：应该恢复到初始状态
        assert torch.is_grad_enabled() == initial_grad_enabled, "Should restore to initial state after exiting enable_grad"
    
    elif nesting_order == "enable_inside_no_grad":
        # 测试场景：enable_grad嵌套在no_grad内部
        with torch.no_grad():
            # 外层：no_grad应该禁用梯度计算
            assert not torch.is_grad_enabled(), "no_grad should disable gradient computation"
            
            # 在外层创建张量
            outer_tensor = test_tensor * 2
            assert not outer_tensor.requires_grad, "Tensor created in no_grad context should not require grad"
            
            with torch.enable_grad():
                # 内层：enable_grad应该启用梯度计算
                assert torch.is_grad_enabled(), "enable_grad should enable gradient computation inside no_grad"
                
                # 在内层创建张量
                inner_tensor = test_tensor * 3
                assert inner_tensor.requires_grad, "Tensor created in enable_grad context should require grad"
                
                # 可以访问外层张量
                combined = outer_tensor + inner_tensor
                # 注意：当与no_grad张量运算时，结果可能不require grad
                # 这是PyTorch的预期行为
            
            # 退出内层后：应该恢复到no_grad状态
            assert not torch.is_grad_enabled(), "Should return to no_grad state after exiting enable_grad"
            
            # 在外层创建新张量
            new_outer_tensor = test_tensor * 4
            assert not new_outer_tensor.requires_grad, "New tensor in no_grad context should not require grad"
        
        # 退出外层后：应该恢复到初始状态
        assert torch.is_grad_enabled() == initial_grad_enabled, "Should restore to initial state after exiting no_grad"
    
    else:
        raise ValueError(f"Unknown nesting_order: {nesting_order}")
    
    # 最终状态验证
    assert torch.is_grad_enabled() == initial_grad_enabled, (
        f"Final gradient enabled state should be {initial_grad_enabled}, "
        f"but got {torch.is_grad_enabled()}"
    )
    
    # 清理
    del test_tensor
# ==== BLOCK:CASE_02 END ====

# ==== BLOCK:CASE_03 START ====
@pytest.mark.parametrize("device,dtype,mode", [
    ("cpu", torch.float32, True),
    ("cpu", torch.float32, False),
])
def test_inference_mode_basic(device, dtype, mode):
    """TC-03: inference_mode基础功能
    
    测试inference_mode上下文管理器的基本功能：
    1. mode参数控制推理模式开关
    2. 在推理模式下创建的张量requires_grad=False
    3. 无异常抛出
    
    Weak asserts:
    - requires_grad_correct: 张量requires_grad属性正确
    - mode_parameter_works: mode参数正常工作
    - no_exception: 无异常抛出
    
    Note: 根据规格书，此测试需要mock
    """
    # 记录初始状态
    initial_grad_enabled = torch.is_grad_enabled()
    
    # 创建输入张量
    input_tensor = create_tensor(device=device, dtype=dtype, requires_grad=True)
    
    # 使用mock来验证torch._C._InferenceMode被正确调用
    with patch('torch._C._InferenceMode') as mock_inference_mode:
        # 测试inference_mode上下文
        with torch.inference_mode(mode=mode):
            # 验证InferenceMode被正确实例化
            mock_inference_mode.assert_called_once_with(mode)
            
            # 创建新张量
            output_tensor = input_tensor * 2
            
            # 断言1: 在inference_mode上下文内创建的新张量requires_grad=False
            # 注意：当mode=True时，inference_mode应该禁用梯度
            # 当mode=False时，行为可能不同，但根据文档，inference_mode(mode=False)应该启用梯度
            if mode:
                assert not output_tensor.requires_grad, (
                    f"Tensor created in inference_mode(mode={mode}) context should have "
                    f"requires_grad=False, but got {output_tensor.requires_grad}"
                )
            else:
                # mode=False时，inference_mode应该不改变梯度状态
                # 但实际行为可能取决于实现，这里我们只检查无异常
                pass
            
            # 执行一些计算操作
            result = output_tensor.sum()
            assert result is not None, "Computation should succeed in inference_mode context"
        
        # InferenceMode RAII guard应该被删除
        # 这通过mock对象的生命周期来隐式验证
    
    # 断言2: 退出上下文后可以正常创建新张量
    new_tensor = torch.tensor([1.0, 2.0, 3.0], device=device, dtype=dtype, requires_grad=True)
    assert new_tensor.requires_grad == True, "New tensor should be able to require grad after exiting inference_mode"
    
    # 清理
    del input_tensor, output_tensor, result, new_tensor
    
    # 额外测试：验证mode参数默认值
    with patch('torch._C._InferenceMode') as mock_inference_mode_default:
        with torch.inference_mode():  # 使用默认mode=True
            mock_inference_mode_default.assert_called_once_with(True)
    
    # 最终状态验证
    assert torch.is_grad_enabled() == initial_grad_enabled, (
        f"Final gradient enabled state should be {initial_grad_enabled}, "
        f"but got {torch.is_grad_enabled()}"
    )
# ==== BLOCK:CASE_03 END ====

# ==== BLOCK:CASE_04 START ====
@pytest.mark.parametrize("device,dtype,mode", [
    ("cpu", torch.float32, True),
    ("cpu", torch.float32, False),
])
def test_set_grad_enabled_parameter(device, dtype, mode):
    """TC-04: set_grad_enabled参数验证
    
    测试set_grad_enabled上下文管理器的参数验证：
    1. mode参数控制梯度计算开关
    2. 必需提供mode参数
    3. 无异常抛出
    
    Weak asserts:
    - grad_enabled_state: 梯度启用状态正确
    - parameter_required: 参数必需性验证
    - no_exception: 无异常抛出
    """
    # 记录初始状态
    initial_grad_enabled = torch.is_grad_enabled()
    
    # 创建测试张量
    test_tensor = create_tensor(device=device, dtype=dtype, requires_grad=True)
    
    # 测试set_grad_enabled上下文
    with torch.set_grad_enabled(mode):
        # 断言1: 梯度启用状态应该与mode参数一致
        assert torch.is_grad_enabled() == mode, (
            f"Gradient enabled state should be {mode} when set_grad_enabled(mode={mode}), "
            f"but got {torch.is_grad_enabled()}"
        )
        
        # 创建新张量
        if mode:
            # mode=True时，新张量可以require grad
            new_tensor = test_tensor * 2
            assert new_tensor.requires_grad, (
                f"Tensor created with set_grad_enabled(mode={mode}) should be able to require grad"
            )
        else:
            # mode=False时，新张量不应该require grad
            new_tensor = test_tensor * 2
            assert not new_tensor.requires_grad, (
                f"Tensor created with set_grad_enabled(mode={mode}) should not require grad"
            )
        
        # 执行计算操作
        result = new_tensor.sum()
        assert result is not None, "Computation should succeed in set_grad_enabled context"
    
    # 断言2: 退出上下文后梯度计算状态恢复
    assert torch.is_grad_enabled() == initial_grad_enabled, (
        f"Gradient enabled state should be restored to initial value {initial_grad_enabled}, "
        f"but got {torch.is_grad_enabled()}"
    )
    
    # 测试参数必需性：set_grad_enabled必须提供mode参数
    # 这通过函数签名来验证，如果调用时不提供参数应该抛出TypeError
    try:
        # 尝试不带参数调用（应该失败）
        with torch.set_grad_enabled():  # type: ignore
            pass
        # 如果到达这里，说明没有抛出异常，这可能是一个问题
        # 但根据PyTorch实现，set_grad_enabled确实需要参数
        # 我们把这个检查放在try-except中，以防实现变化
    except TypeError as e:
        # 期望的异常：缺少必需的参数
        assert "missing" in str(e).lower() or "required" in str(e).lower() or "argument" in str(e).lower(), (
            f"Expected TypeError about missing required argument, but got: {e}"
        )
    except Exception as e:
        # 其他异常也可以接受，只要不是静默成功
        pass
    
    # 额外测试：验证嵌套使用
    original_state = torch.is_grad_enabled()
    
    with torch.set_grad_enabled(True):
        assert torch.is_grad_enabled() == True
        
        with torch.set_grad_enabled(False):
            assert torch.is_grad_enabled() == False
            
            inner_tensor = test_tensor * 3
            assert not inner_tensor.requires_grad
        
        # 应该恢复到True状态
        assert torch.is_grad_enabled() == True
    
    # 应该恢复到原始状态
    assert torch.is_grad_enabled() == original_state
    
    # 清理
    del test_tensor
# ==== BLOCK:CASE_04 END ====

# ==== BLOCK:CASE_05 START ====
@pytest.mark.parametrize(
    "decorator_type,device,dtype",
    [
        ("no_grad", "cpu", torch.float32),
        ("inference_mode", "cpu", torch.float32),
    ],
    ids=["no_grad_decorator", "inference_mode_decorator"]
)
def test_decorator_usage_verification(decorator_type, device, dtype):
    """测试装饰器用法验证 (TC-05)
    
    验证no_grad和inference_mode作为装饰器的正确用法：
    1. 装饰器正确包装函数
    2. 装饰后的函数可调用
    3. 梯度状态正确设置
    """
    # 创建测试张量
    x = torch.randn(3, 4, device=device, dtype=dtype, requires_grad=True)
    
    # 定义原始函数
    def original_func(tensor):
        """原始函数，执行张量操作"""
        return tensor * 2 + 1
    
    # 根据装饰器类型应用装饰器
    if decorator_type == "no_grad":
        decorated_func = torch.no_grad()(original_func)
    elif decorator_type == "inference_mode":
        decorated_func = torch.inference_mode()(original_func)
    else:
        pytest.fail(f"未知的装饰器类型: {decorator_type}")
    
    # 断言1: 装饰器正确包装函数 (decorator_wraps)
    # 检查装饰后的函数是否保留了原始函数的元数据
    assert decorated_func.__name__ == original_func.__name__, \
        f"装饰器应保留函数名: {decorator_type}"
    assert decorated_func.__doc__ == original_func.__doc__, \
        f"装饰器应保留文档字符串: {decorator_type}"
    
    # 断言2: 装饰后的函数可调用 (function_callable)
    # 验证装饰后的函数可以正常调用
    try:
        result = decorated_func(x)
        assert result is not None, f"装饰器函数应返回结果: {decorator_type}"
        assert isinstance(result, torch.Tensor), \
            f"装饰器函数应返回张量: {decorator_type}"
    except Exception as e:
        pytest.fail(f"装饰器函数调用失败 ({decorator_type}): {e}")
    
    # 断言3: 梯度状态正确设置 (grad_state_correct)
    # 验证在装饰器内部梯度被正确禁用
    result = decorated_func(x)
    assert not result.requires_grad, \
        f"装饰器内部应禁用梯度: {decorator_type}"
    
    # 验证装饰器外部梯度状态恢复
    # 在装饰器外部创建新张量，梯度状态应正常
    y = torch.randn(2, 3, device=device, dtype=dtype, requires_grad=True)
    z = y * 2
    assert z.requires_grad, \
        f"装饰器外部梯度状态应恢复: {decorator_type}"
    
    # 额外测试：验证装饰器可以多次调用
    for i in range(3):
        result_i = decorated_func(x + i)
        assert not result_i.requires_grad, \
            f"装饰器多次调用应保持梯度禁用: {decorator_type} (调用{i+1})"
    
    # 测试装饰器与普通函数调用的对比
    # 普通调用（无装饰器）应保持梯度
    if torch.is_grad_enabled():
        normal_result = original_func(x)
        assert normal_result.requires_grad, \
            f"普通函数调用应保持梯度: {decorator_type}"
    
    # 验证装饰器可以应用于不同签名的函数
    def func_with_args(a, b=1):
        return a * b
    
    if decorator_type == "no_grad":
        decorated_with_args = torch.no_grad()(func_with_args)
    else:
        decorated_with_args = torch.inference_mode()(func_with_args)
    
    # 测试带参数的函数调用
    result_args = decorated_with_args(x, 3)
    assert not result_args.requires_grad, \
        f"带参数函数的装饰器应禁用梯度: {decorator_type}"
    
    # 测试装饰器可以应用于类方法
    class TestClass:
        def method(self, tensor):
            return tensor * 3
    
    obj = TestClass()
    if decorator_type == "no_grad":
        decorated_method = torch.no_grad()(obj.method)
    else:
        decorated_method = torch.inference_mode()(obj.method)
    
    result_method = decorated_method(x)
    assert not result_method.requires_grad, \
        f"类方法的装饰器应禁用梯度: {decorator_type}"
# ==== BLOCK:CASE_05 END ====

# ==== BLOCK:FOOTER START ====
# Additional test utilities and fixtures

@pytest.fixture
def random_tensor():
    """Fixture to create a random tensor for testing."""
    torch.manual_seed(42)
    return torch.randn(3, 4, requires_grad=True)

@pytest.fixture(params=[torch.float32, torch.float64])
def dtype_fixture(request):
    """Fixture providing different dtypes for parameterized tests."""
    return request.param

@pytest.fixture(params=['cpu'])
def device_fixture(request):
    """Fixture providing different devices for parameterized tests."""
    return request.param

# Test class for organizing related tests
class TestGradMode:
    """Test class for torch.autograd.grad_mode functionality."""
    
    def test_no_grad_decorator(self):
        """Test no_grad as a decorator."""
        @torch.no_grad()
        def multiply_by_two(x):
            return x * 2
        
        x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
        result = multiply_by_two(x)
        
        # Decorator should work like context manager
        assert not result.requires_grad
        assert multiply_by_two.__name__ == 'multiply_by_two'  # wraps preserves name
    
    def test_enable_grad_decorator(self):
        """Test enable_grad as a decorator."""
        # First disable grad globally
        torch.set_grad_enabled(False)
        
        @torch.enable_grad()
        def multiply_by_three(x):
            return x * 3
        
        x = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)
        result = multiply_by_three(x)
        
        # Decorator should temporarily enable grad
        # Note: The actual behavior might depend on implementation
        # We mainly check that it doesn't crash
        assert result is not None
        
        # Restore grad state
        torch.set_grad_enabled(True)
    
    def test_thread_local_isolation(self):
        """Basic test for thread-local isolation property.
        
        Note: Full thread isolation testing requires multi-threading,
        which is beyond the scope of basic unit tests.
        """
        # Record initial state
        initial_state = torch.is_grad_enabled()
        
        # Change state in this "thread"
        with torch.no_grad():
            assert not torch.is_grad_enabled()
        
        # State should be restored
        assert torch.is_grad_enabled() == initial_state

if __name__ == "__main__":
    # Simple test runner for debugging
    import sys
    pytest.main([__file__] + sys.argv[1:])
# ==== BLOCK:FOOTER END ====