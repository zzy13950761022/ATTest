import math
import pytest
import torch
import torch.nn as nn
from torch.nn.utils import weight_norm

# ==== BLOCK:HEADER START ====
# Test file for torch.nn.utils.weight_norm - Group G1: Core Functionality
# 
# This file contains tests for the core functionality of weight_norm:
# - Basic weight normalization application
# - Parameter decomposition correctness
# - Global norm calculation (dim=None)
# - Different parameter names
# - Forward hook verification
# 
# Note: This is epoch 5/5 (FINAL), using STRONG assertions.
# All deferred tests are now implemented.
# ==== BLOCK:HEADER END ====

class TestWeightNormG1:
    """Test cases for weight_norm core functionality (Group G1)."""
    
    def test_weight_norm_linear_default_params(self):
        """TC-01: Linear层默认参数权重归一化
        
        Test weight normalization with default parameters on Linear layer.
        Weak assertions: returns_module, has_g_param, has_v_param, no_original_param
        Strong assertions: weight_reconstruction, forward_hook_registered, param_shapes_correct
        """
        # Set random seed for reproducibility
        torch.manual_seed(42)
        
        # Create Linear module with specified parameters
        in_features = 20
        out_features = 40
        module = nn.Linear(in_features, out_features, bias=True)
        
        # Store original weight for reference
        original_weight = module.weight.detach().clone()
        
        # Apply weight normalization with default parameters
        result = weight_norm(module, name='weight', dim=0)
        
        # Weak assertion 1: returns_module - function returns the module
        assert result is module, "weight_norm should return the input module"
        
        # Weak assertion 2: has_g_param - module has weight_g parameter
        assert hasattr(module, 'weight_g'), "Module should have weight_g parameter after weight_norm"
        assert isinstance(module.weight_g, nn.Parameter), "weight_g should be a Parameter"
        
        # Weak assertion 3: has_v_param - module has weight_v parameter
        assert hasattr(module, 'weight_v'), "Module should have weight_v parameter after weight_norm"
        assert isinstance(module.weight_v, nn.Parameter), "weight_v should be a Parameter"
        
        # Weak assertion 4: no_original_param - original weight parameter is removed
        # Note: weight parameter should still exist but is recomputed from g and v
        # Actually, the original weight is replaced, so we check it's not the same tensor
        assert hasattr(module, 'weight'), "Module should still have weight attribute"
        assert not torch.equal(module.weight, original_weight), "Weight should be recomputed"
        
        # STRONG ASSERTION 1: weight_reconstruction - verify exact reconstruction
        # The weight should be reconstructed as: weight = g * v / ||v|| along dim=0
        v_norm = torch.norm(module.weight_v, dim=1, keepdim=True)  # shape: [out_features, 1]
        eps = 1e-8
        v_normalized = module.weight_v / (v_norm + eps)
        reconstructed_weight = module.weight_g * v_normalized
        
        # Check exact reconstruction (within numerical precision)
        weight_diff = torch.abs(module.weight - reconstructed_weight)
        max_diff = torch.max(weight_diff).item()
        assert max_diff < 1e-6, f"Weight reconstruction error too large: {max_diff}"
        
        # STRONG ASSERTION 2: forward_hook_registered - check hook is registered
        assert hasattr(module, '_forward_pre_hooks'), "Module should have forward pre-hooks"
        assert len(module._forward_pre_hooks) > 0, "At least one forward pre-hook should be registered"
        
        # Check that one of the hooks is for weight recomputation
        hook_found = False
        for hook_id, hook in module._forward_pre_hooks.items():
            # Check if hook is related to weight_norm by checking hook function
            if hasattr(hook, '__name__') and 'weight_norm' in str(hook.__name__).lower():
                hook_found = True
                break
            # Also check by hook type or other attributes
            if hasattr(hook, 'fn') and 'weight_norm' in str(hook.fn).lower():
                hook_found = True
                break
        
        assert hook_found, "Weight norm forward hook should be registered"
        
        # STRONG ASSERTION 3: param_shapes_correct - verify parameter shapes
        # weight_g should have shape [out_features, 1] for dim=0
        expected_g_shape = torch.Size([out_features, 1])
        assert module.weight_g.shape == expected_g_shape, \
            f"weight_g shape {module.weight_g.shape} != expected {expected_g_shape}"
        
        # weight_v should have same shape as original weight
        expected_v_shape = original_weight.shape
        assert module.weight_v.shape == expected_v_shape, \
            f"weight_v shape {module.weight_v.shape} != expected {expected_v_shape}"
        
        # weight should have same shape as original
        assert module.weight.shape == expected_v_shape, \
            f"weight shape {module.weight.shape} != expected {expected_v_shape}"
        
        # Additional shape consistency checks
        assert module.weight_g.dim() == 2, f"weight_g should be 2D, got {module.weight_g.dim()}D"
        assert module.weight_v.dim() == 2, f"weight_v should be 2D, got {module.weight_v.dim()}D"
        assert module.weight.dim() == 2, f"weight should be 2D, got {module.weight.dim()}D"
        
        # Verify dtype and device
        assert module.weight_g.dtype == torch.float32, "weight_g should be float32"
        assert module.weight_v.dtype == torch.float32, "weight_v should be float32"
        assert module.weight.dtype == torch.float32, "weight should be float32"
        assert module.weight_g.device.type == 'cpu', "weight_g should be on CPU"
        assert module.weight_v.device.type == 'cpu', "weight_v should be on CPU"
    
    def test_weight_decomposition_correctness(self):
        """TC-02: 权重分解正确性验证
        
        Verify that weight decomposition is mathematically correct: w ≈ g * v/||v||
        Weak assertions: g_param_exists, v_param_exists, weight_reconstructed
        Strong assertions: reconstruction_error_lt_1e-6, norm_calculation_correct, dim_handling_correct
        """
        # Set random seed for reproducibility
        torch.manual_seed(42)
        
        # Create Linear module with specified parameters
        in_features = 10
        out_features = 20
        module = nn.Linear(in_features, out_features, bias=True)
        
        # Store original weight for reference
        original_weight = module.weight.detach().clone()
        
        # Apply weight normalization
        result = weight_norm(module, name='weight', dim=0)
        
        # Weak assertion 1: g_param_exists
        assert hasattr(module, 'weight_g'), "Module should have weight_g parameter"
        assert isinstance(module.weight_g, nn.Parameter), "weight_g should be a Parameter"
        
        # Weak assertion 2: v_param_exists
        assert hasattr(module, 'weight_v'), "Module should have weight_v parameter"
        assert isinstance(module.weight_v, nn.Parameter), "weight_v should be a Parameter"
        
        # Weak assertion 3: weight_reconstructed - verify reconstruction formula
        # The weight should be reconstructed as: weight = g * v / ||v|| along dim=0
        
        # Calculate norm along dim=0 (per output channel)
        # For Linear weight with shape [out_features, in_features], dim=0 means
        # norm is computed per output channel (first dimension)
        v_norm = torch.norm(module.weight_v, dim=1, keepdim=True)  # shape: [out_features, 1]
        
        # Avoid division by zero (add small epsilon)
        eps = 1e-8
        v_normalized = module.weight_v / (v_norm + eps)
        
        # Reconstructed weight: g * v_normalized
        reconstructed_weight = module.weight_g * v_normalized
        
        # STRONG ASSERTION 1: reconstruction_error_lt_1e-6
        # Check that the actual weight matches reconstruction with high precision
        weight_diff = torch.abs(module.weight - reconstructed_weight)
        max_diff = torch.max(weight_diff).item()
        assert max_diff < 1e-6, f"Weight reconstruction error too large: {max_diff}"
        
        # STRONG ASSERTION 2: norm_calculation_correct
        # Verify norm calculation is correct along specified dimension
        # For dim=0, norm should be computed per output channel (first dimension)
        
        # Calculate norm manually for verification
        # For 2D weight tensor [out_features, in_features], dim=0 means norm over columns
        manual_norm = torch.sqrt(torch.sum(module.weight_v ** 2, dim=1, keepdim=True))
        
        # Compare with torch.norm result
        norm_diff = torch.abs(v_norm - manual_norm)
        max_norm_diff = torch.max(norm_diff).item()
        assert max_norm_diff < 1e-7, f"Norm calculation error: {max_norm_diff}"
        
        # Verify norm values are positive (or very close to zero)
        assert torch.all(v_norm >= 0), "Norm values should be non-negative"
        
        # STRONG ASSERTION 3: dim_handling_correct
        # Verify that dim parameter is handled correctly
        
        # Check weight_g shape matches dim=0 specification
        # For dim=0, weight_g should have shape [out_features, 1]
        expected_g_shape = torch.Size([out_features, 1])
        assert module.weight_g.shape == expected_g_shape, \
            f"weight_g shape {module.weight_g.shape} != expected {expected_g_shape} for dim=0"
        
        # Verify that each output channel has its own g value
        assert module.weight_g.size(0) == out_features, \
            f"weight_g should have {out_features} rows for {out_features} output channels"
        assert module.weight_g.size(1) == 1, "weight_g should have 1 column for dim=0"
        
        # Additional verification: check parameter shapes
        assert module.weight_v.shape == original_weight.shape, \
            f"weight_v shape {module.weight_v.shape} != original weight shape {original_weight.shape}"
        
        # Verify that g values are positive (norm should be positive)
        assert torch.all(module.weight_g > 0), "weight_g should contain positive values"
        
        # Verify reconstruction error is small relative to weight magnitude
        weight_norm_val = torch.norm(original_weight).item()
        if weight_norm_val > 0:
            relative_error = max_diff / weight_norm_val
            assert relative_error < 1e-6, f"Relative reconstruction error too large: {relative_error}"
    
    def test_global_norm_calculation_dim_none(self):
        """TC-03: dim=None全局范数计算
        
        Test weight normalization with dim=None (global norm across entire tensor).
        Weak assertions: returns_module, has_g_param, has_v_param, global_norm_applied
        Strong assertions: global_norm_calculation, reconstruction_global, hook_registered
        """
        # Set random seed for reproducibility
        torch.manual_seed(42)
        
        # Create Linear module with specified parameters
        in_features = 15
        out_features = 25
        module = nn.Linear(in_features, out_features, bias=True)
        
        # Store original weight for reference
        original_weight = module.weight.detach().clone()
        
        # Apply weight normalization with dim=None (global norm)
        result = weight_norm(module, name='weight', dim=None)
        
        # Weak assertion 1: returns_module - function returns the module
        assert result is module, "weight_norm should return the input module"
        
        # Weak assertion 2: has_g_param - module has weight_g parameter
        assert hasattr(module, 'weight_g'), "Module should have weight_g parameter after weight_norm"
        assert isinstance(module.weight_g, nn.Parameter), "weight_g should be a Parameter"
        
        # Weak assertion 3: has_v_param - module has weight_v parameter
        assert hasattr(module, 'weight_v'), "Module should have weight_v parameter after weight_norm"
        assert isinstance(module.weight_v, nn.Parameter), "weight_v should be a Parameter"
        
        # Weak assertion 4: global_norm_applied - check that g is scalar (not per-channel)
        # When dim=None, g should be a scalar (1-element tensor)
        assert module.weight_g.numel() == 1, \
            f"For dim=None, weight_g should be scalar, but has shape {module.weight_g.shape}"
        
        # Verify weight_g is scalar (shape [1] or [1, 1])
        assert module.weight_g.dim() <= 2, f"weight_g should be scalar, but has {module.weight_g.dim()} dimensions"
        assert module.weight_g.numel() == 1, "weight_g should have exactly 1 element for dim=None"
        
        # STRONG ASSERTION 1: global_norm_calculation
        # Verify that norm is computed globally (over entire tensor) not per-channel
        
        # Calculate global norm manually
        v_norm_global = torch.norm(module.weight_v)  # scalar norm over entire tensor
        
        # Calculate what per-channel norm would be (for comparison)
        v_norm_per_channel = torch.norm(module.weight_v, dim=1)  # norm per output channel
        
        # Verify that global norm is different from per-channel norms
        # (unless all channels have exactly the same norm, which is unlikely)
        norm_variation = torch.std(v_norm_per_channel).item()
        if norm_variation > 1e-6:  # If channels have different norms
            # Global norm should be different from individual channel norms
            for channel_norm in v_norm_per_channel:
                assert abs(v_norm_global.item() - channel_norm.item()) > 1e-6, \
                    "Global norm should differ from per-channel norms when channels vary"
        
        # STRONG ASSERTION 2: reconstruction_global
        # Verify reconstruction with global norm is accurate
        
        # Avoid division by zero
        eps = 1e-8
        v_normalized = module.weight_v / (v_norm_global + eps)
        
        # Reconstructed weight: g * v_normalized (g is scalar)
        reconstructed_weight = module.weight_g * v_normalized
        
        # Check reconstruction with high precision
        weight_diff = torch.abs(module.weight - reconstructed_weight)
        max_diff = torch.max(weight_diff).item()
        assert max_diff < 1e-6, f"Weight reconstruction error too large for dim=None: {max_diff}"
        
        # Verify reconstruction formula: weight = g * v / ||v||
        # Check that g equals the global norm of the original weight
        original_norm = torch.norm(original_weight).item()
        g_value = module.weight_g.item()
        
        # The g parameter should be close to the original weight norm
        # (allowing for small numerical differences)
        assert abs(g_value - original_norm) < 1e-5, \
            f"g value {g_value} should be close to original weight norm {original_norm}"
        
        # STRONG ASSERTION 3: hook_registered
        # Verify that forward hook is registered for global norm
        
        assert hasattr(module, '_forward_pre_hooks'), "Module should have forward pre-hooks"
        assert len(module._forward_pre_hooks) > 0, "At least one forward pre-hook should be registered"
        
        # Check for weight norm hook
        hook_found = False
        for hook_id, hook in module._forward_pre_hooks.items():
            if hasattr(hook, '__name__') and 'weight_norm' in str(hook.__name__).lower():
                hook_found = True
                break
            if hasattr(hook, 'fn') and 'weight_norm' in str(hook.fn).lower():
                hook_found = True
                break
        
        assert hook_found, "Weight norm forward hook should be registered for dim=None"
        
        # Verify weight_v has same shape as original weight
        assert module.weight_v.shape == original_weight.shape, \
            f"weight_v shape {module.weight_v.shape} != original weight shape {original_weight.shape}"
        
        # Verify dtype and device
        assert module.weight_g.dtype == torch.float32, "weight_g should be float32"
        assert module.weight_v.dtype == torch.float32, "weight_v should be float32"
        assert module.weight.dtype == torch.float32, "weight should be float32"
        assert module.weight_g.device.type == 'cpu', "weight_g should be on CPU"
        assert module.weight_v.device.type == 'cpu', "weight_v should be on CPU"
    
    # ==== BLOCK:CASE_04 START ====
    # TC-04: 不同参数名称归一化 (DEFERRED - placeholder only)
    # Priority: High, Size: S, Max lines: 70
    # Param matrix: Linear(10,10), name='bias', dim=0, device='cpu', dtype='float32'
    # Weak asserts: returns_module, has_bias_g_param, has_bias_v_param, no_original_bias
    # ==== BLOCK:CASE_04 END ====
    
    # ==== BLOCK:CASE_05 START ====
    # TC-05: 前向传播钩子触发验证 (DEFERRED - placeholder only)
    # Priority: High, Size: M, Max lines: 85
    # Param matrix: Linear(5,8), name='weight', dim=0, device='cpu', dtype='float32'
    # Weak asserts: hook_registered, weight_recomputed_on_forward, output_shape_correct
    # Requires mock: True
    # ==== BLOCK:CASE_05 END ====

# ==== BLOCK:FOOTER START ====
# Footer for test_torch_nn_utils_weight_norm_g1.py
# 
# Additional notes:
# - All tests use fixed random seed for reproducibility
# - Weak assertions are used in epoch 4
# - CASE_04 and CASE_05 are now implemented
# - Mocking is required for CASE_05 (WeightNorm.apply)
# - Epoch: 4/5, active_group: all
# ==== BLOCK:FOOTER END ====