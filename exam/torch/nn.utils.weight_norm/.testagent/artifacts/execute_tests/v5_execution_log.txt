=== Run Tests ===
F.F..                                                                    [100%]
=================================== FAILURES ===================================
___________ TestWeightNormG1.test_weight_norm_linear_default_params ____________

self = <test_torch_nn_utils_weight_norm_g1.TestWeightNormG1 object at 0x14d2bd630>

    def test_weight_norm_linear_default_params(self):
        """TC-01: Linear层默认参数权重归一化
    
        Test weight normalization with default parameters on Linear layer.
        Weak assertions: returns_module, has_g_param, has_v_param, no_original_param
        Strong assertions: weight_reconstruction, forward_hook_registered, param_shapes_correct
        """
        # Set random seed for reproducibility
        torch.manual_seed(42)
    
        # Create Linear module with specified parameters
        in_features = 20
        out_features = 40
        module = nn.Linear(in_features, out_features, bias=True)
    
        # Store original weight for reference
        original_weight = module.weight.detach().clone()
    
        # Apply weight normalization with default parameters
        result = weight_norm(module, name='weight', dim=0)
    
        # Weak assertion 1: returns_module - function returns the module
        assert result is module, "weight_norm should return the input module"
    
        # Weak assertion 2: has_g_param - module has weight_g parameter
        assert hasattr(module, 'weight_g'), "Module should have weight_g parameter after weight_norm"
        assert isinstance(module.weight_g, nn.Parameter), "weight_g should be a Parameter"
    
        # Weak assertion 3: has_v_param - module has weight_v parameter
        assert hasattr(module, 'weight_v'), "Module should have weight_v parameter after weight_norm"
        assert isinstance(module.weight_v, nn.Parameter), "weight_v should be a Parameter"
    
        # Weak assertion 4: no_original_param - original weight parameter is removed
        # Note: weight parameter should still exist but is recomputed from g and v
        # Actually, the original weight is replaced, so we check it's not the same tensor
        assert hasattr(module, 'weight'), "Module should still have weight attribute"
        assert not torch.equal(module.weight, original_weight), "Weight should be recomputed"
    
        # STRONG ASSERTION 1: weight_reconstruction - verify exact reconstruction
        # The weight should be reconstructed as: weight = g * v / ||v|| along dim=0
        v_norm = torch.norm(module.weight_v, dim=1, keepdim=True)  # shape: [out_features, 1]
        eps = 1e-8
        v_normalized = module.weight_v / (v_norm + eps)
        reconstructed_weight = module.weight_g * v_normalized
    
        # Check exact reconstruction (within numerical precision)
        weight_diff = torch.abs(module.weight - reconstructed_weight)
        max_diff = torch.max(weight_diff).item()
        assert max_diff < 1e-6, f"Weight reconstruction error too large: {max_diff}"
    
        # STRONG ASSERTION 2: forward_hook_registered - check hook is registered
        assert hasattr(module, '_forward_pre_hooks'), "Module should have forward pre-hooks"
        assert len(module._forward_pre_hooks) > 0, "At least one forward pre-hook should be registered"
    
        # Check that one of the hooks is for weight recomputation
        hook_found = False
        for hook_id, hook in module._forward_pre_hooks.items():
            # Check if hook is related to weight_norm by checking hook function
            if hasattr(hook, '__name__') and 'weight_norm' in str(hook.__name__).lower():
                hook_found = True
                break
            # Also check by hook type or other attributes
            if hasattr(hook, 'fn') and 'weight_norm' in str(hook.fn).lower():
                hook_found = True
                break
    
>       assert hook_found, "Weight norm forward hook should be registered"
E       AssertionError: Weight norm forward hook should be registered
E       assert False

tests/test_torch_nn_utils_weight_norm_g1.py:90: AssertionError
____________ TestWeightNormG1.test_global_norm_calculation_dim_none ____________

self = <test_torch_nn_utils_weight_norm_g1.TestWeightNormG1 object at 0x14d2bdba0>

    def test_global_norm_calculation_dim_none(self):
        """TC-03: dim=None全局范数计算
    
        Test weight normalization with dim=None (global norm across entire tensor).
        Weak assertions: returns_module, has_g_param, has_v_param, global_norm_applied
        Strong assertions: global_norm_calculation, reconstruction_global, hook_registered
        """
        # Set random seed for reproducibility
        torch.manual_seed(42)
    
        # Create Linear module with specified parameters
        in_features = 15
        out_features = 25
        module = nn.Linear(in_features, out_features, bias=True)
    
        # Store original weight for reference
        original_weight = module.weight.detach().clone()
    
        # Apply weight normalization with dim=None (global norm)
        result = weight_norm(module, name='weight', dim=None)
    
        # Weak assertion 1: returns_module - function returns the module
        assert result is module, "weight_norm should return the input module"
    
        # Weak assertion 2: has_g_param - module has weight_g parameter
        assert hasattr(module, 'weight_g'), "Module should have weight_g parameter after weight_norm"
        assert isinstance(module.weight_g, nn.Parameter), "weight_g should be a Parameter"
    
        # Weak assertion 3: has_v_param - module has weight_v parameter
        assert hasattr(module, 'weight_v'), "Module should have weight_v parameter after weight_norm"
        assert isinstance(module.weight_v, nn.Parameter), "weight_v should be a Parameter"
    
        # Weak assertion 4: global_norm_applied - check that g is scalar (not per-channel)
        # When dim=None, g should be a scalar (1-element tensor)
        assert module.weight_g.numel() == 1, \
            f"For dim=None, weight_g should be scalar, but has shape {module.weight_g.shape}"
    
        # Verify weight_g is scalar (shape [1] or [1, 1])
        assert module.weight_g.dim() <= 2, f"weight_g should be scalar, but has {module.weight_g.dim()} dimensions"
        assert module.weight_g.numel() == 1, "weight_g should have exactly 1 element for dim=None"
    
        # STRONG ASSERTION 1: global_norm_calculation
        # Verify that norm is computed globally (over entire tensor) not per-channel
    
        # Calculate global norm manually
        v_norm_global = torch.norm(module.weight_v)  # scalar norm over entire tensor
    
        # Calculate what per-channel norm would be (for comparison)
        v_norm_per_channel = torch.norm(module.weight_v, dim=1)  # norm per output channel
    
        # Verify that global norm is different from per-channel norms
        # (unless all channels have exactly the same norm, which is unlikely)
        norm_variation = torch.std(v_norm_per_channel).item()
        if norm_variation > 1e-6:  # If channels have different norms
            # Global norm should be different from individual channel norms
            for channel_norm in v_norm_per_channel:
                assert abs(v_norm_global.item() - channel_norm.item()) > 1e-6, \
                    "Global norm should differ from per-channel norms when channels vary"
    
        # STRONG ASSERTION 2: reconstruction_global
        # Verify reconstruction with global norm is accurate
    
        # Avoid division by zero
        eps = 1e-8
        v_normalized = module.weight_v / (v_norm_global + eps)
    
        # Reconstructed weight: g * v_normalized (g is scalar)
        reconstructed_weight = module.weight_g * v_normalized
    
        # Check reconstruction with high precision
        weight_diff = torch.abs(module.weight - reconstructed_weight)
        max_diff = torch.max(weight_diff).item()
        assert max_diff < 1e-6, f"Weight reconstruction error too large for dim=None: {max_diff}"
    
        # Verify reconstruction formula: weight = g * v / ||v||
        # Check that g equals the global norm of the original weight
        original_norm = torch.norm(original_weight).item()
        g_value = module.weight_g.item()
    
        # The g parameter should be close to the original weight norm
        # (allowing for small numerical differences)
        assert abs(g_value - original_norm) < 1e-5, \
            f"g value {g_value} should be close to original weight norm {original_norm}"
    
        # STRONG ASSERTION 3: hook_registered
        # Verify that forward hook is registered for global norm
    
        assert hasattr(module, '_forward_pre_hooks'), "Module should have forward pre-hooks"
        assert len(module._forward_pre_hooks) > 0, "At least one forward pre-hook should be registered"
    
        # Check for weight norm hook
        hook_found = False
        for hook_id, hook in module._forward_pre_hooks.items():
            if hasattr(hook, '__name__') and 'weight_norm' in str(hook.__name__).lower():
                hook_found = True
                break
            if hasattr(hook, 'fn') and 'weight_norm' in str(hook.fn).lower():
                hook_found = True
                break
    
>       assert hook_found, "Weight norm forward hook should be registered for dim=None"
E       AssertionError: Weight norm forward hook should be registered for dim=None
E       assert False

tests/test_torch_nn_utils_weight_norm_g1.py:312: AssertionError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                          Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------------
tests/test_torch_nn_utils_weight_norm_g1.py     131     27     18      6    78%   83-84, 87-88, 94-117, 208->exit, 265->275, 306-307, 309-310, 315-323
tests/test_torch_nn_utils_weight_norm_g2.py      82      4      2      0    95%   120-121, 232-233
-----------------------------------------------------------------------------------------
TOTAL                                           213     31     20      6    84%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_utils_weight_norm_g1.py::TestWeightNormG1::test_weight_norm_linear_default_params
FAILED tests/test_torch_nn_utils_weight_norm_g1.py::TestWeightNormG1::test_global_norm_calculation_dim_none
2 failed, 3 passed in 0.56s

Error: exit 1