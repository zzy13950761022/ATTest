=== Run Tests ===
FFFFFF...FFF..F                                                          [100%]
=================================== FAILURES ===================================
______ test_export_basic_model_to_file[nn.Module-tuple-file-13-True-True] ______

model_type = 'nn.Module', args_format = 'tuple', output_target = 'file'
opset_version = 13, export_params = True, do_constant_folding = True
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
simple_conv_model = SimpleConvModel(
  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
sample_input_tensor = tensor([[ 0.4485,  0.0330,  1.4503],
        [-0.6936,  0.9967,  0.6131]])
sample_input_tensor_2d = tensor([[[[-9.5632e-01, -1.2476e+00, -7.4994e-01, -5.9219e-01,  1.7744e+00,
           -9.2155e-01,  9.6245e-01, -3.37...  7.4018e-01,  1.4162e+00,  6.8340e-01,
           -1.3825e-01,  9.8639e-01, -3.8926e-01,  6.1381e-01, -2.7863e-01]]]])
temp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmp1_1nghk3.onnx'

    @pytest.mark.parametrize(
        "model_type,args_format,output_target,opset_version,export_params,do_constant_folding",
        [
            # Base case from test plan
            ("nn.Module", "tuple", "file", 13, True, True),
            # Parameter extensions from test plan
            ("nn.Module", "named_tuple", "file", 13, True, False),
            ("nn.Module", "tuple", "file", 7, True, True),
            ("nn.Module", "tuple", "file", 16, False, True),
        ]
    )
    def test_export_basic_model_to_file(
        model_type,
        args_format,
        output_target,
        opset_version,
        export_params,
        do_constant_folding,
        simple_linear_model,
        simple_conv_model,
        sample_input_tensor,
        sample_input_tensor_2d,
        temp_onnx_file,
    ):
        """
        Test basic model export to file with various configurations.
    
        This test covers the core export functionality with different:
        - Model types (nn.Module)
        - Args formats (tuple, named_tuple)
        - Opset versions (7, 13, 16)
        - Export parameters (True/False)
        - Constant folding (True/False)
    
        Weak assertions: file exists, file not empty, no exception.
        """
        # Select model based on type
        if model_type == "nn.Module":
            # Use linear model for 1D input, conv model for 2D input
            if args_format == "tuple":
                model = simple_linear_model
                args = (sample_input_tensor,)
            else:  # named_tuple
                model = simple_conv_model
                args = (sample_input_tensor_2d,)
    
        # Prepare args based on format
        if args_format == "named_tuple":
            # Create args with named parameters
            args = (args[0], {"dummy_param": torch.tensor([1.0])})
    
        # Mock dependencies
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace) as mock_trace, \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph) as mock_graph, \
             mock.patch('io.open', mock.mock_open()) as mock_file:
    
            # Call export
            try:
>               onnx_utils.export(
                    model=model,
                    args=args,
                    f=temp_onnx_file,
                    export_params=export_params,
                    opset_version=opset_version,
                    do_constant_folding=do_constant_folding,
                    verbose=False,
                )

tests/test_torch_onnx_utils.py:210: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:504: in export
    _export(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
args = (tensor([[ 0.4485,  0.0330,  1.4503],
        [-0.6936,  0.9967,  0.6131]]),)
f = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmp1_1nghk3.onnx'
export_params = True, verbose = False, training = <TrainingMode.EVAL: 0>
input_names = None, output_names = None
operator_export_type = <OperatorExportTypes.ONNX: 0>
export_type = 'Saves model in the specified protobuf file.', opset_version = 13
do_constant_folding = True, dynamic_axes = {}
keep_initializers_as_inputs = None, fixed_batch_size = False, custom_opsets = {}
add_node_names = True, onnx_shape_inference = True
export_modules_as_functions = False

    @_beartype.beartype
    def _export(
        model,
        args,
        f,
        export_params=True,
        verbose=False,
        training=_C_onnx.TrainingMode.EVAL,
        input_names=None,
        output_names=None,
        operator_export_type=_C_onnx.OperatorExportTypes.ONNX,
        export_type=None,
        opset_version=None,
        do_constant_folding=True,
        dynamic_axes=None,
        keep_initializers_as_inputs=None,
        fixed_batch_size=False,
        custom_opsets=None,
        add_node_names=True,
        onnx_shape_inference=True,
        export_modules_as_functions=False,
    ):
        assert GLOBALS.in_onnx_export is False
    
        if export_type is None:
            export_type = _exporter_states.ExportTypes.PROTOBUF_FILE
    
        if isinstance(model, torch.nn.DataParallel):
            raise ValueError(
                "torch.nn.DataParallel is not supported by ONNX "
                "exporter, please use 'attribute' module to "
                "unwrap model from torch.nn.DataParallel. Try "
                "torch.onnx.export(model.module, ...)"
            )
    
        GLOBALS.onnx_shape_inference = onnx_shape_inference
    
        if opset_version is None:
            opset_version = _constants.ONNX_DEFAULT_OPSET
    
        if export_modules_as_functions and opset_version < 15:
            raise ValueError(
                "`export_modules_as_functions` is not supported for `opset_version` < 15."
                "This is because `opset_version` < 15 implies IR version < 8, which means "
                "no local function support. "
            )
        if not operator_export_type:
            if _C_onnx._CAFFE2_ATEN_FALLBACK:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK
            else:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX
    
        # By default, training=TrainingMode.EVAL,
        # which is good because running a model in training mode could result in
        # internal buffers getting updated, dropout getting applied, etc.
        # If you really know what you're doing, you can turn
        # training=TrainingMode.TRAINING or training=TrainingMode.PRESERVE,
        # (to preserve whatever the original training mode was.)
        GLOBALS.export_onnx_opset_version = opset_version
        GLOBALS.operator_export_type = operator_export_type
    
        try:
            GLOBALS.in_onnx_export = True
    
            module_typenames_to_export_as_functions: Set[str] = set()
            if isinstance(model, (torch.nn.Module, torch.jit.ScriptModule)):
                module_typenames_to_export_as_functions = _setup_trace_module_map(
                    model, export_modules_as_functions
                )
    
            with exporter_context(model, training, verbose):
                val_keep_init_as_ip = _decide_keep_init_as_input(
                    keep_initializers_as_inputs,
                    operator_export_type,
                    opset_version,
                )
                val_add_node_names = _decide_add_node_names(
                    add_node_names, operator_export_type
                )
                val_do_constant_folding = _decide_constant_folding(
                    do_constant_folding, operator_export_type, training
                )
                # Normally f can be a file-like object, but for large models, the external data format requires a
                # valid `model_file_location`. Code in export.cpp will enforce this.
                if isinstance(f, str):
                    model_file_location = f
                else:
                    model_file_location = ""
                args = _decide_input_format(model, args)
                if dynamic_axes is None:
                    dynamic_axes = {}
                _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)
    
                graph, params_dict, torch_out = _model_to_graph(
                    model,
                    args,
                    verbose,
                    input_names,
                    output_names,
                    operator_export_type,
                    val_do_constant_folding,
                    fixed_batch_size=fixed_batch_size,
                    training=training,
                    dynamic_axes=dynamic_axes,
                )
    
                # TODO: Don't allocate a in-memory string for the protobuf
                defer_weight_export = (
                    export_type is not _exporter_states.ExportTypes.PROTOBUF_FILE
                )
                if custom_opsets is None:
                    custom_opsets = {}
    
>               _C._jit_pass_dce_allow_deleting_nodes_with_side_effects(graph)
E               TypeError: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f864430>

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:1549: TypeError

During handling of the above exception, another exception occurred:

model_type = 'nn.Module', args_format = 'tuple', output_target = 'file'
opset_version = 13, export_params = True, do_constant_folding = True
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
simple_conv_model = SimpleConvModel(
  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
sample_input_tensor = tensor([[ 0.4485,  0.0330,  1.4503],
        [-0.6936,  0.9967,  0.6131]])
sample_input_tensor_2d = tensor([[[[-9.5632e-01, -1.2476e+00, -7.4994e-01, -5.9219e-01,  1.7744e+00,
           -9.2155e-01,  9.6245e-01, -3.37...  7.4018e-01,  1.4162e+00,  6.8340e-01,
           -1.3825e-01,  9.8639e-01, -3.8926e-01,  6.1381e-01, -2.7863e-01]]]])
temp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmp1_1nghk3.onnx'

    @pytest.mark.parametrize(
        "model_type,args_format,output_target,opset_version,export_params,do_constant_folding",
        [
            # Base case from test plan
            ("nn.Module", "tuple", "file", 13, True, True),
            # Parameter extensions from test plan
            ("nn.Module", "named_tuple", "file", 13, True, False),
            ("nn.Module", "tuple", "file", 7, True, True),
            ("nn.Module", "tuple", "file", 16, False, True),
        ]
    )
    def test_export_basic_model_to_file(
        model_type,
        args_format,
        output_target,
        opset_version,
        export_params,
        do_constant_folding,
        simple_linear_model,
        simple_conv_model,
        sample_input_tensor,
        sample_input_tensor_2d,
        temp_onnx_file,
    ):
        """
        Test basic model export to file with various configurations.
    
        This test covers the core export functionality with different:
        - Model types (nn.Module)
        - Args formats (tuple, named_tuple)
        - Opset versions (7, 13, 16)
        - Export parameters (True/False)
        - Constant folding (True/False)
    
        Weak assertions: file exists, file not empty, no exception.
        """
        # Select model based on type
        if model_type == "nn.Module":
            # Use linear model for 1D input, conv model for 2D input
            if args_format == "tuple":
                model = simple_linear_model
                args = (sample_input_tensor,)
            else:  # named_tuple
                model = simple_conv_model
                args = (sample_input_tensor_2d,)
    
        # Prepare args based on format
        if args_format == "named_tuple":
            # Create args with named parameters
            args = (args[0], {"dummy_param": torch.tensor([1.0])})
    
        # Mock dependencies
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace) as mock_trace, \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph) as mock_graph, \
             mock.patch('io.open', mock.mock_open()) as mock_file:
    
            # Call export
            try:
                onnx_utils.export(
                    model=model,
                    args=args,
                    f=temp_onnx_file,
                    export_params=export_params,
                    opset_version=opset_version,
                    do_constant_folding=do_constant_folding,
                    verbose=False,
                )
    
                # Weak assertion 1: No exception raised
                # (implicitly passed if we reach here)
    
                # Weak assertion 2: File exists and is not empty
                # Note: Since we're mocking io.open, we need to check mock calls
                assert mock_file.called, "File was not opened for writing"
    
                # Check that write was called (indicating content was written)
                write_calls = [call for call in mock_file.mock_calls if call[0] == '().write']
                assert len(write_calls) > 0, "No data was written to file"
    
                # Weak assertion 3: Trace was called for nn.Module
                if model_type == "nn.Module":
                    assert mock_trace.called, "torch.jit.trace should be called for nn.Module"
    
                # Weak assertion 4: Graph conversion was called
                assert mock_graph.called, "_model_to_graph should be called"
    
                # Additional weak checks
                assert opset_version >= 7 and opset_version <= 16, \
                    f"Opset version {opset_version} should be in range 7-16"
    
            except Exception as e:
>               pytest.fail(f"Export raised unexpected exception: {e}")
E               Failed: Export raised unexpected exception: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f864430>

tests/test_torch_onnx_utils.py:243: Failed
__ test_export_basic_model_to_file[nn.Module-named_tuple-file-13-True-False] ___

model_type = 'nn.Module', args_format = 'named_tuple', output_target = 'file'
opset_version = 13, export_params = True, do_constant_folding = False
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
simple_conv_model = SimpleConvModel(
  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
sample_input_tensor = tensor([[ 0.1005,  0.2325,  0.7064],
        [ 0.3693, -0.8368, -1.2204]])
sample_input_tensor_2d = tensor([[[[-1.3386e-01, -5.8557e-02,  1.2574e-01, -5.5258e-01,  7.4480e-02,
           -1.4929e-01, -5.5225e-01, -9.34...  4.5689e-01, -6.0341e-01,  4.6268e-01,
            2.4934e-01, -1.7062e-01, -6.1410e-01,  5.3551e-01, -1.6644e+00]]]])
temp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpvlw7l6g2.onnx'

    @pytest.mark.parametrize(
        "model_type,args_format,output_target,opset_version,export_params,do_constant_folding",
        [
            # Base case from test plan
            ("nn.Module", "tuple", "file", 13, True, True),
            # Parameter extensions from test plan
            ("nn.Module", "named_tuple", "file", 13, True, False),
            ("nn.Module", "tuple", "file", 7, True, True),
            ("nn.Module", "tuple", "file", 16, False, True),
        ]
    )
    def test_export_basic_model_to_file(
        model_type,
        args_format,
        output_target,
        opset_version,
        export_params,
        do_constant_folding,
        simple_linear_model,
        simple_conv_model,
        sample_input_tensor,
        sample_input_tensor_2d,
        temp_onnx_file,
    ):
        """
        Test basic model export to file with various configurations.
    
        This test covers the core export functionality with different:
        - Model types (nn.Module)
        - Args formats (tuple, named_tuple)
        - Opset versions (7, 13, 16)
        - Export parameters (True/False)
        - Constant folding (True/False)
    
        Weak assertions: file exists, file not empty, no exception.
        """
        # Select model based on type
        if model_type == "nn.Module":
            # Use linear model for 1D input, conv model for 2D input
            if args_format == "tuple":
                model = simple_linear_model
                args = (sample_input_tensor,)
            else:  # named_tuple
                model = simple_conv_model
                args = (sample_input_tensor_2d,)
    
        # Prepare args based on format
        if args_format == "named_tuple":
            # Create args with named parameters
            args = (args[0], {"dummy_param": torch.tensor([1.0])})
    
        # Mock dependencies
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace) as mock_trace, \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph) as mock_graph, \
             mock.patch('io.open', mock.mock_open()) as mock_file:
    
            # Call export
            try:
>               onnx_utils.export(
                    model=model,
                    args=args,
                    f=temp_onnx_file,
                    export_params=export_params,
                    opset_version=opset_version,
                    do_constant_folding=do_constant_folding,
                    verbose=False,
                )

tests/test_torch_onnx_utils.py:210: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:504: in export
    _export(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = SimpleConvModel(
  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
args = (tensor([[[[-1.3386e-01, -5.8557e-02,  1.2574e-01, -5.5258e-01,  7.4480e-02,
           -1.4929e-01, -5.5225e-01, -9.3...4.5689e-01, -6.0341e-01,  4.6268e-01,
            2.4934e-01, -1.7062e-01, -6.1410e-01,  5.3551e-01, -1.6644e+00]]]]),)
f = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpvlw7l6g2.onnx'
export_params = True, verbose = False, training = <TrainingMode.EVAL: 0>
input_names = None, output_names = None
operator_export_type = <OperatorExportTypes.ONNX: 0>
export_type = 'Saves model in the specified protobuf file.', opset_version = 13
do_constant_folding = False, dynamic_axes = {}
keep_initializers_as_inputs = None, fixed_batch_size = False, custom_opsets = {}
add_node_names = True, onnx_shape_inference = True
export_modules_as_functions = False

    @_beartype.beartype
    def _export(
        model,
        args,
        f,
        export_params=True,
        verbose=False,
        training=_C_onnx.TrainingMode.EVAL,
        input_names=None,
        output_names=None,
        operator_export_type=_C_onnx.OperatorExportTypes.ONNX,
        export_type=None,
        opset_version=None,
        do_constant_folding=True,
        dynamic_axes=None,
        keep_initializers_as_inputs=None,
        fixed_batch_size=False,
        custom_opsets=None,
        add_node_names=True,
        onnx_shape_inference=True,
        export_modules_as_functions=False,
    ):
        assert GLOBALS.in_onnx_export is False
    
        if export_type is None:
            export_type = _exporter_states.ExportTypes.PROTOBUF_FILE
    
        if isinstance(model, torch.nn.DataParallel):
            raise ValueError(
                "torch.nn.DataParallel is not supported by ONNX "
                "exporter, please use 'attribute' module to "
                "unwrap model from torch.nn.DataParallel. Try "
                "torch.onnx.export(model.module, ...)"
            )
    
        GLOBALS.onnx_shape_inference = onnx_shape_inference
    
        if opset_version is None:
            opset_version = _constants.ONNX_DEFAULT_OPSET
    
        if export_modules_as_functions and opset_version < 15:
            raise ValueError(
                "`export_modules_as_functions` is not supported for `opset_version` < 15."
                "This is because `opset_version` < 15 implies IR version < 8, which means "
                "no local function support. "
            )
        if not operator_export_type:
            if _C_onnx._CAFFE2_ATEN_FALLBACK:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK
            else:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX
    
        # By default, training=TrainingMode.EVAL,
        # which is good because running a model in training mode could result in
        # internal buffers getting updated, dropout getting applied, etc.
        # If you really know what you're doing, you can turn
        # training=TrainingMode.TRAINING or training=TrainingMode.PRESERVE,
        # (to preserve whatever the original training mode was.)
        GLOBALS.export_onnx_opset_version = opset_version
        GLOBALS.operator_export_type = operator_export_type
    
        try:
            GLOBALS.in_onnx_export = True
    
            module_typenames_to_export_as_functions: Set[str] = set()
            if isinstance(model, (torch.nn.Module, torch.jit.ScriptModule)):
                module_typenames_to_export_as_functions = _setup_trace_module_map(
                    model, export_modules_as_functions
                )
    
            with exporter_context(model, training, verbose):
                val_keep_init_as_ip = _decide_keep_init_as_input(
                    keep_initializers_as_inputs,
                    operator_export_type,
                    opset_version,
                )
                val_add_node_names = _decide_add_node_names(
                    add_node_names, operator_export_type
                )
                val_do_constant_folding = _decide_constant_folding(
                    do_constant_folding, operator_export_type, training
                )
                # Normally f can be a file-like object, but for large models, the external data format requires a
                # valid `model_file_location`. Code in export.cpp will enforce this.
                if isinstance(f, str):
                    model_file_location = f
                else:
                    model_file_location = ""
                args = _decide_input_format(model, args)
                if dynamic_axes is None:
                    dynamic_axes = {}
                _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)
    
                graph, params_dict, torch_out = _model_to_graph(
                    model,
                    args,
                    verbose,
                    input_names,
                    output_names,
                    operator_export_type,
                    val_do_constant_folding,
                    fixed_batch_size=fixed_batch_size,
                    training=training,
                    dynamic_axes=dynamic_axes,
                )
    
                # TODO: Don't allocate a in-memory string for the protobuf
                defer_weight_export = (
                    export_type is not _exporter_states.ExportTypes.PROTOBUF_FILE
                )
                if custom_opsets is None:
                    custom_opsets = {}
    
>               _C._jit_pass_dce_allow_deleting_nodes_with_side_effects(graph)
E               TypeError: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f999a50>

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:1549: TypeError

During handling of the above exception, another exception occurred:

model_type = 'nn.Module', args_format = 'named_tuple', output_target = 'file'
opset_version = 13, export_params = True, do_constant_folding = False
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
simple_conv_model = SimpleConvModel(
  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
sample_input_tensor = tensor([[ 0.1005,  0.2325,  0.7064],
        [ 0.3693, -0.8368, -1.2204]])
sample_input_tensor_2d = tensor([[[[-1.3386e-01, -5.8557e-02,  1.2574e-01, -5.5258e-01,  7.4480e-02,
           -1.4929e-01, -5.5225e-01, -9.34...  4.5689e-01, -6.0341e-01,  4.6268e-01,
            2.4934e-01, -1.7062e-01, -6.1410e-01,  5.3551e-01, -1.6644e+00]]]])
temp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpvlw7l6g2.onnx'

    @pytest.mark.parametrize(
        "model_type,args_format,output_target,opset_version,export_params,do_constant_folding",
        [
            # Base case from test plan
            ("nn.Module", "tuple", "file", 13, True, True),
            # Parameter extensions from test plan
            ("nn.Module", "named_tuple", "file", 13, True, False),
            ("nn.Module", "tuple", "file", 7, True, True),
            ("nn.Module", "tuple", "file", 16, False, True),
        ]
    )
    def test_export_basic_model_to_file(
        model_type,
        args_format,
        output_target,
        opset_version,
        export_params,
        do_constant_folding,
        simple_linear_model,
        simple_conv_model,
        sample_input_tensor,
        sample_input_tensor_2d,
        temp_onnx_file,
    ):
        """
        Test basic model export to file with various configurations.
    
        This test covers the core export functionality with different:
        - Model types (nn.Module)
        - Args formats (tuple, named_tuple)
        - Opset versions (7, 13, 16)
        - Export parameters (True/False)
        - Constant folding (True/False)
    
        Weak assertions: file exists, file not empty, no exception.
        """
        # Select model based on type
        if model_type == "nn.Module":
            # Use linear model for 1D input, conv model for 2D input
            if args_format == "tuple":
                model = simple_linear_model
                args = (sample_input_tensor,)
            else:  # named_tuple
                model = simple_conv_model
                args = (sample_input_tensor_2d,)
    
        # Prepare args based on format
        if args_format == "named_tuple":
            # Create args with named parameters
            args = (args[0], {"dummy_param": torch.tensor([1.0])})
    
        # Mock dependencies
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace) as mock_trace, \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph) as mock_graph, \
             mock.patch('io.open', mock.mock_open()) as mock_file:
    
            # Call export
            try:
                onnx_utils.export(
                    model=model,
                    args=args,
                    f=temp_onnx_file,
                    export_params=export_params,
                    opset_version=opset_version,
                    do_constant_folding=do_constant_folding,
                    verbose=False,
                )
    
                # Weak assertion 1: No exception raised
                # (implicitly passed if we reach here)
    
                # Weak assertion 2: File exists and is not empty
                # Note: Since we're mocking io.open, we need to check mock calls
                assert mock_file.called, "File was not opened for writing"
    
                # Check that write was called (indicating content was written)
                write_calls = [call for call in mock_file.mock_calls if call[0] == '().write']
                assert len(write_calls) > 0, "No data was written to file"
    
                # Weak assertion 3: Trace was called for nn.Module
                if model_type == "nn.Module":
                    assert mock_trace.called, "torch.jit.trace should be called for nn.Module"
    
                # Weak assertion 4: Graph conversion was called
                assert mock_graph.called, "_model_to_graph should be called"
    
                # Additional weak checks
                assert opset_version >= 7 and opset_version <= 16, \
                    f"Opset version {opset_version} should be in range 7-16"
    
            except Exception as e:
>               pytest.fail(f"Export raised unexpected exception: {e}")
E               Failed: Export raised unexpected exception: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f999a50>

tests/test_torch_onnx_utils.py:243: Failed
______ test_export_basic_model_to_file[nn.Module-tuple-file-7-True-True] _______

model_type = 'nn.Module', args_format = 'tuple', output_target = 'file'
opset_version = 7, export_params = True, do_constant_folding = True
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
simple_conv_model = SimpleConvModel(
  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
sample_input_tensor = tensor([[-0.2662,  1.0295,  0.1327],
        [ 1.8114, -1.0315,  1.1471]])
sample_input_tensor_2d = tensor([[[[-0.8443, -0.3292, -0.1140, -0.8452,  0.5263,  0.1255, -1.7527,
            0.7752,  0.2892,  0.3520],
     ...9],
          [ 0.1342, -1.1246, -1.8830,  1.6258, -0.7038,  0.5581,  0.2723,
           -0.4286,  0.8240, -1.3990]]]])
temp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpyzcq4qwz.onnx'

    @pytest.mark.parametrize(
        "model_type,args_format,output_target,opset_version,export_params,do_constant_folding",
        [
            # Base case from test plan
            ("nn.Module", "tuple", "file", 13, True, True),
            # Parameter extensions from test plan
            ("nn.Module", "named_tuple", "file", 13, True, False),
            ("nn.Module", "tuple", "file", 7, True, True),
            ("nn.Module", "tuple", "file", 16, False, True),
        ]
    )
    def test_export_basic_model_to_file(
        model_type,
        args_format,
        output_target,
        opset_version,
        export_params,
        do_constant_folding,
        simple_linear_model,
        simple_conv_model,
        sample_input_tensor,
        sample_input_tensor_2d,
        temp_onnx_file,
    ):
        """
        Test basic model export to file with various configurations.
    
        This test covers the core export functionality with different:
        - Model types (nn.Module)
        - Args formats (tuple, named_tuple)
        - Opset versions (7, 13, 16)
        - Export parameters (True/False)
        - Constant folding (True/False)
    
        Weak assertions: file exists, file not empty, no exception.
        """
        # Select model based on type
        if model_type == "nn.Module":
            # Use linear model for 1D input, conv model for 2D input
            if args_format == "tuple":
                model = simple_linear_model
                args = (sample_input_tensor,)
            else:  # named_tuple
                model = simple_conv_model
                args = (sample_input_tensor_2d,)
    
        # Prepare args based on format
        if args_format == "named_tuple":
            # Create args with named parameters
            args = (args[0], {"dummy_param": torch.tensor([1.0])})
    
        # Mock dependencies
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace) as mock_trace, \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph) as mock_graph, \
             mock.patch('io.open', mock.mock_open()) as mock_file:
    
            # Call export
            try:
>               onnx_utils.export(
                    model=model,
                    args=args,
                    f=temp_onnx_file,
                    export_params=export_params,
                    opset_version=opset_version,
                    do_constant_folding=do_constant_folding,
                    verbose=False,
                )

tests/test_torch_onnx_utils.py:210: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:504: in export
    _export(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
args = (tensor([[-0.2662,  1.0295,  0.1327],
        [ 1.8114, -1.0315,  1.1471]]),)
f = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpyzcq4qwz.onnx'
export_params = True, verbose = False, training = <TrainingMode.EVAL: 0>
input_names = None, output_names = None
operator_export_type = <OperatorExportTypes.ONNX: 0>
export_type = 'Saves model in the specified protobuf file.', opset_version = 7
do_constant_folding = True, dynamic_axes = {}
keep_initializers_as_inputs = None, fixed_batch_size = False, custom_opsets = {}
add_node_names = True, onnx_shape_inference = True
export_modules_as_functions = False

    @_beartype.beartype
    def _export(
        model,
        args,
        f,
        export_params=True,
        verbose=False,
        training=_C_onnx.TrainingMode.EVAL,
        input_names=None,
        output_names=None,
        operator_export_type=_C_onnx.OperatorExportTypes.ONNX,
        export_type=None,
        opset_version=None,
        do_constant_folding=True,
        dynamic_axes=None,
        keep_initializers_as_inputs=None,
        fixed_batch_size=False,
        custom_opsets=None,
        add_node_names=True,
        onnx_shape_inference=True,
        export_modules_as_functions=False,
    ):
        assert GLOBALS.in_onnx_export is False
    
        if export_type is None:
            export_type = _exporter_states.ExportTypes.PROTOBUF_FILE
    
        if isinstance(model, torch.nn.DataParallel):
            raise ValueError(
                "torch.nn.DataParallel is not supported by ONNX "
                "exporter, please use 'attribute' module to "
                "unwrap model from torch.nn.DataParallel. Try "
                "torch.onnx.export(model.module, ...)"
            )
    
        GLOBALS.onnx_shape_inference = onnx_shape_inference
    
        if opset_version is None:
            opset_version = _constants.ONNX_DEFAULT_OPSET
    
        if export_modules_as_functions and opset_version < 15:
            raise ValueError(
                "`export_modules_as_functions` is not supported for `opset_version` < 15."
                "This is because `opset_version` < 15 implies IR version < 8, which means "
                "no local function support. "
            )
        if not operator_export_type:
            if _C_onnx._CAFFE2_ATEN_FALLBACK:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK
            else:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX
    
        # By default, training=TrainingMode.EVAL,
        # which is good because running a model in training mode could result in
        # internal buffers getting updated, dropout getting applied, etc.
        # If you really know what you're doing, you can turn
        # training=TrainingMode.TRAINING or training=TrainingMode.PRESERVE,
        # (to preserve whatever the original training mode was.)
        GLOBALS.export_onnx_opset_version = opset_version
        GLOBALS.operator_export_type = operator_export_type
    
        try:
            GLOBALS.in_onnx_export = True
    
            module_typenames_to_export_as_functions: Set[str] = set()
            if isinstance(model, (torch.nn.Module, torch.jit.ScriptModule)):
                module_typenames_to_export_as_functions = _setup_trace_module_map(
                    model, export_modules_as_functions
                )
    
            with exporter_context(model, training, verbose):
                val_keep_init_as_ip = _decide_keep_init_as_input(
                    keep_initializers_as_inputs,
                    operator_export_type,
                    opset_version,
                )
                val_add_node_names = _decide_add_node_names(
                    add_node_names, operator_export_type
                )
                val_do_constant_folding = _decide_constant_folding(
                    do_constant_folding, operator_export_type, training
                )
                # Normally f can be a file-like object, but for large models, the external data format requires a
                # valid `model_file_location`. Code in export.cpp will enforce this.
                if isinstance(f, str):
                    model_file_location = f
                else:
                    model_file_location = ""
                args = _decide_input_format(model, args)
                if dynamic_axes is None:
                    dynamic_axes = {}
                _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)
    
                graph, params_dict, torch_out = _model_to_graph(
                    model,
                    args,
                    verbose,
                    input_names,
                    output_names,
                    operator_export_type,
                    val_do_constant_folding,
                    fixed_batch_size=fixed_batch_size,
                    training=training,
                    dynamic_axes=dynamic_axes,
                )
    
                # TODO: Don't allocate a in-memory string for the protobuf
                defer_weight_export = (
                    export_type is not _exporter_states.ExportTypes.PROTOBUF_FILE
                )
                if custom_opsets is None:
                    custom_opsets = {}
    
>               _C._jit_pass_dce_allow_deleting_nodes_with_side_effects(graph)
E               TypeError: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f99db10>

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:1549: TypeError

During handling of the above exception, another exception occurred:

model_type = 'nn.Module', args_format = 'tuple', output_target = 'file'
opset_version = 7, export_params = True, do_constant_folding = True
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
simple_conv_model = SimpleConvModel(
  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
sample_input_tensor = tensor([[-0.2662,  1.0295,  0.1327],
        [ 1.8114, -1.0315,  1.1471]])
sample_input_tensor_2d = tensor([[[[-0.8443, -0.3292, -0.1140, -0.8452,  0.5263,  0.1255, -1.7527,
            0.7752,  0.2892,  0.3520],
     ...9],
          [ 0.1342, -1.1246, -1.8830,  1.6258, -0.7038,  0.5581,  0.2723,
           -0.4286,  0.8240, -1.3990]]]])
temp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpyzcq4qwz.onnx'

    @pytest.mark.parametrize(
        "model_type,args_format,output_target,opset_version,export_params,do_constant_folding",
        [
            # Base case from test plan
            ("nn.Module", "tuple", "file", 13, True, True),
            # Parameter extensions from test plan
            ("nn.Module", "named_tuple", "file", 13, True, False),
            ("nn.Module", "tuple", "file", 7, True, True),
            ("nn.Module", "tuple", "file", 16, False, True),
        ]
    )
    def test_export_basic_model_to_file(
        model_type,
        args_format,
        output_target,
        opset_version,
        export_params,
        do_constant_folding,
        simple_linear_model,
        simple_conv_model,
        sample_input_tensor,
        sample_input_tensor_2d,
        temp_onnx_file,
    ):
        """
        Test basic model export to file with various configurations.
    
        This test covers the core export functionality with different:
        - Model types (nn.Module)
        - Args formats (tuple, named_tuple)
        - Opset versions (7, 13, 16)
        - Export parameters (True/False)
        - Constant folding (True/False)
    
        Weak assertions: file exists, file not empty, no exception.
        """
        # Select model based on type
        if model_type == "nn.Module":
            # Use linear model for 1D input, conv model for 2D input
            if args_format == "tuple":
                model = simple_linear_model
                args = (sample_input_tensor,)
            else:  # named_tuple
                model = simple_conv_model
                args = (sample_input_tensor_2d,)
    
        # Prepare args based on format
        if args_format == "named_tuple":
            # Create args with named parameters
            args = (args[0], {"dummy_param": torch.tensor([1.0])})
    
        # Mock dependencies
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace) as mock_trace, \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph) as mock_graph, \
             mock.patch('io.open', mock.mock_open()) as mock_file:
    
            # Call export
            try:
                onnx_utils.export(
                    model=model,
                    args=args,
                    f=temp_onnx_file,
                    export_params=export_params,
                    opset_version=opset_version,
                    do_constant_folding=do_constant_folding,
                    verbose=False,
                )
    
                # Weak assertion 1: No exception raised
                # (implicitly passed if we reach here)
    
                # Weak assertion 2: File exists and is not empty
                # Note: Since we're mocking io.open, we need to check mock calls
                assert mock_file.called, "File was not opened for writing"
    
                # Check that write was called (indicating content was written)
                write_calls = [call for call in mock_file.mock_calls if call[0] == '().write']
                assert len(write_calls) > 0, "No data was written to file"
    
                # Weak assertion 3: Trace was called for nn.Module
                if model_type == "nn.Module":
                    assert mock_trace.called, "torch.jit.trace should be called for nn.Module"
    
                # Weak assertion 4: Graph conversion was called
                assert mock_graph.called, "_model_to_graph should be called"
    
                # Additional weak checks
                assert opset_version >= 7 and opset_version <= 16, \
                    f"Opset version {opset_version} should be in range 7-16"
    
            except Exception as e:
>               pytest.fail(f"Export raised unexpected exception: {e}")
E               Failed: Export raised unexpected exception: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f99db10>

tests/test_torch_onnx_utils.py:243: Failed
_____ test_export_basic_model_to_file[nn.Module-tuple-file-16-False-True] ______

model_type = 'nn.Module', args_format = 'tuple', output_target = 'file'
opset_version = 16, export_params = False, do_constant_folding = True
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
simple_conv_model = SimpleConvModel(
  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
sample_input_tensor = tensor([[-1.7534, -1.3003,  0.5800],
        [ 0.0158, -1.6756,  1.0759]])
sample_input_tensor_2d = tensor([[[[-0.4210, -0.6637,  1.4599, -1.9324,  0.2951,  1.9531,  0.0623,
           -1.1079, -0.0518,  0.4115],
     ...7],
          [ 1.5941,  2.5384, -0.4962,  1.5591, -1.2024, -0.6351,  0.2057,
            1.2085, -1.2975, -1.7842]]]])
temp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmp3y1vq9tb.onnx'

    @pytest.mark.parametrize(
        "model_type,args_format,output_target,opset_version,export_params,do_constant_folding",
        [
            # Base case from test plan
            ("nn.Module", "tuple", "file", 13, True, True),
            # Parameter extensions from test plan
            ("nn.Module", "named_tuple", "file", 13, True, False),
            ("nn.Module", "tuple", "file", 7, True, True),
            ("nn.Module", "tuple", "file", 16, False, True),
        ]
    )
    def test_export_basic_model_to_file(
        model_type,
        args_format,
        output_target,
        opset_version,
        export_params,
        do_constant_folding,
        simple_linear_model,
        simple_conv_model,
        sample_input_tensor,
        sample_input_tensor_2d,
        temp_onnx_file,
    ):
        """
        Test basic model export to file with various configurations.
    
        This test covers the core export functionality with different:
        - Model types (nn.Module)
        - Args formats (tuple, named_tuple)
        - Opset versions (7, 13, 16)
        - Export parameters (True/False)
        - Constant folding (True/False)
    
        Weak assertions: file exists, file not empty, no exception.
        """
        # Select model based on type
        if model_type == "nn.Module":
            # Use linear model for 1D input, conv model for 2D input
            if args_format == "tuple":
                model = simple_linear_model
                args = (sample_input_tensor,)
            else:  # named_tuple
                model = simple_conv_model
                args = (sample_input_tensor_2d,)
    
        # Prepare args based on format
        if args_format == "named_tuple":
            # Create args with named parameters
            args = (args[0], {"dummy_param": torch.tensor([1.0])})
    
        # Mock dependencies
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace) as mock_trace, \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph) as mock_graph, \
             mock.patch('io.open', mock.mock_open()) as mock_file:
    
            # Call export
            try:
>               onnx_utils.export(
                    model=model,
                    args=args,
                    f=temp_onnx_file,
                    export_params=export_params,
                    opset_version=opset_version,
                    do_constant_folding=do_constant_folding,
                    verbose=False,
                )

tests/test_torch_onnx_utils.py:210: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:504: in export
    _export(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
args = (tensor([[-1.7534, -1.3003,  0.5800],
        [ 0.0158, -1.6756,  1.0759]]),)
f = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmp3y1vq9tb.onnx'
export_params = False, verbose = False, training = <TrainingMode.EVAL: 0>
input_names = None, output_names = None
operator_export_type = <OperatorExportTypes.ONNX: 0>
export_type = 'Saves model in the specified protobuf file.', opset_version = 16
do_constant_folding = True, dynamic_axes = {}
keep_initializers_as_inputs = None, fixed_batch_size = False, custom_opsets = {}
add_node_names = True, onnx_shape_inference = True
export_modules_as_functions = False

    @_beartype.beartype
    def _export(
        model,
        args,
        f,
        export_params=True,
        verbose=False,
        training=_C_onnx.TrainingMode.EVAL,
        input_names=None,
        output_names=None,
        operator_export_type=_C_onnx.OperatorExportTypes.ONNX,
        export_type=None,
        opset_version=None,
        do_constant_folding=True,
        dynamic_axes=None,
        keep_initializers_as_inputs=None,
        fixed_batch_size=False,
        custom_opsets=None,
        add_node_names=True,
        onnx_shape_inference=True,
        export_modules_as_functions=False,
    ):
        assert GLOBALS.in_onnx_export is False
    
        if export_type is None:
            export_type = _exporter_states.ExportTypes.PROTOBUF_FILE
    
        if isinstance(model, torch.nn.DataParallel):
            raise ValueError(
                "torch.nn.DataParallel is not supported by ONNX "
                "exporter, please use 'attribute' module to "
                "unwrap model from torch.nn.DataParallel. Try "
                "torch.onnx.export(model.module, ...)"
            )
    
        GLOBALS.onnx_shape_inference = onnx_shape_inference
    
        if opset_version is None:
            opset_version = _constants.ONNX_DEFAULT_OPSET
    
        if export_modules_as_functions and opset_version < 15:
            raise ValueError(
                "`export_modules_as_functions` is not supported for `opset_version` < 15."
                "This is because `opset_version` < 15 implies IR version < 8, which means "
                "no local function support. "
            )
        if not operator_export_type:
            if _C_onnx._CAFFE2_ATEN_FALLBACK:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK
            else:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX
    
        # By default, training=TrainingMode.EVAL,
        # which is good because running a model in training mode could result in
        # internal buffers getting updated, dropout getting applied, etc.
        # If you really know what you're doing, you can turn
        # training=TrainingMode.TRAINING or training=TrainingMode.PRESERVE,
        # (to preserve whatever the original training mode was.)
        GLOBALS.export_onnx_opset_version = opset_version
        GLOBALS.operator_export_type = operator_export_type
    
        try:
            GLOBALS.in_onnx_export = True
    
            module_typenames_to_export_as_functions: Set[str] = set()
            if isinstance(model, (torch.nn.Module, torch.jit.ScriptModule)):
                module_typenames_to_export_as_functions = _setup_trace_module_map(
                    model, export_modules_as_functions
                )
    
            with exporter_context(model, training, verbose):
                val_keep_init_as_ip = _decide_keep_init_as_input(
                    keep_initializers_as_inputs,
                    operator_export_type,
                    opset_version,
                )
                val_add_node_names = _decide_add_node_names(
                    add_node_names, operator_export_type
                )
                val_do_constant_folding = _decide_constant_folding(
                    do_constant_folding, operator_export_type, training
                )
                # Normally f can be a file-like object, but for large models, the external data format requires a
                # valid `model_file_location`. Code in export.cpp will enforce this.
                if isinstance(f, str):
                    model_file_location = f
                else:
                    model_file_location = ""
                args = _decide_input_format(model, args)
                if dynamic_axes is None:
                    dynamic_axes = {}
                _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)
    
                graph, params_dict, torch_out = _model_to_graph(
                    model,
                    args,
                    verbose,
                    input_names,
                    output_names,
                    operator_export_type,
                    val_do_constant_folding,
                    fixed_batch_size=fixed_batch_size,
                    training=training,
                    dynamic_axes=dynamic_axes,
                )
    
                # TODO: Don't allocate a in-memory string for the protobuf
                defer_weight_export = (
                    export_type is not _exporter_states.ExportTypes.PROTOBUF_FILE
                )
                if custom_opsets is None:
                    custom_opsets = {}
    
>               _C._jit_pass_dce_allow_deleting_nodes_with_side_effects(graph)
E               TypeError: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f8442e0>

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:1549: TypeError

During handling of the above exception, another exception occurred:

model_type = 'nn.Module', args_format = 'tuple', output_target = 'file'
opset_version = 16, export_params = False, do_constant_folding = True
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
simple_conv_model = SimpleConvModel(
  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
sample_input_tensor = tensor([[-1.7534, -1.3003,  0.5800],
        [ 0.0158, -1.6756,  1.0759]])
sample_input_tensor_2d = tensor([[[[-0.4210, -0.6637,  1.4599, -1.9324,  0.2951,  1.9531,  0.0623,
           -1.1079, -0.0518,  0.4115],
     ...7],
          [ 1.5941,  2.5384, -0.4962,  1.5591, -1.2024, -0.6351,  0.2057,
            1.2085, -1.2975, -1.7842]]]])
temp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmp3y1vq9tb.onnx'

    @pytest.mark.parametrize(
        "model_type,args_format,output_target,opset_version,export_params,do_constant_folding",
        [
            # Base case from test plan
            ("nn.Module", "tuple", "file", 13, True, True),
            # Parameter extensions from test plan
            ("nn.Module", "named_tuple", "file", 13, True, False),
            ("nn.Module", "tuple", "file", 7, True, True),
            ("nn.Module", "tuple", "file", 16, False, True),
        ]
    )
    def test_export_basic_model_to_file(
        model_type,
        args_format,
        output_target,
        opset_version,
        export_params,
        do_constant_folding,
        simple_linear_model,
        simple_conv_model,
        sample_input_tensor,
        sample_input_tensor_2d,
        temp_onnx_file,
    ):
        """
        Test basic model export to file with various configurations.
    
        This test covers the core export functionality with different:
        - Model types (nn.Module)
        - Args formats (tuple, named_tuple)
        - Opset versions (7, 13, 16)
        - Export parameters (True/False)
        - Constant folding (True/False)
    
        Weak assertions: file exists, file not empty, no exception.
        """
        # Select model based on type
        if model_type == "nn.Module":
            # Use linear model for 1D input, conv model for 2D input
            if args_format == "tuple":
                model = simple_linear_model
                args = (sample_input_tensor,)
            else:  # named_tuple
                model = simple_conv_model
                args = (sample_input_tensor_2d,)
    
        # Prepare args based on format
        if args_format == "named_tuple":
            # Create args with named parameters
            args = (args[0], {"dummy_param": torch.tensor([1.0])})
    
        # Mock dependencies
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace) as mock_trace, \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph) as mock_graph, \
             mock.patch('io.open', mock.mock_open()) as mock_file:
    
            # Call export
            try:
                onnx_utils.export(
                    model=model,
                    args=args,
                    f=temp_onnx_file,
                    export_params=export_params,
                    opset_version=opset_version,
                    do_constant_folding=do_constant_folding,
                    verbose=False,
                )
    
                # Weak assertion 1: No exception raised
                # (implicitly passed if we reach here)
    
                # Weak assertion 2: File exists and is not empty
                # Note: Since we're mocking io.open, we need to check mock calls
                assert mock_file.called, "File was not opened for writing"
    
                # Check that write was called (indicating content was written)
                write_calls = [call for call in mock_file.mock_calls if call[0] == '().write']
                assert len(write_calls) > 0, "No data was written to file"
    
                # Weak assertion 3: Trace was called for nn.Module
                if model_type == "nn.Module":
                    assert mock_trace.called, "torch.jit.trace should be called for nn.Module"
    
                # Weak assertion 4: Graph conversion was called
                assert mock_graph.called, "_model_to_graph should be called"
    
                # Additional weak checks
                assert opset_version >= 7 and opset_version <= 16, \
                    f"Opset version {opset_version} should be in range 7-16"
    
            except Exception as e:
>               pytest.fail(f"Export raised unexpected exception: {e}")
E               Failed: Export raised unexpected exception: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f8442e0>

tests/test_torch_onnx_utils.py:243: Failed
_____ test_export_model_to_bytesio[nn.Module-tensor-bytesio-13-True-True] ______

model_type = 'nn.Module', args_format = 'tensor', output_target = 'bytesio'
opset_version = 13, export_params = True, do_constant_folding = True
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
sample_input_tensor = tensor([[-0.7328,  0.0113,  0.4797],
        [-0.0365,  1.0878, -0.0177]])

    @pytest.mark.parametrize(
        "model_type,args_format,output_target,opset_version,export_params,do_constant_folding",
        [
            # Base case from test plan
            ("nn.Module", "tensor", "bytesio", 13, True, True),
            # Parameter extension from test plan
            ("ScriptFunction", "tensor", "bytesio", 13, True, True),
        ]
    )
    def test_export_model_to_bytesio(
        model_type,
        args_format,
        output_target,
        opset_version,
        export_params,
        do_constant_folding,
        simple_linear_model,
        sample_input_tensor,
    ):
        """
        Test model export to BytesIO buffer with various configurations.
    
        This test covers:
        - Export to in-memory buffer (BytesIO)
        - Tensor args format
        - Different model types (nn.Module, ScriptFunction)
    
        Weak assertions: buffer not empty, buffer position changed, no exception.
        """
        # Prepare model based on type
        if model_type == "nn.Module":
            model = simple_linear_model
            # For ScriptFunction mock, we'll create a traced version
            script_function_mock = None
        else:  # ScriptFunction
            # Create a mock ScriptFunction
            model = mock.MagicMock(spec=torch.jit.ScriptFunction)
            # Also create a regular model for the trace mock to return
            script_function_mock = simple_linear_model
    
        # Prepare args based on format
        if args_format == "tensor":
            args = sample_input_tensor
    
        # Create BytesIO buffer
        buffer = io.BytesIO()
    
        # Track initial buffer state
        initial_position = buffer.tell()
    
        # Mock dependencies
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace) as mock_trace, \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph) as mock_graph:
    
            # For ScriptFunction, trace should not be called
            if model_type == "ScriptFunction":
                mock_trace.side_effect = None
                mock_trace.return_value = script_function_mock
    
            # Call export
            try:
>               onnx_utils.export(
                    model=model,
                    args=args,
                    f=buffer,
                    export_params=export_params,
                    opset_version=opset_version,
                    do_constant_folding=do_constant_folding,
                    verbose=False,
                )

tests/test_torch_onnx_utils.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:504: in export
    _export(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
args = (tensor([[-0.7328,  0.0113,  0.4797],
        [-0.0365,  1.0878, -0.0177]]),)
f = <_io.BytesIO object at 0x12f8a7dd0>, export_params = True, verbose = False
training = <TrainingMode.EVAL: 0>, input_names = None, output_names = None
operator_export_type = <OperatorExportTypes.ONNX: 0>
export_type = 'Saves model in the specified protobuf file.', opset_version = 13
do_constant_folding = True, dynamic_axes = {}
keep_initializers_as_inputs = None, fixed_batch_size = False, custom_opsets = {}
add_node_names = True, onnx_shape_inference = True
export_modules_as_functions = False

    @_beartype.beartype
    def _export(
        model,
        args,
        f,
        export_params=True,
        verbose=False,
        training=_C_onnx.TrainingMode.EVAL,
        input_names=None,
        output_names=None,
        operator_export_type=_C_onnx.OperatorExportTypes.ONNX,
        export_type=None,
        opset_version=None,
        do_constant_folding=True,
        dynamic_axes=None,
        keep_initializers_as_inputs=None,
        fixed_batch_size=False,
        custom_opsets=None,
        add_node_names=True,
        onnx_shape_inference=True,
        export_modules_as_functions=False,
    ):
        assert GLOBALS.in_onnx_export is False
    
        if export_type is None:
            export_type = _exporter_states.ExportTypes.PROTOBUF_FILE
    
        if isinstance(model, torch.nn.DataParallel):
            raise ValueError(
                "torch.nn.DataParallel is not supported by ONNX "
                "exporter, please use 'attribute' module to "
                "unwrap model from torch.nn.DataParallel. Try "
                "torch.onnx.export(model.module, ...)"
            )
    
        GLOBALS.onnx_shape_inference = onnx_shape_inference
    
        if opset_version is None:
            opset_version = _constants.ONNX_DEFAULT_OPSET
    
        if export_modules_as_functions and opset_version < 15:
            raise ValueError(
                "`export_modules_as_functions` is not supported for `opset_version` < 15."
                "This is because `opset_version` < 15 implies IR version < 8, which means "
                "no local function support. "
            )
        if not operator_export_type:
            if _C_onnx._CAFFE2_ATEN_FALLBACK:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK
            else:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX
    
        # By default, training=TrainingMode.EVAL,
        # which is good because running a model in training mode could result in
        # internal buffers getting updated, dropout getting applied, etc.
        # If you really know what you're doing, you can turn
        # training=TrainingMode.TRAINING or training=TrainingMode.PRESERVE,
        # (to preserve whatever the original training mode was.)
        GLOBALS.export_onnx_opset_version = opset_version
        GLOBALS.operator_export_type = operator_export_type
    
        try:
            GLOBALS.in_onnx_export = True
    
            module_typenames_to_export_as_functions: Set[str] = set()
            if isinstance(model, (torch.nn.Module, torch.jit.ScriptModule)):
                module_typenames_to_export_as_functions = _setup_trace_module_map(
                    model, export_modules_as_functions
                )
    
            with exporter_context(model, training, verbose):
                val_keep_init_as_ip = _decide_keep_init_as_input(
                    keep_initializers_as_inputs,
                    operator_export_type,
                    opset_version,
                )
                val_add_node_names = _decide_add_node_names(
                    add_node_names, operator_export_type
                )
                val_do_constant_folding = _decide_constant_folding(
                    do_constant_folding, operator_export_type, training
                )
                # Normally f can be a file-like object, but for large models, the external data format requires a
                # valid `model_file_location`. Code in export.cpp will enforce this.
                if isinstance(f, str):
                    model_file_location = f
                else:
                    model_file_location = ""
                args = _decide_input_format(model, args)
                if dynamic_axes is None:
                    dynamic_axes = {}
                _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)
    
                graph, params_dict, torch_out = _model_to_graph(
                    model,
                    args,
                    verbose,
                    input_names,
                    output_names,
                    operator_export_type,
                    val_do_constant_folding,
                    fixed_batch_size=fixed_batch_size,
                    training=training,
                    dynamic_axes=dynamic_axes,
                )
    
                # TODO: Don't allocate a in-memory string for the protobuf
                defer_weight_export = (
                    export_type is not _exporter_states.ExportTypes.PROTOBUF_FILE
                )
                if custom_opsets is None:
                    custom_opsets = {}
    
>               _C._jit_pass_dce_allow_deleting_nodes_with_side_effects(graph)
E               TypeError: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f805f90>

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:1549: TypeError

During handling of the above exception, another exception occurred:

model_type = 'nn.Module', args_format = 'tensor', output_target = 'bytesio'
opset_version = 13, export_params = True, do_constant_folding = True
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
sample_input_tensor = tensor([[-0.7328,  0.0113,  0.4797],
        [-0.0365,  1.0878, -0.0177]])

    @pytest.mark.parametrize(
        "model_type,args_format,output_target,opset_version,export_params,do_constant_folding",
        [
            # Base case from test plan
            ("nn.Module", "tensor", "bytesio", 13, True, True),
            # Parameter extension from test plan
            ("ScriptFunction", "tensor", "bytesio", 13, True, True),
        ]
    )
    def test_export_model_to_bytesio(
        model_type,
        args_format,
        output_target,
        opset_version,
        export_params,
        do_constant_folding,
        simple_linear_model,
        sample_input_tensor,
    ):
        """
        Test model export to BytesIO buffer with various configurations.
    
        This test covers:
        - Export to in-memory buffer (BytesIO)
        - Tensor args format
        - Different model types (nn.Module, ScriptFunction)
    
        Weak assertions: buffer not empty, buffer position changed, no exception.
        """
        # Prepare model based on type
        if model_type == "nn.Module":
            model = simple_linear_model
            # For ScriptFunction mock, we'll create a traced version
            script_function_mock = None
        else:  # ScriptFunction
            # Create a mock ScriptFunction
            model = mock.MagicMock(spec=torch.jit.ScriptFunction)
            # Also create a regular model for the trace mock to return
            script_function_mock = simple_linear_model
    
        # Prepare args based on format
        if args_format == "tensor":
            args = sample_input_tensor
    
        # Create BytesIO buffer
        buffer = io.BytesIO()
    
        # Track initial buffer state
        initial_position = buffer.tell()
    
        # Mock dependencies
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace) as mock_trace, \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph) as mock_graph:
    
            # For ScriptFunction, trace should not be called
            if model_type == "ScriptFunction":
                mock_trace.side_effect = None
                mock_trace.return_value = script_function_mock
    
            # Call export
            try:
                onnx_utils.export(
                    model=model,
                    args=args,
                    f=buffer,
                    export_params=export_params,
                    opset_version=opset_version,
                    do_constant_folding=do_constant_folding,
                    verbose=False,
                )
    
                # Weak assertion 1: No exception raised
                # (implicitly passed if we reach here)
    
                # Weak assertion 2: Buffer is not empty
                buffer_size = buffer.getbuffer().nbytes
                assert buffer_size > 0, f"Buffer should not be empty, got size: {buffer_size}"
    
                # Weak assertion 3: Buffer position changed
                final_position = buffer.tell()
                assert final_position > initial_position, \
                    f"Buffer position should change. Initial: {initial_position}, Final: {final_position}"
    
                # Weak assertion 4: Trace called appropriately
                if model_type == "nn.Module":
                    assert mock_trace.called, "torch.jit.trace should be called for nn.Module"
                else:  # ScriptFunction
                    # Trace might still be called internally, but we don't enforce this
                    pass
    
                # Weak assertion 5: Graph conversion was called
                assert mock_graph.called, "_model_to_graph should be called"
    
                # Additional weak check for opset version
                assert opset_version >= 7 and opset_version <= 16, \
                    f"Opset version {opset_version} should be in range 7-16"
    
                # Check buffer content (basic validation)
                buffer_content = buffer.getvalue()
                assert isinstance(buffer_content, bytes), "Buffer content should be bytes"
                assert len(buffer_content) > 0, "Buffer content should have non-zero length"
    
            except Exception as e:
>               pytest.fail(f"Export to BytesIO raised unexpected exception: {e}")
E               Failed: Export to BytesIO raised unexpected exception: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f805f90>

tests/test_torch_onnx_utils.py:352: Failed
___ test_export_model_to_bytesio[ScriptFunction-tensor-bytesio-13-True-True] ___

model_type = 'ScriptFunction', args_format = 'tensor', output_target = 'bytesio'
opset_version = 13, export_params = True, do_constant_folding = True
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
sample_input_tensor = tensor([[-0.6039, -0.3514,  0.5235],
        [ 0.0675, -0.9669, -0.0743]])

    @pytest.mark.parametrize(
        "model_type,args_format,output_target,opset_version,export_params,do_constant_folding",
        [
            # Base case from test plan
            ("nn.Module", "tensor", "bytesio", 13, True, True),
            # Parameter extension from test plan
            ("ScriptFunction", "tensor", "bytesio", 13, True, True),
        ]
    )
    def test_export_model_to_bytesio(
        model_type,
        args_format,
        output_target,
        opset_version,
        export_params,
        do_constant_folding,
        simple_linear_model,
        sample_input_tensor,
    ):
        """
        Test model export to BytesIO buffer with various configurations.
    
        This test covers:
        - Export to in-memory buffer (BytesIO)
        - Tensor args format
        - Different model types (nn.Module, ScriptFunction)
    
        Weak assertions: buffer not empty, buffer position changed, no exception.
        """
        # Prepare model based on type
        if model_type == "nn.Module":
            model = simple_linear_model
            # For ScriptFunction mock, we'll create a traced version
            script_function_mock = None
        else:  # ScriptFunction
            # Create a mock ScriptFunction
            model = mock.MagicMock(spec=torch.jit.ScriptFunction)
            # Also create a regular model for the trace mock to return
            script_function_mock = simple_linear_model
    
        # Prepare args based on format
        if args_format == "tensor":
            args = sample_input_tensor
    
        # Create BytesIO buffer
        buffer = io.BytesIO()
    
        # Track initial buffer state
        initial_position = buffer.tell()
    
        # Mock dependencies
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace) as mock_trace, \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph) as mock_graph:
    
            # For ScriptFunction, trace should not be called
            if model_type == "ScriptFunction":
                mock_trace.side_effect = None
                mock_trace.return_value = script_function_mock
    
            # Call export
            try:
>               onnx_utils.export(
                    model=model,
                    args=args,
                    f=buffer,
                    export_params=export_params,
                    opset_version=opset_version,
                    do_constant_folding=do_constant_folding,
                    verbose=False,
                )

tests/test_torch_onnx_utils.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:504: in export
    _export(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = <MagicMock spec='ScriptFunction' id='5092965248'>
args = (tensor([[-0.6039, -0.3514,  0.5235],
        [ 0.0675, -0.9669, -0.0743]]),)
f = <_io.BytesIO object at 0x12f88ee30>, export_params = True, verbose = False
training = <TrainingMode.EVAL: 0>, input_names = None, output_names = None
operator_export_type = <OperatorExportTypes.ONNX: 0>
export_type = 'Saves model in the specified protobuf file.', opset_version = 13
do_constant_folding = True, dynamic_axes = {}
keep_initializers_as_inputs = None, fixed_batch_size = False, custom_opsets = {}
add_node_names = True, onnx_shape_inference = True
export_modules_as_functions = False

    @_beartype.beartype
    def _export(
        model,
        args,
        f,
        export_params=True,
        verbose=False,
        training=_C_onnx.TrainingMode.EVAL,
        input_names=None,
        output_names=None,
        operator_export_type=_C_onnx.OperatorExportTypes.ONNX,
        export_type=None,
        opset_version=None,
        do_constant_folding=True,
        dynamic_axes=None,
        keep_initializers_as_inputs=None,
        fixed_batch_size=False,
        custom_opsets=None,
        add_node_names=True,
        onnx_shape_inference=True,
        export_modules_as_functions=False,
    ):
        assert GLOBALS.in_onnx_export is False
    
        if export_type is None:
            export_type = _exporter_states.ExportTypes.PROTOBUF_FILE
    
        if isinstance(model, torch.nn.DataParallel):
            raise ValueError(
                "torch.nn.DataParallel is not supported by ONNX "
                "exporter, please use 'attribute' module to "
                "unwrap model from torch.nn.DataParallel. Try "
                "torch.onnx.export(model.module, ...)"
            )
    
        GLOBALS.onnx_shape_inference = onnx_shape_inference
    
        if opset_version is None:
            opset_version = _constants.ONNX_DEFAULT_OPSET
    
        if export_modules_as_functions and opset_version < 15:
            raise ValueError(
                "`export_modules_as_functions` is not supported for `opset_version` < 15."
                "This is because `opset_version` < 15 implies IR version < 8, which means "
                "no local function support. "
            )
        if not operator_export_type:
            if _C_onnx._CAFFE2_ATEN_FALLBACK:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK
            else:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX
    
        # By default, training=TrainingMode.EVAL,
        # which is good because running a model in training mode could result in
        # internal buffers getting updated, dropout getting applied, etc.
        # If you really know what you're doing, you can turn
        # training=TrainingMode.TRAINING or training=TrainingMode.PRESERVE,
        # (to preserve whatever the original training mode was.)
        GLOBALS.export_onnx_opset_version = opset_version
        GLOBALS.operator_export_type = operator_export_type
    
        try:
            GLOBALS.in_onnx_export = True
    
            module_typenames_to_export_as_functions: Set[str] = set()
            if isinstance(model, (torch.nn.Module, torch.jit.ScriptModule)):
                module_typenames_to_export_as_functions = _setup_trace_module_map(
                    model, export_modules_as_functions
                )
    
            with exporter_context(model, training, verbose):
                val_keep_init_as_ip = _decide_keep_init_as_input(
                    keep_initializers_as_inputs,
                    operator_export_type,
                    opset_version,
                )
                val_add_node_names = _decide_add_node_names(
                    add_node_names, operator_export_type
                )
                val_do_constant_folding = _decide_constant_folding(
                    do_constant_folding, operator_export_type, training
                )
                # Normally f can be a file-like object, but for large models, the external data format requires a
                # valid `model_file_location`. Code in export.cpp will enforce this.
                if isinstance(f, str):
                    model_file_location = f
                else:
                    model_file_location = ""
                args = _decide_input_format(model, args)
                if dynamic_axes is None:
                    dynamic_axes = {}
                _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)
    
                graph, params_dict, torch_out = _model_to_graph(
                    model,
                    args,
                    verbose,
                    input_names,
                    output_names,
                    operator_export_type,
                    val_do_constant_folding,
                    fixed_batch_size=fixed_batch_size,
                    training=training,
                    dynamic_axes=dynamic_axes,
                )
    
                # TODO: Don't allocate a in-memory string for the protobuf
                defer_weight_export = (
                    export_type is not _exporter_states.ExportTypes.PROTOBUF_FILE
                )
                if custom_opsets is None:
                    custom_opsets = {}
    
>               _C._jit_pass_dce_allow_deleting_nodes_with_side_effects(graph)
E               TypeError: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f84d660>

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:1549: TypeError

During handling of the above exception, another exception occurred:

model_type = 'ScriptFunction', args_format = 'tensor', output_target = 'bytesio'
opset_version = 13, export_params = True, do_constant_folding = True
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
sample_input_tensor = tensor([[-0.6039, -0.3514,  0.5235],
        [ 0.0675, -0.9669, -0.0743]])

    @pytest.mark.parametrize(
        "model_type,args_format,output_target,opset_version,export_params,do_constant_folding",
        [
            # Base case from test plan
            ("nn.Module", "tensor", "bytesio", 13, True, True),
            # Parameter extension from test plan
            ("ScriptFunction", "tensor", "bytesio", 13, True, True),
        ]
    )
    def test_export_model_to_bytesio(
        model_type,
        args_format,
        output_target,
        opset_version,
        export_params,
        do_constant_folding,
        simple_linear_model,
        sample_input_tensor,
    ):
        """
        Test model export to BytesIO buffer with various configurations.
    
        This test covers:
        - Export to in-memory buffer (BytesIO)
        - Tensor args format
        - Different model types (nn.Module, ScriptFunction)
    
        Weak assertions: buffer not empty, buffer position changed, no exception.
        """
        # Prepare model based on type
        if model_type == "nn.Module":
            model = simple_linear_model
            # For ScriptFunction mock, we'll create a traced version
            script_function_mock = None
        else:  # ScriptFunction
            # Create a mock ScriptFunction
            model = mock.MagicMock(spec=torch.jit.ScriptFunction)
            # Also create a regular model for the trace mock to return
            script_function_mock = simple_linear_model
    
        # Prepare args based on format
        if args_format == "tensor":
            args = sample_input_tensor
    
        # Create BytesIO buffer
        buffer = io.BytesIO()
    
        # Track initial buffer state
        initial_position = buffer.tell()
    
        # Mock dependencies
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace) as mock_trace, \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph) as mock_graph:
    
            # For ScriptFunction, trace should not be called
            if model_type == "ScriptFunction":
                mock_trace.side_effect = None
                mock_trace.return_value = script_function_mock
    
            # Call export
            try:
                onnx_utils.export(
                    model=model,
                    args=args,
                    f=buffer,
                    export_params=export_params,
                    opset_version=opset_version,
                    do_constant_folding=do_constant_folding,
                    verbose=False,
                )
    
                # Weak assertion 1: No exception raised
                # (implicitly passed if we reach here)
    
                # Weak assertion 2: Buffer is not empty
                buffer_size = buffer.getbuffer().nbytes
                assert buffer_size > 0, f"Buffer should not be empty, got size: {buffer_size}"
    
                # Weak assertion 3: Buffer position changed
                final_position = buffer.tell()
                assert final_position > initial_position, \
                    f"Buffer position should change. Initial: {initial_position}, Final: {final_position}"
    
                # Weak assertion 4: Trace called appropriately
                if model_type == "nn.Module":
                    assert mock_trace.called, "torch.jit.trace should be called for nn.Module"
                else:  # ScriptFunction
                    # Trace might still be called internally, but we don't enforce this
                    pass
    
                # Weak assertion 5: Graph conversion was called
                assert mock_graph.called, "_model_to_graph should be called"
    
                # Additional weak check for opset version
                assert opset_version >= 7 and opset_version <= 16, \
                    f"Opset version {opset_version} should be in range 7-16"
    
                # Check buffer content (basic validation)
                buffer_content = buffer.getvalue()
                assert isinstance(buffer_content, bytes), "Buffer content should be bytes"
                assert len(buffer_content) > 0, "Buffer content should have non-zero length"
    
            except Exception as e:
>               pytest.fail(f"Export to BytesIO raised unexpected exception: {e}")
E               Failed: Export to BytesIO raised unexpected exception: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f84d660>

tests/test_torch_onnx_utils.py:352: Failed
___________ TestONNXUtilsExport.test_export_with_default_parameters ____________

self = <test_torch_onnx_utils.TestONNXUtilsExport object at 0x12f806320>
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
sample_input_tensor = tensor([[-0.0280, -0.4115,  0.8032],
        [-0.9688,  0.1715, -0.5394]])
temp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmp0hx52f66.onnx'

    def test_export_with_default_parameters(self, simple_linear_model, sample_input_tensor, temp_onnx_file):
        """Test export with minimal required parameters."""
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace), \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph), \
             mock.patch('io.open', mock.mock_open()):
    
            # Should not raise any exception
>           onnx_utils.export(
                model=simple_linear_model,
                args=(sample_input_tensor,),
                f=temp_onnx_file,
            )

tests/test_torch_onnx_utils.py:508: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:504: in export
    _export(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
args = (tensor([[-0.0280, -0.4115,  0.8032],
        [-0.9688,  0.1715, -0.5394]]),)
f = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmp0hx52f66.onnx'
export_params = True, verbose = False, training = <TrainingMode.EVAL: 0>
input_names = None, output_names = None
operator_export_type = <OperatorExportTypes.ONNX: 0>
export_type = 'Saves model in the specified protobuf file.', opset_version = 14
do_constant_folding = True, dynamic_axes = {}
keep_initializers_as_inputs = None, fixed_batch_size = False, custom_opsets = {}
add_node_names = True, onnx_shape_inference = True
export_modules_as_functions = False

    @_beartype.beartype
    def _export(
        model,
        args,
        f,
        export_params=True,
        verbose=False,
        training=_C_onnx.TrainingMode.EVAL,
        input_names=None,
        output_names=None,
        operator_export_type=_C_onnx.OperatorExportTypes.ONNX,
        export_type=None,
        opset_version=None,
        do_constant_folding=True,
        dynamic_axes=None,
        keep_initializers_as_inputs=None,
        fixed_batch_size=False,
        custom_opsets=None,
        add_node_names=True,
        onnx_shape_inference=True,
        export_modules_as_functions=False,
    ):
        assert GLOBALS.in_onnx_export is False
    
        if export_type is None:
            export_type = _exporter_states.ExportTypes.PROTOBUF_FILE
    
        if isinstance(model, torch.nn.DataParallel):
            raise ValueError(
                "torch.nn.DataParallel is not supported by ONNX "
                "exporter, please use 'attribute' module to "
                "unwrap model from torch.nn.DataParallel. Try "
                "torch.onnx.export(model.module, ...)"
            )
    
        GLOBALS.onnx_shape_inference = onnx_shape_inference
    
        if opset_version is None:
            opset_version = _constants.ONNX_DEFAULT_OPSET
    
        if export_modules_as_functions and opset_version < 15:
            raise ValueError(
                "`export_modules_as_functions` is not supported for `opset_version` < 15."
                "This is because `opset_version` < 15 implies IR version < 8, which means "
                "no local function support. "
            )
        if not operator_export_type:
            if _C_onnx._CAFFE2_ATEN_FALLBACK:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK
            else:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX
    
        # By default, training=TrainingMode.EVAL,
        # which is good because running a model in training mode could result in
        # internal buffers getting updated, dropout getting applied, etc.
        # If you really know what you're doing, you can turn
        # training=TrainingMode.TRAINING or training=TrainingMode.PRESERVE,
        # (to preserve whatever the original training mode was.)
        GLOBALS.export_onnx_opset_version = opset_version
        GLOBALS.operator_export_type = operator_export_type
    
        try:
            GLOBALS.in_onnx_export = True
    
            module_typenames_to_export_as_functions: Set[str] = set()
            if isinstance(model, (torch.nn.Module, torch.jit.ScriptModule)):
                module_typenames_to_export_as_functions = _setup_trace_module_map(
                    model, export_modules_as_functions
                )
    
            with exporter_context(model, training, verbose):
                val_keep_init_as_ip = _decide_keep_init_as_input(
                    keep_initializers_as_inputs,
                    operator_export_type,
                    opset_version,
                )
                val_add_node_names = _decide_add_node_names(
                    add_node_names, operator_export_type
                )
                val_do_constant_folding = _decide_constant_folding(
                    do_constant_folding, operator_export_type, training
                )
                # Normally f can be a file-like object, but for large models, the external data format requires a
                # valid `model_file_location`. Code in export.cpp will enforce this.
                if isinstance(f, str):
                    model_file_location = f
                else:
                    model_file_location = ""
                args = _decide_input_format(model, args)
                if dynamic_axes is None:
                    dynamic_axes = {}
                _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)
    
                graph, params_dict, torch_out = _model_to_graph(
                    model,
                    args,
                    verbose,
                    input_names,
                    output_names,
                    operator_export_type,
                    val_do_constant_folding,
                    fixed_batch_size=fixed_batch_size,
                    training=training,
                    dynamic_axes=dynamic_axes,
                )
    
                # TODO: Don't allocate a in-memory string for the protobuf
                defer_weight_export = (
                    export_type is not _exporter_states.ExportTypes.PROTOBUF_FILE
                )
                if custom_opsets is None:
                    custom_opsets = {}
    
>               _C._jit_pass_dce_allow_deleting_nodes_with_side_effects(graph)
E               TypeError: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f96afe0>

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:1549: TypeError
________ TestONNXUtilsExport.test_export_with_custom_input_output_names ________

self = <test_torch_onnx_utils.TestONNXUtilsExport object at 0x12f806140>
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
sample_input_tensor = tensor([[ 0.4123, -0.2202, -1.7519],
        [-0.2982, -1.6451, -0.6181]])
temp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpuznd6fth.onnx'

    def test_export_with_custom_input_output_names(self, simple_linear_model, sample_input_tensor, temp_onnx_file):
        """Test export with custom input and output names."""
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace), \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph), \
             mock.patch('io.open', mock.mock_open()):
    
>           onnx_utils.export(
                model=simple_linear_model,
                args=(sample_input_tensor,),
                f=temp_onnx_file,
                input_names=["input_tensor"],
                output_names=["output_tensor"],
            )

tests/test_torch_onnx_utils.py:520: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:504: in export
    _export(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
args = (tensor([[ 0.4123, -0.2202, -1.7519],
        [-0.2982, -1.6451, -0.6181]]),)
f = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpuznd6fth.onnx'
export_params = True, verbose = False, training = <TrainingMode.EVAL: 0>
input_names = ['input_tensor'], output_names = ['output_tensor']
operator_export_type = <OperatorExportTypes.ONNX: 0>
export_type = 'Saves model in the specified protobuf file.', opset_version = 14
do_constant_folding = True, dynamic_axes = {}
keep_initializers_as_inputs = None, fixed_batch_size = False, custom_opsets = {}
add_node_names = True, onnx_shape_inference = True
export_modules_as_functions = False

    @_beartype.beartype
    def _export(
        model,
        args,
        f,
        export_params=True,
        verbose=False,
        training=_C_onnx.TrainingMode.EVAL,
        input_names=None,
        output_names=None,
        operator_export_type=_C_onnx.OperatorExportTypes.ONNX,
        export_type=None,
        opset_version=None,
        do_constant_folding=True,
        dynamic_axes=None,
        keep_initializers_as_inputs=None,
        fixed_batch_size=False,
        custom_opsets=None,
        add_node_names=True,
        onnx_shape_inference=True,
        export_modules_as_functions=False,
    ):
        assert GLOBALS.in_onnx_export is False
    
        if export_type is None:
            export_type = _exporter_states.ExportTypes.PROTOBUF_FILE
    
        if isinstance(model, torch.nn.DataParallel):
            raise ValueError(
                "torch.nn.DataParallel is not supported by ONNX "
                "exporter, please use 'attribute' module to "
                "unwrap model from torch.nn.DataParallel. Try "
                "torch.onnx.export(model.module, ...)"
            )
    
        GLOBALS.onnx_shape_inference = onnx_shape_inference
    
        if opset_version is None:
            opset_version = _constants.ONNX_DEFAULT_OPSET
    
        if export_modules_as_functions and opset_version < 15:
            raise ValueError(
                "`export_modules_as_functions` is not supported for `opset_version` < 15."
                "This is because `opset_version` < 15 implies IR version < 8, which means "
                "no local function support. "
            )
        if not operator_export_type:
            if _C_onnx._CAFFE2_ATEN_FALLBACK:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK
            else:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX
    
        # By default, training=TrainingMode.EVAL,
        # which is good because running a model in training mode could result in
        # internal buffers getting updated, dropout getting applied, etc.
        # If you really know what you're doing, you can turn
        # training=TrainingMode.TRAINING or training=TrainingMode.PRESERVE,
        # (to preserve whatever the original training mode was.)
        GLOBALS.export_onnx_opset_version = opset_version
        GLOBALS.operator_export_type = operator_export_type
    
        try:
            GLOBALS.in_onnx_export = True
    
            module_typenames_to_export_as_functions: Set[str] = set()
            if isinstance(model, (torch.nn.Module, torch.jit.ScriptModule)):
                module_typenames_to_export_as_functions = _setup_trace_module_map(
                    model, export_modules_as_functions
                )
    
            with exporter_context(model, training, verbose):
                val_keep_init_as_ip = _decide_keep_init_as_input(
                    keep_initializers_as_inputs,
                    operator_export_type,
                    opset_version,
                )
                val_add_node_names = _decide_add_node_names(
                    add_node_names, operator_export_type
                )
                val_do_constant_folding = _decide_constant_folding(
                    do_constant_folding, operator_export_type, training
                )
                # Normally f can be a file-like object, but for large models, the external data format requires a
                # valid `model_file_location`. Code in export.cpp will enforce this.
                if isinstance(f, str):
                    model_file_location = f
                else:
                    model_file_location = ""
                args = _decide_input_format(model, args)
                if dynamic_axes is None:
                    dynamic_axes = {}
                _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)
    
                graph, params_dict, torch_out = _model_to_graph(
                    model,
                    args,
                    verbose,
                    input_names,
                    output_names,
                    operator_export_type,
                    val_do_constant_folding,
                    fixed_batch_size=fixed_batch_size,
                    training=training,
                    dynamic_axes=dynamic_axes,
                )
    
                # TODO: Don't allocate a in-memory string for the protobuf
                defer_weight_export = (
                    export_type is not _exporter_states.ExportTypes.PROTOBUF_FILE
                )
                if custom_opsets is None:
                    custom_opsets = {}
    
>               _C._jit_pass_dce_allow_deleting_nodes_with_side_effects(graph)
E               TypeError: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f8fee00>

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:1549: TypeError
______________ TestONNXUtilsExport.test_export_with_verbose_mode _______________

self = <test_torch_onnx_utils.TestONNXUtilsExport object at 0x12f805c00>
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
sample_input_tensor = tensor([[ 0.7818, -1.0138, -2.0971],
        [ 0.1009,  0.6100,  1.4594]])
temp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpexs7l2vm.onnx'

    def test_export_with_verbose_mode(self, simple_linear_model, sample_input_tensor, temp_onnx_file):
        """Test export with verbose mode enabled."""
        with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace), \
             mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph), \
             mock.patch('io.open', mock.mock_open()):
    
            # Capture warnings/prints if needed
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
>               onnx_utils.export(
                    model=simple_linear_model,
                    args=(sample_input_tensor,),
                    f=temp_onnx_file,
                    verbose=True,
                )

tests/test_torch_onnx_utils.py:537: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:504: in export
    _export(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
args = (tensor([[ 0.7818, -1.0138, -2.0971],
        [ 0.1009,  0.6100,  1.4594]]),)
f = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpexs7l2vm.onnx'
export_params = True, verbose = True, training = <TrainingMode.EVAL: 0>
input_names = None, output_names = None
operator_export_type = <OperatorExportTypes.ONNX: 0>
export_type = 'Saves model in the specified protobuf file.', opset_version = 14
do_constant_folding = True, dynamic_axes = {}
keep_initializers_as_inputs = None, fixed_batch_size = False, custom_opsets = {}
add_node_names = True, onnx_shape_inference = True
export_modules_as_functions = False

    @_beartype.beartype
    def _export(
        model,
        args,
        f,
        export_params=True,
        verbose=False,
        training=_C_onnx.TrainingMode.EVAL,
        input_names=None,
        output_names=None,
        operator_export_type=_C_onnx.OperatorExportTypes.ONNX,
        export_type=None,
        opset_version=None,
        do_constant_folding=True,
        dynamic_axes=None,
        keep_initializers_as_inputs=None,
        fixed_batch_size=False,
        custom_opsets=None,
        add_node_names=True,
        onnx_shape_inference=True,
        export_modules_as_functions=False,
    ):
        assert GLOBALS.in_onnx_export is False
    
        if export_type is None:
            export_type = _exporter_states.ExportTypes.PROTOBUF_FILE
    
        if isinstance(model, torch.nn.DataParallel):
            raise ValueError(
                "torch.nn.DataParallel is not supported by ONNX "
                "exporter, please use 'attribute' module to "
                "unwrap model from torch.nn.DataParallel. Try "
                "torch.onnx.export(model.module, ...)"
            )
    
        GLOBALS.onnx_shape_inference = onnx_shape_inference
    
        if opset_version is None:
            opset_version = _constants.ONNX_DEFAULT_OPSET
    
        if export_modules_as_functions and opset_version < 15:
            raise ValueError(
                "`export_modules_as_functions` is not supported for `opset_version` < 15."
                "This is because `opset_version` < 15 implies IR version < 8, which means "
                "no local function support. "
            )
        if not operator_export_type:
            if _C_onnx._CAFFE2_ATEN_FALLBACK:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK
            else:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX
    
        # By default, training=TrainingMode.EVAL,
        # which is good because running a model in training mode could result in
        # internal buffers getting updated, dropout getting applied, etc.
        # If you really know what you're doing, you can turn
        # training=TrainingMode.TRAINING or training=TrainingMode.PRESERVE,
        # (to preserve whatever the original training mode was.)
        GLOBALS.export_onnx_opset_version = opset_version
        GLOBALS.operator_export_type = operator_export_type
    
        try:
            GLOBALS.in_onnx_export = True
    
            module_typenames_to_export_as_functions: Set[str] = set()
            if isinstance(model, (torch.nn.Module, torch.jit.ScriptModule)):
                module_typenames_to_export_as_functions = _setup_trace_module_map(
                    model, export_modules_as_functions
                )
    
            with exporter_context(model, training, verbose):
                val_keep_init_as_ip = _decide_keep_init_as_input(
                    keep_initializers_as_inputs,
                    operator_export_type,
                    opset_version,
                )
                val_add_node_names = _decide_add_node_names(
                    add_node_names, operator_export_type
                )
                val_do_constant_folding = _decide_constant_folding(
                    do_constant_folding, operator_export_type, training
                )
                # Normally f can be a file-like object, but for large models, the external data format requires a
                # valid `model_file_location`. Code in export.cpp will enforce this.
                if isinstance(f, str):
                    model_file_location = f
                else:
                    model_file_location = ""
                args = _decide_input_format(model, args)
                if dynamic_axes is None:
                    dynamic_axes = {}
                _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)
    
                graph, params_dict, torch_out = _model_to_graph(
                    model,
                    args,
                    verbose,
                    input_names,
                    output_names,
                    operator_export_type,
                    val_do_constant_folding,
                    fixed_batch_size=fixed_batch_size,
                    training=training,
                    dynamic_axes=dynamic_axes,
                )
    
                # TODO: Don't allocate a in-memory string for the protobuf
                defer_weight_export = (
                    export_type is not _exporter_states.ExportTypes.PROTOBUF_FILE
                )
                if custom_opsets is None:
                    custom_opsets = {}
    
>               _C._jit_pass_dce_allow_deleting_nodes_with_side_effects(graph)
E               TypeError: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f8bdab0>

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:1549: TypeError
_________ TestONNXUtilsEdgeCases.test_export_to_nonexistent_directory __________

self = <test_torch_onnx_utils.TestONNXUtilsEdgeCases object at 0x12f806a40>
simple_linear_model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
sample_input_tensor = tensor([[ 0.2328, -1.2158,  2.8345],
        [-0.3159, -0.1452, -0.8379]])

    def test_export_to_nonexistent_directory(self, simple_linear_model, sample_input_tensor):
        """Test export to non-existent directory."""
        invalid_path = "/nonexistent/path/model.onnx"
    
        # This should raise an error when trying to open the file
        with pytest.raises((OSError, IOError, FileNotFoundError)):
            with mock.patch('torch.jit.trace', side_effect=mock_torch_jit_trace), \
                 mock.patch('torch.onnx.utils._model_to_graph', side_effect=mock_model_to_graph):
                # Don't mock io.open for this test
>               onnx_utils.export(
                    model=simple_linear_model,
                    args=(sample_input_tensor,),
                    f=invalid_path,
                )

tests/test_torch_onnx_utils.py:588: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:504: in export
    _export(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = SimpleLinearModel(
  (linear): Linear(in_features=3, out_features=2, bias=True)
)
args = (tensor([[ 0.2328, -1.2158,  2.8345],
        [-0.3159, -0.1452, -0.8379]]),)
f = '/nonexistent/path/model.onnx', export_params = True, verbose = False
training = <TrainingMode.EVAL: 0>, input_names = None, output_names = None
operator_export_type = <OperatorExportTypes.ONNX: 0>
export_type = 'Saves model in the specified protobuf file.', opset_version = 14
do_constant_folding = True, dynamic_axes = {}
keep_initializers_as_inputs = None, fixed_batch_size = False, custom_opsets = {}
add_node_names = True, onnx_shape_inference = True
export_modules_as_functions = False

    @_beartype.beartype
    def _export(
        model,
        args,
        f,
        export_params=True,
        verbose=False,
        training=_C_onnx.TrainingMode.EVAL,
        input_names=None,
        output_names=None,
        operator_export_type=_C_onnx.OperatorExportTypes.ONNX,
        export_type=None,
        opset_version=None,
        do_constant_folding=True,
        dynamic_axes=None,
        keep_initializers_as_inputs=None,
        fixed_batch_size=False,
        custom_opsets=None,
        add_node_names=True,
        onnx_shape_inference=True,
        export_modules_as_functions=False,
    ):
        assert GLOBALS.in_onnx_export is False
    
        if export_type is None:
            export_type = _exporter_states.ExportTypes.PROTOBUF_FILE
    
        if isinstance(model, torch.nn.DataParallel):
            raise ValueError(
                "torch.nn.DataParallel is not supported by ONNX "
                "exporter, please use 'attribute' module to "
                "unwrap model from torch.nn.DataParallel. Try "
                "torch.onnx.export(model.module, ...)"
            )
    
        GLOBALS.onnx_shape_inference = onnx_shape_inference
    
        if opset_version is None:
            opset_version = _constants.ONNX_DEFAULT_OPSET
    
        if export_modules_as_functions and opset_version < 15:
            raise ValueError(
                "`export_modules_as_functions` is not supported for `opset_version` < 15."
                "This is because `opset_version` < 15 implies IR version < 8, which means "
                "no local function support. "
            )
        if not operator_export_type:
            if _C_onnx._CAFFE2_ATEN_FALLBACK:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK
            else:
                operator_export_type = _C_onnx.OperatorExportTypes.ONNX
    
        # By default, training=TrainingMode.EVAL,
        # which is good because running a model in training mode could result in
        # internal buffers getting updated, dropout getting applied, etc.
        # If you really know what you're doing, you can turn
        # training=TrainingMode.TRAINING or training=TrainingMode.PRESERVE,
        # (to preserve whatever the original training mode was.)
        GLOBALS.export_onnx_opset_version = opset_version
        GLOBALS.operator_export_type = operator_export_type
    
        try:
            GLOBALS.in_onnx_export = True
    
            module_typenames_to_export_as_functions: Set[str] = set()
            if isinstance(model, (torch.nn.Module, torch.jit.ScriptModule)):
                module_typenames_to_export_as_functions = _setup_trace_module_map(
                    model, export_modules_as_functions
                )
    
            with exporter_context(model, training, verbose):
                val_keep_init_as_ip = _decide_keep_init_as_input(
                    keep_initializers_as_inputs,
                    operator_export_type,
                    opset_version,
                )
                val_add_node_names = _decide_add_node_names(
                    add_node_names, operator_export_type
                )
                val_do_constant_folding = _decide_constant_folding(
                    do_constant_folding, operator_export_type, training
                )
                # Normally f can be a file-like object, but for large models, the external data format requires a
                # valid `model_file_location`. Code in export.cpp will enforce this.
                if isinstance(f, str):
                    model_file_location = f
                else:
                    model_file_location = ""
                args = _decide_input_format(model, args)
                if dynamic_axes is None:
                    dynamic_axes = {}
                _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)
    
                graph, params_dict, torch_out = _model_to_graph(
                    model,
                    args,
                    verbose,
                    input_names,
                    output_names,
                    operator_export_type,
                    val_do_constant_folding,
                    fixed_batch_size=fixed_batch_size,
                    training=training,
                    dynamic_axes=dynamic_axes,
                )
    
                # TODO: Don't allocate a in-memory string for the protobuf
                defer_weight_export = (
                    export_type is not _exporter_states.ExportTypes.PROTOBUF_FILE
                )
                if custom_opsets is None:
                    custom_opsets = {}
    
>               _C._jit_pass_dce_allow_deleting_nodes_with_side_effects(graph)
E               TypeError: _jit_pass_dce_allow_deleting_nodes_with_side_effects(): incompatible function arguments. The following argument types are supported:
E                   1. (arg0: torch::jit::Graph) -> None
E               
E               Invoked with: <test_torch_onnx_utils.mock_model_to_graph.<locals>.MockGraph object at 0x12f8df0d0>

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py:1549: TypeError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                             Stmts   Miss Branch BrPart  Cover   Missing
----------------------------------------------------------------------------
tests/test_torch_onnx_utils.py     240     68     32      5    70%   77->exit, 83-96, 128-134, 141-142, 147-148, 189->199, 225-239, 290->294, 324-349, 355-359, 426-427, 598, 606-617, 622-630, 636-642
----------------------------------------------------------------------------
TOTAL                              240     68     32      5    70%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_onnx_utils.py::test_export_basic_model_to_file[nn.Module-tuple-file-13-True-True]
FAILED tests/test_torch_onnx_utils.py::test_export_basic_model_to_file[nn.Module-named_tuple-file-13-True-False]
FAILED tests/test_torch_onnx_utils.py::test_export_basic_model_to_file[nn.Module-tuple-file-7-True-True]
FAILED tests/test_torch_onnx_utils.py::test_export_basic_model_to_file[nn.Module-tuple-file-16-False-True]
FAILED tests/test_torch_onnx_utils.py::test_export_model_to_bytesio[nn.Module-tensor-bytesio-13-True-True]
FAILED tests/test_torch_onnx_utils.py::test_export_model_to_bytesio[ScriptFunction-tensor-bytesio-13-True-True]
FAILED tests/test_torch_onnx_utils.py::TestONNXUtilsExport::test_export_with_default_parameters
FAILED tests/test_torch_onnx_utils.py::TestONNXUtilsExport::test_export_with_custom_input_output_names
FAILED tests/test_torch_onnx_utils.py::TestONNXUtilsExport::test_export_with_verbose_mode
FAILED tests/test_torch_onnx_utils.py::TestONNXUtilsEdgeCases::test_export_to_nonexistent_directory
10 failed, 5 passed in 0.91s

Error: exit 1