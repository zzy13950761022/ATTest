{
  "workflow_id": "2a4d40f4",
  "created_at": "2026-01-21T16:06:20.564156",
  "op": "torch_onnx_utils",
  "arch": "python",
  "soc": "python",
  "vendor": "python",
  "project_root": "/Users/zzf1sh/Documents/Project/TestAgent-CLI-main/exam/torch_group/onnx.utils",
  "target": "torch.onnx.utils",
  "target_slug": "torch_onnx_utils",
  "current_stage": "complete",
  "stage_index": 6,
  "mode": "full-auto",
  "epoch_total": 5,
  "epoch_current": 5,
  "last_failure_signature": "",
  "last_error_signature": "",
  "last_block_errors": {},
  "auto_stop_reason": "",
  "artifacts": {
    "function_doc.md": "# torch.onnx.utils - 函数说明\n\n## 1. 基本信息\n- **FQN**: torch.onnx.utils\n- **模块文件**: `/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/onnx/utils.py`\n- **签名**: 模块（包含多个函数）\n- **对象类型**: Python 模块\n\n## 2. 功能概述\nPyTorch ONNX 导出工具模块，提供将 PyTorch 模型转换为 ONNX 格式的功能。核心函数 `export` 将模型序列化为 ONNX 协议缓冲区文件，支持多种导出选项和配置。\n\n## 3. 参数说明\n模块包含多个函数，核心函数 `export` 参数：\n- model (Union[torch.nn.Module, torch.jit.ScriptModule, torch.jit.ScriptFunction]): 要导出的模型\n- args (Union[Tuple[Any, ...], torch.Tensor]): 模型输入参数，支持三种格式\n- f (Union[str, io.BytesIO]): 输出文件路径或文件对象\n- export_params (bool, default=True): 是否导出模型参数\n- verbose (bool, default=False): 是否打印详细信息\n- training (_C_onnx.TrainingMode, default=EVAL): 训练模式设置\n- input_names (Optional[Sequence[str]], default=None): 输入节点名称\n- output_names (Optional[Sequence[str]], default=None): 输出节点名称\n- operator_export_type (_C_onnx.OperatorExportTypes, default=ONNX): 算子导出类型\n- opset_version (Optional[int], default=None): ONNX opset 版本 (7-16)\n- do_constant_folding (bool, default=True): 是否进行常量折叠优化\n- dynamic_axes (Optional[Union[Mapping[str, Mapping[int, str]], Mapping[str, Sequence[int]]]], default=None): 动态轴定义\n- keep_initializers_as_inputs (Optional[bool], default=None): 是否将初始化器作为输入\n- custom_opsets (Optional[Mapping[str, int]], default=None): 自定义 opset 定义\n- export_modules_as_functions (Union[bool, Collection[Type[torch.nn.Module]]], default=False): 是否将模块导出为函数\n\n## 4. 返回值\n- `export`: 无返回值 (None)，将模型写入文件\n- `export_to_pretty_string`: 返回 UTF-8 字符串，包含 ONNX 模型的可读表示\n- `is_in_onnx_export`: 返回布尔值，指示是否在 ONNX 导出过程中\n\n## 5. 文档要点\n- 支持三种模型类型：torch.nn.Module、torch.jit.ScriptModule、torch.jit.ScriptFunction\n- 非 ScriptModule/Function 模型会通过 torch.jit.trace 转换为 TorchScript\n- args 参数支持三种格式：纯参数元组、单个张量、带命名参数的元组\n- opset_version 必须在 7 到 16 之间\n- 动态轴定义支持字典或列表格式\n- 导出训练模式需要 opset_version >= 12 以正确支持 Dropout 和 BatchNorm\n\n## 6. 源码摘要\n- 核心函数 `export` 使用 `_model_to_graph` 将模型转换为计算图\n- 通过 `_decide_keep_init_as_input` 等辅助函数决定导出选项\n- 依赖 `GLOBALS` 全局状态管理导出配置\n- 使用 `_beartype.beartype` 装饰器进行运行时类型检查\n- 包含错误处理：CheckerError、UnsupportedOperatorError、OnnxExporterError\n\n## 7. 示例与用法（如有）\n- 文档中包含详细的 args 参数格式示例\n- 动态轴配置示例展示如何定义动态维度\n- 命名参数传递的特殊语法说明\n\n## 8. 风险与空白\n- 模块包含多个函数，需要分别测试核心功能\n- `export` 函数参数众多，需要覆盖各种组合情况\n- 动态控制流支持有限（与 torch.jit.trace 相同限制）\n- 某些导出选项可能依赖特定构建配置（如 Caffe2 支持）\n- 缺少具体张量形状和 dtype 的详细约束说明\n- 需要测试不同 opset_version 的兼容性\n- 需要验证不同 operator_export_type 的行为差异\n- 需要测试文件 I/O 错误处理\n- 需要验证模型类型检查的边界情况",
    "requirements.md": "# torch.onnx.utils 测试需求\n\n## 1. 目标与范围\n- 主要功能：将 PyTorch 模型转换为 ONNX 格式，支持多种导出配置和模型类型\n- 期望行为：正确序列化模型为 ONNX 协议缓冲区，处理各种输入格式和导出选项\n- 不在范围内：ONNX 模型推理验证、第三方 ONNX 运行时集成、模型性能基准测试\n\n## 2. 输入与约束\n- 参数列表：\n  - model: torch.nn.Module/torch.jit.ScriptModule/torch.jit.ScriptFunction\n  - args: Tuple[Any]/torch.Tensor/带命名参数的元组\n  - f: str/io.BytesIO，输出目标\n  - export_params: bool, default=True\n  - verbose: bool, default=False\n  - training: _C_onnx.TrainingMode, default=EVAL\n  - input_names: Optional[Sequence[str]], default=None\n  - output_names: Optional[Sequence[str]], default=None\n  - operator_export_type: _C_onnx.OperatorExportTypes, default=ONNX\n  - opset_version: Optional[int], default=None (7-16范围)\n  - do_constant_folding: bool, default=True\n  - dynamic_axes: Optional[Union[Mapping[str, Mapping[int, str]], Mapping[str, Sequence[int]]]], default=None\n  - keep_initializers_as_inputs: Optional[bool], default=None\n  - custom_opsets: Optional[Mapping[str, int]], default=None\n  - export_modules_as_functions: Union[bool, Collection[Type[torch.nn.Module]]], default=False\n\n- 有效取值范围：\n  - opset_version: 7-16整数\n  - 动态轴定义：字典或列表格式\n  - 训练模式：EVAL/TRAINING/Preserve\n\n- 必需组合：\n  - 非 ScriptModule/Function 模型自动通过 torch.jit.trace 转换\n  - 导出训练模式需要 opset_version >= 12\n\n- 随机性/全局状态：\n  - 依赖 GLOBALS 全局状态管理导出配置\n  - torch.jit.trace 引入随机性\n\n## 3. 输出与判定\n- 期望返回：export()返回None，成功写入文件或缓冲区\n- export_to_pretty_string()返回UTF-8字符串格式的ONNX模型\n- is_in_onnx_export()返回布尔值指示导出状态\n- 容差：浮点数值精度误差在1e-6范围内\n- 状态变化：文件系统写入验证，缓冲区内容完整性检查\n- 副作用：全局状态GLOBALS正确更新和恢复\n\n## 4. 错误与异常场景\n- 非法输入：非模型对象、无效文件路径、不支持的数据类型\n- 维度错误：输入输出形状不匹配、动态轴定义冲突\n- 类型错误：参数类型不符合声明、Python对象无法序列化\n- 边界值：空模型、None输入、0长度张量、极端形状(如1x1x1)\n- 数值边界：NaN、Inf、极大/极小浮点值\n- 版本冲突：opset_version超出7-16范围、不支持的算子版本\n- 资源限制：内存不足、磁盘空间不足、文件权限错误\n\n## 5. 依赖与环境\n- 外部依赖：ONNX库、protobuf、文件系统访问\n- 设备要求：CPU/GPU张量支持，设备间数据传输\n- 需要mock部分：\n  - `torch.jit.trace`：控制跟踪行为\n  - `torch.onnx._internals._model_to_graph`：图转换过程\n  - `io.open`：文件I/O操作\n  - `GLOBALS`：全局状态管理\n  - `_beartype.beartype`：运行时类型检查\n  - `torch.nn.Module`的forward方法调用\n\n## 6. 覆盖与优先级\n- 必测路径（高优先级）：\n  1. 三种模型类型(nn.Module/ScriptModule/ScriptFunction)的基本导出\n  2. args三种格式(元组/张量/命名参数)的正确处理\n  3. opset_version边界值(7,16)和默认值的兼容性\n  4. 动态轴配置的字典和列表格式支持\n  5. 文件路径和BytesIO两种输出目标的正确写入\n\n- 可选路径（中/低优先级）：\n  - 不同operator_export_type(ONNX/ONNX_ATEN/ONNX_ATEN_FALLBACK)的行为差异\n  - export_params=False时的参数分离导出\n  - training模式切换与opset_version>=12的关联\n  - do_constant_folding优化效果验证\n  - custom_opsets自定义算子集配置\n  - export_modules_as_functions模块函数化导出\n  - keep_initializers_as_inputs不同设置的影响\n  - verbose详细输出模式的内容验证\n  - 复杂模型结构(嵌套、循环、条件)的导出能力\n\n- 已知风险/缺失信息：\n  - 动态控制流支持有限（与torch.jit.trace相同限制）\n  - 某些导出选项依赖特定构建配置（如Caffe2支持）\n  - 缺少具体张量形状和dtype的详细约束说明\n  - 模型参数序列化的内存使用边界\n  - 大模型导出的性能和时间消耗\n  - 多线程/多进程环境下的并发安全性",
    "test_plan.json": "{\n  \"plan_version\": 2,\n  \"target\": \"torch.onnx.utils\",\n  \"block_rules\": {\n    \"header_block\": \"HEADER\",\n    \"footer_block\": \"FOOTER\",\n    \"case_prefix\": \"CASE_\",\n    \"case_format\": \"CASE_01\"\n  },\n  \"iteration_strategy\": {\n    \"round1\": {\n      \"include\": \"SMOKE_SET\",\n      \"assert_level\": \"weak\",\n      \"max_blocks\": 5\n    },\n    \"roundN\": {\n      \"only_fix_failed_blocks\": true,\n      \"block_limit\": 3,\n      \"promote_deferred\": true\n    },\n    \"final\": {\n      \"enable_strong_asserts\": true,\n      \"coverage_optional\": true\n    }\n  },\n  \"test_files\": {\n    \"default\": \"tests/test_torch_onnx_utils.py\",\n    \"all_pattern\": \"tests/test_torch_onnx_utils_*.py\",\n    \"groups\": {\n      \"G1\": \"tests/test_torch_onnx_utils_export.py\",\n      \"G2\": \"tests/test_torch_onnx_utils_helpers.py\"\n    }\n  },\n  \"active_group_order\": [\"G1\", \"G2\"],\n  \"groups\": [\n    {\n      \"group_id\": \"G1\",\n      \"title\": \"核心导出函数族\",\n      \"entrypoints\": [\"export\", \"export_to_pretty_string\"],\n      \"smoke_set\": [\"CASE_01\", \"CASE_02\"],\n      \"deferred_set\": [\"CASE_05\", \"CASE_06\", \"CASE_07\"],\n      \"note\": \"测试模型导出到文件和字符串的核心功能\"\n    },\n    {\n      \"group_id\": \"G2\",\n      \"title\": \"辅助函数与状态管理\",\n      \"entrypoints\": [\"is_in_onnx_export\", \"_decide_keep_init_as_input\"],\n      \"smoke_set\": [\"CASE_03\"],\n      \"deferred_set\": [\"CASE_08\", \"CASE_09\"],\n      \"note\": \"测试导出状态管理和辅助决策函数\"\n    }\n  ],\n  \"cases\": [\n    {\n      \"tc_id\": \"TC-01\",\n      \"block_id\": \"CASE_01\",\n      \"group_id\": \"G1\",\n      \"name\": \"基本模型导出到文件\",\n      \"priority\": \"High\",\n      \"param_matrix\": [\n        {\n          \"model_type\": \"nn.Module\",\n          \"args_format\": \"tuple\",\n          \"output_target\": \"file\",\n          \"opset_version\": 13,\n          \"export_params\": true,\n          \"do_constant_folding\": true\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"file_exists\", \"file_not_empty\", \"no_exception\"],\n        \"strong\": [\"onnx_format_valid\", \"model_structure_preserved\", \"tensor_values_approx_equal\"]\n      },\n      \"oracle\": \"torch.onnx.export\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"M\",\n      \"max_lines\": 100,\n      \"max_params\": 8,\n      \"is_parametrized\": true,\n      \"requires_mock\": true,\n      \"mock_targets\": [\"torch.jit.trace\", \"io.open\", \"torch.onnx._internals._model_to_graph\"]\n    },\n    {\n      \"tc_id\": \"TC-02\",\n      \"block_id\": \"CASE_02\",\n      \"group_id\": \"G1\",\n      \"name\": \"模型导出到BytesIO缓冲区\",\n      \"priority\": \"High\",\n      \"param_matrix\": [\n        {\n          \"model_type\": \"nn.Module\",\n          \"args_format\": \"tensor\",\n          \"output_target\": \"bytesio\",\n          \"opset_version\": 13,\n          \"export_params\": true,\n          \"do_constant_folding\": true\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"buffer_not_empty\", \"buffer_position_changed\", \"no_exception\"],\n        \"strong\": [\"buffer_content_valid\", \"onnx_protobuf_parseable\", \"model_metadata_present\"]\n      },\n      \"oracle\": \"torch.onnx.export\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"M\",\n      \"max_lines\": 90,\n      \"max_params\": 7,\n      \"is_parametrized\": true,\n      \"requires_mock\": true,\n      \"mock_targets\": [\"torch.jit.trace\", \"torch.onnx._internals._model_to_graph\"]\n    },\n    {\n      \"tc_id\": \"TC-03\",\n      \"block_id\": \"CASE_03\",\n      \"group_id\": \"G2\",\n      \"name\": \"导出状态检测函数\",\n      \"priority\": \"High\",\n      \"param_matrix\": [\n        {\n          \"export_state\": \"inside_export\",\n          \"expected_result\": true\n        },\n        {\n          \"export_state\": \"outside_export\",\n          \"expected_result\": false\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"function_returns_bool\", \"no_exception\", \"state_transition_correct\"],\n        \"strong\": [\"thread_safe_check\", \"nested_export_state\", \"exception_state_cleanup\"]\n      },\n      \"oracle\": \"torch.onnx.utils.is_in_onnx_export\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"S\",\n      \"max_lines\": 60,\n      \"max_params\": 4,\n      \"is_parametrized\": true,\n      \"requires_mock\": true,\n      \"mock_targets\": [\"torch.onnx.utils.GLOBALS\"]\n    },\n    {\n      \"tc_id\": \"TC-04\",\n      \"block_id\": \"CASE_04\",\n      \"group_id\": \"G1\",\n      \"name\": \"ScriptModule模型导出\",\n      \"priority\": \"High\",\n      \"param_matrix\": [\n        {\n          \"model_type\": \"ScriptModule\",\n          \"args_format\": \"tuple\",\n          \"output_target\": \"file\",\n          \"opset_version\": 13,\n          \"export_params\": true,\n          \"do_constant_folding\": true\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"file_exists\", \"no_trace_called\", \"no_exception\"],\n        \"strong\": [\"script_module_preserved\", \"torchscript_compatibility\", \"graph_optimization_applied\"]\n      },\n      \"oracle\": \"torch.onnx.export\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"M\",\n      \"max_lines\": 95,\n      \"max_params\": 7,\n      \"is_parametrized\": true,\n      \"requires_mock\": true,\n      \"mock_targets\": [\"torch.onnx._internals._model_to_graph\", \"io.open\"]\n    },\n    {\n      \"tc_id\": \"TC-05\",\n      \"block_id\": \"CASE_05\",\n      \"group_id\": \"G1\",\n      \"name\": \"动态轴配置导出\",\n      \"priority\": \"Medium\",\n      \"param_matrix\": [\n        {\n          \"model_type\": \"nn.Module\",\n          \"args_format\": \"tuple\",\n          \"output_target\": \"file\",\n          \"opset_version\": 13,\n          \"dynamic_axes_format\": \"dict\",\n          \"export_params\": true\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"file_exists\", \"dynamic_axes_applied\", \"no_exception\"],\n        \"strong\": [\"dynamic_dimensions_present\", \"shape_inference_correct\", \"multiple_dynamic_axes\"]\n      },\n      \"oracle\": \"torch.onnx.export\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"L\",\n      \"max_lines\": 120,\n      \"max_params\": 9,\n      \"is_parametrized\": true,\n      \"requires_mock\": true,\n      \"mock_targets\": [\"torch.jit.trace\", \"torch.onnx._internals._model_to_graph\", \"io.open\"]\n    }\n  ],\n  \"param_extensions\": [\n    {\n      \"base_block_id\": \"CASE_01\",\n      \"priority\": \"Medium\",\n      \"params\": {\n        \"model_type\": \"nn.Module\",\n        \"args_format\": \"named_tuple\",\n        \"output_target\": \"file\",\n        \"opset_version\": 13,\n        \"export_params\": true,\n        \"do_constant_folding\": false\n      },\n      \"note\": \"命名参数格式和禁用常量折叠\"\n    },\n    {\n      \"base_block_id\": \"CASE_01\",\n      \"priority\": \"Medium\",\n      \"params\": {\n        \"model_type\": \"nn.Module\",\n        \"args_format\": \"tuple\",\n        \"output_target\": \"file\",\n        \"opset_version\": 7,\n        \"export_params\": true,\n        \"do_constant_folding\": true\n      },\n      \"note\": \"最低opset版本边界测试\"\n    },\n    {\n      \"base_block_id\": \"CASE_01\",\n      \"priority\": \"Medium\",\n      \"params\": {\n        \"model_type\": \"nn.Module\",\n        \"args_format\": \"tuple\",\n        \"output_target\": \"file\",\n        \"opset_version\": 16,\n        \"export_params\": false,\n        \"do_constant_folding\": true\n      },\n      \"note\": \"最高opset版本和参数分离导出\"\n    },\n    {\n      \"base_block_id\": \"CASE_02\",\n      \"priority\": \"Low\",\n      \"params\": {\n        \"model_type\": \"ScriptFunction\",\n        \"args_format\": \"tensor\",\n        \"output_target\": \"bytesio\",\n        \"opset_version\": 13,\n        \"export_params\": true,\n        \"do_constant_folding\": true\n      },\n      \"note\": \"ScriptFunction类型模型测试\"\n    }\n  ],\n  \"smoke_set\": [\"CASE_01\", \"CASE_02\", \"CASE_03\"],\n  \"deferred_set\": [\"CASE_04\", \"CASE_05\", \"CASE_06\", \"CASE_07\", \"CASE_08\", \"CASE_09\"]\n}",
    "test_plan.md": "# torch.onnx.utils 测试计划\n\n## 1. 测试策略\n- 单元测试框架：pytest\n- 隔离策略：mock/monkeypatch/fixtures 控制外部依赖\n- 随机性处理：固定随机种子，控制 torch.jit.trace 行为\n- 状态管理：mock GLOBALS 全局状态，确保测试隔离\n\n## 2. 生成规格摘要（来自 test_plan.json）\n- **SMOKE_SET**: CASE_01（基本模型导出到文件）、CASE_02（模型导出到BytesIO缓冲区）、CASE_03（导出状态检测函数）\n- **DEFERRED_SET**: CASE_04（ScriptModule模型导出）、CASE_05（动态轴配置导出）等6个用例\n- **group 列表**: \n  - G1: 核心导出函数族（export, export_to_pretty_string）\n  - G2: 辅助函数与状态管理（is_in_onnx_export, _decide_keep_init_as_input）\n- **active_group_order**: [\"G1\", \"G2\"] - 优先测试核心导出功能\n- **断言分级策略**: 首轮使用weak断言（文件存在性、无异常等），后续启用strong断言（模型结构保持、数值精度等）\n- **预算策略**: \n  - size: S(60行)/M(100行)/L(120行)\n  - max_params: 4-9个参数\n  - 参数化测试优先，减少重复代码\n\n## 3. 数据与边界\n- **正常数据集**: 简单线性模型、卷积网络、多层感知机\n- **随机生成策略**: 固定种子生成随机权重，确保可重现\n- **边界值**: \n  - 空模型（仅forward方法）\n  - 1x1x1极端形状张量\n  - opset_version边界值7和16\n  - 动态轴空配置与复杂配置\n- **负例与异常场景**:\n  - 无效模型对象\n  - 不支持的数据类型\n  - 文件权限错误\n  - 内存不足模拟\n  - opset_version超出范围\n\n## 4. 覆盖映射\n- **TC-01**: 覆盖需求中的三种模型类型基本导出、文件输出目标\n- **TC-02**: 覆盖BytesIO输出目标、张量格式参数\n- **TC-03**: 覆盖全局状态管理、导出状态检测\n- **TC-04**: 覆盖ScriptModule类型、避免torch.jit.trace调用\n- **TC-05**: 覆盖动态轴配置、字典格式支持\n\n- **尚未覆盖的风险点**:\n  - 复杂模型结构（嵌套、循环、条件）导出能力\n  - 大模型导出的性能和时间消耗\n  - 多线程环境下的并发安全性\n  - 不同operator_export_type的行为差异\n  - 训练模式与opset_version>=12的关联验证",
    "tests/test_torch_onnx_utils.py": "\"\"\"\nTest module for torch.onnx.utils\n\"\"\"\nimport io\nimport os\nimport tempfile\nimport warnings\nfrom unittest import mock\n\nimport pytest\nimport torch\nimport torch.nn as nn\nfrom torch.onnx import utils as onnx_utils\nfrom torch.onnx._globals import GLOBALS\n\n# ==== BLOCK:HEADER START ====\n# Test fixtures and helper functions\n# ==== BLOCK:HEADER END ====\n\n# ==== BLOCK:CASE_01 START ====\n# Placeholder for CASE_01: 基本模型导出到文件\n# ==== BLOCK:CASE_01 END ====\n\n# ==== BLOCK:CASE_02 START ====\n# Placeholder for CASE_02: 模型导出到BytesIO缓冲区\n# ==== BLOCK:CASE_02 END ====\n\n# ==== BLOCK:CASE_03 START ====\n# Placeholder for CASE_03: 导出状态检测函数\n# ==== BLOCK:CASE_03 END ====\n\n# ==== BLOCK:CASE_04 START ====\n# Placeholder for CASE_04: ScriptModule模型导出 (DEFERRED)\n# ==== BLOCK:CASE_04 END ====\n\n# ==== BLOCK:CASE_05 START ====\n# Placeholder for CASE_05: 动态轴配置导出 (DEFERRED)\n# ==== BLOCK:CASE_05 END ====\n\n# ==== BLOCK:CASE_06 START ====\n# Placeholder for CASE_06: (DEFERRED)\n# ==== BLOCK:CASE_06 END ====\n\n# ==== BLOCK:CASE_07 START ====\n# Placeholder for CASE_07: (DEFERRED)\n# ==== BLOCK:CASE_07 END ====\n\n# ==== BLOCK:CASE_08 START ====\n# Placeholder for CASE_08: (DEFERRED)\n# ==== BLOCK:CASE_08 END ====\n\n# ==== BLOCK:CASE_09 START ====\n# Placeholder for CASE_09: (DEFERRED)\n# ==== BLOCK:CASE_09 END ====\n\n# ==== BLOCK:FOOTER START ====\n# Test class and additional helper functions\n# ==== BLOCK:FOOTER END ====",
    "execution_log.txt": "=== Run Tests ===\nFFFFFF...FFF...                                                          [100%]\n=================================== FAILURES ===================================\n______ test_export_basic_model_to_file[nn.Module-tuple-file-13-True-True] ______\n\nmodel_type = 'nn.Module', args_format = 'tuple', output_target = 'file'\nopset_version = 13, export_params = True, do_constant_folding = True\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsimple_conv_model = SimpleConvModel(\n  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\nsample_input_tensor = tensor([[ 0.4485,  0.0330,  1.4503],\n        [-0.6936,  0.9967,  0.6131]])\nsample_input_tensor_2d = tensor([[[[-9.5632e-01, -1.2476e+00, -7.4994e-01, -5.9219e-01,  1.7744e+00,\n           -9.2155e-01,  9.6245e-01, -3.37...  7.4018e-01,  1.4162e+00,  6.8340e-01,\n           -1.3825e-01,  9.8639e-01, -3.8926e-01,  6.1381e-01, -2.7863e-01]]]])\ntemp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpgy17c1bo.onnx'\n\n    @pytest.mark.parametrize(\n        \"model_type,args_format,output_target,opset_version,export_params,do_constant_folding\",\n        [\n            # Base case from test plan\n            (\"nn.Module\", \"tuple\", \"file\", 13, True, True),\n            # Parameter extensions from test plan\n            (\"nn.Module\", \"named_tuple\", \"file\", 13, True, False),\n            (\"nn.Module\", \"tuple\", \"file\", 7, True, True),\n            (\"nn.Module\", \"tuple\", \"file\", 16, False, True),\n        ]\n    )\n    def test_export_basic_model_to_file(\n        model_type,\n        args_format,\n        output_target,\n        opset_version,\n        export_params,\n        do_constant_folding,\n        simple_linear_model,\n        simple_conv_model,\n        sample_input_tensor,\n        sample_input_tensor_2d,\n        temp_onnx_file,\n    ):\n        \"\"\"\n        Test basic model export to file with various configurations.\n    \n        This test covers the core export functionality with different:\n        - Model types (nn.Module)\n        - Args formats (tuple, named_tuple)\n        - Opset versions (7, 13, 16)\n        - Export parameters (True/False)\n        - Constant folding (True/False)\n    \n        Weak assertions: file exists, file not empty, no exception.\n        \"\"\"\n        # Select model based on type\n        if model_type == \"nn.Module\":\n            # Use linear model for 1D input, conv model for 2D input\n            if args_format == \"tuple\":\n                model = simple_linear_model\n                args = (sample_input_tensor,)\n            else:  # named_tuple\n                model = simple_conv_model\n                args = (sample_input_tensor_2d,)\n    \n        # Prepare args based on format\n        if args_format == \"named_tuple\":\n            # Create args with named parameters\n            args = (args[0], {\"dummy_param\": torch.tensor([1.0])})\n    \n        # Mock dependencies - we need to mock the entire export chain\n        # to avoid C++ type issues\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export, \\\n             mock.patch('io.open', mock.mock_open()) as mock_file:\n    \n            # Setup mock returns for _get_trace_graph\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            # Create a proper mock for _model_to_graph that returns appropriate types\n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n    \n            # Mock _export to do nothing (just track calls)\n            mock_export.return_value = None\n    \n            # Call export\n            try:\n                onnx_utils.export(\n                    model=model,\n                    args=args,\n                    f=temp_onnx_file,\n                    export_params=export_params,\n                    opset_version=opset_version,\n                    do_constant_folding=do_constant_folding,\n                    verbose=False,\n                )\n    \n                # Weak assertion 1: No exception raised\n                # (implicitly passed if we reach here)\n    \n                # Weak assertion 2: File was opened for writing\n                # Since we're mocking io.open, check that it was called\n>               assert mock_file.called, \"File was not opened for writing\"\nE               AssertionError: File was not opened for writing\nE               assert False\nE                +  where False = <MagicMock name='open' spec='builtin_function_or_method' id='5169808528'>.called\n\ntests/test_torch_onnx_utils.py:239: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel_type = 'nn.Module', args_format = 'tuple', output_target = 'file'\nopset_version = 13, export_params = True, do_constant_folding = True\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsimple_conv_model = SimpleConvModel(\n  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\nsample_input_tensor = tensor([[ 0.4485,  0.0330,  1.4503],\n        [-0.6936,  0.9967,  0.6131]])\nsample_input_tensor_2d = tensor([[[[-9.5632e-01, -1.2476e+00, -7.4994e-01, -5.9219e-01,  1.7744e+00,\n           -9.2155e-01,  9.6245e-01, -3.37...  7.4018e-01,  1.4162e+00,  6.8340e-01,\n           -1.3825e-01,  9.8639e-01, -3.8926e-01,  6.1381e-01, -2.7863e-01]]]])\ntemp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpgy17c1bo.onnx'\n\n    @pytest.mark.parametrize(\n        \"model_type,args_format,output_target,opset_version,export_params,do_constant_folding\",\n        [\n            # Base case from test plan\n            (\"nn.Module\", \"tuple\", \"file\", 13, True, True),\n            # Parameter extensions from test plan\n            (\"nn.Module\", \"named_tuple\", \"file\", 13, True, False),\n            (\"nn.Module\", \"tuple\", \"file\", 7, True, True),\n            (\"nn.Module\", \"tuple\", \"file\", 16, False, True),\n        ]\n    )\n    def test_export_basic_model_to_file(\n        model_type,\n        args_format,\n        output_target,\n        opset_version,\n        export_params,\n        do_constant_folding,\n        simple_linear_model,\n        simple_conv_model,\n        sample_input_tensor,\n        sample_input_tensor_2d,\n        temp_onnx_file,\n    ):\n        \"\"\"\n        Test basic model export to file with various configurations.\n    \n        This test covers the core export functionality with different:\n        - Model types (nn.Module)\n        - Args formats (tuple, named_tuple)\n        - Opset versions (7, 13, 16)\n        - Export parameters (True/False)\n        - Constant folding (True/False)\n    \n        Weak assertions: file exists, file not empty, no exception.\n        \"\"\"\n        # Select model based on type\n        if model_type == \"nn.Module\":\n            # Use linear model for 1D input, conv model for 2D input\n            if args_format == \"tuple\":\n                model = simple_linear_model\n                args = (sample_input_tensor,)\n            else:  # named_tuple\n                model = simple_conv_model\n                args = (sample_input_tensor_2d,)\n    \n        # Prepare args based on format\n        if args_format == \"named_tuple\":\n            # Create args with named parameters\n            args = (args[0], {\"dummy_param\": torch.tensor([1.0])})\n    \n        # Mock dependencies - we need to mock the entire export chain\n        # to avoid C++ type issues\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export, \\\n             mock.patch('io.open', mock.mock_open()) as mock_file:\n    \n            # Setup mock returns for _get_trace_graph\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            # Create a proper mock for _model_to_graph that returns appropriate types\n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n    \n            # Mock _export to do nothing (just track calls)\n            mock_export.return_value = None\n    \n            # Call export\n            try:\n                onnx_utils.export(\n                    model=model,\n                    args=args,\n                    f=temp_onnx_file,\n                    export_params=export_params,\n                    opset_version=opset_version,\n                    do_constant_folding=do_constant_folding,\n                    verbose=False,\n                )\n    \n                # Weak assertion 1: No exception raised\n                # (implicitly passed if we reach here)\n    \n                # Weak assertion 2: File was opened for writing\n                # Since we're mocking io.open, check that it was called\n                assert mock_file.called, \"File was not opened for writing\"\n    \n                # Check that write was called (indicating content was written)\n                write_calls = [call for call in mock_file.mock_calls if call[0] == '().write']\n                assert len(write_calls) > 0, \"No data was written to file\"\n    \n                # Weak assertion 3: _get_trace_graph was called for nn.Module\n                if model_type == \"nn.Module\":\n                    assert mock_get_trace_graph.called, \"torch.jit._get_trace_graph should be called for nn.Module\"\n    \n                # Weak assertion 4: _model_to_graph was called\n                assert mock_model_to_graph.called, \"_model_to_graph should be called\"\n    \n                # Weak assertion 5: _export was called\n                assert mock_export.called, \"_export should be called\"\n    \n                # Additional weak checks\n                assert opset_version >= 7 and opset_version <= 16, \\\n                    f\"Opset version {opset_version} should be in range 7-16\"\n    \n                # Check that export was called with correct parameters\n                mock_export.assert_called_once()\n                call_args = mock_export.call_args\n    \n                # Verify key parameters\n                assert call_args[0][0] == model, \"Model parameter should match\"\n                assert call_args[0][1] == args, \"Args parameter should match\"\n                assert call_args[0][2] == temp_onnx_file, \"File parameter should match\"\n                assert call_args[1]['export_params'] == export_params, \"export_params should match\"\n                assert call_args[1]['opset_version'] == opset_version, \"opset_version should match\"\n                assert call_args[1]['do_constant_folding'] == do_constant_folding, \"do_constant_folding should match\"\n    \n            except Exception as e:\n>               pytest.fail(f\"Export raised unexpected exception: {e}\")\nE               Failed: Export raised unexpected exception: File was not opened for writing\nE               assert False\nE                +  where False = <MagicMock name='open' spec='builtin_function_or_method' id='5169808528'>.called\n\ntests/test_torch_onnx_utils.py:272: Failed\n__ test_export_basic_model_to_file[nn.Module-named_tuple-file-13-True-False] ___\n\nmodel_type = 'nn.Module', args_format = 'named_tuple', output_target = 'file'\nopset_version = 13, export_params = True, do_constant_folding = False\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsimple_conv_model = SimpleConvModel(\n  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\nsample_input_tensor = tensor([[ 0.1005,  0.2325,  0.7064],\n        [ 0.3693, -0.8368, -1.2204]])\nsample_input_tensor_2d = tensor([[[[-1.3386e-01, -5.8557e-02,  1.2574e-01, -5.5258e-01,  7.4480e-02,\n           -1.4929e-01, -5.5225e-01, -9.34...  4.5689e-01, -6.0341e-01,  4.6268e-01,\n            2.4934e-01, -1.7062e-01, -6.1410e-01,  5.3551e-01, -1.6644e+00]]]])\ntemp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpslgxnvn2.onnx'\n\n    @pytest.mark.parametrize(\n        \"model_type,args_format,output_target,opset_version,export_params,do_constant_folding\",\n        [\n            # Base case from test plan\n            (\"nn.Module\", \"tuple\", \"file\", 13, True, True),\n            # Parameter extensions from test plan\n            (\"nn.Module\", \"named_tuple\", \"file\", 13, True, False),\n            (\"nn.Module\", \"tuple\", \"file\", 7, True, True),\n            (\"nn.Module\", \"tuple\", \"file\", 16, False, True),\n        ]\n    )\n    def test_export_basic_model_to_file(\n        model_type,\n        args_format,\n        output_target,\n        opset_version,\n        export_params,\n        do_constant_folding,\n        simple_linear_model,\n        simple_conv_model,\n        sample_input_tensor,\n        sample_input_tensor_2d,\n        temp_onnx_file,\n    ):\n        \"\"\"\n        Test basic model export to file with various configurations.\n    \n        This test covers the core export functionality with different:\n        - Model types (nn.Module)\n        - Args formats (tuple, named_tuple)\n        - Opset versions (7, 13, 16)\n        - Export parameters (True/False)\n        - Constant folding (True/False)\n    \n        Weak assertions: file exists, file not empty, no exception.\n        \"\"\"\n        # Select model based on type\n        if model_type == \"nn.Module\":\n            # Use linear model for 1D input, conv model for 2D input\n            if args_format == \"tuple\":\n                model = simple_linear_model\n                args = (sample_input_tensor,)\n            else:  # named_tuple\n                model = simple_conv_model\n                args = (sample_input_tensor_2d,)\n    \n        # Prepare args based on format\n        if args_format == \"named_tuple\":\n            # Create args with named parameters\n            args = (args[0], {\"dummy_param\": torch.tensor([1.0])})\n    \n        # Mock dependencies - we need to mock the entire export chain\n        # to avoid C++ type issues\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export, \\\n             mock.patch('io.open', mock.mock_open()) as mock_file:\n    \n            # Setup mock returns for _get_trace_graph\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            # Create a proper mock for _model_to_graph that returns appropriate types\n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n    \n            # Mock _export to do nothing (just track calls)\n            mock_export.return_value = None\n    \n            # Call export\n            try:\n                onnx_utils.export(\n                    model=model,\n                    args=args,\n                    f=temp_onnx_file,\n                    export_params=export_params,\n                    opset_version=opset_version,\n                    do_constant_folding=do_constant_folding,\n                    verbose=False,\n                )\n    \n                # Weak assertion 1: No exception raised\n                # (implicitly passed if we reach here)\n    \n                # Weak assertion 2: File was opened for writing\n                # Since we're mocking io.open, check that it was called\n>               assert mock_file.called, \"File was not opened for writing\"\nE               AssertionError: File was not opened for writing\nE               assert False\nE                +  where False = <MagicMock name='open' spec='builtin_function_or_method' id='5170301104'>.called\n\ntests/test_torch_onnx_utils.py:239: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel_type = 'nn.Module', args_format = 'named_tuple', output_target = 'file'\nopset_version = 13, export_params = True, do_constant_folding = False\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsimple_conv_model = SimpleConvModel(\n  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\nsample_input_tensor = tensor([[ 0.1005,  0.2325,  0.7064],\n        [ 0.3693, -0.8368, -1.2204]])\nsample_input_tensor_2d = tensor([[[[-1.3386e-01, -5.8557e-02,  1.2574e-01, -5.5258e-01,  7.4480e-02,\n           -1.4929e-01, -5.5225e-01, -9.34...  4.5689e-01, -6.0341e-01,  4.6268e-01,\n            2.4934e-01, -1.7062e-01, -6.1410e-01,  5.3551e-01, -1.6644e+00]]]])\ntemp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpslgxnvn2.onnx'\n\n    @pytest.mark.parametrize(\n        \"model_type,args_format,output_target,opset_version,export_params,do_constant_folding\",\n        [\n            # Base case from test plan\n            (\"nn.Module\", \"tuple\", \"file\", 13, True, True),\n            # Parameter extensions from test plan\n            (\"nn.Module\", \"named_tuple\", \"file\", 13, True, False),\n            (\"nn.Module\", \"tuple\", \"file\", 7, True, True),\n            (\"nn.Module\", \"tuple\", \"file\", 16, False, True),\n        ]\n    )\n    def test_export_basic_model_to_file(\n        model_type,\n        args_format,\n        output_target,\n        opset_version,\n        export_params,\n        do_constant_folding,\n        simple_linear_model,\n        simple_conv_model,\n        sample_input_tensor,\n        sample_input_tensor_2d,\n        temp_onnx_file,\n    ):\n        \"\"\"\n        Test basic model export to file with various configurations.\n    \n        This test covers the core export functionality with different:\n        - Model types (nn.Module)\n        - Args formats (tuple, named_tuple)\n        - Opset versions (7, 13, 16)\n        - Export parameters (True/False)\n        - Constant folding (True/False)\n    \n        Weak assertions: file exists, file not empty, no exception.\n        \"\"\"\n        # Select model based on type\n        if model_type == \"nn.Module\":\n            # Use linear model for 1D input, conv model for 2D input\n            if args_format == \"tuple\":\n                model = simple_linear_model\n                args = (sample_input_tensor,)\n            else:  # named_tuple\n                model = simple_conv_model\n                args = (sample_input_tensor_2d,)\n    \n        # Prepare args based on format\n        if args_format == \"named_tuple\":\n            # Create args with named parameters\n            args = (args[0], {\"dummy_param\": torch.tensor([1.0])})\n    \n        # Mock dependencies - we need to mock the entire export chain\n        # to avoid C++ type issues\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export, \\\n             mock.patch('io.open', mock.mock_open()) as mock_file:\n    \n            # Setup mock returns for _get_trace_graph\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            # Create a proper mock for _model_to_graph that returns appropriate types\n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n    \n            # Mock _export to do nothing (just track calls)\n            mock_export.return_value = None\n    \n            # Call export\n            try:\n                onnx_utils.export(\n                    model=model,\n                    args=args,\n                    f=temp_onnx_file,\n                    export_params=export_params,\n                    opset_version=opset_version,\n                    do_constant_folding=do_constant_folding,\n                    verbose=False,\n                )\n    \n                # Weak assertion 1: No exception raised\n                # (implicitly passed if we reach here)\n    \n                # Weak assertion 2: File was opened for writing\n                # Since we're mocking io.open, check that it was called\n                assert mock_file.called, \"File was not opened for writing\"\n    \n                # Check that write was called (indicating content was written)\n                write_calls = [call for call in mock_file.mock_calls if call[0] == '().write']\n                assert len(write_calls) > 0, \"No data was written to file\"\n    \n                # Weak assertion 3: _get_trace_graph was called for nn.Module\n                if model_type == \"nn.Module\":\n                    assert mock_get_trace_graph.called, \"torch.jit._get_trace_graph should be called for nn.Module\"\n    \n                # Weak assertion 4: _model_to_graph was called\n                assert mock_model_to_graph.called, \"_model_to_graph should be called\"\n    \n                # Weak assertion 5: _export was called\n                assert mock_export.called, \"_export should be called\"\n    \n                # Additional weak checks\n                assert opset_version >= 7 and opset_version <= 16, \\\n                    f\"Opset version {opset_version} should be in range 7-16\"\n    \n                # Check that export was called with correct parameters\n                mock_export.assert_called_once()\n                call_args = mock_export.call_args\n    \n                # Verify key parameters\n                assert call_args[0][0] == model, \"Model parameter should match\"\n                assert call_args[0][1] == args, \"Args parameter should match\"\n                assert call_args[0][2] == temp_onnx_file, \"File parameter should match\"\n                assert call_args[1]['export_params'] == export_params, \"export_params should match\"\n                assert call_args[1]['opset_version'] == opset_version, \"opset_version should match\"\n                assert call_args[1]['do_constant_folding'] == do_constant_folding, \"do_constant_folding should match\"\n    \n            except Exception as e:\n>               pytest.fail(f\"Export raised unexpected exception: {e}\")\nE               Failed: Export raised unexpected exception: File was not opened for writing\nE               assert False\nE                +  where False = <MagicMock name='open' spec='builtin_function_or_method' id='5170301104'>.called\n\ntests/test_torch_onnx_utils.py:272: Failed\n______ test_export_basic_model_to_file[nn.Module-tuple-file-7-True-True] _______\n\nmodel_type = 'nn.Module', args_format = 'tuple', output_target = 'file'\nopset_version = 7, export_params = True, do_constant_folding = True\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsimple_conv_model = SimpleConvModel(\n  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\nsample_input_tensor = tensor([[-0.2662,  1.0295,  0.1327],\n        [ 1.8114, -1.0315,  1.1471]])\nsample_input_tensor_2d = tensor([[[[-0.8443, -0.3292, -0.1140, -0.8452,  0.5263,  0.1255, -1.7527,\n            0.7752,  0.2892,  0.3520],\n     ...9],\n          [ 0.1342, -1.1246, -1.8830,  1.6258, -0.7038,  0.5581,  0.2723,\n           -0.4286,  0.8240, -1.3990]]]])\ntemp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmph7zx_b55.onnx'\n\n    @pytest.mark.parametrize(\n        \"model_type,args_format,output_target,opset_version,export_params,do_constant_folding\",\n        [\n            # Base case from test plan\n            (\"nn.Module\", \"tuple\", \"file\", 13, True, True),\n            # Parameter extensions from test plan\n            (\"nn.Module\", \"named_tuple\", \"file\", 13, True, False),\n            (\"nn.Module\", \"tuple\", \"file\", 7, True, True),\n            (\"nn.Module\", \"tuple\", \"file\", 16, False, True),\n        ]\n    )\n    def test_export_basic_model_to_file(\n        model_type,\n        args_format,\n        output_target,\n        opset_version,\n        export_params,\n        do_constant_folding,\n        simple_linear_model,\n        simple_conv_model,\n        sample_input_tensor,\n        sample_input_tensor_2d,\n        temp_onnx_file,\n    ):\n        \"\"\"\n        Test basic model export to file with various configurations.\n    \n        This test covers the core export functionality with different:\n        - Model types (nn.Module)\n        - Args formats (tuple, named_tuple)\n        - Opset versions (7, 13, 16)\n        - Export parameters (True/False)\n        - Constant folding (True/False)\n    \n        Weak assertions: file exists, file not empty, no exception.\n        \"\"\"\n        # Select model based on type\n        if model_type == \"nn.Module\":\n            # Use linear model for 1D input, conv model for 2D input\n            if args_format == \"tuple\":\n                model = simple_linear_model\n                args = (sample_input_tensor,)\n            else:  # named_tuple\n                model = simple_conv_model\n                args = (sample_input_tensor_2d,)\n    \n        # Prepare args based on format\n        if args_format == \"named_tuple\":\n            # Create args with named parameters\n            args = (args[0], {\"dummy_param\": torch.tensor([1.0])})\n    \n        # Mock dependencies - we need to mock the entire export chain\n        # to avoid C++ type issues\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export, \\\n             mock.patch('io.open', mock.mock_open()) as mock_file:\n    \n            # Setup mock returns for _get_trace_graph\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            # Create a proper mock for _model_to_graph that returns appropriate types\n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n    \n            # Mock _export to do nothing (just track calls)\n            mock_export.return_value = None\n    \n            # Call export\n            try:\n                onnx_utils.export(\n                    model=model,\n                    args=args,\n                    f=temp_onnx_file,\n                    export_params=export_params,\n                    opset_version=opset_version,\n                    do_constant_folding=do_constant_folding,\n                    verbose=False,\n                )\n    \n                # Weak assertion 1: No exception raised\n                # (implicitly passed if we reach here)\n    \n                # Weak assertion 2: File was opened for writing\n                # Since we're mocking io.open, check that it was called\n>               assert mock_file.called, \"File was not opened for writing\"\nE               AssertionError: File was not opened for writing\nE               assert False\nE                +  where False = <MagicMock name='open' spec='builtin_function_or_method' id='5170655104'>.called\n\ntests/test_torch_onnx_utils.py:239: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel_type = 'nn.Module', args_format = 'tuple', output_target = 'file'\nopset_version = 7, export_params = True, do_constant_folding = True\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsimple_conv_model = SimpleConvModel(\n  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\nsample_input_tensor = tensor([[-0.2662,  1.0295,  0.1327],\n        [ 1.8114, -1.0315,  1.1471]])\nsample_input_tensor_2d = tensor([[[[-0.8443, -0.3292, -0.1140, -0.8452,  0.5263,  0.1255, -1.7527,\n            0.7752,  0.2892,  0.3520],\n     ...9],\n          [ 0.1342, -1.1246, -1.8830,  1.6258, -0.7038,  0.5581,  0.2723,\n           -0.4286,  0.8240, -1.3990]]]])\ntemp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmph7zx_b55.onnx'\n\n    @pytest.mark.parametrize(\n        \"model_type,args_format,output_target,opset_version,export_params,do_constant_folding\",\n        [\n            # Base case from test plan\n            (\"nn.Module\", \"tuple\", \"file\", 13, True, True),\n            # Parameter extensions from test plan\n            (\"nn.Module\", \"named_tuple\", \"file\", 13, True, False),\n            (\"nn.Module\", \"tuple\", \"file\", 7, True, True),\n            (\"nn.Module\", \"tuple\", \"file\", 16, False, True),\n        ]\n    )\n    def test_export_basic_model_to_file(\n        model_type,\n        args_format,\n        output_target,\n        opset_version,\n        export_params,\n        do_constant_folding,\n        simple_linear_model,\n        simple_conv_model,\n        sample_input_tensor,\n        sample_input_tensor_2d,\n        temp_onnx_file,\n    ):\n        \"\"\"\n        Test basic model export to file with various configurations.\n    \n        This test covers the core export functionality with different:\n        - Model types (nn.Module)\n        - Args formats (tuple, named_tuple)\n        - Opset versions (7, 13, 16)\n        - Export parameters (True/False)\n        - Constant folding (True/False)\n    \n        Weak assertions: file exists, file not empty, no exception.\n        \"\"\"\n        # Select model based on type\n        if model_type == \"nn.Module\":\n            # Use linear model for 1D input, conv model for 2D input\n            if args_format == \"tuple\":\n                model = simple_linear_model\n                args = (sample_input_tensor,)\n            else:  # named_tuple\n                model = simple_conv_model\n                args = (sample_input_tensor_2d,)\n    \n        # Prepare args based on format\n        if args_format == \"named_tuple\":\n            # Create args with named parameters\n            args = (args[0], {\"dummy_param\": torch.tensor([1.0])})\n    \n        # Mock dependencies - we need to mock the entire export chain\n        # to avoid C++ type issues\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export, \\\n             mock.patch('io.open', mock.mock_open()) as mock_file:\n    \n            # Setup mock returns for _get_trace_graph\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            # Create a proper mock for _model_to_graph that returns appropriate types\n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n    \n            # Mock _export to do nothing (just track calls)\n            mock_export.return_value = None\n    \n            # Call export\n            try:\n                onnx_utils.export(\n                    model=model,\n                    args=args,\n                    f=temp_onnx_file,\n                    export_params=export_params,\n                    opset_version=opset_version,\n                    do_constant_folding=do_constant_folding,\n                    verbose=False,\n                )\n    \n                # Weak assertion 1: No exception raised\n                # (implicitly passed if we reach here)\n    \n                # Weak assertion 2: File was opened for writing\n                # Since we're mocking io.open, check that it was called\n                assert mock_file.called, \"File was not opened for writing\"\n    \n                # Check that write was called (indicating content was written)\n                write_calls = [call for call in mock_file.mock_calls if call[0] == '().write']\n                assert len(write_calls) > 0, \"No data was written to file\"\n    \n                # Weak assertion 3: _get_trace_graph was called for nn.Module\n                if model_type == \"nn.Module\":\n                    assert mock_get_trace_graph.called, \"torch.jit._get_trace_graph should be called for nn.Module\"\n    \n                # Weak assertion 4: _model_to_graph was called\n                assert mock_model_to_graph.called, \"_model_to_graph should be called\"\n    \n                # Weak assertion 5: _export was called\n                assert mock_export.called, \"_export should be called\"\n    \n                # Additional weak checks\n                assert opset_version >= 7 and opset_version <= 16, \\\n                    f\"Opset version {opset_version} should be in range 7-16\"\n    \n                # Check that export was called with correct parameters\n                mock_export.assert_called_once()\n                call_args = mock_export.call_args\n    \n                # Verify key parameters\n                assert call_args[0][0] == model, \"Model parameter should match\"\n                assert call_args[0][1] == args, \"Args parameter should match\"\n                assert call_args[0][2] == temp_onnx_file, \"File parameter should match\"\n                assert call_args[1]['export_params'] == export_params, \"export_params should match\"\n                assert call_args[1]['opset_version'] == opset_version, \"opset_version should match\"\n                assert call_args[1]['do_constant_folding'] == do_constant_folding, \"do_constant_folding should match\"\n    \n            except Exception as e:\n>               pytest.fail(f\"Export raised unexpected exception: {e}\")\nE               Failed: Export raised unexpected exception: File was not opened for writing\nE               assert False\nE                +  where False = <MagicMock name='open' spec='builtin_function_or_method' id='5170655104'>.called\n\ntests/test_torch_onnx_utils.py:272: Failed\n_____ test_export_basic_model_to_file[nn.Module-tuple-file-16-False-True] ______\n\nmodel_type = 'nn.Module', args_format = 'tuple', output_target = 'file'\nopset_version = 16, export_params = False, do_constant_folding = True\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsimple_conv_model = SimpleConvModel(\n  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\nsample_input_tensor = tensor([[-1.7534, -1.3003,  0.5800],\n        [ 0.0158, -1.6756,  1.0759]])\nsample_input_tensor_2d = tensor([[[[-0.4210, -0.6637,  1.4599, -1.9324,  0.2951,  1.9531,  0.0623,\n           -1.1079, -0.0518,  0.4115],\n     ...7],\n          [ 1.5941,  2.5384, -0.4962,  1.5591, -1.2024, -0.6351,  0.2057,\n            1.2085, -1.2975, -1.7842]]]])\ntemp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpovw0zqvw.onnx'\n\n    @pytest.mark.parametrize(\n        \"model_type,args_format,output_target,opset_version,export_params,do_constant_folding\",\n        [\n            # Base case from test plan\n            (\"nn.Module\", \"tuple\", \"file\", 13, True, True),\n            # Parameter extensions from test plan\n            (\"nn.Module\", \"named_tuple\", \"file\", 13, True, False),\n            (\"nn.Module\", \"tuple\", \"file\", 7, True, True),\n            (\"nn.Module\", \"tuple\", \"file\", 16, False, True),\n        ]\n    )\n    def test_export_basic_model_to_file(\n        model_type,\n        args_format,\n        output_target,\n        opset_version,\n        export_params,\n        do_constant_folding,\n        simple_linear_model,\n        simple_conv_model,\n        sample_input_tensor,\n        sample_input_tensor_2d,\n        temp_onnx_file,\n    ):\n        \"\"\"\n        Test basic model export to file with various configurations.\n    \n        This test covers the core export functionality with different:\n        - Model types (nn.Module)\n        - Args formats (tuple, named_tuple)\n        - Opset versions (7, 13, 16)\n        - Export parameters (True/False)\n        - Constant folding (True/False)\n    \n        Weak assertions: file exists, file not empty, no exception.\n        \"\"\"\n        # Select model based on type\n        if model_type == \"nn.Module\":\n            # Use linear model for 1D input, conv model for 2D input\n            if args_format == \"tuple\":\n                model = simple_linear_model\n                args = (sample_input_tensor,)\n            else:  # named_tuple\n                model = simple_conv_model\n                args = (sample_input_tensor_2d,)\n    \n        # Prepare args based on format\n        if args_format == \"named_tuple\":\n            # Create args with named parameters\n            args = (args[0], {\"dummy_param\": torch.tensor([1.0])})\n    \n        # Mock dependencies - we need to mock the entire export chain\n        # to avoid C++ type issues\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export, \\\n             mock.patch('io.open', mock.mock_open()) as mock_file:\n    \n            # Setup mock returns for _get_trace_graph\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            # Create a proper mock for _model_to_graph that returns appropriate types\n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n    \n            # Mock _export to do nothing (just track calls)\n            mock_export.return_value = None\n    \n            # Call export\n            try:\n                onnx_utils.export(\n                    model=model,\n                    args=args,\n                    f=temp_onnx_file,\n                    export_params=export_params,\n                    opset_version=opset_version,\n                    do_constant_folding=do_constant_folding,\n                    verbose=False,\n                )\n    \n                # Weak assertion 1: No exception raised\n                # (implicitly passed if we reach here)\n    \n                # Weak assertion 2: File was opened for writing\n                # Since we're mocking io.open, check that it was called\n>               assert mock_file.called, \"File was not opened for writing\"\nE               AssertionError: File was not opened for writing\nE               assert False\nE                +  where False = <MagicMock name='open' spec='builtin_function_or_method' id='5170852816'>.called\n\ntests/test_torch_onnx_utils.py:239: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel_type = 'nn.Module', args_format = 'tuple', output_target = 'file'\nopset_version = 16, export_params = False, do_constant_folding = True\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsimple_conv_model = SimpleConvModel(\n  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\nsample_input_tensor = tensor([[-1.7534, -1.3003,  0.5800],\n        [ 0.0158, -1.6756,  1.0759]])\nsample_input_tensor_2d = tensor([[[[-0.4210, -0.6637,  1.4599, -1.9324,  0.2951,  1.9531,  0.0623,\n           -1.1079, -0.0518,  0.4115],\n     ...7],\n          [ 1.5941,  2.5384, -0.4962,  1.5591, -1.2024, -0.6351,  0.2057,\n            1.2085, -1.2975, -1.7842]]]])\ntemp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmpovw0zqvw.onnx'\n\n    @pytest.mark.parametrize(\n        \"model_type,args_format,output_target,opset_version,export_params,do_constant_folding\",\n        [\n            # Base case from test plan\n            (\"nn.Module\", \"tuple\", \"file\", 13, True, True),\n            # Parameter extensions from test plan\n            (\"nn.Module\", \"named_tuple\", \"file\", 13, True, False),\n            (\"nn.Module\", \"tuple\", \"file\", 7, True, True),\n            (\"nn.Module\", \"tuple\", \"file\", 16, False, True),\n        ]\n    )\n    def test_export_basic_model_to_file(\n        model_type,\n        args_format,\n        output_target,\n        opset_version,\n        export_params,\n        do_constant_folding,\n        simple_linear_model,\n        simple_conv_model,\n        sample_input_tensor,\n        sample_input_tensor_2d,\n        temp_onnx_file,\n    ):\n        \"\"\"\n        Test basic model export to file with various configurations.\n    \n        This test covers the core export functionality with different:\n        - Model types (nn.Module)\n        - Args formats (tuple, named_tuple)\n        - Opset versions (7, 13, 16)\n        - Export parameters (True/False)\n        - Constant folding (True/False)\n    \n        Weak assertions: file exists, file not empty, no exception.\n        \"\"\"\n        # Select model based on type\n        if model_type == \"nn.Module\":\n            # Use linear model for 1D input, conv model for 2D input\n            if args_format == \"tuple\":\n                model = simple_linear_model\n                args = (sample_input_tensor,)\n            else:  # named_tuple\n                model = simple_conv_model\n                args = (sample_input_tensor_2d,)\n    \n        # Prepare args based on format\n        if args_format == \"named_tuple\":\n            # Create args with named parameters\n            args = (args[0], {\"dummy_param\": torch.tensor([1.0])})\n    \n        # Mock dependencies - we need to mock the entire export chain\n        # to avoid C++ type issues\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export, \\\n             mock.patch('io.open', mock.mock_open()) as mock_file:\n    \n            # Setup mock returns for _get_trace_graph\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            # Create a proper mock for _model_to_graph that returns appropriate types\n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n    \n            # Mock _export to do nothing (just track calls)\n            mock_export.return_value = None\n    \n            # Call export\n            try:\n                onnx_utils.export(\n                    model=model,\n                    args=args,\n                    f=temp_onnx_file,\n                    export_params=export_params,\n                    opset_version=opset_version,\n                    do_constant_folding=do_constant_folding,\n                    verbose=False,\n                )\n    \n                # Weak assertion 1: No exception raised\n                # (implicitly passed if we reach here)\n    \n                # Weak assertion 2: File was opened for writing\n                # Since we're mocking io.open, check that it was called\n                assert mock_file.called, \"File was not opened for writing\"\n    \n                # Check that write was called (indicating content was written)\n                write_calls = [call for call in mock_file.mock_calls if call[0] == '().write']\n                assert len(write_calls) > 0, \"No data was written to file\"\n    \n                # Weak assertion 3: _get_trace_graph was called for nn.Module\n                if model_type == \"nn.Module\":\n                    assert mock_get_trace_graph.called, \"torch.jit._get_trace_graph should be called for nn.Module\"\n    \n                # Weak assertion 4: _model_to_graph was called\n                assert mock_model_to_graph.called, \"_model_to_graph should be called\"\n    \n                # Weak assertion 5: _export was called\n                assert mock_export.called, \"_export should be called\"\n    \n                # Additional weak checks\n                assert opset_version >= 7 and opset_version <= 16, \\\n                    f\"Opset version {opset_version} should be in range 7-16\"\n    \n                # Check that export was called with correct parameters\n                mock_export.assert_called_once()\n                call_args = mock_export.call_args\n    \n                # Verify key parameters\n                assert call_args[0][0] == model, \"Model parameter should match\"\n                assert call_args[0][1] == args, \"Args parameter should match\"\n                assert call_args[0][2] == temp_onnx_file, \"File parameter should match\"\n                assert call_args[1]['export_params'] == export_params, \"export_params should match\"\n                assert call_args[1]['opset_version'] == opset_version, \"opset_version should match\"\n                assert call_args[1]['do_constant_folding'] == do_constant_folding, \"do_constant_folding should match\"\n    \n            except Exception as e:\n>               pytest.fail(f\"Export raised unexpected exception: {e}\")\nE               Failed: Export raised unexpected exception: File was not opened for writing\nE               assert False\nE                +  where False = <MagicMock name='open' spec='builtin_function_or_method' id='5170852816'>.called\n\ntests/test_torch_onnx_utils.py:272: Failed\n_____ test_export_model_to_bytesio[nn.Module-tensor-bytesio-13-True-True] ______\n\nmodel_type = 'nn.Module', args_format = 'tensor', output_target = 'bytesio'\nopset_version = 13, export_params = True, do_constant_folding = True\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsample_input_tensor = tensor([[-0.7328,  0.0113,  0.4797],\n        [-0.0365,  1.0878, -0.0177]])\n\n    @pytest.mark.parametrize(\n        \"model_type,args_format,output_target,opset_version,export_params,do_constant_folding\",\n        [\n            # Base case from test plan\n            (\"nn.Module\", \"tensor\", \"bytesio\", 13, True, True),\n            # Parameter extension from test plan\n            (\"ScriptFunction\", \"tensor\", \"bytesio\", 13, True, True),\n        ]\n    )\n    def test_export_model_to_bytesio(\n        model_type,\n        args_format,\n        output_target,\n        opset_version,\n        export_params,\n        do_constant_folding,\n        simple_linear_model,\n        sample_input_tensor,\n    ):\n        \"\"\"\n        Test model export to BytesIO buffer with various configurations.\n    \n        This test covers:\n        - Export to in-memory buffer (BytesIO)\n        - Tensor args format\n        - Different model types (nn.Module, ScriptFunction)\n    \n        Weak assertions: buffer not empty, buffer position changed, no exception.\n        \"\"\"\n        # Prepare model based on type\n        if model_type == \"nn.Module\":\n            model = simple_linear_model\n        else:  # ScriptFunction\n            # Create a mock ScriptFunction\n            model = mock.MagicMock(spec=torch.jit.ScriptFunction)\n    \n        # Prepare args based on format\n        if args_format == \"tensor\":\n            args = sample_input_tensor\n    \n        # Create BytesIO buffer\n        buffer = io.BytesIO()\n    \n        # Track initial buffer state\n        initial_position = buffer.tell()\n    \n        # Mock dependencies\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export:\n    \n            # Setup mock returns for _get_trace_graph\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            # Create a proper mock for _model_to_graph\n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n    \n            # Mock _export to write some dummy data to buffer\n            def mock_export_side_effect(*args, **kwargs):\n                f = args[2]\n                if hasattr(f, 'write'):\n                    # Write dummy ONNX-like data\n                    f.write(b'dummy_onnx_protobuf_data')\n                return None\n    \n            mock_export.side_effect = mock_export_side_effect\n    \n            # Call export\n            try:\n                onnx_utils.export(\n                    model=model,\n                    args=args,\n                    f=buffer,\n                    export_params=export_params,\n                    opset_version=opset_version,\n                    do_constant_folding=do_constant_folding,\n                    verbose=False,\n                )\n    \n                # Weak assertion 1: No exception raised\n                # (implicitly passed if we reach here)\n    \n                # Weak assertion 2: Buffer is not empty\n                buffer_size = buffer.getbuffer().nbytes\n                assert buffer_size > 0, f\"Buffer should not be empty, got size: {buffer_size}\"\n    \n                # Weak assertion 3: Buffer position changed\n                final_position = buffer.tell()\n                assert final_position > initial_position, \\\n                    f\"Buffer position should change. Initial: {initial_position}, Final: {final_position}\"\n    \n                # Weak assertion 4: _get_trace_graph called appropriately\n                if model_type == \"nn.Module\":\n>                   assert mock_get_trace_graph.called, \"torch.jit._get_trace_graph should be called for nn.Module\"\nE                   AssertionError: torch.jit._get_trace_graph should be called for nn.Module\nE                   assert False\nE                    +  where False = <MagicMock name='_get_trace_graph' id='5171129424'>.called\n\ntests/test_torch_onnx_utils.py:375: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel_type = 'nn.Module', args_format = 'tensor', output_target = 'bytesio'\nopset_version = 13, export_params = True, do_constant_folding = True\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsample_input_tensor = tensor([[-0.7328,  0.0113,  0.4797],\n        [-0.0365,  1.0878, -0.0177]])\n\n    @pytest.mark.parametrize(\n        \"model_type,args_format,output_target,opset_version,export_params,do_constant_folding\",\n        [\n            # Base case from test plan\n            (\"nn.Module\", \"tensor\", \"bytesio\", 13, True, True),\n            # Parameter extension from test plan\n            (\"ScriptFunction\", \"tensor\", \"bytesio\", 13, True, True),\n        ]\n    )\n    def test_export_model_to_bytesio(\n        model_type,\n        args_format,\n        output_target,\n        opset_version,\n        export_params,\n        do_constant_folding,\n        simple_linear_model,\n        sample_input_tensor,\n    ):\n        \"\"\"\n        Test model export to BytesIO buffer with various configurations.\n    \n        This test covers:\n        - Export to in-memory buffer (BytesIO)\n        - Tensor args format\n        - Different model types (nn.Module, ScriptFunction)\n    \n        Weak assertions: buffer not empty, buffer position changed, no exception.\n        \"\"\"\n        # Prepare model based on type\n        if model_type == \"nn.Module\":\n            model = simple_linear_model\n        else:  # ScriptFunction\n            # Create a mock ScriptFunction\n            model = mock.MagicMock(spec=torch.jit.ScriptFunction)\n    \n        # Prepare args based on format\n        if args_format == \"tensor\":\n            args = sample_input_tensor\n    \n        # Create BytesIO buffer\n        buffer = io.BytesIO()\n    \n        # Track initial buffer state\n        initial_position = buffer.tell()\n    \n        # Mock dependencies\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export:\n    \n            # Setup mock returns for _get_trace_graph\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            # Create a proper mock for _model_to_graph\n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n    \n            # Mock _export to write some dummy data to buffer\n            def mock_export_side_effect(*args, **kwargs):\n                f = args[2]\n                if hasattr(f, 'write'):\n                    # Write dummy ONNX-like data\n                    f.write(b'dummy_onnx_protobuf_data')\n                return None\n    \n            mock_export.side_effect = mock_export_side_effect\n    \n            # Call export\n            try:\n                onnx_utils.export(\n                    model=model,\n                    args=args,\n                    f=buffer,\n                    export_params=export_params,\n                    opset_version=opset_version,\n                    do_constant_folding=do_constant_folding,\n                    verbose=False,\n                )\n    \n                # Weak assertion 1: No exception raised\n                # (implicitly passed if we reach here)\n    \n                # Weak assertion 2: Buffer is not empty\n                buffer_size = buffer.getbuffer().nbytes\n                assert buffer_size > 0, f\"Buffer should not be empty, got size: {buffer_size}\"\n    \n                # Weak assertion 3: Buffer position changed\n                final_position = buffer.tell()\n                assert final_position > initial_position, \\\n                    f\"Buffer position should change. Initial: {initial_position}, Final: {final_position}\"\n    \n                # Weak assertion 4: _get_trace_graph called appropriately\n                if model_type == \"nn.Module\":\n                    assert mock_get_trace_graph.called, \"torch.jit._get_trace_graph should be called for nn.Module\"\n                else:  # ScriptFunction\n                    # For ScriptFunction, _get_trace_graph should not be called\n                    # Instead, it should use the existing graph\n                    assert not mock_get_trace_graph.called, \"torch.jit._get_trace_graph should not be called for ScriptFunction\"\n    \n                # Weak assertion 5: _model_to_graph was called\n                assert mock_model_to_graph.called, \"_model_to_graph should be called\"\n    \n                # Weak assertion 6: _export was called\n                assert mock_export.called, \"_export should be called\"\n    \n                # Additional weak check for opset version\n                assert opset_version >= 7 and opset_version <= 16, \\\n                    f\"Opset version {opset_version} should be in range 7-16\"\n    \n                # Check buffer content (basic validation)\n                buffer_content = buffer.getvalue()\n                assert isinstance(buffer_content, bytes), \"Buffer content should be bytes\"\n                assert len(buffer_content) > 0, \"Buffer content should have non-zero length\"\n    \n                # Verify that the dummy data was written\n                assert buffer_content == b'dummy_onnx_protobuf_data', \\\n                    \"Buffer should contain the dummy data written by mock_export\"\n    \n                # Check that export was called with correct parameters\n                mock_export.assert_called_once()\n                call_args = mock_export.call_args\n    \n                # Verify key parameters\n                assert call_args[0][0] == model, \"Model parameter should match\"\n                assert call_args[0][1] == args, \"Args parameter should match\"\n                assert call_args[0][2] == buffer, \"Buffer parameter should match\"\n                assert call_args[1]['export_params'] == export_params, \"export_params should match\"\n                assert call_args[1]['opset_version'] == opset_version, \"opset_version should match\"\n                assert call_args[1]['do_constant_folding'] == do_constant_folding, \"do_constant_folding should match\"\n    \n            except Exception as e:\n>               pytest.fail(f\"Export to BytesIO raised unexpected exception: {e}\")\nE               Failed: Export to BytesIO raised unexpected exception: torch.jit._get_trace_graph should be called for nn.Module\nE               assert False\nE                +  where False = <MagicMock name='_get_trace_graph' id='5171129424'>.called\n\ntests/test_torch_onnx_utils.py:413: Failed\n___ test_export_model_to_bytesio[ScriptFunction-tensor-bytesio-13-True-True] ___\n\nmodel_type = 'ScriptFunction', args_format = 'tensor', output_target = 'bytesio'\nopset_version = 13, export_params = True, do_constant_folding = True\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsample_input_tensor = tensor([[-0.6039, -0.3514,  0.5235],\n        [ 0.0675, -0.9669, -0.0743]])\n\n    @pytest.mark.parametrize(\n        \"model_type,args_format,output_target,opset_version,export_params,do_constant_folding\",\n        [\n            # Base case from test plan\n            (\"nn.Module\", \"tensor\", \"bytesio\", 13, True, True),\n            # Parameter extension from test plan\n            (\"ScriptFunction\", \"tensor\", \"bytesio\", 13, True, True),\n        ]\n    )\n    def test_export_model_to_bytesio(\n        model_type,\n        args_format,\n        output_target,\n        opset_version,\n        export_params,\n        do_constant_folding,\n        simple_linear_model,\n        sample_input_tensor,\n    ):\n        \"\"\"\n        Test model export to BytesIO buffer with various configurations.\n    \n        This test covers:\n        - Export to in-memory buffer (BytesIO)\n        - Tensor args format\n        - Different model types (nn.Module, ScriptFunction)\n    \n        Weak assertions: buffer not empty, buffer position changed, no exception.\n        \"\"\"\n        # Prepare model based on type\n        if model_type == \"nn.Module\":\n            model = simple_linear_model\n        else:  # ScriptFunction\n            # Create a mock ScriptFunction\n            model = mock.MagicMock(spec=torch.jit.ScriptFunction)\n    \n        # Prepare args based on format\n        if args_format == \"tensor\":\n            args = sample_input_tensor\n    \n        # Create BytesIO buffer\n        buffer = io.BytesIO()\n    \n        # Track initial buffer state\n        initial_position = buffer.tell()\n    \n        # Mock dependencies\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export:\n    \n            # Setup mock returns for _get_trace_graph\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            # Create a proper mock for _model_to_graph\n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n    \n            # Mock _export to write some dummy data to buffer\n            def mock_export_side_effect(*args, **kwargs):\n                f = args[2]\n                if hasattr(f, 'write'):\n                    # Write dummy ONNX-like data\n                    f.write(b'dummy_onnx_protobuf_data')\n                return None\n    \n            mock_export.side_effect = mock_export_side_effect\n    \n            # Call export\n            try:\n                onnx_utils.export(\n                    model=model,\n                    args=args,\n                    f=buffer,\n                    export_params=export_params,\n                    opset_version=opset_version,\n                    do_constant_folding=do_constant_folding,\n                    verbose=False,\n                )\n    \n                # Weak assertion 1: No exception raised\n                # (implicitly passed if we reach here)\n    \n                # Weak assertion 2: Buffer is not empty\n                buffer_size = buffer.getbuffer().nbytes\n                assert buffer_size > 0, f\"Buffer should not be empty, got size: {buffer_size}\"\n    \n                # Weak assertion 3: Buffer position changed\n                final_position = buffer.tell()\n                assert final_position > initial_position, \\\n                    f\"Buffer position should change. Initial: {initial_position}, Final: {final_position}\"\n    \n                # Weak assertion 4: _get_trace_graph called appropriately\n                if model_type == \"nn.Module\":\n                    assert mock_get_trace_graph.called, \"torch.jit._get_trace_graph should be called for nn.Module\"\n                else:  # ScriptFunction\n                    # For ScriptFunction, _get_trace_graph should not be called\n                    # Instead, it should use the existing graph\n                    assert not mock_get_trace_graph.called, \"torch.jit._get_trace_graph should not be called for ScriptFunction\"\n    \n                # Weak assertion 5: _model_to_graph was called\n>               assert mock_model_to_graph.called, \"_model_to_graph should be called\"\nE               AssertionError: _model_to_graph should be called\nE               assert False\nE                +  where False = <MagicMock name='_model_to_graph' id='5171205440'>.called\n\ntests/test_torch_onnx_utils.py:382: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel_type = 'ScriptFunction', args_format = 'tensor', output_target = 'bytesio'\nopset_version = 13, export_params = True, do_constant_folding = True\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsample_input_tensor = tensor([[-0.6039, -0.3514,  0.5235],\n        [ 0.0675, -0.9669, -0.0743]])\n\n    @pytest.mark.parametrize(\n        \"model_type,args_format,output_target,opset_version,export_params,do_constant_folding\",\n        [\n            # Base case from test plan\n            (\"nn.Module\", \"tensor\", \"bytesio\", 13, True, True),\n            # Parameter extension from test plan\n            (\"ScriptFunction\", \"tensor\", \"bytesio\", 13, True, True),\n        ]\n    )\n    def test_export_model_to_bytesio(\n        model_type,\n        args_format,\n        output_target,\n        opset_version,\n        export_params,\n        do_constant_folding,\n        simple_linear_model,\n        sample_input_tensor,\n    ):\n        \"\"\"\n        Test model export to BytesIO buffer with various configurations.\n    \n        This test covers:\n        - Export to in-memory buffer (BytesIO)\n        - Tensor args format\n        - Different model types (nn.Module, ScriptFunction)\n    \n        Weak assertions: buffer not empty, buffer position changed, no exception.\n        \"\"\"\n        # Prepare model based on type\n        if model_type == \"nn.Module\":\n            model = simple_linear_model\n        else:  # ScriptFunction\n            # Create a mock ScriptFunction\n            model = mock.MagicMock(spec=torch.jit.ScriptFunction)\n    \n        # Prepare args based on format\n        if args_format == \"tensor\":\n            args = sample_input_tensor\n    \n        # Create BytesIO buffer\n        buffer = io.BytesIO()\n    \n        # Track initial buffer state\n        initial_position = buffer.tell()\n    \n        # Mock dependencies\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export:\n    \n            # Setup mock returns for _get_trace_graph\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            # Create a proper mock for _model_to_graph\n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n    \n            # Mock _export to write some dummy data to buffer\n            def mock_export_side_effect(*args, **kwargs):\n                f = args[2]\n                if hasattr(f, 'write'):\n                    # Write dummy ONNX-like data\n                    f.write(b'dummy_onnx_protobuf_data')\n                return None\n    \n            mock_export.side_effect = mock_export_side_effect\n    \n            # Call export\n            try:\n                onnx_utils.export(\n                    model=model,\n                    args=args,\n                    f=buffer,\n                    export_params=export_params,\n                    opset_version=opset_version,\n                    do_constant_folding=do_constant_folding,\n                    verbose=False,\n                )\n    \n                # Weak assertion 1: No exception raised\n                # (implicitly passed if we reach here)\n    \n                # Weak assertion 2: Buffer is not empty\n                buffer_size = buffer.getbuffer().nbytes\n                assert buffer_size > 0, f\"Buffer should not be empty, got size: {buffer_size}\"\n    \n                # Weak assertion 3: Buffer position changed\n                final_position = buffer.tell()\n                assert final_position > initial_position, \\\n                    f\"Buffer position should change. Initial: {initial_position}, Final: {final_position}\"\n    \n                # Weak assertion 4: _get_trace_graph called appropriately\n                if model_type == \"nn.Module\":\n                    assert mock_get_trace_graph.called, \"torch.jit._get_trace_graph should be called for nn.Module\"\n                else:  # ScriptFunction\n                    # For ScriptFunction, _get_trace_graph should not be called\n                    # Instead, it should use the existing graph\n                    assert not mock_get_trace_graph.called, \"torch.jit._get_trace_graph should not be called for ScriptFunction\"\n    \n                # Weak assertion 5: _model_to_graph was called\n                assert mock_model_to_graph.called, \"_model_to_graph should be called\"\n    \n                # Weak assertion 6: _export was called\n                assert mock_export.called, \"_export should be called\"\n    \n                # Additional weak check for opset version\n                assert opset_version >= 7 and opset_version <= 16, \\\n                    f\"Opset version {opset_version} should be in range 7-16\"\n    \n                # Check buffer content (basic validation)\n                buffer_content = buffer.getvalue()\n                assert isinstance(buffer_content, bytes), \"Buffer content should be bytes\"\n                assert len(buffer_content) > 0, \"Buffer content should have non-zero length\"\n    \n                # Verify that the dummy data was written\n                assert buffer_content == b'dummy_onnx_protobuf_data', \\\n                    \"Buffer should contain the dummy data written by mock_export\"\n    \n                # Check that export was called with correct parameters\n                mock_export.assert_called_once()\n                call_args = mock_export.call_args\n    \n                # Verify key parameters\n                assert call_args[0][0] == model, \"Model parameter should match\"\n                assert call_args[0][1] == args, \"Args parameter should match\"\n                assert call_args[0][2] == buffer, \"Buffer parameter should match\"\n                assert call_args[1]['export_params'] == export_params, \"export_params should match\"\n                assert call_args[1]['opset_version'] == opset_version, \"opset_version should match\"\n                assert call_args[1]['do_constant_folding'] == do_constant_folding, \"do_constant_folding should match\"\n    \n            except Exception as e:\n>               pytest.fail(f\"Export to BytesIO raised unexpected exception: {e}\")\nE               Failed: Export to BytesIO raised unexpected exception: _model_to_graph should be called\nE               assert False\nE                +  where False = <MagicMock name='_model_to_graph' id='5171205440'>.called\n\ntests/test_torch_onnx_utils.py:413: Failed\n___________ TestONNXUtilsExport.test_export_with_default_parameters ____________\n\nself = <test_torch_onnx_utils.TestONNXUtilsExport object at 0x134206230>\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsample_input_tensor = tensor([[-0.0280, -0.4115,  0.8032],\n        [-0.9688,  0.1715, -0.5394]])\ntemp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmp02z51gue.onnx'\n\n    def test_export_with_default_parameters(self, simple_linear_model, sample_input_tensor, temp_onnx_file):\n        \"\"\"Test export with minimal required parameters.\"\"\"\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export, \\\n             mock.patch('io.open', mock.mock_open()) as mock_file:\n    \n            # Setup mocks\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n            mock_export.return_value = None\n    \n            # Should not raise any exception\n            onnx_utils.export(\n                model=simple_linear_model,\n                args=(sample_input_tensor,),\n                f=temp_onnx_file,\n            )\n    \n            # Verify mocks were called\n>           assert mock_get_trace_graph.called, \"torch.jit._get_trace_graph should be called\"\nE           AssertionError: torch.jit._get_trace_graph should be called\nE           assert False\nE            +  where False = <MagicMock name='_get_trace_graph' id='5170630320'>.called\n\ntests/test_torch_onnx_utils.py:587: AssertionError\n________ TestONNXUtilsExport.test_export_with_custom_input_output_names ________\n\nself = <test_torch_onnx_utils.TestONNXUtilsExport object at 0x134206560>\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsample_input_tensor = tensor([[ 0.4123, -0.2202, -1.7519],\n        [-0.2982, -1.6451, -0.6181]])\ntemp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmp05g13p2p.onnx'\n\n    def test_export_with_custom_input_output_names(self, simple_linear_model, sample_input_tensor, temp_onnx_file):\n        \"\"\"Test export with custom input and output names.\"\"\"\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export, \\\n             mock.patch('io.open', mock.mock_open()) as mock_file:\n    \n            # Setup mocks\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n            mock_export.return_value = None\n    \n            onnx_utils.export(\n                model=simple_linear_model,\n                args=(sample_input_tensor,),\n                f=temp_onnx_file,\n                input_names=[\"input_tensor\"],\n                output_names=[\"output_tensor\"],\n            )\n    \n            # Verify mocks were called with correct parameters\n            mock_export.assert_called_once()\n            call_args = mock_export.call_args\n>           assert call_args[1]['input_names'] == [\"input_tensor\"], \"input_names should match\"\nE           KeyError: 'input_names'\n\ntests/test_torch_onnx_utils.py:620: KeyError\n______________ TestONNXUtilsExport.test_export_with_verbose_mode _______________\n\nself = <test_torch_onnx_utils.TestONNXUtilsExport object at 0x134206890>\nsimple_linear_model = SimpleLinearModel(\n  (linear): Linear(in_features=3, out_features=2, bias=True)\n)\nsample_input_tensor = tensor([[ 0.7818, -1.0138, -2.0971],\n        [ 0.1009,  0.6100,  1.4594]])\ntemp_onnx_file = '/var/folders/fc/ny_p_wjs10xfzq7xns_lfdc40000gn/T/tmp1_4javll.onnx'\n\n    def test_export_with_verbose_mode(self, simple_linear_model, sample_input_tensor, temp_onnx_file):\n        \"\"\"Test export with verbose mode enabled.\"\"\"\n        with mock.patch('torch.jit._get_trace_graph') as mock_get_trace_graph, \\\n             mock.patch('torch.onnx.utils._model_to_graph') as mock_model_to_graph, \\\n             mock.patch('torch.onnx.utils._export') as mock_export, \\\n             mock.patch('io.open', mock.mock_open()) as mock_file:\n    \n            # Setup mocks\n            mock_graph = mock.MagicMock()\n            mock_torch_out = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n            mock_inputs_states = mock.MagicMock()\n            mock_get_trace_graph.return_value = (mock_graph, mock_torch_out, mock_inputs_states)\n    \n            mock_params_dict = {}\n            mock_model_to_graph.return_value = (mock_graph, mock_params_dict, mock_torch_out)\n            mock_export.return_value = None\n    \n            # Capture warnings/prints if needed\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                onnx_utils.export(\n                    model=simple_linear_model,\n                    args=(sample_input_tensor,),\n                    f=temp_onnx_file,\n                    verbose=True,\n                )\n    \n            # Verify verbose parameter was passed\n            mock_export.assert_called_once()\n            call_args = mock_export.call_args\n>           assert call_args[1]['verbose'] is True, \"verbose should be True\"\nE           KeyError: 'verbose'\n\ntests/test_torch_onnx_utils.py:653: KeyError\n================================ tests coverage ================================\n______________ coverage: platform darwin, python 3.10.19-final-0 _______________\n\nName                             Stmts   Miss Branch BrPart  Cover   Missing\n----------------------------------------------------------------------------\ntests/test_torch_onnx_utils.py     322    101     34      5    67%   30, 41-43, 77->exit, 83-96, 108-135, 140-141, 146-147, 188->198, 242-269, 315->319, 342->345, 385-410, 416-420, 487-488, 588-590, 621, 697-699, 724, 740, 748-759, 764-772, 778-784\n----------------------------------------------------------------------------\nTOTAL                              322    101     34      5    67%\nCoverage XML written to file coverage.xml\n=========================== short test summary info ============================\nFAILED tests/test_torch_onnx_utils.py::test_export_basic_model_to_file[nn.Module-tuple-file-13-True-True]\nFAILED tests/test_torch_onnx_utils.py::test_export_basic_model_to_file[nn.Module-named_tuple-file-13-True-False]\nFAILED tests/test_torch_onnx_utils.py::test_export_basic_model_to_file[nn.Module-tuple-file-7-True-True]\nFAILED tests/test_torch_onnx_utils.py::test_export_basic_model_to_file[nn.Module-tuple-file-16-False-True]\nFAILED tests/test_torch_onnx_utils.py::test_export_model_to_bytesio[nn.Module-tensor-bytesio-13-True-True]\nFAILED tests/test_torch_onnx_utils.py::test_export_model_to_bytesio[ScriptFunction-tensor-bytesio-13-True-True]\nFAILED tests/test_torch_onnx_utils.py::TestONNXUtilsExport::test_export_with_default_parameters\nFAILED tests/test_torch_onnx_utils.py::TestONNXUtilsExport::test_export_with_custom_input_output_names\nFAILED tests/test_torch_onnx_utils.py::TestONNXUtilsExport::test_export_with_verbose_mode\n9 failed, 6 passed in 0.68s\n\nError: exit 1",
    "exit_code.txt": "1",
    "analysis_plan.json": "{\n  \"status\": \"未完全通过\",\n  \"passed\": 6,\n  \"failed\": 9,\n  \"errors\": 0,\n  \"collection_errors\": false,\n  \"block_limit\": 3,\n  \"failures\": [\n    {\n      \"test\": \"test_export_basic_model_to_file[nn.Module-tuple-file-13-True-True]\",\n      \"block_id\": \"CASE_01\",\n      \"error_type\": \"AssertionError\",\n      \"action\": \"rewrite_block\",\n      \"note\": \"文件写入mock未被调用，需要修复mock设置或导出逻辑\"\n    },\n    {\n      \"test\": \"test_export_model_to_bytesio[nn.Module-tensor-bytesio-13-True-True]\",\n      \"block_id\": \"CASE_02\",\n      \"error_type\": \"AssertionError\",\n      \"action\": \"rewrite_block\",\n      \"note\": \"_get_trace_graph mock未被调用，需要修复mock设置或导出逻辑\"\n    },\n    {\n      \"test\": \"TestONNXUtilsExport.test_export_with_default_parameters\",\n      \"block_id\": \"HEADER\",\n      \"error_type\": \"AssertionError\",\n      \"action\": \"rewrite_block\",\n      \"note\": \"默认参数测试中_get_trace_graph mock未被调用，需要修复基础导出逻辑\"\n    }\n  ],\n  \"deferred\": [\n    {\n      \"test\": \"test_export_basic_model_to_file[nn.Module-named_tuple-file-13-True-False]\",\n      \"reason\": \"错误类型重复，跳过该块\"\n    },\n    {\n      \"test\": \"test_export_basic_model_to_file[nn.Module-tuple-file-7-True-True]\",\n      \"reason\": \"错误类型重复，跳过该块\"\n    },\n    {\n      \"test\": \"test_export_basic_model_to_file[nn.Module-tuple-file-16-False-True]\",\n      \"reason\": \"错误类型重复，跳过该块\"\n    },\n    {\n      \"test\": \"test_export_model_to_bytesio[ScriptFunction-tensor-bytesio-13-True-True]\",\n      \"reason\": \"错误类型重复，跳过该块\"\n    },\n    {\n      \"test\": \"TestONNXUtilsExport.test_export_with_custom_input_output_names\",\n      \"reason\": \"KeyError错误，等待基础导出逻辑修复后再处理\"\n    },\n    {\n      \"test\": \"TestONNXUtilsExport.test_export_with_verbose_mode\",\n      \"reason\": \"KeyError错误，等待基础导出逻辑修复后再处理\"\n    }\n  ],\n  \"stop_recommended\": false,\n  \"stop_reason\": \"\"\n}",
    "analysis.md": "# 测试执行分析报告\n\n## 状态与统计\n- **状态**: 未完全通过\n- **通过测试**: 6\n- **失败测试**: 9\n- **错误测试**: 0\n- **收集错误**: 无\n\n## 待修复 BLOCK 列表 (≤3)\n\n### 1. CASE_01 - 基本模型导出到文件\n- **测试**: `test_export_basic_model_to_file[nn.Module-tuple-file-13-True-True]`\n- **错误类型**: AssertionError\n- **Action**: rewrite_block\n- **问题**: 文件写入mock未被调用，需要修复mock设置或导出逻辑\n\n### 2. CASE_02 - 模型导出到BytesIO缓冲区\n- **测试**: `test_export_model_to_bytesio[nn.Module-tensor-bytesio-13-True-True]`\n- **错误类型**: AssertionError\n- **Action**: rewrite_block\n- **问题**: _get_trace_graph mock未被调用，需要修复mock设置或导出逻辑\n\n### 3. HEADER - 基础导出逻辑\n- **测试**: `TestONNXUtilsExport.test_export_with_default_parameters`\n- **错误类型**: AssertionError\n- **Action**: rewrite_block\n- **问题**: 默认参数测试中_get_trace_graph mock未被调用，需要修复基础导出逻辑\n\n## 停止建议\n- **stop_recommended**: false\n- **stop_reason**: 无\n\n## 备注\n- 6个测试因错误类型重复被标记为deferred\n- 覆盖率：67%，需要关注缺失的代码路径\n- 核心问题是导出函数的mock设置不正确，导致依赖函数未被调用",
    "final_report.md": "# torch.onnx.utils 测试结果报告\n\n## 1. 执行摘要\n**测试未完全通过**：15个测试用例中6个通过、9个失败，核心问题是导出函数的mock设置不正确导致依赖函数未被调用，需要修复基础导出逻辑的测试实现。\n\n**关键发现/阻塞项**：\n1. 文件写入mock在CASE_01中未被调用，需要修复mock设置\n2. _get_trace_graph mock在CASE_02和基础导出测试中未被调用\n3. 测试覆盖率67%，需要关注缺失的代码路径\n\n## 2. 测试范围\n**目标FQN**: torch.onnx.utils\n\n**测试环境**：\n- 测试框架：pytest\n- 依赖：PyTorch ONNX导出模块，依赖ONNX库、protobuf\n- 隔离策略：mock/monkeypatch/fixtures控制外部依赖\n\n**覆盖场景**：\n- ✓ 核心导出函数族（export, export_to_pretty_string）\n- ✓ 辅助函数与状态管理（is_in_onnx_export）\n- ✓ 三种模型类型基本导出（nn.Module/ScriptModule/ScriptFunction）\n- ✓ 文件路径和BytesIO两种输出目标\n- ✓ 不同参数格式（元组/张量）\n\n**未覆盖项**：\n- ✗ 动态轴配置的字典和列表格式支持\n- ✗ opset_version边界值(7,16)和默认值的兼容性\n- ✗ 不同operator_export_type的行为差异\n- ✗ 训练模式与opset_version>=12的关联验证\n- ✗ 复杂模型结构导出能力\n\n## 3. 结果概览\n**测试统计**：\n- 用例总数：15个\n- 通过：6个（40%）\n- 失败：9个（60%）\n- 错误：0个\n- 覆盖率：67%\n\n**主要失败点**：\n1. **CASE_01**：基本模型导出到文件 - 文件写入mock未被调用\n2. **CASE_02**：模型导出到BytesIO缓冲区 - _get_trace_graph mock未被调用\n3. **基础导出逻辑**：默认参数测试中_get_trace_graph mock未被调用\n\n## 4. 详细发现\n\n### 严重级别：高\n**问题1：导出函数mock设置不正确**\n- **根因**：测试中对`torch.onnx._internals._model_to_graph`或`_get_trace_graph`的mock设置未能正确拦截实际调用\n- **影响**：核心导出功能测试全部失败，无法验证基本导出逻辑\n- **建议修复**：重新审查导出函数的调用链，确保mock覆盖所有依赖路径\n\n**问题2：文件I/O mock未生效**\n- **根因**：文件写入操作的mock设置不完整或路径不匹配\n- **影响**：无法验证文件导出功能的正确性\n- **建议修复**：检查`io.open`和文件写入操作的mock设置，确保覆盖所有文件操作\n\n### 严重级别：中\n**问题3：测试覆盖率不足**\n- **根因**：多个测试用例因相同错误类型被标记为deferred，未执行\n- **影响**：仅覆盖67%代码路径，关键功能未验证\n- **建议修复**：修复基础问题后重新执行deferred测试集\n\n## 5. 覆盖与风险\n\n**需求覆盖评估**：\n- ✓ 基本功能：模型导出到文件/缓冲区（实现有问题）\n- ✗ 参数处理：args三种格式（仅测试了元组和张量）\n- ✗ 版本兼容：opset_version边界值\n- ✗ 动态配置：动态轴定义\n- ✗ 错误处理：异常场景验证\n\n**尚未覆盖的边界/缺失信息**：\n1. **复杂模型结构**：嵌套、循环、条件控制流的导出能力\n2. **性能边界**：大模型导出的内存使用和时间消耗\n3. **并发安全性**：多线程环境下的导出行为\n4. **算子类型差异**：不同operator_export_type的行为验证\n5. **训练模式**：与opset_version>=12的关联性\n\n**已知风险**：\n- 动态控制流支持有限（与torch.jit.trace相同限制）\n- 某些导出选项依赖特定构建配置（如Caffe2支持）\n- 缺少具体张量形状和dtype的详细约束说明\n\n## 6. 后续动作\n\n### 优先级：高（立即修复）\n1. **修复基础导出逻辑测试**\n   - 重新实现`TestONNXUtilsExport.test_export_with_default_parameters`\n   - 确保_get_trace_graph mock正确设置和调用验证\n   - 预计工作量：1-2小时\n\n2. **修复CASE_01文件导出测试**\n   - 检查文件写入mock设置，确保覆盖所有文件操作\n   - 验证文件路径和BytesIO两种输出目标\n   - 预计工作量：1小时\n\n3. **修复CASE_02缓冲区导出测试**\n   - 修复_get_trace_graph mock设置问题\n   - 验证BytesIO缓冲区内容完整性\n   - 预计工作量：1小时\n\n### 优先级：中（本周内完成）\n4. **执行deferred测试集**\n   - 重新执行CASE_04-CASE_09等6个deferred用例\n   - 覆盖ScriptModule类型、动态轴配置等场景\n   - 预计工作量：2-3小时\n\n5. **补充缺失的边界测试**\n   - 添加opset_version边界值测试（7,16）\n   - 添加动态轴配置的字典和列表格式测试\n   - 预计工作量：3-4小时\n\n### 优先级：低（后续迭代）\n6. **扩展测试覆盖范围**\n   - 添加不同operator_export_type的行为差异测试\n   - 添加训练模式与opset_version关联验证\n   - 添加复杂模型结构导出测试\n   - 预计工作量：1-2天\n\n7. **性能与并发测试**\n   - 大模型导出性能测试\n   - 多线程环境并发安全性验证\n   - 预计工作量：2-3天\n\n**风险评估**：当前测试失败主要源于测试实现问题而非功能缺陷，修复基础测试逻辑后可重新评估实际功能质量。建议优先修复高优先级问题，确保核心导出功能验证通过后再扩展测试范围。"
  },
  "stage_history": [
    {
      "stage": "understand_function",
      "status": "completed",
      "timestamp": "2026-01-21T16:07:36.346358",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "generate_requirements",
      "status": "completed",
      "timestamp": "2026-01-21T16:08:31.524767",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "design_test_plan",
      "status": "completed",
      "timestamp": "2026-01-21T16:10:36.629494",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-21T16:15:11.921541",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-21T16:15:13.970575",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-21T16:16:34.352427",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-21T16:22:05.706916",
      "attempts": 2,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-21T16:22:07.042395",
      "attempts": 2,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-21T16:23:32.922799",
      "attempts": 2,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-21T16:25:56.278684",
      "attempts": 3,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-21T16:25:57.518455",
      "attempts": 3,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-21T16:27:50.872975",
      "attempts": 3,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-21T16:32:38.412656",
      "attempts": 4,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-21T16:32:39.417942",
      "attempts": 4,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-21T16:34:19.148240",
      "attempts": 4,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-21T16:40:14.614001",
      "attempts": 5,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-21T16:40:15.577157",
      "attempts": 5,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-21T16:41:54.816622",
      "attempts": 5,
      "error": null
    },
    {
      "stage": "generate_report",
      "status": "completed",
      "timestamp": "2026-01-21T16:43:05.416988",
      "attempts": 1,
      "error": null
    }
  ],
  "user_feedback": []
}