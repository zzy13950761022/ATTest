=== Run Tests ===
.F...FF.                                                                 [100%]
=================================== FAILURES ===================================
_ TestEmbeddingBag.test_embeddingbag_sum_mode[10-3-max-None-input_shape1-offsets1-dtype1-cpu] _

self = <test_torch_nn_modules_sparse_embeddingbag.TestEmbeddingBag object at 0x12d0a1960>
set_random_seed = 42, num_embeddings = 10, embedding_dim = 3, mode = 'max'
padding_idx = None, input_shape = (8,), offsets = [0, 3, 8], dtype = torch.int64
device = 'cpu'

    @pytest.mark.parametrize("num_embeddings,embedding_dim,mode,padding_idx,input_shape,offsets,dtype,device", [
        (10, 3, "sum", None, (8,), [0, 3, 8], torch.long, "cpu"),
        # Parameter extension: max mode (Medium priority)
        (10, 3, "max", None, (8,), [0, 3, 8], torch.long, "cpu"),
    ])
    def test_embeddingbag_sum_mode(self, set_random_seed, num_embeddings, embedding_dim,
                                  mode, padding_idx, input_shape, offsets, dtype, device):
        """TC-03: EmbeddingBag sum 模式（包含 max 模式扩展）"""
        # Create EmbeddingBag module
        embedding_bag = EmbeddingBag(
            num_embeddings=num_embeddings,
            embedding_dim=embedding_dim,
            mode=mode,
            padding_idx=padding_idx,
            sparse=False  # Use dense gradients for simplicity
        )
    
        # Create test input
        indices = torch.randint(0, num_embeddings, input_shape, dtype=dtype, device=device)
        offsets_tensor = torch.tensor(offsets, dtype=torch.long, device=device)
    
        # Forward pass
        output = embedding_bag(indices, offsets_tensor)
    
        # Weak assertions
        # 1. Shape assertion
        expected_batch_size = len(offsets)
        assert output.shape == (expected_batch_size, embedding_dim), \
            f"Output shape mismatch: {output.shape} != ({expected_batch_size}, {embedding_dim})"
    
        # 2. Dtype assertion
        assert output.dtype == torch.float32, \
            f"Output dtype should be float32, got {output.dtype}"
    
        # 3. Finite values assertion
        assert torch.isfinite(output).all(), "Output contains NaN or Inf values"
    
        # 4. Aggregation correctness assertion (weak)
        # Verify that output values are within reasonable range
        # For sum mode, values should be sum of embedding vectors
        # For max mode, values should be max of embedding vectors
        # We'll do a basic sanity check
        weight_norm = torch.norm(embedding_bag.weight, dim=1)
        max_weight_norm = weight_norm.max().item()
    
        if mode == "sum":
            # Sum of up to (input_shape[0] / len(offsets)) embeddings per bag
            max_bag_size = max(offsets[i+1] - offsets[i] for i in range(len(offsets)-1)) if len(offsets) > 1 else input_shape[0]
            expected_max_norm = max_weight_norm * max_bag_size
            output_norms = torch.norm(output, dim=1)
            assert (output_norms <= expected_max_norm * 1.5).all(), \
                f"Sum mode output norms too large: max {output_norms.max().item()} > {expected_max_norm * 1.5}"
    
        elif mode == "max":
            # Max mode output should not exceed individual embedding norms
            output_norms = torch.norm(output, dim=1)
>           assert (output_norms <= max_weight_norm * 1.1).all(), \
                f"Max mode output norms too large: max {output_norms.max().item()} > {max_weight_norm * 1.1}"
E           AssertionError: Max mode output norms too large: max 3.2297418117523193 > 2.8549836397171022
E           assert tensor(False)
E            +  where tensor(False) = <built-in method all of Tensor object at 0x12d0a9120>()
E            +    where <built-in method all of Tensor object at 0x12d0a9120> = tensor([2.2006, 3.2297, 0.0000], grad_fn=<NormBackward1>) <= (2.5954396724700928 * 1.1).all

tests/test_torch_nn_modules_sparse_embeddingbag.py:102: AssertionError
_____________________ test_embeddingbag_invalid_parameters _____________________

    def test_embeddingbag_invalid_parameters():
        """Test invalid parameter combinations for EmbeddingBag"""
        # Test 1: num_embeddings <= 0 should raise ValueError
>       with pytest.raises(ValueError, match="num_embeddings must be positive"):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_torch_nn_modules_sparse_embeddingbag.py:239: Failed
_________________________ test_embeddingbag_edge_cases _________________________

    def test_embeddingbag_edge_cases():
        """Test edge cases for EmbeddingBag"""
        # Test 1: Single bag (no offsets needed)
        torch.manual_seed(42)
        embedding_bag = EmbeddingBag(num_embeddings=10, embedding_dim=3, mode="sum")
        indices = torch.randint(0, 10, (5,), dtype=torch.long)
    
        # When offsets is not provided, it's assumed to be [0]
>       output = embedding_bag(indices)

tests/test_torch_nn_modules_sparse_embeddingbag.py:278: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/sparse.py:387: in forward
    return F.embedding_bag(input, self.weight, offsets,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([3, 4, 3, 7, 0])
weight = Parameter containing:
tensor([[ 1.9269,  1.4873,  0.9007],
        [-2.1055,  0.6784, -1.2345],
        [-0.0431, -1.6...4245, -0.8286,  0.3309],
        [-1.5576,  0.9956, -0.8798],
        [-0.6011, -1.2742,  2.1228]], requires_grad=True)
offsets = None, max_norm = None, norm_type = 2.0, scale_grad_by_freq = False
mode = 'sum', sparse = False, per_sample_weights = None
include_last_offset = False, padding_idx = None

    def embedding_bag(
        input: Tensor,
        weight: Tensor,
        offsets: Optional[Tensor] = None,
        max_norm: Optional[float] = None,
        norm_type: float = 2,
        scale_grad_by_freq: bool = False,
        mode: str = "mean",
        sparse: bool = False,
        per_sample_weights: Optional[Tensor] = None,
        include_last_offset: bool = False,
        padding_idx: Optional[int] = None,
    ) -> Tensor:
        r"""Computes sums, means or maxes of `bags` of embeddings, without instantiating the
        intermediate embeddings.
    
        See :class:`torch.nn.EmbeddingBag` for more details.
    
        Note:
            {backward_reproducibility_note}
    
        Args:
            input (LongTensor): Tensor containing bags of indices into the embedding matrix
            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
                and number of columns equal to the embedding size
            offsets (LongTensor, optional): Only used when :attr:`input` is 1D. :attr:`offsets` determines
                                 the starting index position of each bag (sequence) in :attr:`input`.
            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                        is renormalized to have norm :attr:`max_norm`.
                                        Note: this will modify :attr:`weight` in-place.
            norm_type (float, optional): The ``p`` in the ``p``-norm to compute for the :attr:`max_norm` option.
                                         Default ``2``.
            scale_grad_by_freq (bool, optional): if given, this will scale gradients by the inverse of frequency of
                                                    the words in the mini-batch. Default ``False``.
                                                    Note: this option is not supported when ``mode="max"``.
            mode (str, optional): ``"sum"``, ``"mean"`` or ``"max"``. Specifies the way to reduce the bag.
                                     Default: ``"mean"``
            sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.
                                     Note: this option is not supported when ``mode="max"``.
            per_sample_weights (Tensor, optional): a tensor of float / double weights, or None
                to indicate all weights should be taken to be 1. If specified, :attr:`per_sample_weights`
                must have exactly the same shape as input and is treated as having the same
                :attr:`offsets`, if those are not None.
    
            include_last_offset (bool, optional): if ``True``, the size of offsets is equal to the number of bags + 1.
                The last element is the size of the input, or the ending index position of the last bag (sequence).
    
            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the
                                         gradient; therefore, the embedding vector at :attr:`padding_idx` is not updated
                                         during training, i.e. it remains as a fixed "pad". Note that the embedding
                                         vector at :attr:`padding_idx` is excluded from the reduction.
    
        Shape:
            - :attr:`input` (LongTensor) and :attr:`offsets` (LongTensor, optional)
    
              - If :attr:`input` is 2D of shape `(B, N)`, it will be treated as ``B`` bags (sequences)
                each of fixed length ``N``, and this will return ``B`` values aggregated in a way
                depending on the :attr:`mode`. :attr:`offsets` is ignored and required to be ``None`` in this case.
    
              - If :attr:`input` is 1D of shape `(N)`, it will be treated as a concatenation of
                multiple bags (sequences). :attr:`offsets` is required to be a 1D tensor containing
                the starting index positions of each bag in :attr:`input`. Therefore, for :attr:`offsets`
                of shape `(B)`, :attr:`input` will be viewed as having ``B`` bags.
                Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.
    
            - :attr:`weight` (Tensor): the learnable weights of the module of shape `(num_embeddings, embedding_dim)`
    
            - :attr:`per_sample_weights` (Tensor, optional). Has the same shape as :attr:`input`.
    
            - :attr:`output`: aggregated embedding values of shape `(B, embedding_dim)`
    
        Examples::
    
            >>> # an Embedding module containing 10 tensors of size 3
            >>> embedding_matrix = torch.rand(10, 3)
            >>> # a batch of 2 samples of 4 indices each
            >>> input = torch.tensor([1,2,4,5,4,3,2,9])
            >>> offsets = torch.tensor([0,4])
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> F.embedding_bag(input, embedding_matrix, offsets)
            tensor([[ 0.3397,  0.3552,  0.5545],
                    [ 0.5893,  0.4386,  0.5882]])
    
            >>> # example with padding_idx
            >>> embedding_matrix = torch.rand(10, 3)
            >>> input = torch.tensor([2, 2, 2, 2, 4, 3, 2, 9])
            >>> offsets = torch.tensor([0,4])
            >>> F.embedding_bag(input, embedding_matrix, offsets, padding_idx=2, mode='sum')
            tensor([[ 0.0000,  0.0000,  0.0000],
                    [-0.7082,  3.2145, -2.6251]])
        """
        if has_torch_function_variadic(input, weight, offsets, per_sample_weights):
            return handle_torch_function(
                embedding_bag,
                (input, weight, offsets, per_sample_weights),
                input,
                weight,
                offsets=offsets,
                max_norm=max_norm,
                norm_type=norm_type,
                scale_grad_by_freq=scale_grad_by_freq,
                mode=mode,
                sparse=sparse,
                per_sample_weights=per_sample_weights,
                include_last_offset=include_last_offset,
                padding_idx=padding_idx,
            )
        # Check for backward compatibility.
        # Used to be embedding_bag(weight, input, ...)
        # Now is     embedding_bag(input, weight, ...)
        if weight.dtype == torch.long and input.is_floating_point():
            warnings.warn(
                "Argument order of nn.functional.embedding_bag was changed. "
                "Usage `embedding_bag(weight, input, ...)` is deprecated, "
                "and should now be `embedding_bag(input, weight, ...)`."
            )
            weight, input = input, weight
    
        if per_sample_weights is not None and input.size() != per_sample_weights.size():
            raise ValueError(
                "embedding_bag: If per_sample_weights ({}) is not None, "
                "then it must have the same shape as the input ({})".format(per_sample_weights.shape, input.shape)
            )
    
        if input.dim() == 2:
            if offsets is not None:
                type_str = "<unknown>"
                # TODO: Remove this once script supports type() calls
                if not torch.jit.is_scripting():
                    type_str = str(type(offsets))
                raise ValueError(
                    "if input is 2D, then offsets has to be None"
                    ", as input is treated is a mini-batch of"
                    " fixed length sequences. However, found "
                    "offsets of type {}".format(type_str)
                )
            offsets = torch.arange(0, input.numel(), input.size(1), dtype=input.dtype, device=input.device)
    
            input = input.reshape(-1)
            if per_sample_weights is not None:
                per_sample_weights = per_sample_weights.reshape(-1)
        elif input.dim() == 1:
            if offsets is None:
>               raise ValueError("offsets has to be a 1D Tensor but got None")
E               ValueError: offsets has to be a 1D Tensor but got None

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/functional.py:2357: ValueError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                                 Stmts   Miss Branch BrPart  Cover   Missing
------------------------------------------------------------------------------------------------
tests/test_torch_nn_modules_sparse_embeddingbag.py     139     45      6      1    67%   19-29, 33-39, 99->106, 242-268, 279-305
------------------------------------------------------------------------------------------------
TOTAL                                                  139     45      6      1    67%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_sparse_embeddingbag.py::TestEmbeddingBag::test_embeddingbag_sum_mode[10-3-max-None-input_shape1-offsets1-dtype1-cpu]
FAILED tests/test_torch_nn_modules_sparse_embeddingbag.py::test_embeddingbag_invalid_parameters
FAILED tests/test_torch_nn_modules_sparse_embeddingbag.py::test_embeddingbag_edge_cases
3 failed, 5 passed in 0.79s

Error: exit 1