=== Run Tests ===
....FF.F...FF.                                                           [100%]
=================================== FAILURES ===================================
______________________ test_embedding_invalid_parameters _______________________

    def test_embedding_invalid_parameters():
        """Test invalid parameter combinations"""
        # Test 1: num_embeddings <= 0 should raise ValueError
>       with pytest.raises(ValueError, match="num_embeddings must be positive"):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_torch_nn_modules_sparse_embedding.py:232: Failed
__________________________ test_embedding_edge_cases ___________________________

    def test_embedding_edge_cases():
        """Test edge cases for Embedding"""
        torch.manual_seed(42)
    
        # Test 1: Single element input
        embedding = Embedding(num_embeddings=10, embedding_dim=3)
        indices = torch.tensor([5], dtype=torch.long)
        output = embedding(indices)
        assert output.shape == (1, 3)
        assert torch.isfinite(output).all()
    
        # Test 2: 2D input (batch processing)
        embedding = Embedding(num_embeddings=10, embedding_dim=3)
        indices = torch.randint(0, 10, (2, 4), dtype=torch.long)
        output = embedding(indices)
        assert output.shape == (2, 4, 3)
    
        # Test 3: Large dimensions
        embedding = Embedding(num_embeddings=1000, embedding_dim=256)
        indices = torch.randint(0, 1000, (32, 10), dtype=torch.long)  # batch of 32, seq length 10
        output = embedding(indices)
        assert output.shape == (32, 10, 256)
        assert torch.isfinite(output).all()
    
        # Test 4: Negative padding_idx
        embedding = Embedding(num_embeddings=10, embedding_dim=3, padding_idx=-1)
        indices = torch.tensor([1, 2, -1, 4], dtype=torch.long)
>       output = embedding(indices)

tests/test_torch_nn_modules_sparse_embedding.py:291: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/sparse.py:160: in forward
    return F.embedding(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([ 1,  2, -1,  4])
weight = Parameter containing:
tensor([[-0.4001, -0.0233, -0.1459],
        [ 0.7031, -0.3245, -0.5097],
        [-0.8257, -0.9...2373,  0.8104, -0.2995],
        [-0.9664,  0.7101,  1.2301],
        [ 0.0000,  0.0000,  0.0000]], requires_grad=True)
padding_idx = 9, max_norm = None, norm_type = 2.0, scale_grad_by_freq = False
sparse = False

    def embedding(
        input: Tensor,
        weight: Tensor,
        padding_idx: Optional[int] = None,
        max_norm: Optional[float] = None,
        norm_type: float = 2.0,
        scale_grad_by_freq: bool = False,
        sparse: bool = False,
    ) -> Tensor:
        r"""A simple lookup table that looks up embeddings in a fixed dictionary and size.
    
        This module is often used to retrieve word embeddings using indices.
        The input to the module is a list of indices, and the embedding matrix,
        and the output is the corresponding word embeddings.
    
        See :class:`torch.nn.Embedding` for more details.
    
        Args:
            input (LongTensor): Tensor containing indices into the embedding matrix
            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
                and number of columns equal to the embedding size
            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
                                         i.e. it remains as a fixed "pad".
            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                        is renormalized to have norm :attr:`max_norm`.
                                        Note: this will modify :attr:`weight` in-place.
            norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
            scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of
                                                    the words in the mini-batch. Default ``False``.
            sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.
    
        Shape:
            - Input: LongTensor of arbitrary shape containing the indices to extract
            - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,
              where V = maximum index + 1 and embedding_dim = the embedding size
            - Output: `(*, embedding_dim)`, where `*` is the input shape
    
        Examples::
    
            >>> # a batch of 2 samples of 4 indices each
            >>> input = torch.tensor([[1, 2, 4, 5], [4, 3, 2, 9]])
            >>> # an embedding matrix containing 10 tensors of size 3
            >>> embedding_matrix = torch.rand(10, 3)
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> F.embedding(input, embedding_matrix)
            tensor([[[ 0.8490,  0.9625,  0.6753],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.6246,  0.9751,  0.3618],
                     [ 0.4161,  0.2419,  0.7383]],
    
                    [[ 0.6246,  0.9751,  0.3618],
                     [ 0.0237,  0.7794,  0.0528],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.3385,  0.8612,  0.1867]]])
    
            >>> # example with padding_idx
            >>> weights = torch.rand(10, 3)
            >>> weights[0, :].zero_()
            >>> embedding_matrix = weights
            >>> input = torch.tensor([[0,2,0,5]])
            >>> F.embedding(input, embedding_matrix, padding_idx=0)
            tensor([[[ 0.0000,  0.0000,  0.0000],
                     [ 0.5609,  0.5384,  0.8720],
                     [ 0.0000,  0.0000,  0.0000],
                     [ 0.6262,  0.2438,  0.7471]]])
        """
    
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(
                embedding,
                (input, weight),
                input,
                weight,
                padding_idx=padding_idx,
                max_norm=max_norm,
                norm_type=norm_type,
                scale_grad_by_freq=scale_grad_by_freq,
                sparse=sparse,
            )
        if padding_idx is not None:
            if padding_idx > 0:
                assert padding_idx < weight.size(0), "Padding_idx must be within num_embeddings"
            elif padding_idx < 0:
                assert padding_idx >= -weight.size(0), "Padding_idx must be within num_embeddings"
                padding_idx = weight.size(0) + padding_idx
        else:
            padding_idx = -1
        if max_norm is not None:
            # Note [embedding_renorm contiguous]
            # `embedding_renorm_` will call .contiguous() on input anyways, so we
            # call it here and take advantage of the improved locality in the
            # `embedding` call below too.
            input = input.contiguous()
            # Note [embedding_renorm set_grad_enabled]
            # XXX: equivalent to
            # with torch.no_grad():
            #   torch.embedding_renorm_
            # remove once script supports set_grad_enabled
            _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
>       return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
E       IndexError: index out of range in self

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/functional.py:2210: IndexError
_ TestEmbeddingBag.test_embeddingbag_sum_mode[10-3-max-None-input_shape1-offsets1-dtype1-cpu] _

self = <test_torch_nn_modules_sparse_embeddingbag.TestEmbeddingBag object at 0x10e7c5b70>
set_random_seed = 42, num_embeddings = 10, embedding_dim = 3, mode = 'max'
padding_idx = None, input_shape = (8,), offsets = [0, 3, 8], dtype = torch.int64
device = 'cpu'

    @pytest.mark.parametrize("num_embeddings,embedding_dim,mode,padding_idx,input_shape,offsets,dtype,device", [
        (10, 3, "sum", None, (8,), [0, 3, 8], torch.long, "cpu"),
        # Parameter extension: max mode (Medium priority)
        (10, 3, "max", None, (8,), [0, 3, 8], torch.long, "cpu"),
    ])
    def test_embeddingbag_sum_mode(self, set_random_seed, num_embeddings, embedding_dim,
                                  mode, padding_idx, input_shape, offsets, dtype, device):
        """TC-03: EmbeddingBag sum 模式（包含 max 模式扩展）"""
        # Create EmbeddingBag module
        embedding_bag = EmbeddingBag(
            num_embeddings=num_embeddings,
            embedding_dim=embedding_dim,
            mode=mode,
            padding_idx=padding_idx,
            sparse=False  # Use dense gradients for simplicity
        )
    
        # Create test input
        indices = torch.randint(0, num_embeddings, input_shape, dtype=dtype, device=device)
        offsets_tensor = torch.tensor(offsets, dtype=torch.long, device=device)
    
        # Forward pass
        output = embedding_bag(indices, offsets_tensor)
    
        # Weak assertions
        # 1. Shape assertion
        expected_batch_size = len(offsets)
        assert output.shape == (expected_batch_size, embedding_dim), \
            f"Output shape mismatch: {output.shape} != ({expected_batch_size}, {embedding_dim})"
    
        # 2. Dtype assertion
        assert output.dtype == torch.float32, \
            f"Output dtype should be float32, got {output.dtype}"
    
        # 3. Finite values assertion
        assert torch.isfinite(output).all(), "Output contains NaN or Inf values"
    
        # 4. Aggregation correctness assertion (weak)
        # Verify that output values are within reasonable range
        # For sum mode, values should be sum of embedding vectors
        # For max mode, values should be max of embedding vectors
        # We'll do a basic sanity check
        weight_norm = torch.norm(embedding_bag.weight, dim=1)
        max_weight_norm = weight_norm.max().item()
    
        if mode == "sum":
            # Sum of up to (input_shape[0] / len(offsets)) embeddings per bag
            max_bag_size = max(offsets[i+1] - offsets[i] for i in range(len(offsets)-1)) if len(offsets) > 1 else input_shape[0]
            expected_max_norm = max_weight_norm * max_bag_size
            output_norms = torch.norm(output, dim=1)
            assert (output_norms <= expected_max_norm * 1.5).all(), \
                f"Sum mode output norms too large: max {output_norms.max().item()} > {expected_max_norm * 1.5}"
    
        elif mode == "max":
            # Max mode output should not exceed individual embedding norms
            output_norms = torch.norm(output, dim=1)
>           assert (output_norms <= max_weight_norm * 1.1).all(), \
                f"Max mode output norms too large: max {output_norms.max().item()} > {max_weight_norm * 1.1}"
E           AssertionError: Max mode output norms too large: max 3.2297418117523193 > 2.8549836397171022
E           assert tensor(False)
E            +  where tensor(False) = <built-in method all of Tensor object at 0x102ee5490>()
E            +    where <built-in method all of Tensor object at 0x102ee5490> = tensor([2.2006, 3.2297, 0.0000], grad_fn=<NormBackward1>) <= (2.5954396724700928 * 1.1).all

tests/test_torch_nn_modules_sparse_embeddingbag.py:102: AssertionError
_____________________ test_embeddingbag_invalid_parameters _____________________

    def test_embeddingbag_invalid_parameters():
        """Test invalid parameter combinations for EmbeddingBag"""
        # Test 1: num_embeddings <= 0 should raise RuntimeError (not ValueError)
        # EmbeddingBag constructor doesn't validate num_embeddings directly,
        # but creating weight tensor with invalid dimensions will fail
>       with pytest.raises((RuntimeError, ValueError)) as exc_info:
E       Failed: DID NOT RAISE any of (<class 'RuntimeError'>, <class 'ValueError'>)

tests/test_torch_nn_modules_sparse_embeddingbag.py:241: Failed
_________________________ test_embeddingbag_edge_cases _________________________

    def test_embeddingbag_edge_cases():
        """Test edge cases for EmbeddingBag"""
        # Test 1: Single bag with explicit offsets
        torch.manual_seed(42)
        embedding_bag = EmbeddingBag(num_embeddings=10, embedding_dim=3, mode="sum")
        indices = torch.randint(0, 10, (5,), dtype=torch.long)
    
        # When offsets is not provided for 1D input, it raises ValueError
        # So we need to provide offsets explicitly
        offsets = torch.tensor([0], dtype=torch.long)
        output = embedding_bag(indices, offsets)
        assert output.shape == (1, 3)
        assert torch.isfinite(output).all()
    
        # Test 2: Single element per bag
        embedding_bag = EmbeddingBag(num_embeddings=10, embedding_dim=3, mode="mean")
        indices = torch.tensor([1, 2, 3, 4], dtype=torch.long)
        offsets = torch.tensor([0, 1, 2, 3], dtype=torch.long)
    
        output = embedding_bag(indices, offsets)
        assert output.shape == (4, 3)
    
        # Test 3: Large num_embeddings and embedding_dim
        embedding_bag = EmbeddingBag(num_embeddings=1000, embedding_dim=256, mode="sum")
        indices = torch.randint(0, 1000, (100,), dtype=torch.long)
        offsets = torch.tensor([0, 50, 100], dtype=torch.long)
    
        output = embedding_bag(indices, offsets)
        assert output.shape == (3, 256)
        assert torch.isfinite(output).all()
    
        # Test 4: Negative padding_idx
        embedding_bag = EmbeddingBag(num_embeddings=10, embedding_dim=3, padding_idx=-1, mode="sum")
        indices = torch.tensor([1, 2, -1, 4], dtype=torch.long)
        offsets = torch.tensor([0, 2, 4], dtype=torch.long)
    
>       output = embedding_bag(indices, offsets)

tests/test_torch_nn_modules_sparse_embeddingbag.py:314: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/sparse.py:387: in forward
    return F.embedding_bag(input, self.weight, offsets,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([ 1,  2, -1,  4])
weight = Parameter containing:
tensor([[ 0.1416, -1.2835,  0.0746],
        [ 0.1165, -0.9246,  1.7482],
        [ 0.6768,  0.7...4726,  0.2986,  0.3352],
        [ 0.1059,  1.9793,  1.1869],
        [ 0.0000,  0.0000,  0.0000]], requires_grad=True)
offsets = tensor([0, 2, 4]), max_norm = None, norm_type = 2.0
scale_grad_by_freq = False, mode = 'sum', sparse = False
per_sample_weights = None, include_last_offset = False, padding_idx = 9

    def embedding_bag(
        input: Tensor,
        weight: Tensor,
        offsets: Optional[Tensor] = None,
        max_norm: Optional[float] = None,
        norm_type: float = 2,
        scale_grad_by_freq: bool = False,
        mode: str = "mean",
        sparse: bool = False,
        per_sample_weights: Optional[Tensor] = None,
        include_last_offset: bool = False,
        padding_idx: Optional[int] = None,
    ) -> Tensor:
        r"""Computes sums, means or maxes of `bags` of embeddings, without instantiating the
        intermediate embeddings.
    
        See :class:`torch.nn.EmbeddingBag` for more details.
    
        Note:
            {backward_reproducibility_note}
    
        Args:
            input (LongTensor): Tensor containing bags of indices into the embedding matrix
            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
                and number of columns equal to the embedding size
            offsets (LongTensor, optional): Only used when :attr:`input` is 1D. :attr:`offsets` determines
                                 the starting index position of each bag (sequence) in :attr:`input`.
            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                        is renormalized to have norm :attr:`max_norm`.
                                        Note: this will modify :attr:`weight` in-place.
            norm_type (float, optional): The ``p`` in the ``p``-norm to compute for the :attr:`max_norm` option.
                                         Default ``2``.
            scale_grad_by_freq (bool, optional): if given, this will scale gradients by the inverse of frequency of
                                                    the words in the mini-batch. Default ``False``.
                                                    Note: this option is not supported when ``mode="max"``.
            mode (str, optional): ``"sum"``, ``"mean"`` or ``"max"``. Specifies the way to reduce the bag.
                                     Default: ``"mean"``
            sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.
                                     Note: this option is not supported when ``mode="max"``.
            per_sample_weights (Tensor, optional): a tensor of float / double weights, or None
                to indicate all weights should be taken to be 1. If specified, :attr:`per_sample_weights`
                must have exactly the same shape as input and is treated as having the same
                :attr:`offsets`, if those are not None.
    
            include_last_offset (bool, optional): if ``True``, the size of offsets is equal to the number of bags + 1.
                The last element is the size of the input, or the ending index position of the last bag (sequence).
    
            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the
                                         gradient; therefore, the embedding vector at :attr:`padding_idx` is not updated
                                         during training, i.e. it remains as a fixed "pad". Note that the embedding
                                         vector at :attr:`padding_idx` is excluded from the reduction.
    
        Shape:
            - :attr:`input` (LongTensor) and :attr:`offsets` (LongTensor, optional)
    
              - If :attr:`input` is 2D of shape `(B, N)`, it will be treated as ``B`` bags (sequences)
                each of fixed length ``N``, and this will return ``B`` values aggregated in a way
                depending on the :attr:`mode`. :attr:`offsets` is ignored and required to be ``None`` in this case.
    
              - If :attr:`input` is 1D of shape `(N)`, it will be treated as a concatenation of
                multiple bags (sequences). :attr:`offsets` is required to be a 1D tensor containing
                the starting index positions of each bag in :attr:`input`. Therefore, for :attr:`offsets`
                of shape `(B)`, :attr:`input` will be viewed as having ``B`` bags.
                Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.
    
            - :attr:`weight` (Tensor): the learnable weights of the module of shape `(num_embeddings, embedding_dim)`
    
            - :attr:`per_sample_weights` (Tensor, optional). Has the same shape as :attr:`input`.
    
            - :attr:`output`: aggregated embedding values of shape `(B, embedding_dim)`
    
        Examples::
    
            >>> # an Embedding module containing 10 tensors of size 3
            >>> embedding_matrix = torch.rand(10, 3)
            >>> # a batch of 2 samples of 4 indices each
            >>> input = torch.tensor([1,2,4,5,4,3,2,9])
            >>> offsets = torch.tensor([0,4])
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> F.embedding_bag(input, embedding_matrix, offsets)
            tensor([[ 0.3397,  0.3552,  0.5545],
                    [ 0.5893,  0.4386,  0.5882]])
    
            >>> # example with padding_idx
            >>> embedding_matrix = torch.rand(10, 3)
            >>> input = torch.tensor([2, 2, 2, 2, 4, 3, 2, 9])
            >>> offsets = torch.tensor([0,4])
            >>> F.embedding_bag(input, embedding_matrix, offsets, padding_idx=2, mode='sum')
            tensor([[ 0.0000,  0.0000,  0.0000],
                    [-0.7082,  3.2145, -2.6251]])
        """
        if has_torch_function_variadic(input, weight, offsets, per_sample_weights):
            return handle_torch_function(
                embedding_bag,
                (input, weight, offsets, per_sample_weights),
                input,
                weight,
                offsets=offsets,
                max_norm=max_norm,
                norm_type=norm_type,
                scale_grad_by_freq=scale_grad_by_freq,
                mode=mode,
                sparse=sparse,
                per_sample_weights=per_sample_weights,
                include_last_offset=include_last_offset,
                padding_idx=padding_idx,
            )
        # Check for backward compatibility.
        # Used to be embedding_bag(weight, input, ...)
        # Now is     embedding_bag(input, weight, ...)
        if weight.dtype == torch.long and input.is_floating_point():
            warnings.warn(
                "Argument order of nn.functional.embedding_bag was changed. "
                "Usage `embedding_bag(weight, input, ...)` is deprecated, "
                "and should now be `embedding_bag(input, weight, ...)`."
            )
            weight, input = input, weight
    
        if per_sample_weights is not None and input.size() != per_sample_weights.size():
            raise ValueError(
                "embedding_bag: If per_sample_weights ({}) is not None, "
                "then it must have the same shape as the input ({})".format(per_sample_weights.shape, input.shape)
            )
    
        if input.dim() == 2:
            if offsets is not None:
                type_str = "<unknown>"
                # TODO: Remove this once script supports type() calls
                if not torch.jit.is_scripting():
                    type_str = str(type(offsets))
                raise ValueError(
                    "if input is 2D, then offsets has to be None"
                    ", as input is treated is a mini-batch of"
                    " fixed length sequences. However, found "
                    "offsets of type {}".format(type_str)
                )
            offsets = torch.arange(0, input.numel(), input.size(1), dtype=input.dtype, device=input.device)
    
            input = input.reshape(-1)
            if per_sample_weights is not None:
                per_sample_weights = per_sample_weights.reshape(-1)
        elif input.dim() == 1:
            if offsets is None:
                raise ValueError("offsets has to be a 1D Tensor but got None")
            if offsets.dim() != 1:
                raise ValueError("offsets has to be a 1D Tensor")
        else:
            raise ValueError("input has to be 1D or 2D Tensor," " but got Tensor of dimension {}".format(input.dim()))
        if mode == "sum":
            mode_enum = 0
        elif mode == "mean":
            mode_enum = 1
        elif mode == "max":
            mode_enum = 2
    
            if scale_grad_by_freq:
                raise ValueError("max mode does not support scaling the gradient by the frequency")
    
            if sparse:
                raise ValueError("max mode does not support sparse weights")
    
        else:
            raise ValueError("mode has to be one of sum, mean or max")
    
        if max_norm is not None:
            # XXX: equivalent to
            # with torch.no_grad():
            #   torch.nembedding_renorm_
            # remove once script supports set_grad_enabled
            _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
    
        if per_sample_weights is not None and mode != "sum":
            raise NotImplementedError(
                "embedding_bag: per_sample_weights was not None. "
                "per_sample_weights is only supported for mode='sum' "
                "(got mode='{}'). Please open a feature request on GitHub.".format(mode)
            )
    
>       ret, _, _, _ = torch.embedding_bag(
            weight, input, offsets, scale_grad_by_freq, mode_enum, sparse, per_sample_weights, include_last_offset, padding_idx
        )
E       RuntimeError: embedding_bag: Expected idx >= 0 && idx < num_embeddings but found idx to be -1

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/functional.py:2392: RuntimeError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                                 Stmts   Miss Branch BrPart  Cover   Missing
------------------------------------------------------------------------------------------------
tests/test_torch_nn_modules_sparse_embedding.py        138     47     20      5    62%   19-32, 36-42, 109->exit, 161->168, 171->186, 177->186, 180->179, 235-262, 292-317
tests/test_torch_nn_modules_sparse_embeddingbag.py     144     32      6      1    77%   19-29, 33-39, 99->106, 244-276, 315
------------------------------------------------------------------------------------------------
TOTAL                                                  282     79     26      6    69%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_sparse_embedding.py::test_embedding_invalid_parameters
FAILED tests/test_torch_nn_modules_sparse_embedding.py::test_embedding_edge_cases
FAILED tests/test_torch_nn_modules_sparse_embeddingbag.py::TestEmbeddingBag::test_embeddingbag_sum_mode[10-3-max-None-input_shape1-offsets1-dtype1-cpu]
FAILED tests/test_torch_nn_modules_sparse_embeddingbag.py::test_embeddingbag_invalid_parameters
FAILED tests/test_torch_nn_modules_sparse_embeddingbag.py::test_embeddingbag_edge_cases
5 failed, 9 passed in 0.95s

Error: exit 1