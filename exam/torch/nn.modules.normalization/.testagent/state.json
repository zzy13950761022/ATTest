{
  "workflow_id": "83868475",
  "created_at": "2026-01-18T23:18:09.439956",
  "op": "torch_nn_modules_normalization",
  "arch": "python",
  "soc": "python",
  "vendor": "python",
  "project_root": "/Users/zzf1sh/Documents/Project/TestAgent-CLI-main/exam/torch_group/nn.modules.normalization",
  "target": "torch.nn.modules.normalization",
  "target_slug": "torch_nn_modules_normalization",
  "current_stage": "complete",
  "stage_index": 6,
  "mode": "full-auto",
  "epoch_total": 5,
  "epoch_current": 5,
  "last_failure_signature": "6c5a7cfb8a7e3430f0ec19c21dc7186fdfa787f9",
  "last_error_signature": "6dd40b8fc86fa90ba1c0725011c5f82a9c9d3fcc",
  "last_block_errors": {
    "CASE_01": [
      "RuntimeError"
    ],
    "CASE_03": [
      "AssertionError"
    ],
    "CASE_04": [
      "AssertionError"
    ]
  },
  "auto_stop_reason": "",
  "artifacts": {
    "function_doc.md": "# torch.nn.modules.normalization - 函数说明\n\n## 1. 基本信息\n- **FQN**: torch.nn.modules.normalization\n- **模块文件**: `/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/normalization.py`\n- **签名**: 模块（包含多个类）\n- **对象类型**: Python 模块\n\n## 2. 功能概述\nPyTorch 归一化层模块，提供四种归一化方法：\n- LocalResponseNorm：局部响应归一化，跨通道归一化\n- CrossMapLRN2d：跨通道 LRN 2D 版本\n- LayerNorm：层归一化，在最后 D 维计算统计量\n- GroupNorm：组归一化，将通道分组后归一化\n\n## 3. 参数说明\n模块包含四个主要类，各有不同参数：\n\n**LocalResponseNorm**\n- size (int): 用于归一化的相邻通道数\n- alpha (float=1e-4): 乘法因子\n- beta (float=0.75): 指数\n- k (float=1): 加法因子\n\n**LayerNorm**\n- normalized_shape (int/list/torch.Size): 归一化形状\n- eps (float=1e-5): 数值稳定性分母项\n- elementwise_affine (bool=True): 是否启用逐元素仿射变换\n- device/dtype: 可选设备/数据类型参数\n\n**GroupNorm**\n- num_groups (int): 分组数\n- num_channels (int): 输入通道数\n- eps (float=1e-5): 数值稳定性分母项\n- affine (bool=True): 是否启用仿射变换\n- device/dtype: 可选设备/数据类型参数\n\n## 4. 返回值\n- 各类的 forward 方法返回 Tensor，形状与输入相同\n- 归一化后的张量\n\n## 5. 文档要点\n- LocalResponseNorm：输入形状 (N, C, *)，输出形状相同\n- LayerNorm：在最后 D 维计算均值和标准差，D 为 normalized_shape 维度\n- GroupNorm：num_channels 必须能被 num_groups 整除\n- 所有归一化层在训练和评估模式都使用输入数据统计量\n- 标准差计算使用有偏估计器（torch.var(input, unbiased=False)）\n\n## 6. 源码摘要\n- LocalResponseNorm.forward：调用 F.local_response_norm\n- CrossMapLRN2d.forward：调用 _cross_map_lrn2d.apply\n- LayerNorm.forward：调用 F.layer_norm\n- GroupNorm.forward：调用 F.group_norm\n- 初始化时检查参数有效性（如整除性）\n- 仿射参数使用 init.ones_ 和 init.zeros_ 初始化\n\n## 7. 示例与用法（如有）\n- LocalResponseNorm：支持 2D 和 4D 信号\n- LayerNorm：NLP 和图像处理示例\n- GroupNorm：不同分组策略示例（等效于 InstanceNorm/LayerNorm）\n\n## 8. 风险与空白\n- 目标为模块而非单个函数，包含四个主要类\n- CrossMapLRN2d 文档字符串缺失\n- 需要测试多实体情况：四个类各有不同参数和行为\n- 边界情况：GroupNorm 的整除性检查\n- 设备/数据类型参数的具体约束未详细说明\n- 缺少对 CrossMapLRN2d 与 LocalResponseNorm 差异的明确说明",
    "requirements.md": "# torch.nn.modules.normalization 测试需求\n\n## 1. 目标与范围\n- 测试四种归一化层：LocalResponseNorm、CrossMapLRN2d、LayerNorm、GroupNorm\n- 验证前向传播正确性、参数有效性检查、设备/数据类型支持\n- 不在范围：反向传播梯度计算、优化器集成、自定义初始化\n\n## 2. 输入与约束\n- **LocalResponseNorm**: size(int), alpha(float=1e-4), beta(float=0.75), k(float=1)\n- **LayerNorm**: normalized_shape(int/list/torch.Size), eps(float=1e-5), elementwise_affine(bool=True), device/dtype可选\n- **GroupNorm**: num_groups(int), num_channels(int), eps(float=1e-5), affine(bool=True), device/dtype可选\n- **形状约束**: GroupNorm要求num_channels能被num_groups整除\n- **设备要求**: 支持CPU/CUDA，数据类型支持float32/float64\n- **随机性**: 无全局状态依赖，仅依赖输入数据统计量\n\n## 3. 输出与判定\n- 所有类forward方法返回Tensor，形状与输入相同\n- 浮点容差：相对误差1e-5，绝对误差1e-7\n- 状态变化：无副作用，训练/评估模式行为一致\n- 仿射参数：当affine=True时，scale/bias参数应存在且可训练\n\n## 4. 错误与异常场景\n- GroupNorm：num_channels不能被num_groups整除时抛出ValueError\n- 非法输入：非Tensor输入、维度不匹配、无效数据类型\n- 边界值：size=0或负数、eps=0或负数、极端alpha/beta值\n- 形状异常：normalized_shape与输入最后D维不匹配\n- 设备不匹配：参数与输入张量设备不一致\n\n## 5. 依赖与环境\n- 外部依赖：torch、torch.nn.functional\n- 设备依赖：需要CUDA环境测试GPU支持\n- Mock需求：无需外部资源，纯计算模块\n- 环境要求：Python 3.8+，PyTorch 1.9+\n\n## 6. 覆盖与优先级\n- **必测路径（高优先级）**：\n  1. GroupNorm整除性检查异常处理\n  2. LayerNorm不同normalized_shape形状支持\n  3. LocalResponseNorm跨通道归一化正确性\n  4. 设备/数据类型参数化测试\n  5. affine/elementwise_affine参数开关测试\n\n- **可选路径（中/低优先级）**：\n  - CrossMapLRN2d与LocalResponseNorm差异对比\n  - 极端输入值（极大/极小数值）稳定性\n  - 批量大小=1边界情况\n  - 不同维度输入（2D/3D/4D）支持\n  - 训练/评估模式一致性验证\n\n- **已知风险/缺失信息**：\n  - CrossMapLRN2d文档字符串缺失\n  - 设备/数据类型参数具体约束未详细说明\n  - 缺少对CrossMapLRN2d与LocalResponseNorm差异的明确说明\n  - 标准差计算使用有偏估计器的具体影响",
    "test_plan.json": "{\n  \"plan_version\": 2,\n  \"target\": \"torch.nn.modules.normalization\",\n  \"block_rules\": {\n    \"header_block\": \"HEADER\",\n    \"footer_block\": \"FOOTER\",\n    \"case_prefix\": \"CASE_\",\n    \"case_format\": \"CASE_01\"\n  },\n  \"iteration_strategy\": {\n    \"round1\": {\n      \"include\": \"SMOKE_SET\",\n      \"assert_level\": \"weak\",\n      \"max_blocks\": 5\n    },\n    \"roundN\": {\n      \"only_fix_failed_blocks\": true,\n      \"block_limit\": 3,\n      \"promote_deferred\": true\n    },\n    \"final\": {\n      \"enable_strong_asserts\": true,\n      \"coverage_optional\": true\n    }\n  },\n  \"test_files\": {\n    \"default\": \"tests/test_torch_nn_modules_normalization.py\",\n    \"all_pattern\": \"tests/test_torch_nn_modules_normalization_*.py\",\n    \"groups\": {\n      \"G1\": \"tests/test_torch_nn_modules_normalization_g1.py\",\n      \"G2\": \"tests/test_torch_nn_modules_normalization_g2.py\",\n      \"G3\": \"tests/test_torch_nn_modules_normalization_g3.py\"\n    }\n  },\n  \"active_group_order\": [\"G1\", \"G2\", \"G3\"],\n  \"groups\": [\n    {\n      \"group_id\": \"G1\",\n      \"title\": \"GroupNorm 核心功能\",\n      \"entrypoints\": [\"GroupNorm\"],\n      \"smoke_set\": [\"CASE_01\", \"CASE_02\"],\n      \"deferred_set\": [\"CASE_05\", \"CASE_06\"],\n      \"note\": \"测试分组归一化的基本功能和整除性检查\"\n    },\n    {\n      \"group_id\": \"G2\",\n      \"title\": \"LayerNorm 核心功能\",\n      \"entrypoints\": [\"LayerNorm\"],\n      \"smoke_set\": [\"CASE_03\"],\n      \"deferred_set\": [\"CASE_07\", \"CASE_08\"],\n      \"note\": \"测试层归一化的形状支持和仿射变换\"\n    },\n    {\n      \"group_id\": \"G3\",\n      \"title\": \"LocalResponseNorm 与 CrossMapLRN2d\",\n      \"entrypoints\": [\"LocalResponseNorm\", \"CrossMapLRN2d\"],\n      \"smoke_set\": [\"CASE_04\"],\n      \"deferred_set\": [\"CASE_09\", \"CASE_10\"],\n      \"note\": \"测试局部响应归一化的跨通道计算\"\n    }\n  ],\n  \"cases\": [\n    {\n      \"tc_id\": \"TC-01\",\n      \"block_id\": \"CASE_01\",\n      \"group_id\": \"G1\",\n      \"name\": \"GroupNorm 基本前向传播\",\n      \"priority\": \"High\",\n      \"param_matrix\": [\n        {\n          \"num_groups\": 2,\n          \"num_channels\": 4,\n          \"eps\": 1e-5,\n          \"affine\": true,\n          \"dtype\": \"float32\",\n          \"device\": \"cpu\",\n          \"shape\": [2, 4, 8, 8],\n          \"flags\": []\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"shape\", \"dtype\", \"finite\", \"basic_property\"],\n        \"strong\": [\"approx_equal\", \"group_statistics\"]\n      },\n      \"oracle\": \"torch.nn.functional.group_norm\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"S\",\n      \"max_lines\": 80,\n      \"max_params\": 6,\n      \"is_parametrized\": true,\n      \"requires_mock\": false\n    },\n    {\n      \"tc_id\": \"TC-02\",\n      \"block_id\": \"CASE_02\",\n      \"group_id\": \"G1\",\n      \"name\": \"GroupNorm 整除性异常检查\",\n      \"priority\": \"High\",\n      \"param_matrix\": [\n        {\n          \"num_groups\": 3,\n          \"num_channels\": 5,\n          \"eps\": 1e-5,\n          \"affine\": true,\n          \"dtype\": \"float32\",\n          \"device\": \"cpu\",\n          \"shape\": [2, 5, 8, 8],\n          \"flags\": [\"expect_error\"]\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"exception_type\", \"exception_message\"],\n        \"strong\": [\"exception_detail\"]\n      },\n      \"oracle\": \"ValueError\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"S\",\n      \"max_lines\": 60,\n      \"max_params\": 5,\n      \"is_parametrized\": false,\n      \"requires_mock\": false\n    },\n    {\n      \"tc_id\": \"TC-03\",\n      \"block_id\": \"CASE_03\",\n      \"group_id\": \"G2\",\n      \"name\": \"LayerNorm 基本前向传播\",\n      \"priority\": \"High\",\n      \"param_matrix\": [\n        {\n          \"normalized_shape\": [8, 8],\n          \"eps\": 1e-5,\n          \"elementwise_affine\": true,\n          \"dtype\": \"float32\",\n          \"device\": \"cpu\",\n          \"shape\": [2, 4, 8, 8],\n          \"flags\": []\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"shape\", \"dtype\", \"finite\", \"basic_property\"],\n        \"strong\": [\"approx_equal\", \"layer_statistics\"]\n      },\n      \"oracle\": \"torch.nn.functional.layer_norm\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"S\",\n      \"max_lines\": 80,\n      \"max_params\": 6,\n      \"is_parametrized\": true,\n      \"requires_mock\": false\n    },\n    {\n      \"tc_id\": \"TC-04\",\n      \"block_id\": \"CASE_04\",\n      \"group_id\": \"G3\",\n      \"name\": \"LocalResponseNorm 基本前向传播\",\n      \"priority\": \"High\",\n      \"param_matrix\": [\n        {\n          \"size\": 5,\n          \"alpha\": 1e-4,\n          \"beta\": 0.75,\n          \"k\": 1.0,\n          \"dtype\": \"float32\",\n          \"device\": \"cpu\",\n          \"shape\": [2, 16, 8, 8],\n          \"flags\": []\n        }\n      ],\n      \"asserts\": {\n        \"weak\": [\"shape\", \"dtype\", \"finite\", \"basic_property\"],\n        \"strong\": [\"approx_equal\", \"response_normalization\"]\n      },\n      \"oracle\": \"torch.nn.functional.local_response_norm\",\n      \"assertion_level\": \"weak\",\n      \"size\": \"S\",\n      \"max_lines\": 80,\n      \"max_params\": 7,\n      \"is_parametrized\": true,\n      \"requires_mock\": false\n    }\n  ],\n  \"param_extensions\": [\n    {\n      \"base_block_id\": \"CASE_01\",\n      \"priority\": \"Medium\",\n      \"params\": {\n        \"num_groups\": 4,\n        \"num_channels\": 8,\n        \"eps\": 1e-6,\n        \"affine\": false,\n        \"dtype\": \"float64\",\n        \"device\": \"cpu\",\n        \"shape\": [4, 8, 16, 16],\n        \"flags\": []\n      },\n      \"note\": \"作为参数扩展：不同分组数、无仿射变换、float64\"\n    },\n    {\n      \"base_block_id\": \"CASE_01\",\n      \"priority\": \"Low\",\n      \"params\": {\n        \"num_groups\": 1,\n        \"num_channels\": 4,\n        \"eps\": 1e-7,\n        \"affine\": true,\n        \"dtype\": \"float32\",\n        \"device\": \"cuda\",\n        \"shape\": [1, 4, 4, 4],\n        \"flags\": [\"cuda_available\"]\n      },\n      \"note\": \"作为参数扩展：单组、CUDA设备、批量大小=1\"\n    },\n    {\n      \"base_block_id\": \"CASE_03\",\n      \"priority\": \"Medium\",\n      \"params\": {\n        \"normalized_shape\": 8,\n        \"eps\": 1e-6,\n        \"elementwise_affine\": false,\n        \"dtype\": \"float64\",\n        \"device\": \"cpu\",\n        \"shape\": [2, 4, 8],\n        \"flags\": []\n      },\n      \"note\": \"作为参数扩展：1D归一化形状、无仿射变换、float64\"\n    }\n  ],\n  \"smoke_set\": [\"CASE_01\", \"CASE_02\", \"CASE_03\", \"CASE_04\"],\n  \"deferred_set\": [\"CASE_05\", \"CASE_06\", \"CASE_07\", \"CASE_08\", \"CASE_09\", \"CASE_10\"]\n}",
    "test_plan.md": "# torch.nn.modules.normalization 测试计划\n\n## 1. 测试策略\n- 单元测试框架：pytest\n- 隔离策略：每个测试用例独立实例化归一化层\n- 随机性处理：固定随机种子，使用 torch.manual_seed\n- 设备隔离：CPU测试为主，CUDA测试作为参数扩展\n\n## 2. 生成规格摘要（来自 test_plan.json）\n- **SMOKE_SET**: CASE_01, CASE_02, CASE_03, CASE_04\n- **DEFERRED_SET**: CASE_05, CASE_06, CASE_07, CASE_08, CASE_09, CASE_10\n- **group 列表**: G1(GroupNorm), G2(LayerNorm), G3(LocalResponseNorm/CrossMapLRN2d)\n- **active_group_order**: G1 → G2 → G3\n- **断言分级策略**: 首轮使用weak断言（形状、数据类型、有限性、基本属性）\n- **预算策略**: 每个CASE最多80行代码，最多6个参数，优先参数化测试\n\n## 3. 数据与边界\n- **正常数据集**: 随机生成正态分布张量，形状符合各归一化层要求\n- **边界值**: eps极小值(1e-7)、批量大小=1、极端alpha/beta值\n- **极端形状**: 大尺寸输入(64x64)、小尺寸输入(4x4)、不同维度(2D/3D/4D)\n- **空输入**: 不适用（归一化层需要有效输入）\n- **负例与异常场景**:\n  - GroupNorm整除性异常\n  - 非法normalized_shape形状\n  - 无效参数值（负数size、零eps）\n  - 设备/数据类型不匹配\n\n## 4. 覆盖映射\n| TC ID | 对应需求 | 覆盖约束 |\n|-------|----------|----------|\n| TC-01 | GroupNorm基本功能 | 高优先级必测路径 |\n| TC-02 | GroupNorm异常处理 | 整除性检查异常 |\n| TC-03 | LayerNorm基本功能 | 不同normalized_shape支持 |\n| TC-04 | LocalResponseNorm基本功能 | 跨通道归一化正确性 |\n\n**尚未覆盖的风险点**:\n- CrossMapLRN2d与LocalResponseNorm差异对比\n- 训练/评估模式一致性验证\n- 极端数值稳定性（极大/极小值）\n- 不同维度输入全面支持（2D/3D/4D）",
    "tests/test_torch_nn_modules_normalization.py": "import math\nimport pytest\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.normalization import (\n    LocalResponseNorm,\n    CrossMapLRN2d,\n    LayerNorm,\n    GroupNorm\n)\n\n# ==== BLOCK:HEADER START ====\n# Test fixtures and helper functions\n@pytest.fixture(scope=\"function\")\ndef set_random_seed():\n    \"\"\"Set random seed for reproducibility\"\"\"\n    torch.manual_seed(42)\n    return 42\n\ndef assert_tensor_properties(tensor, expected_shape=None, expected_dtype=None, \n                           expected_device=None, name=\"\"):\n    \"\"\"Helper to assert tensor properties\"\"\"\n    assert torch.is_tensor(tensor), f\"{name}: Output is not a tensor\"\n    assert torch.all(torch.isfinite(tensor)), f\"{name}: Tensor contains NaN or Inf\"\n    \n    if expected_shape is not None:\n        assert tensor.shape == expected_shape, \\\n            f\"{name}: Shape mismatch: {tensor.shape} != {expected_shape}\"\n    \n    if expected_dtype is not None:\n        assert tensor.dtype == expected_dtype, \\\n            f\"{name}: Dtype mismatch: {tensor.dtype} != {expected_dtype}\"\n    \n    if expected_device is not None:\n        assert tensor.device == expected_device, \\\n            f\"{name}: Device mismatch: {tensor.device} != {expected_device}\"\n    \n    return True\n# ==== BLOCK:HEADER END ====\n\n# ==== BLOCK:CASE_01 START ====\n# TC-01: GroupNorm 基本前向传播\n# ==== BLOCK:CASE_01 END ====\n\n# ==== BLOCK:CASE_02 START ====\n# TC-02: GroupNorm 整除性异常检查\n# ==== BLOCK:CASE_02 END ====\n\n# ==== BLOCK:CASE_03 START ====\n# TC-03: LayerNorm 基本前向传播\n# ==== BLOCK:CASE_03 END ====\n\n# ==== BLOCK:CASE_04 START ====\n# TC-04: LocalResponseNorm 基本前向传播\n# ==== BLOCK:CASE_04 END ====\n\n# ==== BLOCK:CASE_05 START ====\n# TC-05: DEFERRED - GroupNorm 参数扩展测试\n# ==== BLOCK:CASE_05 END ====\n\n# ==== BLOCK:CASE_06 START ====\n# TC-06: DEFERRED - GroupNorm 设备/数据类型测试\n# ==== BLOCK:CASE_06 END ====\n\n# ==== BLOCK:CASE_07 START ====\n# TC-07: DEFERRED - LayerNorm 参数扩展测试\n# ==== BLOCK:CASE_07 END ====\n\n# ==== BLOCK:CASE_08 START ====\n# TC-08: DEFERRED - LayerNorm 异常形状测试\n# ==== BLOCK:CASE_08 END ====\n\n# ==== BLOCK:CASE_09 START ====\n# TC-09: DEFERRED - CrossMapLRN2d 基本功能测试\n# ==== BLOCK:CASE_09 END ====\n\n# ==== BLOCK:CASE_10 START ====\n# TC-10: DEFERRED - LocalResponseNorm 边界值测试\n# ==== BLOCK:CASE_10 END ====\n\n# ==== BLOCK:FOOTER START ====\n# Additional test classes and helper functions can be added here\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n# ==== BLOCK:FOOTER END ====",
    "execution_log.txt": "=== Run Tests ===\n.......F.ss........F.....FF.FFFFFFF                                      [100%]\n=================================== FAILURES ===================================\n___________________ test_groupnorm_device_dtype[dtype0-cpu] ____________________\n\ndtype = torch.float32, device = 'cpu', set_random_seed = 42\n\n    @pytest.mark.parametrize(\"dtype,device\", [\n        # Test different data types on CPU\n        (torch.float32, \"cpu\"),\n        (torch.float64, \"cpu\"),\n        # Test CUDA if available\n        pytest.param(torch.float32, \"cuda\", marks=pytest.mark.skipif(not torch.cuda.is_available(), reason=\"CUDA not available\")),\n        pytest.param(torch.float64, \"cuda\", marks=pytest.mark.skipif(not torch.cuda.is_available(), reason=\"CUDA not available\")),\n    ])\n    def test_groupnorm_device_dtype(dtype, device, set_random_seed):\n        \"\"\"Test GroupNorm with different devices and data types\"\"\"\n        # Common parameters\n        num_groups = 2\n        num_channels = 8\n        eps = 1e-5\n        affine = True\n        shape = (2, 8, 16, 16)\n    \n        # Create input tensor\n        torch.manual_seed(42)\n        input_tensor = torch.randn(*shape, dtype=dtype, device=device)\n    \n        # Create GroupNorm layer\n        gn = GroupNorm(\n            num_groups=num_groups,\n            num_channels=num_channels,\n            eps=eps,\n            affine=affine\n        ).to(device=device, dtype=dtype)\n    \n        # Forward pass\n        output = gn(input_tensor)\n    \n        # Weak assertions\n        # 1. Shape assertion\n        assert output.shape == input_tensor.shape, \\\n            f\"Output shape {output.shape} != input shape {input_tensor.shape}\"\n    \n        # 2. Dtype assertion\n        assert output.dtype == dtype, \\\n            f\"Output dtype {output.dtype} != expected {dtype}\"\n    \n        # 3. Device assertion\n        assert output.device.type == device, \\\n            f\"Output device {output.device.type} != expected {device}\"\n    \n        # 4. Finite values assertion\n        assert torch.all(torch.isfinite(output)), \\\n            \"Output contains NaN or infinite values\"\n    \n        # 5. Basic normalization property\n        batch_size = shape[0]\n        channels_per_group = num_channels // num_groups\n        spatial_dims = shape[2:]\n    \n        # Reshape for group-wise statistics\n        output_reshaped = output.view(batch_size, num_groups, channels_per_group, *spatial_dims)\n    \n        # Check that each group is normalized\n        for b in range(batch_size):\n            for g in range(num_groups):\n                group_output = output_reshaped[b, g].flatten()\n    \n                # Mean should be close to 0\n                mean_abs = torch.abs(torch.mean(group_output))\n                assert mean_abs < 0.2, f\"Group mean too large: {mean_abs}\"\n    \n                # Std should be close to 1\n                std = torch.std(group_output)\n                assert 0.8 < std < 1.2, f\"Group std out of range: {std}\"\n    \n        # 6. Test parameter device/dtype consistency\n        if affine:\n            # Weight and bias should be on correct device and dtype\n            assert gn.weight.device.type == device, \\\n                f\"Weight device {gn.weight.device.type} != expected {device}\"\n            assert gn.weight.dtype == dtype, \\\n                f\"Weight dtype {gn.weight.dtype} != expected {dtype}\"\n    \n            assert gn.bias.device.type == device, \\\n                f\"Bias device {gn.bias.device.type} != expected {device}\"\n            assert gn.bias.dtype == dtype, \\\n                f\"Bias dtype {gn.bias.dtype} != expected {dtype}\"\n    \n        # 7. Test with mixed precision - FIXED: PyTorch doesn't support automatic dtype conversion\n        # between input and layer parameters. We'll skip this test or handle it differently.\n        # Instead, we'll test that creating a layer with one dtype and using input with another\n        # dtype either works (with casting) or fails gracefully.\n    \n        # Create a new layer for mixed precision test\n        gn_mixed = GroupNorm(\n            num_groups=num_groups,\n            num_channels=num_channels,\n            eps=eps,\n            affine=affine\n        ).to(device=device, dtype=dtype)\n    \n        # Create input with different dtype\n        if dtype == torch.float32:\n            mixed_dtype = torch.float64\n        else:\n            mixed_dtype = torch.float32\n    \n        mixed_input = torch.randn(*shape, dtype=mixed_dtype, device=device)\n    \n        # Try the mixed precision forward pass\n        # PyTorch may or may not support this automatically\n        try:\n            mixed_output = gn_mixed(mixed_input)\n            # If it works, check basic properties\n            assert mixed_output.shape == shape\n            assert mixed_output.device.type == device\n    \n            # Output dtype should match layer dtype, not input dtype\n            # This is PyTorch's behavior: it casts to layer's dtype\n            assert mixed_output.dtype == dtype, \\\n                f\"Mixed precision output dtype {mixed_output.dtype} != layer dtype {dtype}\"\n    \n            print(f\"Mixed precision test passed: input {mixed_dtype}, layer {dtype}, output {mixed_output.dtype}\")\n    \n        except RuntimeError as e:\n            # If PyTorch doesn't support mixed precision, that's OK\n            # Check if it's the expected \"expected scalar type\" error\n            error_msg = str(e).lower()\n            if 'expected scalar type' in error_msg or 'dtype' in error_msg:\n                print(f\"Mixed precision not supported (expected): {e}\")\n                # This is acceptable - PyTorch requires matching dtypes\n            else:\n                # Some other error - re-raise\n                raise\n    \n        # 8. Test device transfer\n        if device == \"cpu\":\n            # Test moving to CPU (already there)\n            gn_cpu = gn.cpu()\n            assert gn_cpu.weight.device.type == \"cpu\"\n    \n            # Test with CPU input\n            cpu_input = torch.randn(*shape, dtype=dtype, device=\"cpu\")\n            cpu_output = gn_cpu(cpu_input)\n            assert cpu_output.device.type == \"cpu\"\n    \n        # 9. Test dtype conversion\n        if dtype == torch.float32:\n            # Convert to float64\n            gn_fp64 = gn.double()\n            assert gn_fp64.weight.dtype == torch.float64\n    \n            fp64_input = torch.randn(*shape, dtype=torch.float64, device=device)\n            fp64_output = gn_fp64(fp64_input)\n            assert fp64_output.dtype == torch.float64\n    \n        # 10. Test with requires_grad\n        input_tensor.requires_grad_(True)\n>       output = gn(input_tensor)\n\ntests/test_torch_nn_modules_normalization_g1.py:506: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl\n    return forward_call(*input, **kwargs)\n/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/normalization.py:273: in forward\n    return F.group_norm(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ninput = tensor([[[[ 1.9269e+00,  1.4873e+00,  9.0072e-01,  ..., -5.5943e-01,\n           -7.6884e-01,  7.6245e-01],\n          [...[ 5.6279e-01, -2.5226e-01,  1.4472e+00,  ..., -1.4031e-01,\n            9.4162e-01, -1.1843e-02]]]], requires_grad=True)\nnum_groups = 2\nweight = Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64,\n       requires_grad=True)\nbias = Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64,\n       requires_grad=True)\neps = 1e-05\n\n    def group_norm(\n        input: Tensor, num_groups: int, weight: Optional[Tensor] = None, bias: Optional[Tensor] = None, eps: float = 1e-5\n    ) -> Tensor:\n        r\"\"\"Applies Group Normalization for last certain number of dimensions.\n    \n        See :class:`~torch.nn.GroupNorm` for details.\n        \"\"\"\n        if has_torch_function_variadic(input, weight, bias):\n            return handle_torch_function(group_norm, (input, weight, bias,), input, num_groups, weight=weight, bias=bias, eps=eps)\n        _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))\n>       return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)\nE       RuntimeError: expected scalar type Float but found Double\n\n/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/functional.py:2528: RuntimeError\n----------------------------- Captured stdout call -----------------------------\nMixed precision not supported (expected): expected scalar type Double but found Float\n_______________________ test_layernorm_exception_shapes ________________________\n\n    def test_layernorm_exception_shapes():\n        \"\"\"Test LayerNorm with invalid shapes and parameters\"\"\"\n        # Test 1: normalized_shape doesn't match input shape\n        normalized_shape = [8, 8]\n        eps = 1e-5\n        elementwise_affine = True\n    \n        # Create LayerNorm layer\n        ln = LayerNorm(\n            normalized_shape=normalized_shape,\n            eps=eps,\n            elementwise_affine=elementwise_affine\n        )\n    \n        # Test with input that has wrong last dimensions\n        # LayerNorm expects last 2 dimensions to be 8x8, but we give 4x4\n        wrong_shape_input = torch.randn(2, 4, 4, 4)\n    \n        # This should raise a RuntimeError during forward pass\n        # LayerNorm checks that input shape ends with normalized_shape\n        with pytest.raises(RuntimeError) as exc_info:\n            ln(wrong_shape_input)\n    \n        error_msg = str(exc_info.value).lower()\n        assert any(keyword in error_msg for keyword in\n                  ['shape', 'size', 'dimension', 'normalized', 'expected']), \\\n            f\"Error message doesn't mention shape mismatch: {error_msg}\"\n    \n        # Test 2: Input with too few dimensions\n        # If normalized_shape has 2 elements, input needs at least 3 dimensions\n        input_2d = torch.randn(8, 8)  # Only 2 dimensions\n    \n>       with pytest.raises(RuntimeError) as exc_info:\nE       Failed: DID NOT RAISE <class 'RuntimeError'>\n\ntests/test_torch_nn_modules_normalization_g2.py:370: Failed\n_ test_localresponsenorm_boundary_values[3-0.0001-0.75-1.0-dtype1-cpu-shape1-small_size] _\n\nsize = 3, alpha = 0.0001, beta = 0.75, k = 1.0, dtype = torch.float32\ndevice = 'cpu', shape = (2, 8, 8, 8), test_type = 'small_size'\nset_random_seed = 42\n\n    @pytest.mark.parametrize(\"size,alpha,beta,k,dtype,device,shape,test_type\", [\n        # Test boundary values for size\n        (1, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"min_size\"),\n        (3, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"small_size\"),\n        (15, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_size\"),\n        # Test boundary values for alpha - avoid alpha=0 which can cause issues\n        (5, 1e-10, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"tiny_alpha\"),\n        (5, 1.0, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_alpha\"),\n        # Test boundary values for beta - avoid beta=0 which can cause issues\n        (5, 1e-4, 0.5, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_beta\"),\n        (5, 1e-4, 2.0, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_beta\"),\n        # Test boundary values for k - avoid k=0 which can cause division by zero\n        (5, 1e-4, 0.75, 0.1, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_k\"),\n        (5, 1e-4, 0.75, 10.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_k\"),\n        # Test with different data types\n        (5, 1e-4, 0.75, 1.0, torch.float64, \"cpu\", (2, 16, 8, 8), \"float64\"),\n        # Test with small batch size\n        (5, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (1, 16, 8, 8), \"batch_size_1\"),\n    ])\n    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):\n        \"\"\"Test LocalResponseNorm with boundary values for parameters\"\"\"\n        # Skip CUDA tests if device not available\n        if device == \"cuda\" and not torch.cuda.is_available():\n            pytest.skip(\"CUDA not available\")\n    \n        # Validate parameters based on test type\n        if test_type == \"min_size\":\n            assert size == 1, \"Test requires size=1\"\n        elif test_type == \"large_size\":\n            # Size should be odd and <= number of channels\n            assert size % 2 == 1, \"Size should be odd\"\n            assert size <= shape[1], f\"Size {size} should not exceed channels {shape[1]}\"\n    \n        # Create input tensor\n        torch.manual_seed(42)\n        input_tensor = torch.randn(*shape, dtype=dtype, device=device)\n    \n        # Create LocalResponseNorm layer\n        lrn = LocalResponseNorm(\n            size=size,\n            alpha=alpha,\n            beta=beta,\n            k=k\n        ).to(device=device)\n    \n        # Forward pass\n        output = lrn(input_tensor)\n    \n        # Basic assertions for all boundary tests\n        # 1. Shape assertion\n        assert output.shape == input_tensor.shape, \\\n            f\"Output shape {output.shape} != input shape {input_tensor.shape}\"\n    \n        # 2. Dtype assertion\n        assert output.dtype == input_tensor.dtype, \\\n            f\"Output dtype {output.dtype} != input dtype {input_tensor.dtype}\"\n    \n        # 3. Finite values assertion\n        assert torch.all(torch.isfinite(output)), \\\n            f\"Output contains NaN or infinite values for test_type={test_type}\"\n    \n        # 4. Sign preservation (allow small numerical errors)\n        # Check that signs are preserved for most elements\n        sign_match = ((input_tensor >= 0) == (output >= 0))\n        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()\n        assert sign_match_ratio > 0.99, \\\n            f\"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}\"\n    \n        # Test-specific assertions\n        if test_type == \"tiny_alpha\":\n            # With very small alpha, normalization effect is minimal\n            # Output should be close to input / (k^beta)\n            expected = input_tensor / (k**beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.1, f\"Tiny alpha test failed: diff={diff}\"\n    \n        elif test_type == \"large_alpha\":\n            # With large alpha, normalization effect is stronger\n            # Output magnitude should be reduced\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n            reduction_ratio = output_norm / input_norm\n    \n            # With alpha=1.0, reduction should be noticeable\n            assert reduction_ratio < 1.0, \\\n                f\"Large alpha should reduce norm: reduction_ratio={reduction_ratio}\"\n    \n        elif test_type == \"small_beta\":\n            # With small beta (0.5), normalization is less aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Smaller beta should produce larger output (less reduction)\n            assert output_norm > normal_norm * 0.9, \\\n                f\"Small beta should reduce output less: beta={beta} vs 0.75\"\n    \n        elif test_type == \"large_beta\":\n            # With large beta (2.0), normalization is more aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Larger beta should produce smaller output\n            assert output_norm < normal_norm * 1.1, \\\n                f\"Large beta should reduce output more: beta={beta} vs 0.75\"\n    \n        elif test_type == \"min_size\":\n            # With size=1, only self-channel is considered\n            # Output should be approximately input / (k^beta + alpha * input^2)^beta\n            # For size=1, the sum over neighbors is just self\n            squared = input_tensor ** 2\n            denominator = k + alpha * squared\n            expected = input_tensor / (denominator ** beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.2, f\"Size=1 test failed: diff={diff}\"\n    \n        elif test_type == \"small_k\":\n            # With small k, normalization denominator is small\n            # Output magnitude could be large\n            assert torch.all(torch.isfinite(output)), \"Small k test produced non-finite values\"\n    \n        # Test with all positive input\n        positive_input = torch.abs(input_tensor) + 0.1\n        positive_output = lrn(positive_input)\n        # Check that output is mostly positive (allow small numerical errors)\n        positive_ratio = torch.sum(positive_output >= -1e-7).item() / positive_output.numel()\n        assert positive_ratio > 0.99, \\\n            f\"Positive input should produce non-negative output for test_type={test_type}: ratio={positive_ratio}\"\n    \n        # Test with all negative input\n        negative_input = -torch.abs(input_tensor) - 0.1\n        negative_output = lrn(negative_input)\n        # Check that output is mostly negative (allow small numerical errors)\n        negative_ratio = torch.sum(negative_output <= 1e-7).item() / negative_output.numel()\n        assert negative_ratio > 0.99, \\\n            f\"Negative input should produce non-positive output for test_type={test_type}: ratio={negative_ratio}\"\n    \n        # Test with constant input\n        constant_value = 5.0\n        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)\n        constant_output = lrn(constant_input)\n    \n        # For constant input, output should be scaled version of input\n        # All values should be approximately equal\n        output_std = torch.std(constant_output)\n>       assert output_std < 1e-5, \\\n            f\"Constant input should produce constant output for test_type={test_type}: std={output_std}\"\nE       AssertionError: Constant input should produce constant output for test_type=small_size: std=0.0013487441465258598\nE       assert tensor(0.0013) < 1e-05\n\ntests/test_torch_nn_modules_normalization_g3.py:440: AssertionError\n_ test_localresponsenorm_boundary_values[15-0.0001-0.75-1.0-dtype2-cpu-shape2-large_size] _\n\nsize = 15, alpha = 0.0001, beta = 0.75, k = 1.0, dtype = torch.float32\ndevice = 'cpu', shape = (2, 16, 8, 8), test_type = 'large_size'\nset_random_seed = 42\n\n    @pytest.mark.parametrize(\"size,alpha,beta,k,dtype,device,shape,test_type\", [\n        # Test boundary values for size\n        (1, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"min_size\"),\n        (3, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"small_size\"),\n        (15, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_size\"),\n        # Test boundary values for alpha - avoid alpha=0 which can cause issues\n        (5, 1e-10, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"tiny_alpha\"),\n        (5, 1.0, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_alpha\"),\n        # Test boundary values for beta - avoid beta=0 which can cause issues\n        (5, 1e-4, 0.5, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_beta\"),\n        (5, 1e-4, 2.0, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_beta\"),\n        # Test boundary values for k - avoid k=0 which can cause division by zero\n        (5, 1e-4, 0.75, 0.1, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_k\"),\n        (5, 1e-4, 0.75, 10.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_k\"),\n        # Test with different data types\n        (5, 1e-4, 0.75, 1.0, torch.float64, \"cpu\", (2, 16, 8, 8), \"float64\"),\n        # Test with small batch size\n        (5, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (1, 16, 8, 8), \"batch_size_1\"),\n    ])\n    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):\n        \"\"\"Test LocalResponseNorm with boundary values for parameters\"\"\"\n        # Skip CUDA tests if device not available\n        if device == \"cuda\" and not torch.cuda.is_available():\n            pytest.skip(\"CUDA not available\")\n    \n        # Validate parameters based on test type\n        if test_type == \"min_size\":\n            assert size == 1, \"Test requires size=1\"\n        elif test_type == \"large_size\":\n            # Size should be odd and <= number of channels\n            assert size % 2 == 1, \"Size should be odd\"\n            assert size <= shape[1], f\"Size {size} should not exceed channels {shape[1]}\"\n    \n        # Create input tensor\n        torch.manual_seed(42)\n        input_tensor = torch.randn(*shape, dtype=dtype, device=device)\n    \n        # Create LocalResponseNorm layer\n        lrn = LocalResponseNorm(\n            size=size,\n            alpha=alpha,\n            beta=beta,\n            k=k\n        ).to(device=device)\n    \n        # Forward pass\n        output = lrn(input_tensor)\n    \n        # Basic assertions for all boundary tests\n        # 1. Shape assertion\n        assert output.shape == input_tensor.shape, \\\n            f\"Output shape {output.shape} != input shape {input_tensor.shape}\"\n    \n        # 2. Dtype assertion\n        assert output.dtype == input_tensor.dtype, \\\n            f\"Output dtype {output.dtype} != input dtype {input_tensor.dtype}\"\n    \n        # 3. Finite values assertion\n        assert torch.all(torch.isfinite(output)), \\\n            f\"Output contains NaN or infinite values for test_type={test_type}\"\n    \n        # 4. Sign preservation (allow small numerical errors)\n        # Check that signs are preserved for most elements\n        sign_match = ((input_tensor >= 0) == (output >= 0))\n        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()\n        assert sign_match_ratio > 0.99, \\\n            f\"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}\"\n    \n        # Test-specific assertions\n        if test_type == \"tiny_alpha\":\n            # With very small alpha, normalization effect is minimal\n            # Output should be close to input / (k^beta)\n            expected = input_tensor / (k**beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.1, f\"Tiny alpha test failed: diff={diff}\"\n    \n        elif test_type == \"large_alpha\":\n            # With large alpha, normalization effect is stronger\n            # Output magnitude should be reduced\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n            reduction_ratio = output_norm / input_norm\n    \n            # With alpha=1.0, reduction should be noticeable\n            assert reduction_ratio < 1.0, \\\n                f\"Large alpha should reduce norm: reduction_ratio={reduction_ratio}\"\n    \n        elif test_type == \"small_beta\":\n            # With small beta (0.5), normalization is less aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Smaller beta should produce larger output (less reduction)\n            assert output_norm > normal_norm * 0.9, \\\n                f\"Small beta should reduce output less: beta={beta} vs 0.75\"\n    \n        elif test_type == \"large_beta\":\n            # With large beta (2.0), normalization is more aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Larger beta should produce smaller output\n            assert output_norm < normal_norm * 1.1, \\\n                f\"Large beta should reduce output more: beta={beta} vs 0.75\"\n    \n        elif test_type == \"min_size\":\n            # With size=1, only self-channel is considered\n            # Output should be approximately input / (k^beta + alpha * input^2)^beta\n            # For size=1, the sum over neighbors is just self\n            squared = input_tensor ** 2\n            denominator = k + alpha * squared\n            expected = input_tensor / (denominator ** beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.2, f\"Size=1 test failed: diff={diff}\"\n    \n        elif test_type == \"small_k\":\n            # With small k, normalization denominator is small\n            # Output magnitude could be large\n            assert torch.all(torch.isfinite(output)), \"Small k test produced non-finite values\"\n    \n        # Test with all positive input\n        positive_input = torch.abs(input_tensor) + 0.1\n        positive_output = lrn(positive_input)\n        # Check that output is mostly positive (allow small numerical errors)\n        positive_ratio = torch.sum(positive_output >= -1e-7).item() / positive_output.numel()\n        assert positive_ratio > 0.99, \\\n            f\"Positive input should produce non-negative output for test_type={test_type}: ratio={positive_ratio}\"\n    \n        # Test with all negative input\n        negative_input = -torch.abs(input_tensor) - 0.1\n        negative_output = lrn(negative_input)\n        # Check that output is mostly negative (allow small numerical errors)\n        negative_ratio = torch.sum(negative_output <= 1e-7).item() / negative_output.numel()\n        assert negative_ratio > 0.99, \\\n            f\"Negative input should produce non-positive output for test_type={test_type}: ratio={negative_ratio}\"\n    \n        # Test with constant input\n        constant_value = 5.0\n        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)\n        constant_output = lrn(constant_input)\n    \n        # For constant input, output should be scaled version of input\n        # All values should be approximately equal\n        output_std = torch.std(constant_output)\n>       assert output_std < 1e-5, \\\n            f\"Constant input should produce constant output for test_type={test_type}: std={output_std}\"\nE       AssertionError: Constant input should produce constant output for test_type=large_size: std=0.0014275847934186459\nE       assert tensor(0.0014) < 1e-05\n\ntests/test_torch_nn_modules_normalization_g3.py:440: AssertionError\n_ test_localresponsenorm_boundary_values[5-1.0-0.75-1.0-dtype4-cpu-shape4-large_alpha] _\n\nsize = 5, alpha = 1.0, beta = 0.75, k = 1.0, dtype = torch.float32\ndevice = 'cpu', shape = (2, 16, 8, 8), test_type = 'large_alpha'\nset_random_seed = 42\n\n    @pytest.mark.parametrize(\"size,alpha,beta,k,dtype,device,shape,test_type\", [\n        # Test boundary values for size\n        (1, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"min_size\"),\n        (3, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"small_size\"),\n        (15, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_size\"),\n        # Test boundary values for alpha - avoid alpha=0 which can cause issues\n        (5, 1e-10, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"tiny_alpha\"),\n        (5, 1.0, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_alpha\"),\n        # Test boundary values for beta - avoid beta=0 which can cause issues\n        (5, 1e-4, 0.5, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_beta\"),\n        (5, 1e-4, 2.0, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_beta\"),\n        # Test boundary values for k - avoid k=0 which can cause division by zero\n        (5, 1e-4, 0.75, 0.1, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_k\"),\n        (5, 1e-4, 0.75, 10.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_k\"),\n        # Test with different data types\n        (5, 1e-4, 0.75, 1.0, torch.float64, \"cpu\", (2, 16, 8, 8), \"float64\"),\n        # Test with small batch size\n        (5, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (1, 16, 8, 8), \"batch_size_1\"),\n    ])\n    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):\n        \"\"\"Test LocalResponseNorm with boundary values for parameters\"\"\"\n        # Skip CUDA tests if device not available\n        if device == \"cuda\" and not torch.cuda.is_available():\n            pytest.skip(\"CUDA not available\")\n    \n        # Validate parameters based on test type\n        if test_type == \"min_size\":\n            assert size == 1, \"Test requires size=1\"\n        elif test_type == \"large_size\":\n            # Size should be odd and <= number of channels\n            assert size % 2 == 1, \"Size should be odd\"\n            assert size <= shape[1], f\"Size {size} should not exceed channels {shape[1]}\"\n    \n        # Create input tensor\n        torch.manual_seed(42)\n        input_tensor = torch.randn(*shape, dtype=dtype, device=device)\n    \n        # Create LocalResponseNorm layer\n        lrn = LocalResponseNorm(\n            size=size,\n            alpha=alpha,\n            beta=beta,\n            k=k\n        ).to(device=device)\n    \n        # Forward pass\n        output = lrn(input_tensor)\n    \n        # Basic assertions for all boundary tests\n        # 1. Shape assertion\n        assert output.shape == input_tensor.shape, \\\n            f\"Output shape {output.shape} != input shape {input_tensor.shape}\"\n    \n        # 2. Dtype assertion\n        assert output.dtype == input_tensor.dtype, \\\n            f\"Output dtype {output.dtype} != input dtype {input_tensor.dtype}\"\n    \n        # 3. Finite values assertion\n        assert torch.all(torch.isfinite(output)), \\\n            f\"Output contains NaN or infinite values for test_type={test_type}\"\n    \n        # 4. Sign preservation (allow small numerical errors)\n        # Check that signs are preserved for most elements\n        sign_match = ((input_tensor >= 0) == (output >= 0))\n        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()\n        assert sign_match_ratio > 0.99, \\\n            f\"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}\"\n    \n        # Test-specific assertions\n        if test_type == \"tiny_alpha\":\n            # With very small alpha, normalization effect is minimal\n            # Output should be close to input / (k^beta)\n            expected = input_tensor / (k**beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.1, f\"Tiny alpha test failed: diff={diff}\"\n    \n        elif test_type == \"large_alpha\":\n            # With large alpha, normalization effect is stronger\n            # Output magnitude should be reduced\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n            reduction_ratio = output_norm / input_norm\n    \n            # With alpha=1.0, reduction should be noticeable\n            assert reduction_ratio < 1.0, \\\n                f\"Large alpha should reduce norm: reduction_ratio={reduction_ratio}\"\n    \n        elif test_type == \"small_beta\":\n            # With small beta (0.5), normalization is less aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Smaller beta should produce larger output (less reduction)\n            assert output_norm > normal_norm * 0.9, \\\n                f\"Small beta should reduce output less: beta={beta} vs 0.75\"\n    \n        elif test_type == \"large_beta\":\n            # With large beta (2.0), normalization is more aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Larger beta should produce smaller output\n            assert output_norm < normal_norm * 1.1, \\\n                f\"Large beta should reduce output more: beta={beta} vs 0.75\"\n    \n        elif test_type == \"min_size\":\n            # With size=1, only self-channel is considered\n            # Output should be approximately input / (k^beta + alpha * input^2)^beta\n            # For size=1, the sum over neighbors is just self\n            squared = input_tensor ** 2\n            denominator = k + alpha * squared\n            expected = input_tensor / (denominator ** beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.2, f\"Size=1 test failed: diff={diff}\"\n    \n        elif test_type == \"small_k\":\n            # With small k, normalization denominator is small\n            # Output magnitude could be large\n            assert torch.all(torch.isfinite(output)), \"Small k test produced non-finite values\"\n    \n        # Test with all positive input\n        positive_input = torch.abs(input_tensor) + 0.1\n        positive_output = lrn(positive_input)\n        # Check that output is mostly positive (allow small numerical errors)\n        positive_ratio = torch.sum(positive_output >= -1e-7).item() / positive_output.numel()\n        assert positive_ratio > 0.99, \\\n            f\"Positive input should produce non-negative output for test_type={test_type}: ratio={positive_ratio}\"\n    \n        # Test with all negative input\n        negative_input = -torch.abs(input_tensor) - 0.1\n        negative_output = lrn(negative_input)\n        # Check that output is mostly negative (allow small numerical errors)\n        negative_ratio = torch.sum(negative_output <= 1e-7).item() / negative_output.numel()\n        assert negative_ratio > 0.99, \\\n            f\"Negative input should produce non-positive output for test_type={test_type}: ratio={negative_ratio}\"\n    \n        # Test with constant input\n        constant_value = 5.0\n        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)\n        constant_output = lrn(constant_input)\n    \n        # For constant input, output should be scaled version of input\n        # All values should be approximately equal\n        output_std = torch.std(constant_output)\n>       assert output_std < 1e-5, \\\n            f\"Constant input should produce constant output for test_type={test_type}: std={output_std}\"\nE       AssertionError: Constant input should produce constant output for test_type=large_alpha: std=0.06445513665676117\nE       assert tensor(0.0645) < 1e-05\n\ntests/test_torch_nn_modules_normalization_g3.py:440: AssertionError\n_ test_localresponsenorm_boundary_values[5-0.0001-0.5-1.0-dtype5-cpu-shape5-small_beta] _\n\nsize = 5, alpha = 0.0001, beta = 0.5, k = 1.0, dtype = torch.float32\ndevice = 'cpu', shape = (2, 16, 8, 8), test_type = 'small_beta'\nset_random_seed = 42\n\n    @pytest.mark.parametrize(\"size,alpha,beta,k,dtype,device,shape,test_type\", [\n        # Test boundary values for size\n        (1, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"min_size\"),\n        (3, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"small_size\"),\n        (15, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_size\"),\n        # Test boundary values for alpha - avoid alpha=0 which can cause issues\n        (5, 1e-10, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"tiny_alpha\"),\n        (5, 1.0, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_alpha\"),\n        # Test boundary values for beta - avoid beta=0 which can cause issues\n        (5, 1e-4, 0.5, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_beta\"),\n        (5, 1e-4, 2.0, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_beta\"),\n        # Test boundary values for k - avoid k=0 which can cause division by zero\n        (5, 1e-4, 0.75, 0.1, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_k\"),\n        (5, 1e-4, 0.75, 10.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_k\"),\n        # Test with different data types\n        (5, 1e-4, 0.75, 1.0, torch.float64, \"cpu\", (2, 16, 8, 8), \"float64\"),\n        # Test with small batch size\n        (5, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (1, 16, 8, 8), \"batch_size_1\"),\n    ])\n    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):\n        \"\"\"Test LocalResponseNorm with boundary values for parameters\"\"\"\n        # Skip CUDA tests if device not available\n        if device == \"cuda\" and not torch.cuda.is_available():\n            pytest.skip(\"CUDA not available\")\n    \n        # Validate parameters based on test type\n        if test_type == \"min_size\":\n            assert size == 1, \"Test requires size=1\"\n        elif test_type == \"large_size\":\n            # Size should be odd and <= number of channels\n            assert size % 2 == 1, \"Size should be odd\"\n            assert size <= shape[1], f\"Size {size} should not exceed channels {shape[1]}\"\n    \n        # Create input tensor\n        torch.manual_seed(42)\n        input_tensor = torch.randn(*shape, dtype=dtype, device=device)\n    \n        # Create LocalResponseNorm layer\n        lrn = LocalResponseNorm(\n            size=size,\n            alpha=alpha,\n            beta=beta,\n            k=k\n        ).to(device=device)\n    \n        # Forward pass\n        output = lrn(input_tensor)\n    \n        # Basic assertions for all boundary tests\n        # 1. Shape assertion\n        assert output.shape == input_tensor.shape, \\\n            f\"Output shape {output.shape} != input shape {input_tensor.shape}\"\n    \n        # 2. Dtype assertion\n        assert output.dtype == input_tensor.dtype, \\\n            f\"Output dtype {output.dtype} != input dtype {input_tensor.dtype}\"\n    \n        # 3. Finite values assertion\n        assert torch.all(torch.isfinite(output)), \\\n            f\"Output contains NaN or infinite values for test_type={test_type}\"\n    \n        # 4. Sign preservation (allow small numerical errors)\n        # Check that signs are preserved for most elements\n        sign_match = ((input_tensor >= 0) == (output >= 0))\n        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()\n        assert sign_match_ratio > 0.99, \\\n            f\"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}\"\n    \n        # Test-specific assertions\n        if test_type == \"tiny_alpha\":\n            # With very small alpha, normalization effect is minimal\n            # Output should be close to input / (k^beta)\n            expected = input_tensor / (k**beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.1, f\"Tiny alpha test failed: diff={diff}\"\n    \n        elif test_type == \"large_alpha\":\n            # With large alpha, normalization effect is stronger\n            # Output magnitude should be reduced\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n            reduction_ratio = output_norm / input_norm\n    \n            # With alpha=1.0, reduction should be noticeable\n            assert reduction_ratio < 1.0, \\\n                f\"Large alpha should reduce norm: reduction_ratio={reduction_ratio}\"\n    \n        elif test_type == \"small_beta\":\n            # With small beta (0.5), normalization is less aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Smaller beta should produce larger output (less reduction)\n            assert output_norm > normal_norm * 0.9, \\\n                f\"Small beta should reduce output less: beta={beta} vs 0.75\"\n    \n        elif test_type == \"large_beta\":\n            # With large beta (2.0), normalization is more aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Larger beta should produce smaller output\n            assert output_norm < normal_norm * 1.1, \\\n                f\"Large beta should reduce output more: beta={beta} vs 0.75\"\n    \n        elif test_type == \"min_size\":\n            # With size=1, only self-channel is considered\n            # Output should be approximately input / (k^beta + alpha * input^2)^beta\n            # For size=1, the sum over neighbors is just self\n            squared = input_tensor ** 2\n            denominator = k + alpha * squared\n            expected = input_tensor / (denominator ** beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.2, f\"Size=1 test failed: diff={diff}\"\n    \n        elif test_type == \"small_k\":\n            # With small k, normalization denominator is small\n            # Output magnitude could be large\n            assert torch.all(torch.isfinite(output)), \"Small k test produced non-finite values\"\n    \n        # Test with all positive input\n        positive_input = torch.abs(input_tensor) + 0.1\n        positive_output = lrn(positive_input)\n        # Check that output is mostly positive (allow small numerical errors)\n        positive_ratio = torch.sum(positive_output >= -1e-7).item() / positive_output.numel()\n        assert positive_ratio > 0.99, \\\n            f\"Positive input should produce non-negative output for test_type={test_type}: ratio={positive_ratio}\"\n    \n        # Test with all negative input\n        negative_input = -torch.abs(input_tensor) - 0.1\n        negative_output = lrn(negative_input)\n        # Check that output is mostly negative (allow small numerical errors)\n        negative_ratio = torch.sum(negative_output <= 1e-7).item() / negative_output.numel()\n        assert negative_ratio > 0.99, \\\n            f\"Negative input should produce non-positive output for test_type={test_type}: ratio={negative_ratio}\"\n    \n        # Test with constant input\n        constant_value = 5.0\n        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)\n        constant_output = lrn(constant_input)\n    \n        # For constant input, output should be scaled version of input\n        # All values should be approximately equal\n        output_std = torch.std(constant_output)\n>       assert output_std < 1e-5, \\\n            f\"Constant input should produce constant output for test_type={test_type}: std={output_std}\"\nE       AssertionError: Constant input should produce constant output for test_type=small_beta: std=0.0008674598066136241\nE       assert tensor(0.0009) < 1e-05\n\ntests/test_torch_nn_modules_normalization_g3.py:440: AssertionError\n_ test_localresponsenorm_boundary_values[5-0.0001-2.0-1.0-dtype6-cpu-shape6-large_beta] _\n\nsize = 5, alpha = 0.0001, beta = 2.0, k = 1.0, dtype = torch.float32\ndevice = 'cpu', shape = (2, 16, 8, 8), test_type = 'large_beta'\nset_random_seed = 42\n\n    @pytest.mark.parametrize(\"size,alpha,beta,k,dtype,device,shape,test_type\", [\n        # Test boundary values for size\n        (1, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"min_size\"),\n        (3, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"small_size\"),\n        (15, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_size\"),\n        # Test boundary values for alpha - avoid alpha=0 which can cause issues\n        (5, 1e-10, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"tiny_alpha\"),\n        (5, 1.0, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_alpha\"),\n        # Test boundary values for beta - avoid beta=0 which can cause issues\n        (5, 1e-4, 0.5, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_beta\"),\n        (5, 1e-4, 2.0, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_beta\"),\n        # Test boundary values for k - avoid k=0 which can cause division by zero\n        (5, 1e-4, 0.75, 0.1, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_k\"),\n        (5, 1e-4, 0.75, 10.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_k\"),\n        # Test with different data types\n        (5, 1e-4, 0.75, 1.0, torch.float64, \"cpu\", (2, 16, 8, 8), \"float64\"),\n        # Test with small batch size\n        (5, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (1, 16, 8, 8), \"batch_size_1\"),\n    ])\n    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):\n        \"\"\"Test LocalResponseNorm with boundary values for parameters\"\"\"\n        # Skip CUDA tests if device not available\n        if device == \"cuda\" and not torch.cuda.is_available():\n            pytest.skip(\"CUDA not available\")\n    \n        # Validate parameters based on test type\n        if test_type == \"min_size\":\n            assert size == 1, \"Test requires size=1\"\n        elif test_type == \"large_size\":\n            # Size should be odd and <= number of channels\n            assert size % 2 == 1, \"Size should be odd\"\n            assert size <= shape[1], f\"Size {size} should not exceed channels {shape[1]}\"\n    \n        # Create input tensor\n        torch.manual_seed(42)\n        input_tensor = torch.randn(*shape, dtype=dtype, device=device)\n    \n        # Create LocalResponseNorm layer\n        lrn = LocalResponseNorm(\n            size=size,\n            alpha=alpha,\n            beta=beta,\n            k=k\n        ).to(device=device)\n    \n        # Forward pass\n        output = lrn(input_tensor)\n    \n        # Basic assertions for all boundary tests\n        # 1. Shape assertion\n        assert output.shape == input_tensor.shape, \\\n            f\"Output shape {output.shape} != input shape {input_tensor.shape}\"\n    \n        # 2. Dtype assertion\n        assert output.dtype == input_tensor.dtype, \\\n            f\"Output dtype {output.dtype} != input dtype {input_tensor.dtype}\"\n    \n        # 3. Finite values assertion\n        assert torch.all(torch.isfinite(output)), \\\n            f\"Output contains NaN or infinite values for test_type={test_type}\"\n    \n        # 4. Sign preservation (allow small numerical errors)\n        # Check that signs are preserved for most elements\n        sign_match = ((input_tensor >= 0) == (output >= 0))\n        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()\n        assert sign_match_ratio > 0.99, \\\n            f\"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}\"\n    \n        # Test-specific assertions\n        if test_type == \"tiny_alpha\":\n            # With very small alpha, normalization effect is minimal\n            # Output should be close to input / (k^beta)\n            expected = input_tensor / (k**beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.1, f\"Tiny alpha test failed: diff={diff}\"\n    \n        elif test_type == \"large_alpha\":\n            # With large alpha, normalization effect is stronger\n            # Output magnitude should be reduced\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n            reduction_ratio = output_norm / input_norm\n    \n            # With alpha=1.0, reduction should be noticeable\n            assert reduction_ratio < 1.0, \\\n                f\"Large alpha should reduce norm: reduction_ratio={reduction_ratio}\"\n    \n        elif test_type == \"small_beta\":\n            # With small beta (0.5), normalization is less aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Smaller beta should produce larger output (less reduction)\n            assert output_norm > normal_norm * 0.9, \\\n                f\"Small beta should reduce output less: beta={beta} vs 0.75\"\n    \n        elif test_type == \"large_beta\":\n            # With large beta (2.0), normalization is more aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Larger beta should produce smaller output\n            assert output_norm < normal_norm * 1.1, \\\n                f\"Large beta should reduce output more: beta={beta} vs 0.75\"\n    \n        elif test_type == \"min_size\":\n            # With size=1, only self-channel is considered\n            # Output should be approximately input / (k^beta + alpha * input^2)^beta\n            # For size=1, the sum over neighbors is just self\n            squared = input_tensor ** 2\n            denominator = k + alpha * squared\n            expected = input_tensor / (denominator ** beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.2, f\"Size=1 test failed: diff={diff}\"\n    \n        elif test_type == \"small_k\":\n            # With small k, normalization denominator is small\n            # Output magnitude could be large\n            assert torch.all(torch.isfinite(output)), \"Small k test produced non-finite values\"\n    \n        # Test with all positive input\n        positive_input = torch.abs(input_tensor) + 0.1\n        positive_output = lrn(positive_input)\n        # Check that output is mostly positive (allow small numerical errors)\n        positive_ratio = torch.sum(positive_output >= -1e-7).item() / positive_output.numel()\n        assert positive_ratio > 0.99, \\\n            f\"Positive input should produce non-negative output for test_type={test_type}: ratio={positive_ratio}\"\n    \n        # Test with all negative input\n        negative_input = -torch.abs(input_tensor) - 0.1\n        negative_output = lrn(negative_input)\n        # Check that output is mostly negative (allow small numerical errors)\n        negative_ratio = torch.sum(negative_output <= 1e-7).item() / negative_output.numel()\n        assert negative_ratio > 0.99, \\\n            f\"Negative input should produce non-positive output for test_type={test_type}: ratio={negative_ratio}\"\n    \n        # Test with constant input\n        constant_value = 5.0\n        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)\n        constant_output = lrn(constant_input)\n    \n        # For constant input, output should be scaled version of input\n        # All values should be approximately equal\n        output_std = torch.std(constant_output)\n>       assert output_std < 1e-5, \\\n            f\"Constant input should produce constant output for test_type={test_type}: std={output_std}\"\nE       AssertionError: Constant input should produce constant output for test_type=large_beta: std=0.0034596342593431473\nE       assert tensor(0.0035) < 1e-05\n\ntests/test_torch_nn_modules_normalization_g3.py:440: AssertionError\n_ test_localresponsenorm_boundary_values[5-0.0001-0.75-0.1-dtype7-cpu-shape7-small_k] _\n\nsize = 5, alpha = 0.0001, beta = 0.75, k = 0.1, dtype = torch.float32\ndevice = 'cpu', shape = (2, 16, 8, 8), test_type = 'small_k'\nset_random_seed = 42\n\n    @pytest.mark.parametrize(\"size,alpha,beta,k,dtype,device,shape,test_type\", [\n        # Test boundary values for size\n        (1, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"min_size\"),\n        (3, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"small_size\"),\n        (15, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_size\"),\n        # Test boundary values for alpha - avoid alpha=0 which can cause issues\n        (5, 1e-10, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"tiny_alpha\"),\n        (5, 1.0, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_alpha\"),\n        # Test boundary values for beta - avoid beta=0 which can cause issues\n        (5, 1e-4, 0.5, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_beta\"),\n        (5, 1e-4, 2.0, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_beta\"),\n        # Test boundary values for k - avoid k=0 which can cause division by zero\n        (5, 1e-4, 0.75, 0.1, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_k\"),\n        (5, 1e-4, 0.75, 10.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_k\"),\n        # Test with different data types\n        (5, 1e-4, 0.75, 1.0, torch.float64, \"cpu\", (2, 16, 8, 8), \"float64\"),\n        # Test with small batch size\n        (5, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (1, 16, 8, 8), \"batch_size_1\"),\n    ])\n    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):\n        \"\"\"Test LocalResponseNorm with boundary values for parameters\"\"\"\n        # Skip CUDA tests if device not available\n        if device == \"cuda\" and not torch.cuda.is_available():\n            pytest.skip(\"CUDA not available\")\n    \n        # Validate parameters based on test type\n        if test_type == \"min_size\":\n            assert size == 1, \"Test requires size=1\"\n        elif test_type == \"large_size\":\n            # Size should be odd and <= number of channels\n            assert size % 2 == 1, \"Size should be odd\"\n            assert size <= shape[1], f\"Size {size} should not exceed channels {shape[1]}\"\n    \n        # Create input tensor\n        torch.manual_seed(42)\n        input_tensor = torch.randn(*shape, dtype=dtype, device=device)\n    \n        # Create LocalResponseNorm layer\n        lrn = LocalResponseNorm(\n            size=size,\n            alpha=alpha,\n            beta=beta,\n            k=k\n        ).to(device=device)\n    \n        # Forward pass\n        output = lrn(input_tensor)\n    \n        # Basic assertions for all boundary tests\n        # 1. Shape assertion\n        assert output.shape == input_tensor.shape, \\\n            f\"Output shape {output.shape} != input shape {input_tensor.shape}\"\n    \n        # 2. Dtype assertion\n        assert output.dtype == input_tensor.dtype, \\\n            f\"Output dtype {output.dtype} != input dtype {input_tensor.dtype}\"\n    \n        # 3. Finite values assertion\n        assert torch.all(torch.isfinite(output)), \\\n            f\"Output contains NaN or infinite values for test_type={test_type}\"\n    \n        # 4. Sign preservation (allow small numerical errors)\n        # Check that signs are preserved for most elements\n        sign_match = ((input_tensor >= 0) == (output >= 0))\n        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()\n        assert sign_match_ratio > 0.99, \\\n            f\"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}\"\n    \n        # Test-specific assertions\n        if test_type == \"tiny_alpha\":\n            # With very small alpha, normalization effect is minimal\n            # Output should be close to input / (k^beta)\n            expected = input_tensor / (k**beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.1, f\"Tiny alpha test failed: diff={diff}\"\n    \n        elif test_type == \"large_alpha\":\n            # With large alpha, normalization effect is stronger\n            # Output magnitude should be reduced\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n            reduction_ratio = output_norm / input_norm\n    \n            # With alpha=1.0, reduction should be noticeable\n            assert reduction_ratio < 1.0, \\\n                f\"Large alpha should reduce norm: reduction_ratio={reduction_ratio}\"\n    \n        elif test_type == \"small_beta\":\n            # With small beta (0.5), normalization is less aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Smaller beta should produce larger output (less reduction)\n            assert output_norm > normal_norm * 0.9, \\\n                f\"Small beta should reduce output less: beta={beta} vs 0.75\"\n    \n        elif test_type == \"large_beta\":\n            # With large beta (2.0), normalization is more aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Larger beta should produce smaller output\n            assert output_norm < normal_norm * 1.1, \\\n                f\"Large beta should reduce output more: beta={beta} vs 0.75\"\n    \n        elif test_type == \"min_size\":\n            # With size=1, only self-channel is considered\n            # Output should be approximately input / (k^beta + alpha * input^2)^beta\n            # For size=1, the sum over neighbors is just self\n            squared = input_tensor ** 2\n            denominator = k + alpha * squared\n            expected = input_tensor / (denominator ** beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.2, f\"Size=1 test failed: diff={diff}\"\n    \n        elif test_type == \"small_k\":\n            # With small k, normalization denominator is small\n            # Output magnitude could be large\n            assert torch.all(torch.isfinite(output)), \"Small k test produced non-finite values\"\n    \n        # Test with all positive input\n        positive_input = torch.abs(input_tensor) + 0.1\n        positive_output = lrn(positive_input)\n        # Check that output is mostly positive (allow small numerical errors)\n        positive_ratio = torch.sum(positive_output >= -1e-7).item() / positive_output.numel()\n        assert positive_ratio > 0.99, \\\n            f\"Positive input should produce non-negative output for test_type={test_type}: ratio={positive_ratio}\"\n    \n        # Test with all negative input\n        negative_input = -torch.abs(input_tensor) - 0.1\n        negative_output = lrn(negative_input)\n        # Check that output is mostly negative (allow small numerical errors)\n        negative_ratio = torch.sum(negative_output <= 1e-7).item() / negative_output.numel()\n        assert negative_ratio > 0.99, \\\n            f\"Negative input should produce non-positive output for test_type={test_type}: ratio={negative_ratio}\"\n    \n        # Test with constant input\n        constant_value = 5.0\n        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)\n        constant_output = lrn(constant_input)\n    \n        # For constant input, output should be scaled version of input\n        # All values should be approximately equal\n        output_std = torch.std(constant_output)\n>       assert output_std < 1e-5, \\\n            f\"Constant input should produce constant output for test_type={test_type}: std={output_std}\"\nE       AssertionError: Constant input should produce constant output for test_type=small_k: std=0.07085202634334564\nE       assert tensor(0.0709) < 1e-05\n\ntests/test_torch_nn_modules_normalization_g3.py:440: AssertionError\n_ test_localresponsenorm_boundary_values[5-0.0001-0.75-10.0-dtype8-cpu-shape8-large_k] _\n\nsize = 5, alpha = 0.0001, beta = 0.75, k = 10.0, dtype = torch.float32\ndevice = 'cpu', shape = (2, 16, 8, 8), test_type = 'large_k'\nset_random_seed = 42\n\n    @pytest.mark.parametrize(\"size,alpha,beta,k,dtype,device,shape,test_type\", [\n        # Test boundary values for size\n        (1, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"min_size\"),\n        (3, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"small_size\"),\n        (15, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_size\"),\n        # Test boundary values for alpha - avoid alpha=0 which can cause issues\n        (5, 1e-10, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"tiny_alpha\"),\n        (5, 1.0, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_alpha\"),\n        # Test boundary values for beta - avoid beta=0 which can cause issues\n        (5, 1e-4, 0.5, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_beta\"),\n        (5, 1e-4, 2.0, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_beta\"),\n        # Test boundary values for k - avoid k=0 which can cause division by zero\n        (5, 1e-4, 0.75, 0.1, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_k\"),\n        (5, 1e-4, 0.75, 10.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_k\"),\n        # Test with different data types\n        (5, 1e-4, 0.75, 1.0, torch.float64, \"cpu\", (2, 16, 8, 8), \"float64\"),\n        # Test with small batch size\n        (5, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (1, 16, 8, 8), \"batch_size_1\"),\n    ])\n    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):\n        \"\"\"Test LocalResponseNorm with boundary values for parameters\"\"\"\n        # Skip CUDA tests if device not available\n        if device == \"cuda\" and not torch.cuda.is_available():\n            pytest.skip(\"CUDA not available\")\n    \n        # Validate parameters based on test type\n        if test_type == \"min_size\":\n            assert size == 1, \"Test requires size=1\"\n        elif test_type == \"large_size\":\n            # Size should be odd and <= number of channels\n            assert size % 2 == 1, \"Size should be odd\"\n            assert size <= shape[1], f\"Size {size} should not exceed channels {shape[1]}\"\n    \n        # Create input tensor\n        torch.manual_seed(42)\n        input_tensor = torch.randn(*shape, dtype=dtype, device=device)\n    \n        # Create LocalResponseNorm layer\n        lrn = LocalResponseNorm(\n            size=size,\n            alpha=alpha,\n            beta=beta,\n            k=k\n        ).to(device=device)\n    \n        # Forward pass\n        output = lrn(input_tensor)\n    \n        # Basic assertions for all boundary tests\n        # 1. Shape assertion\n        assert output.shape == input_tensor.shape, \\\n            f\"Output shape {output.shape} != input shape {input_tensor.shape}\"\n    \n        # 2. Dtype assertion\n        assert output.dtype == input_tensor.dtype, \\\n            f\"Output dtype {output.dtype} != input dtype {input_tensor.dtype}\"\n    \n        # 3. Finite values assertion\n        assert torch.all(torch.isfinite(output)), \\\n            f\"Output contains NaN or infinite values for test_type={test_type}\"\n    \n        # 4. Sign preservation (allow small numerical errors)\n        # Check that signs are preserved for most elements\n        sign_match = ((input_tensor >= 0) == (output >= 0))\n        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()\n        assert sign_match_ratio > 0.99, \\\n            f\"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}\"\n    \n        # Test-specific assertions\n        if test_type == \"tiny_alpha\":\n            # With very small alpha, normalization effect is minimal\n            # Output should be close to input / (k^beta)\n            expected = input_tensor / (k**beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.1, f\"Tiny alpha test failed: diff={diff}\"\n    \n        elif test_type == \"large_alpha\":\n            # With large alpha, normalization effect is stronger\n            # Output magnitude should be reduced\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n            reduction_ratio = output_norm / input_norm\n    \n            # With alpha=1.0, reduction should be noticeable\n            assert reduction_ratio < 1.0, \\\n                f\"Large alpha should reduce norm: reduction_ratio={reduction_ratio}\"\n    \n        elif test_type == \"small_beta\":\n            # With small beta (0.5), normalization is less aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Smaller beta should produce larger output (less reduction)\n            assert output_norm > normal_norm * 0.9, \\\n                f\"Small beta should reduce output less: beta={beta} vs 0.75\"\n    \n        elif test_type == \"large_beta\":\n            # With large beta (2.0), normalization is more aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Larger beta should produce smaller output\n            assert output_norm < normal_norm * 1.1, \\\n                f\"Large beta should reduce output more: beta={beta} vs 0.75\"\n    \n        elif test_type == \"min_size\":\n            # With size=1, only self-channel is considered\n            # Output should be approximately input / (k^beta + alpha * input^2)^beta\n            # For size=1, the sum over neighbors is just self\n            squared = input_tensor ** 2\n            denominator = k + alpha * squared\n            expected = input_tensor / (denominator ** beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.2, f\"Size=1 test failed: diff={diff}\"\n    \n        elif test_type == \"small_k\":\n            # With small k, normalization denominator is small\n            # Output magnitude could be large\n            assert torch.all(torch.isfinite(output)), \"Small k test produced non-finite values\"\n    \n        # Test with all positive input\n        positive_input = torch.abs(input_tensor) + 0.1\n        positive_output = lrn(positive_input)\n        # Check that output is mostly positive (allow small numerical errors)\n        positive_ratio = torch.sum(positive_output >= -1e-7).item() / positive_output.numel()\n        assert positive_ratio > 0.99, \\\n            f\"Positive input should produce non-negative output for test_type={test_type}: ratio={positive_ratio}\"\n    \n        # Test with all negative input\n        negative_input = -torch.abs(input_tensor) - 0.1\n        negative_output = lrn(negative_input)\n        # Check that output is mostly negative (allow small numerical errors)\n        negative_ratio = torch.sum(negative_output <= 1e-7).item() / negative_output.numel()\n        assert negative_ratio > 0.99, \\\n            f\"Negative input should produce non-positive output for test_type={test_type}: ratio={negative_ratio}\"\n    \n        # Test with constant input\n        constant_value = 5.0\n        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)\n        constant_output = lrn(constant_input)\n    \n        # For constant input, output should be scaled version of input\n        # All values should be approximately equal\n        output_std = torch.std(constant_output)\n>       assert output_std < 1e-5, \\\n            f\"Constant input should produce constant output for test_type={test_type}: std={output_std}\"\nE       AssertionError: Constant input should produce constant output for test_type=large_k: std=2.31948488362832e-05\nE       assert tensor(2.3195e-05) < 1e-05\n\ntests/test_torch_nn_modules_normalization_g3.py:440: AssertionError\n_ test_localresponsenorm_boundary_values[5-0.0001-0.75-1.0-dtype9-cpu-shape9-float64] _\n\nsize = 5, alpha = 0.0001, beta = 0.75, k = 1.0, dtype = torch.float64\ndevice = 'cpu', shape = (2, 16, 8, 8), test_type = 'float64'\nset_random_seed = 42\n\n    @pytest.mark.parametrize(\"size,alpha,beta,k,dtype,device,shape,test_type\", [\n        # Test boundary values for size\n        (1, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"min_size\"),\n        (3, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"small_size\"),\n        (15, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_size\"),\n        # Test boundary values for alpha - avoid alpha=0 which can cause issues\n        (5, 1e-10, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"tiny_alpha\"),\n        (5, 1.0, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_alpha\"),\n        # Test boundary values for beta - avoid beta=0 which can cause issues\n        (5, 1e-4, 0.5, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_beta\"),\n        (5, 1e-4, 2.0, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_beta\"),\n        # Test boundary values for k - avoid k=0 which can cause division by zero\n        (5, 1e-4, 0.75, 0.1, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_k\"),\n        (5, 1e-4, 0.75, 10.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_k\"),\n        # Test with different data types\n        (5, 1e-4, 0.75, 1.0, torch.float64, \"cpu\", (2, 16, 8, 8), \"float64\"),\n        # Test with small batch size\n        (5, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (1, 16, 8, 8), \"batch_size_1\"),\n    ])\n    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):\n        \"\"\"Test LocalResponseNorm with boundary values for parameters\"\"\"\n        # Skip CUDA tests if device not available\n        if device == \"cuda\" and not torch.cuda.is_available():\n            pytest.skip(\"CUDA not available\")\n    \n        # Validate parameters based on test type\n        if test_type == \"min_size\":\n            assert size == 1, \"Test requires size=1\"\n        elif test_type == \"large_size\":\n            # Size should be odd and <= number of channels\n            assert size % 2 == 1, \"Size should be odd\"\n            assert size <= shape[1], f\"Size {size} should not exceed channels {shape[1]}\"\n    \n        # Create input tensor\n        torch.manual_seed(42)\n        input_tensor = torch.randn(*shape, dtype=dtype, device=device)\n    \n        # Create LocalResponseNorm layer\n        lrn = LocalResponseNorm(\n            size=size,\n            alpha=alpha,\n            beta=beta,\n            k=k\n        ).to(device=device)\n    \n        # Forward pass\n        output = lrn(input_tensor)\n    \n        # Basic assertions for all boundary tests\n        # 1. Shape assertion\n        assert output.shape == input_tensor.shape, \\\n            f\"Output shape {output.shape} != input shape {input_tensor.shape}\"\n    \n        # 2. Dtype assertion\n        assert output.dtype == input_tensor.dtype, \\\n            f\"Output dtype {output.dtype} != input dtype {input_tensor.dtype}\"\n    \n        # 3. Finite values assertion\n        assert torch.all(torch.isfinite(output)), \\\n            f\"Output contains NaN or infinite values for test_type={test_type}\"\n    \n        # 4. Sign preservation (allow small numerical errors)\n        # Check that signs are preserved for most elements\n        sign_match = ((input_tensor >= 0) == (output >= 0))\n        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()\n        assert sign_match_ratio > 0.99, \\\n            f\"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}\"\n    \n        # Test-specific assertions\n        if test_type == \"tiny_alpha\":\n            # With very small alpha, normalization effect is minimal\n            # Output should be close to input / (k^beta)\n            expected = input_tensor / (k**beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.1, f\"Tiny alpha test failed: diff={diff}\"\n    \n        elif test_type == \"large_alpha\":\n            # With large alpha, normalization effect is stronger\n            # Output magnitude should be reduced\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n            reduction_ratio = output_norm / input_norm\n    \n            # With alpha=1.0, reduction should be noticeable\n            assert reduction_ratio < 1.0, \\\n                f\"Large alpha should reduce norm: reduction_ratio={reduction_ratio}\"\n    \n        elif test_type == \"small_beta\":\n            # With small beta (0.5), normalization is less aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Smaller beta should produce larger output (less reduction)\n            assert output_norm > normal_norm * 0.9, \\\n                f\"Small beta should reduce output less: beta={beta} vs 0.75\"\n    \n        elif test_type == \"large_beta\":\n            # With large beta (2.0), normalization is more aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Larger beta should produce smaller output\n            assert output_norm < normal_norm * 1.1, \\\n                f\"Large beta should reduce output more: beta={beta} vs 0.75\"\n    \n        elif test_type == \"min_size\":\n            # With size=1, only self-channel is considered\n            # Output should be approximately input / (k^beta + alpha * input^2)^beta\n            # For size=1, the sum over neighbors is just self\n            squared = input_tensor ** 2\n            denominator = k + alpha * squared\n            expected = input_tensor / (denominator ** beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.2, f\"Size=1 test failed: diff={diff}\"\n    \n        elif test_type == \"small_k\":\n            # With small k, normalization denominator is small\n            # Output magnitude could be large\n            assert torch.all(torch.isfinite(output)), \"Small k test produced non-finite values\"\n    \n        # Test with all positive input\n        positive_input = torch.abs(input_tensor) + 0.1\n        positive_output = lrn(positive_input)\n        # Check that output is mostly positive (allow small numerical errors)\n        positive_ratio = torch.sum(positive_output >= -1e-7).item() / positive_output.numel()\n        assert positive_ratio > 0.99, \\\n            f\"Positive input should produce non-negative output for test_type={test_type}: ratio={positive_ratio}\"\n    \n        # Test with all negative input\n        negative_input = -torch.abs(input_tensor) - 0.1\n        negative_output = lrn(negative_input)\n        # Check that output is mostly negative (allow small numerical errors)\n        negative_ratio = torch.sum(negative_output <= 1e-7).item() / negative_output.numel()\n        assert negative_ratio > 0.99, \\\n            f\"Negative input should produce non-positive output for test_type={test_type}: ratio={negative_ratio}\"\n    \n        # Test with constant input\n        constant_value = 5.0\n        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)\n        constant_output = lrn(constant_input)\n    \n        # For constant input, output should be scaled version of input\n        # All values should be approximately equal\n        output_std = torch.std(constant_output)\n>       assert output_std < 1e-5, \\\n            f\"Constant input should produce constant output for test_type={test_type}: std={output_std}\"\nE       AssertionError: Constant input should produce constant output for test_type=float64: std=0.0013006162728712114\nE       assert tensor(0.0013, dtype=torch.float64) < 1e-05\n\ntests/test_torch_nn_modules_normalization_g3.py:440: AssertionError\n_ test_localresponsenorm_boundary_values[5-0.0001-0.75-1.0-dtype10-cpu-shape10-batch_size_1] _\n\nsize = 5, alpha = 0.0001, beta = 0.75, k = 1.0, dtype = torch.float32\ndevice = 'cpu', shape = (1, 16, 8, 8), test_type = 'batch_size_1'\nset_random_seed = 42\n\n    @pytest.mark.parametrize(\"size,alpha,beta,k,dtype,device,shape,test_type\", [\n        # Test boundary values for size\n        (1, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"min_size\"),\n        (3, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 8, 8, 8), \"small_size\"),\n        (15, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_size\"),\n        # Test boundary values for alpha - avoid alpha=0 which can cause issues\n        (5, 1e-10, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"tiny_alpha\"),\n        (5, 1.0, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_alpha\"),\n        # Test boundary values for beta - avoid beta=0 which can cause issues\n        (5, 1e-4, 0.5, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_beta\"),\n        (5, 1e-4, 2.0, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_beta\"),\n        # Test boundary values for k - avoid k=0 which can cause division by zero\n        (5, 1e-4, 0.75, 0.1, torch.float32, \"cpu\", (2, 16, 8, 8), \"small_k\"),\n        (5, 1e-4, 0.75, 10.0, torch.float32, \"cpu\", (2, 16, 8, 8), \"large_k\"),\n        # Test with different data types\n        (5, 1e-4, 0.75, 1.0, torch.float64, \"cpu\", (2, 16, 8, 8), \"float64\"),\n        # Test with small batch size\n        (5, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (1, 16, 8, 8), \"batch_size_1\"),\n    ])\n    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):\n        \"\"\"Test LocalResponseNorm with boundary values for parameters\"\"\"\n        # Skip CUDA tests if device not available\n        if device == \"cuda\" and not torch.cuda.is_available():\n            pytest.skip(\"CUDA not available\")\n    \n        # Validate parameters based on test type\n        if test_type == \"min_size\":\n            assert size == 1, \"Test requires size=1\"\n        elif test_type == \"large_size\":\n            # Size should be odd and <= number of channels\n            assert size % 2 == 1, \"Size should be odd\"\n            assert size <= shape[1], f\"Size {size} should not exceed channels {shape[1]}\"\n    \n        # Create input tensor\n        torch.manual_seed(42)\n        input_tensor = torch.randn(*shape, dtype=dtype, device=device)\n    \n        # Create LocalResponseNorm layer\n        lrn = LocalResponseNorm(\n            size=size,\n            alpha=alpha,\n            beta=beta,\n            k=k\n        ).to(device=device)\n    \n        # Forward pass\n        output = lrn(input_tensor)\n    \n        # Basic assertions for all boundary tests\n        # 1. Shape assertion\n        assert output.shape == input_tensor.shape, \\\n            f\"Output shape {output.shape} != input shape {input_tensor.shape}\"\n    \n        # 2. Dtype assertion\n        assert output.dtype == input_tensor.dtype, \\\n            f\"Output dtype {output.dtype} != input dtype {input_tensor.dtype}\"\n    \n        # 3. Finite values assertion\n        assert torch.all(torch.isfinite(output)), \\\n            f\"Output contains NaN or infinite values for test_type={test_type}\"\n    \n        # 4. Sign preservation (allow small numerical errors)\n        # Check that signs are preserved for most elements\n        sign_match = ((input_tensor >= 0) == (output >= 0))\n        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()\n        assert sign_match_ratio > 0.99, \\\n            f\"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}\"\n    \n        # Test-specific assertions\n        if test_type == \"tiny_alpha\":\n            # With very small alpha, normalization effect is minimal\n            # Output should be close to input / (k^beta)\n            expected = input_tensor / (k**beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.1, f\"Tiny alpha test failed: diff={diff}\"\n    \n        elif test_type == \"large_alpha\":\n            # With large alpha, normalization effect is stronger\n            # Output magnitude should be reduced\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n            reduction_ratio = output_norm / input_norm\n    \n            # With alpha=1.0, reduction should be noticeable\n            assert reduction_ratio < 1.0, \\\n                f\"Large alpha should reduce norm: reduction_ratio={reduction_ratio}\"\n    \n        elif test_type == \"small_beta\":\n            # With small beta (0.5), normalization is less aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Smaller beta should produce larger output (less reduction)\n            assert output_norm > normal_norm * 0.9, \\\n                f\"Small beta should reduce output less: beta={beta} vs 0.75\"\n    \n        elif test_type == \"large_beta\":\n            # With large beta (2.0), normalization is more aggressive\n            input_norm = torch.norm(input_tensor)\n            output_norm = torch.norm(output)\n    \n            # Compare with beta=0.75 case\n            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)\n            normal_output = lrn_normal(input_tensor)\n            normal_norm = torch.norm(normal_output)\n    \n            # Larger beta should produce smaller output\n            assert output_norm < normal_norm * 1.1, \\\n                f\"Large beta should reduce output more: beta={beta} vs 0.75\"\n    \n        elif test_type == \"min_size\":\n            # With size=1, only self-channel is considered\n            # Output should be approximately input / (k^beta + alpha * input^2)^beta\n            # For size=1, the sum over neighbors is just self\n            squared = input_tensor ** 2\n            denominator = k + alpha * squared\n            expected = input_tensor / (denominator ** beta)\n            diff = torch.norm(output - expected) / torch.norm(expected)\n            assert diff < 0.2, f\"Size=1 test failed: diff={diff}\"\n    \n        elif test_type == \"small_k\":\n            # With small k, normalization denominator is small\n            # Output magnitude could be large\n            assert torch.all(torch.isfinite(output)), \"Small k test produced non-finite values\"\n    \n        # Test with all positive input\n        positive_input = torch.abs(input_tensor) + 0.1\n        positive_output = lrn(positive_input)\n        # Check that output is mostly positive (allow small numerical errors)\n        positive_ratio = torch.sum(positive_output >= -1e-7).item() / positive_output.numel()\n        assert positive_ratio > 0.99, \\\n            f\"Positive input should produce non-negative output for test_type={test_type}: ratio={positive_ratio}\"\n    \n        # Test with all negative input\n        negative_input = -torch.abs(input_tensor) - 0.1\n        negative_output = lrn(negative_input)\n        # Check that output is mostly negative (allow small numerical errors)\n        negative_ratio = torch.sum(negative_output <= 1e-7).item() / negative_output.numel()\n        assert negative_ratio > 0.99, \\\n            f\"Negative input should produce non-positive output for test_type={test_type}: ratio={negative_ratio}\"\n    \n        # Test with constant input\n        constant_value = 5.0\n        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)\n        constant_output = lrn(constant_input)\n    \n        # For constant input, output should be scaled version of input\n        # All values should be approximately equal\n        output_std = torch.std(constant_output)\n>       assert output_std < 1e-5, \\\n            f\"Constant input should produce constant output for test_type={test_type}: std={output_std}\"\nE       AssertionError: Constant input should produce constant output for test_type=batch_size_1: std=0.0013009667163714767\nE       assert tensor(0.0013) < 1e-05\n\ntests/test_torch_nn_modules_normalization_g3.py:440: AssertionError\n================================ tests coverage ================================\n______________ coverage: platform darwin, python 3.10.19-final-0 _______________\n\nName                                              Stmts   Miss Branch BrPart  Cover   Missing\n---------------------------------------------------------------------------------------------\ntests/test_torch_nn_modules_normalization_g1.py     216     25     48      9    85%   24-39, 54, 101->96, 129-130, 181->186, 199-200, 212-213, 225-226, 256, 424->442, 462-470, 481, 484->495, 513->518, 525\ntests/test_torch_nn_modules_normalization_g2.py     234     87     42      3    64%   24-39, 54, 143-144, 190, 373-538, 544\ntests/test_torch_nn_modules_normalization_g3.py     205     17     46      6    88%   24-39, 52, 117-118, 128->143, 177, 239->254, 309, 468-471, 484\n---------------------------------------------------------------------------------------------\nTOTAL                                               655    129    136     18    79%\nCoverage XML written to file coverage.xml\n=========================== short test summary info ============================\nFAILED tests/test_torch_nn_modules_normalization_g1.py::test_groupnorm_device_dtype[dtype0-cpu]\nFAILED tests/test_torch_nn_modules_normalization_g2.py::test_layernorm_exception_shapes\nFAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[3-0.0001-0.75-1.0-dtype1-cpu-shape1-small_size]\nFAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[15-0.0001-0.75-1.0-dtype2-cpu-shape2-large_size]\nFAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-1.0-0.75-1.0-dtype4-cpu-shape4-large_alpha]\nFAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-0.0001-0.5-1.0-dtype5-cpu-shape5-small_beta]\nFAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-0.0001-2.0-1.0-dtype6-cpu-shape6-large_beta]\nFAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-0.0001-0.75-0.1-dtype7-cpu-shape7-small_k]\nFAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-0.0001-0.75-10.0-dtype8-cpu-shape8-large_k]\nFAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-0.0001-0.75-1.0-dtype9-cpu-shape9-float64]\nFAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-0.0001-0.75-1.0-dtype10-cpu-shape10-batch_size_1]\n11 failed, 22 passed, 2 skipped in 1.11s\n\nError: exit 1",
    "exit_code.txt": "1",
    "analysis_plan.json": "{\n  \"status\": \"未完全通过\",\n  \"passed\": 22,\n  \"failed\": 11,\n  \"errors\": 0,\n  \"collection_errors\": false,\n  \"block_limit\": 3,\n  \"failures\": [\n    {\n      \"test\": \"test_groupnorm_device_dtype[dtype0-cpu]\",\n      \"block_id\": \"CASE_01\",\n      \"error_type\": \"RuntimeError\",\n      \"action\": \"rewrite_block\",\n      \"note\": \"dtype不匹配：layer参数为float64但输入为float32\"\n    },\n    {\n      \"test\": \"test_layernorm_exception_shapes\",\n      \"block_id\": \"CASE_03\",\n      \"error_type\": \"AssertionError\",\n      \"action\": \"adjust_assertion\",\n      \"note\": \"异常测试未按预期抛出RuntimeError\"\n    },\n    {\n      \"test\": \"test_localresponsenorm_boundary_values[3-0.0001-0.75-1.0-dtype1-cpu-shape1-small_size]\",\n      \"block_id\": \"CASE_04\",\n      \"error_type\": \"AssertionError\",\n      \"action\": \"adjust_assertion\",\n      \"note\": \"常数输入测试断言过于严格，需要放宽容差\"\n    }\n  ],\n  \"deferred\": [\n    {\n      \"test\": \"test_localresponsenorm_boundary_values[15-0.0001-0.75-1.0-dtype2-cpu-shape2-large_size]\",\n      \"reason\": \"错误类型重复，跳过该块\"\n    },\n    {\n      \"test\": \"test_localresponsenorm_boundary_values[5-1.0-0.75-1.0-dtype4-cpu-shape4-large_alpha]\",\n      \"reason\": \"错误类型重复，跳过该块\"\n    },\n    {\n      \"test\": \"test_localresponsenorm_boundary_values[5-0.0001-0.5-1.0-dtype5-cpu-shape5-small_beta]\",\n      \"reason\": \"错误类型重复，跳过该块\"\n    },\n    {\n      \"test\": \"test_localresponsenorm_boundary_values[5-0.0001-2.0-1.0-dtype6-cpu-shape6-large_beta]\",\n      \"reason\": \"错误类型重复，跳过该块\"\n    },\n    {\n      \"test\": \"test_localresponsenorm_boundary_values[5-0.0001-0.75-0.1-dtype7-cpu-shape7-small_k]\",\n      \"reason\": \"错误类型重复，跳过该块\"\n    },\n    {\n      \"test\": \"test_localresponsenorm_boundary_values[5-0.0001-0.75-10.0-dtype8-cpu-shape8-large_k]\",\n      \"reason\": \"错误类型重复，跳过该块\"\n    },\n    {\n      \"test\": \"test_localresponsenorm_boundary_values[5-0.0001-0.75-1.0-dtype9-cpu-shape9-float64]\",\n      \"reason\": \"错误类型重复，跳过该块\"\n    },\n    {\n      \"test\": \"test_localresponsenorm_boundary_values[5-0.0001-0.75-1.0-dtype10-cpu-shape10-batch_size_1]\",\n      \"reason\": \"错误类型重复，跳过该块\"\n    }\n  ],\n  \"stop_recommended\": false,\n  \"stop_reason\": \"\"\n}",
    "analysis.md": "# 测试结果分析\n\n## 状态与统计\n- **状态**: 未完全通过\n- **通过**: 22 个测试\n- **失败**: 11 个测试\n- **错误**: 0 个\n- **跳过**: 2 个\n\n## 待修复 BLOCK 列表 (≤3)\n\n### 1. CASE_01 - GroupNorm 基本前向传播\n- **测试**: test_groupnorm_device_dtype[dtype0-cpu]\n- **错误类型**: RuntimeError\n- **Action**: rewrite_block\n- **原因**: dtype不匹配 - layer参数为float64但输入为float32\n\n### 2. CASE_03 - LayerNorm 基本前向传播  \n- **测试**: test_layernorm_exception_shapes\n- **错误类型**: AssertionError\n- **Action**: adjust_assertion\n- **原因**: 异常测试未按预期抛出RuntimeError\n\n### 3. CASE_04 - LocalResponseNorm 基本前向传播\n- **测试**: test_localresponsenorm_boundary_values[3-0.0001-0.75-1.0-dtype1-cpu-shape1-small_size]\n- **错误类型**: AssertionError\n- **Action**: adjust_assertion\n- **原因**: 常数输入测试断言过于严格，需要放宽容差\n\n## 停止建议\n- **stop_recommended**: false\n- **stop_reason**: 无",
    "tests/test_torch_nn_modules_normalization_g2.py": "import math\nimport pytest\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.normalization import (\n    LocalResponseNorm,\n    CrossMapLRN2d,\n    LayerNorm,\n    GroupNorm\n)\n\n# ==== BLOCK:HEADER START ====\n# Test fixtures and helper functions for G2 group\n@pytest.fixture(scope=\"function\")\ndef set_random_seed():\n    \"\"\"Set random seed for reproducibility\"\"\"\n    torch.manual_seed(42)\n    return 42\n\ndef assert_tensor_properties(tensor, expected_shape=None, expected_dtype=None, \n                           expected_device=None, name=\"\"):\n    \"\"\"Helper to assert tensor properties\"\"\"\n    assert torch.is_tensor(tensor), f\"{name}: Output is not a tensor\"\n    assert torch.all(torch.isfinite(tensor)), f\"{name}: Tensor contains NaN or Inf\"\n    \n    if expected_shape is not None:\n        assert tensor.shape == expected_shape, \\\n            f\"{name}: Shape mismatch: {tensor.shape} != {expected_shape}\"\n    \n    if expected_dtype is not None:\n        assert tensor.dtype == expected_dtype, \\\n            f\"{name}: Dtype mismatch: {tensor.dtype} != {expected_dtype}\"\n    \n    if expected_device is not None:\n        assert tensor.device == expected_device, \\\n            f\"{name}: Device mismatch: {tensor.device} != {expected_device}\"\n    \n    return True\n# ==== BLOCK:HEADER END ====\n\n# ==== BLOCK:CASE_03 START ====\n# TC-03: LayerNorm 基本前向传播\n@pytest.mark.parametrize(\"normalized_shape,eps,elementwise_affine,dtype,device,shape\", [\n    # Base case from test plan: 2D normalized shape\n    ([8, 8], 1e-5, True, torch.float32, \"cpu\", (2, 4, 8, 8)),\n    # Parameter extension: 1D normalized shape, no affine, float64\n    (8, 1e-6, False, torch.float64, \"cpu\", (2, 4, 8)),\n])\ndef test_layernorm_forward_basic(normalized_shape, eps, elementwise_affine, dtype, device, shape, set_random_seed):\n    \"\"\"Test basic forward pass of LayerNorm\"\"\"\n    # Skip CUDA tests if device not available\n    if device == \"cuda\" and not torch.cuda.is_available():\n        pytest.skip(\"CUDA not available\")\n    \n    # Create input tensor\n    torch.manual_seed(42)\n    input_tensor = torch.randn(*shape, dtype=dtype, device=device)\n    \n    # Create LayerNorm layer\n    ln = LayerNorm(\n        normalized_shape=normalized_shape,\n        eps=eps,\n        elementwise_affine=elementwise_affine\n    ).to(device=device, dtype=dtype)\n    \n    # Forward pass\n    output = ln(input_tensor)\n    \n    # Weak assertions\n    # 1. Shape assertion\n    assert output.shape == input_tensor.shape, \\\n        f\"Output shape {output.shape} != input shape {input_tensor.shape}\"\n    \n    # 2. Dtype assertion\n    assert output.dtype == dtype, \\\n        f\"Output dtype {output.dtype} != expected {dtype}\"\n    \n    # 3. Finite values assertion\n    assert torch.all(torch.isfinite(output)), \\\n        \"Output contains NaN or infinite values\"\n    \n    # 4. Basic property: normalized dimensions should have mean ~0 and std ~1\n    # Determine which dimensions to normalize over\n    if isinstance(normalized_shape, int):\n        normalized_dims = (-1,)\n        normalized_size = normalized_shape\n    else:\n        normalized_dims = tuple(range(-len(normalized_shape), 0))\n        normalized_size = math.prod(normalized_shape)\n    \n    # Reshape for statistics calculation\n    # Flatten the normalized dimensions\n    batch_dims = shape[:len(shape) - len(normalized_dims)]\n    batch_size = math.prod(batch_dims) if batch_dims else 1\n    \n    output_reshaped = output.reshape(batch_size, normalized_size)\n    \n    # Check statistics for each batch element\n    for i in range(batch_size):\n        batch_output = output_reshaped[i]\n        \n        # Mean should be close to 0\n        mean_abs = torch.abs(torch.mean(batch_output))\n        assert mean_abs < 0.1, f\"Batch element {i} mean too large: {mean_abs}\"\n        \n        # Std should be close to 1 (with eps adjustment)\n        std = torch.std(batch_output)\n        # Allow some tolerance for numerical precision\n        assert 0.9 < std < 1.1, f\"Batch element {i} std out of range: {std}\"\n    \n    # 5. Check affine parameters if enabled\n    if elementwise_affine:\n        assert hasattr(ln, 'weight'), \"Elementwise affine enabled but weight parameter missing\"\n        assert hasattr(ln, 'bias'), \"Elementwise affine enabled but bias parameter missing\"\n        \n        # Weight and bias should have normalized_shape\n        expected_shape = normalized_shape if isinstance(normalized_shape, tuple) else (normalized_shape,)\n        assert ln.weight.shape == expected_shape, \\\n            f\"Weight shape {ln.weight.shape} != expected {expected_shape}\"\n        assert ln.bias.shape == expected_shape, \\\n            f\"Bias shape {ln.bias.shape} != expected {expected_shape}\"\n    \n    # 6. Compare with functional implementation (weak comparison)\n    if elementwise_affine:\n        # When elementwise_affine=True, use weight and bias\n        try:\n            functional_output = F.layer_norm(\n                input_tensor, normalized_shape, ln.weight, ln.bias, eps\n            )\n            # Basic shape check\n            assert functional_output.shape == output.shape\n        except Exception as e:\n            pytest.fail(f\"Functional layer_norm failed: {e}\")\n    else:\n        # When elementwise_affine=False, compare directly\n        functional_output = F.layer_norm(\n            input_tensor, normalized_shape, None, None, eps\n        )\n        # Weak comparison: just check shapes match\n        assert functional_output.shape == output.shape\n    \n    # 7. Test with different input values\n    # Test with all zeros (should produce zeros output)\n    zeros_input = torch.zeros(*shape, dtype=dtype, device=device)\n    zeros_output = ln(zeros_input)\n    \n    # For zero input with affine=False, output should be zeros (0/eps = 0)\n    if not elementwise_affine:\n        assert torch.allclose(zeros_output, torch.zeros_like(zeros_output), atol=1e-7), \\\n            \"Zero input should produce zero output when affine=False\"\n    \n    # Test with all ones (should normalize)\n    ones_input = torch.ones(*shape, dtype=dtype, device=device)\n    ones_output = ln(ones_input)\n    \n    # Should still have correct shape and finite values\n    assert ones_output.shape == shape\n    assert torch.all(torch.isfinite(ones_output))\n# ==== BLOCK:CASE_03 END ====\n\n# ==== BLOCK:CASE_07 START ====\n# TC-07: DEFERRED - LayerNorm 参数扩展测试\n# ==== BLOCK:CASE_07 END ====\n\n# ==== BLOCK:CASE_08 START ====\n# TC-08: DEFERRED - LayerNorm 异常形状测试\n# ==== BLOCK:CASE_08 END ====\n\n# ==== BLOCK:FOOTER START ====\n# Additional test classes and helper functions can be added here\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n# ==== BLOCK:FOOTER END ====",
    "tests/test_torch_nn_modules_normalization_g1.py": "import math\nimport pytest\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.normalization import (\n    LocalResponseNorm,\n    CrossMapLRN2d,\n    LayerNorm,\n    GroupNorm\n)\n\n# ==== BLOCK:HEADER START ====\n# Test fixtures and helper functions for G1 group\n@pytest.fixture(scope=\"function\")\ndef set_random_seed():\n    \"\"\"Set random seed for reproducibility\"\"\"\n    torch.manual_seed(42)\n    return 42\n\ndef assert_tensor_properties(tensor, expected_shape=None, expected_dtype=None, \n                           expected_device=None, name=\"\"):\n    \"\"\"Helper to assert tensor properties\"\"\"\n    assert torch.is_tensor(tensor), f\"{name}: Output is not a tensor\"\n    assert torch.all(torch.isfinite(tensor)), f\"{name}: Tensor contains NaN or Inf\"\n    \n    if expected_shape is not None:\n        assert tensor.shape == expected_shape, \\\n            f\"{name}: Shape mismatch: {tensor.shape} != {expected_shape}\"\n    \n    if expected_dtype is not None:\n        assert tensor.dtype == expected_dtype, \\\n            f\"{name}: Dtype mismatch: {tensor.dtype} != {expected_dtype}\"\n    \n    if expected_device is not None:\n        assert tensor.device == expected_device, \\\n            f\"{name}: Device mismatch: {tensor.device} != {expected_device}\"\n    \n    return True\n# ==== BLOCK:HEADER END ====\n\n# ==== BLOCK:CASE_01 START ====\n# TC-01: GroupNorm 基本前向传播\n@pytest.mark.parametrize(\"num_groups,num_channels,eps,affine,dtype,device,shape\", [\n    # Base case from test plan\n    (2, 4, 1e-5, True, torch.float32, \"cpu\", (2, 4, 8, 8)),\n    # Parameter extensions\n    (4, 8, 1e-6, False, torch.float64, \"cpu\", (4, 8, 16, 16)),\n])\ndef test_groupnorm_forward_basic(num_groups, num_channels, eps, affine, dtype, device, shape, set_random_seed):\n    \"\"\"Test basic forward pass of GroupNorm\"\"\"\n    # Skip CUDA tests if device not available\n    if device == \"cuda\" and not torch.cuda.is_available():\n        pytest.skip(\"CUDA not available\")\n    \n    # Create input tensor\n    torch.manual_seed(42)\n    input_tensor = torch.randn(*shape, dtype=dtype, device=device)\n    \n    # Create GroupNorm layer\n    gn = GroupNorm(\n        num_groups=num_groups,\n        num_channels=num_channels,\n        eps=eps,\n        affine=affine\n    ).to(device=device, dtype=dtype)\n    \n    # Forward pass\n    output = gn(input_tensor)\n    \n    # Weak assertions\n    # 1. Shape assertion\n    assert output.shape == input_tensor.shape, \\\n        f\"Output shape {output.shape} != input shape {input_tensor.shape}\"\n    \n    # 2. Dtype assertion\n    assert output.dtype == dtype, \\\n        f\"Output dtype {output.dtype} != expected {dtype}\"\n    \n    # 3. Finite values assertion\n    assert torch.all(torch.isfinite(output)), \\\n        \"Output contains NaN or infinite values\"\n    \n    # 4. Basic property: output should be normalized per group\n    # Reshape to (batch, groups, channels_per_group, *spatial)\n    batch_size = shape[0]\n    channels_per_group = num_channels // num_groups\n    spatial_dims = shape[2:]\n    \n    # Reshape input and output for group-wise statistics\n    input_reshaped = input_tensor.view(batch_size, num_groups, channels_per_group, *spatial_dims)\n    output_reshaped = output.view(batch_size, num_groups, channels_per_group, *spatial_dims)\n    \n    # Check that each group is normalized (mean close to 0, std close to 1)\n    for b in range(batch_size):\n        for g in range(num_groups):\n            group_input = input_reshaped[b, g].flatten()\n            group_output = output_reshaped[b, g].flatten()\n            \n            # Skip if group has zero variance (unlikely with random data)\n            if torch.std(group_input) > 1e-7:\n                # Mean should be close to 0\n                mean_abs = torch.abs(torch.mean(group_output))\n                assert mean_abs < 0.1, f\"Group mean too large: {mean_abs}\"\n                \n                # Std should be close to 1\n                std = torch.std(group_output)\n                assert 0.9 < std < 1.1, f\"Group std out of range: {std}\"\n    \n    # 5. Check affine parameters if enabled\n    if affine:\n        assert hasattr(gn, 'weight'), \"Affine enabled but weight parameter missing\"\n        assert hasattr(gn, 'bias'), \"Affine enabled but bias parameter missing\"\n        assert gn.weight.shape == (num_channels,), \\\n            f\"Weight shape {gn.weight.shape} != expected ({num_channels},)\"\n        assert gn.bias.shape == (num_channels,), \\\n            f\"Bias shape {gn.bias.shape} != expected ({num_channels},)\"\n    \n    # 6. Compare with functional implementation (weak comparison)\n    if affine:\n        # When affine=True, we need to handle scale and bias\n        # For weak assertion, just verify functional call doesn't crash\n        try:\n            functional_output = F.group_norm(\n                input_tensor, num_groups, gn.weight, gn.bias, eps\n            )\n            # Basic shape check\n            assert functional_output.shape == output.shape\n        except Exception as e:\n            pytest.fail(f\"Functional group_norm failed: {e}\")\n    else:\n        # When affine=False, compare directly\n        functional_output = F.group_norm(\n            input_tensor, num_groups, None, None, eps\n        )\n        # Weak comparison: just check shapes match\n        assert functional_output.shape == output.shape\n# ==== BLOCK:CASE_01 END ====\n\n# ==== BLOCK:CASE_02 START ====\n# TC-02: GroupNorm 整除性异常检查\ndef test_groupnorm_divisibility_exception():\n    \"\"\"Test that GroupNorm raises ValueError when num_channels not divisible by num_groups\"\"\"\n    # Test case from test plan: num_groups=3, num_channels=5 (not divisible)\n    num_groups = 3\n    num_channels = 5\n    eps = 1e-5\n    affine = True\n    \n    # Weak assertions: exception type and message\n    with pytest.raises(ValueError) as exc_info:\n        GroupNorm(\n            num_groups=num_groups,\n            num_channels=num_channels,\n            eps=eps,\n            affine=affine\n        )\n    \n    # Check exception type\n    exception = exc_info.value\n    assert isinstance(exception, ValueError), \\\n        f\"Expected ValueError, got {type(exception).__name__}\"\n    \n    # Check exception message contains relevant information\n    error_msg = str(exception).lower()\n    \n    # Should mention divisibility or groups\n    assert any(keyword in error_msg for keyword in \n              ['divisible', 'group', 'channel', 'num_groups', 'num_channels']), \\\n        f\"Error message doesn't mention divisibility: {error_msg}\"\n    \n    # Should mention the actual numbers (3 and 5)\n    assert '3' in error_msg or '5' in error_msg, \\\n        f\"Error message should mention group/channel counts: {error_msg}\"\n    \n    # Additional test: valid case should not raise exception\n    num_groups_valid = 2\n    num_channels_valid = 4  # 4 is divisible by 2\n    \n    try:\n        gn = GroupNorm(\n            num_groups=num_groups_valid,\n            num_channels=num_channels_valid,\n            eps=eps,\n            affine=affine\n        )\n        # Should reach here without exception\n        assert gn.num_groups == num_groups_valid\n        assert gn.num_channels == num_channels_valid\n    except ValueError as e:\n        pytest.fail(f\"Valid GroupNorm raised unexpected ValueError: {e}\")\n    \n    # Test edge case: num_groups = 1 (always divisible)\n    try:\n        gn = GroupNorm(\n            num_groups=1,\n            num_channels=7,  # Any number is divisible by 1\n            eps=eps,\n            affine=affine\n        )\n        assert gn.num_groups == 1\n        assert gn.num_channels == 7\n    except ValueError as e:\n        pytest.fail(f\"GroupNorm with num_groups=1 raised unexpected ValueError: {e}\")\n    \n    # Test edge case: num_groups = num_channels (divisible)\n    try:\n        gn = GroupNorm(\n            num_groups=4,\n            num_channels=4,  # Each group has 1 channel\n            eps=eps,\n            affine=affine\n        )\n        assert gn.num_groups == 4\n        assert gn.num_channels == 4\n    except ValueError as e:\n        pytest.fail(f\"GroupNorm with num_groups=num_channels raised unexpected ValueError: {e}\")\n    \n    # Test with invalid input tensor shape (should fail during forward, not init)\n    gn = GroupNorm(num_groups=2, num_channels=4, eps=eps, affine=affine)\n    invalid_input = torch.randn(2, 3, 8, 8)  # 3 channels, but layer expects 4\n    \n    # This might raise a runtime error during forward pass\n    # For weak assertion, we just verify it doesn't crash in unexpected ways\n    try:\n        output = gn(invalid_input)\n        # If it reaches here, the error might be caught later or not at all\n        # This is acceptable for weak assertions\n    except Exception as e:\n        # Any exception is acceptable for invalid input\n        pass\n# ==== BLOCK:CASE_02 END ====\n\n# ==== BLOCK:CASE_05 START ====\n# TC-05: DEFERRED - GroupNorm 参数扩展测试\n# ==== BLOCK:CASE_05 END ====\n\n# ==== BLOCK:CASE_06 START ====\n# TC-06: DEFERRED - GroupNorm 设备/数据类型测试\n# ==== BLOCK:CASE_06 END ====\n\n# ==== BLOCK:FOOTER START ====\n# Additional test classes and helper functions can be added here\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n# ==== BLOCK:FOOTER END ====",
    "tests/test_torch_nn_modules_normalization_g3.py": "import math\nimport pytest\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.normalization import (\n    LocalResponseNorm,\n    CrossMapLRN2d,\n    LayerNorm,\n    GroupNorm\n)\n\n# ==== BLOCK:HEADER START ====\n# Test fixtures and helper functions for G3 group\n@pytest.fixture(scope=\"function\")\ndef set_random_seed():\n    \"\"\"Set random seed for reproducibility\"\"\"\n    torch.manual_seed(42)\n    return 42\n\ndef assert_tensor_properties(tensor, expected_shape=None, expected_dtype=None, \n                           expected_device=None, name=\"\"):\n    \"\"\"Helper to assert tensor properties\"\"\"\n    assert torch.is_tensor(tensor), f\"{name}: Output is not a tensor\"\n    assert torch.all(torch.isfinite(tensor)), f\"{name}: Tensor contains NaN or Inf\"\n    \n    if expected_shape is not None:\n        assert tensor.shape == expected_shape, \\\n            f\"{name}: Shape mismatch: {tensor.shape} != {expected_shape}\"\n    \n    if expected_dtype is not None:\n        assert tensor.dtype == expected_dtype, \\\n            f\"{name}: Dtype mismatch: {tensor.dtype} != {expected_dtype}\"\n    \n    if expected_device is not None:\n        assert tensor.device == expected_device, \\\n            f\"{name}: Device mismatch: {tensor.device} != {expected_device}\"\n    \n    return True\n# ==== BLOCK:HEADER END ====\n\n# ==== BLOCK:CASE_04 START ====\n# TC-04: LocalResponseNorm 基本前向传播\n@pytest.mark.parametrize(\"size,alpha,beta,k,dtype,device,shape\", [\n    # Base case from test plan\n    (5, 1e-4, 0.75, 1.0, torch.float32, \"cpu\", (2, 16, 8, 8)),\n])\ndef test_localresponsenorm_forward_basic(size, alpha, beta, k, dtype, device, shape, set_random_seed):\n    \"\"\"Test basic forward pass of LocalResponseNorm\"\"\"\n    # Skip CUDA tests if device not available\n    if device == \"cuda\" and not torch.cuda.is_available():\n        pytest.skip(\"CUDA not available\")\n    \n    # Create input tensor\n    torch.manual_seed(42)\n    input_tensor = torch.randn(*shape, dtype=dtype, device=device)\n    \n    # Create LocalResponseNorm layer\n    lrn = LocalResponseNorm(\n        size=size,\n        alpha=alpha,\n        beta=beta,\n        k=k\n    ).to(device=device)\n    \n    # Forward pass\n    output = lrn(input_tensor)\n    \n    # Weak assertions\n    # 1. Shape assertion\n    assert output.shape == input_tensor.shape, \\\n        f\"Output shape {output.shape} != input shape {input_tensor.shape}\"\n    \n    # 2. Dtype assertion - LRN doesn't change dtype\n    assert output.dtype == input_tensor.dtype, \\\n        f\"Output dtype {output.dtype} != input dtype {input_tensor.dtype}\"\n    \n    # 3. Finite values assertion\n    assert torch.all(torch.isfinite(output)), \\\n        \"Output contains NaN or infinite values\"\n    \n    # 4. Basic property: output should have same sign as input\n    # LRN is a pointwise normalization, so sign should be preserved\n    sign_preserved = torch.all((input_tensor * output) >= 0)\n    assert sign_preserved, \"LocalResponseNorm should preserve sign of input\"\n    \n    # 5. Basic property: output magnitude should be reduced or similar\n    # LRN reduces magnitude based on neighboring channels\n    input_norm = torch.norm(input_tensor)\n    output_norm = torch.norm(output)\n    \n    # Output norm should not be larger than input norm (normalization reduces)\n    # Allow small numerical differences\n    assert output_norm <= input_norm * 1.1, \\\n        f\"Output norm {output_norm} > input norm {input_norm} * 1.1\"\n    \n    # 6. Check layer parameters\n    assert lrn.size == size, f\"Layer size {lrn.size} != expected {size}\"\n    assert abs(lrn.alpha - alpha) < 1e-10, f\"Layer alpha {lrn.alpha} != expected {alpha}\"\n    assert abs(lrn.beta - beta) < 1e-10, f\"Layer beta {lrn.beta} != expected {beta}\"\n    assert abs(lrn.k - k) < 1e-10, f\"Layer k {lrn.k} != expected {k}\"\n    \n    # 7. Compare with functional implementation (weak comparison)\n    try:\n        functional_output = F.local_response_norm(\n            input_tensor, size, alpha, beta, k\n        )\n        # Basic shape check\n        assert functional_output.shape == output.shape\n        \n        # Weak comparison: check they're close within reasonable tolerance\n        # LRN can have numerical differences due to implementation\n        diff_norm = torch.norm(functional_output - output)\n        rel_diff = diff_norm / (torch.norm(output) + 1e-10)\n        assert rel_diff < 0.01, \\\n            f\"Functional and layer outputs differ significantly: rel_diff={rel_diff}\"\n    except Exception as e:\n        pytest.fail(f\"Functional local_response_norm failed: {e}\")\n    \n    # 8. Test edge cases\n    \n    # Test with all positive values\n    positive_input = torch.abs(input_tensor) + 0.1  # Ensure all positive\n    positive_output = lrn(positive_input)\n    assert torch.all(positive_output >= 0), \"All positive input should produce non-negative output\"\n    \n    # Test with small size (size=1 means only self-channel)\n    if shape[1] >= 3:  # Need at least 3 channels for size=1 to make sense\n        lrn_small = LocalResponseNorm(size=1, alpha=alpha, beta=beta, k=k).to(device=device)\n        small_output = lrn_small(input_tensor)\n        assert small_output.shape == shape\n        \n        # With size=1, normalization should be minimal\n        # Check that output is close to input/(k^beta) when only considering self\n        expected_small = input_tensor / (k**beta)\n        diff = torch.norm(small_output - expected_small) / torch.norm(expected_small)\n        # Allow some difference due to implementation details\n        assert diff < 0.1, f\"Size=1 LRN differs from expected: diff={diff}\"\n    \n    # 9. Test parameter boundaries\n    \n    # Test with different alpha values\n    for test_alpha in [1e-5, 1e-3, 1e-2]:\n        lrn_alpha = LocalResponseNorm(size=size, alpha=test_alpha, beta=beta, k=k).to(device=device)\n        output_alpha = lrn_alpha(input_tensor)\n        assert output_alpha.shape == shape\n        assert torch.all(torch.isfinite(output_alpha))\n    \n    # Test with different beta values\n    for test_beta in [0.5, 0.75, 1.0]:\n        lrn_beta = LocalResponseNorm(size=size, alpha=alpha, beta=test_beta, k=k).to(device=device)\n        output_beta = lrn_beta(input_tensor)\n        assert output_beta.shape == shape\n        assert torch.all(torch.isfinite(output_beta))\n    \n    # Test with different k values\n    for test_k in [0.5, 1.0, 2.0]:\n        lrn_k = LocalResponseNorm(size=size, alpha=alpha, beta=beta, k=test_k).to(device=device)\n        output_k = lrn_k(input_tensor)\n        assert output_k.shape == shape\n        assert torch.all(torch.isfinite(output_k))\n# ==== BLOCK:CASE_04 END ====\n\n# ==== BLOCK:CASE_09 START ====\n# TC-09: DEFERRED - CrossMapLRN2d 基本功能测试\n# ==== BLOCK:CASE_09 END ====\n\n# ==== BLOCK:CASE_10 START ====\n# TC-10: DEFERRED - LocalResponseNorm 边界值测试\n# ==== BLOCK:CASE_10 END ====\n\n# ==== BLOCK:FOOTER START ====\n# Additional test classes and helper functions can be added here\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n# ==== BLOCK:FOOTER END ====",
    "final_report.md": "# torch.nn.modules.normalization 测试报告\n\n## 1. 执行摘要\n测试基本通过但存在11个失败用例，主要阻塞项为数据类型不匹配、异常断言过严和常数输入容差问题。\n\n**关键发现**：\n- GroupNorm设备/数据类型参数化测试存在dtype不匹配问题\n- LayerNorm异常形状测试断言需要调整\n- LocalResponseNorm常数输入测试容差过严\n- CrossMapLRN2d完全未覆盖，存在文档缺失风险\n\n## 2. 测试范围\n**目标FQN**: torch.nn.modules.normalization\n\n**测试环境**:\n- 框架：pytest\n- 依赖：torch, torch.nn.functional\n- 设备：CPU为主（CUDA测试作为参数扩展）\n- Python版本：3.10+\n\n**覆盖场景**:\n- ✓ GroupNorm基本功能与异常处理\n- ✓ LayerNorm不同normalized_shape支持\n- ✓ LocalResponseNorm跨通道归一化\n- ✓ 设备/数据类型参数化测试\n- ✓ affine/elementwise_affine参数开关\n\n**未覆盖项**:\n- ✗ CrossMapLRN2d所有功能（文档缺失，测试计划未包含）\n- ✗ 训练/评估模式一致性验证\n- ✗ 极端数值稳定性测试\n- ✗ 不同维度输入全面支持（2D/3D/4D）\n- ✗ 批量大小=1边界情况\n\n## 3. 结果概览\n- **用例总数**: 35个（22通过 + 11失败 + 2跳过）\n- **通过率**: 62.9%\n- **主要失败点**:\n  1. GroupNorm设备/数据类型测试：dtype不匹配（RuntimeError）\n  2. LayerNorm异常测试：未按预期抛出RuntimeError（AssertionError）\n  3. LocalResponseNorm边界值测试：常数输入容差过严（AssertionError）\n\n## 4. 详细发现\n\n### 高优先级问题\n**P1: GroupNorm设备/数据类型不匹配**\n- **现象**: test_groupnorm_device_dtype[dtype0-cpu] 失败\n- **根因**: layer参数为float64但输入为float32，导致RuntimeError\n- **影响**: 设备/数据类型参数化测试无法执行\n- **建议**: 确保输入张量与layer参数dtype一致，或调整测试逻辑\n\n**P1: LayerNorm异常断言过严**\n- **现象**: test_layernorm_exception_shapes 失败\n- **根因**: 异常测试未按预期抛出RuntimeError\n- **影响**: 异常处理逻辑验证不完整\n- **建议**: 检查异常类型或调整断言条件\n\n**P2: LocalResponseNorm容差过严**\n- **现象**: test_localresponsenorm_boundary_values 失败\n- **根因**: 常数输入测试断言过于严格\n- **影响**: 边界值测试覆盖不全\n- **建议**: 放宽常数输入测试的容差要求\n\n### 中优先级风险\n**P3: CrossMapLRN2d完全未测试**\n- **风险**: 文档字符串缺失，功能未验证\n- **影响**: 跨通道LRN 2D版本可能存在未知问题\n- **建议**: 补充CrossMapLRN2d基础测试用例\n\n**P4: 训练/评估模式未验证**\n- **风险**: 所有归一化层在两种模式下行为一致性未测试\n- **影响**: 实际使用中可能存在模式切换问题\n- **建议**: 添加训练/评估模式切换测试\n\n## 5. 覆盖与风险\n\n**需求覆盖评估**:\n- ✓ 高优先级必测路径：4/5（缺少CrossMapLRN2d）\n- ✓ 异常处理：部分覆盖（GroupNorm整除性检查通过）\n- ✓ 设备/数据类型：部分覆盖（存在dtype不匹配问题）\n- ✗ 可选路径：基本未覆盖\n\n**尚未覆盖的边界/缺失信息**:\n1. **CrossMapLRN2d与LocalResponseNorm差异**：文档缺失，功能对比不明\n2. **极端数值稳定性**：极大/极小值输入未测试\n3. **不同维度支持**：仅测试了部分形状，未全面覆盖2D/3D/4D\n4. **标准差计算影响**：有偏估计器的具体影响未验证\n5. **设备/数据类型约束**：具体限制条件未详细说明\n\n**已知风险**:\n- CrossMapLRN2d可能存在隐藏bug\n- 极端输入值可能导致数值不稳定\n- 不同设备间行为一致性未验证\n\n## 6. 后续动作\n\n### 立即修复（P0-P1）\n1. **修复GroupNorm dtype不匹配**（P1）\n   - 确保测试中layer参数与输入dtype一致\n   - 添加dtype转换检查逻辑\n   - 预计工时：0.5天\n\n2. **调整LayerNorm异常断言**（P1）\n   - 检查实际抛出的异常类型\n   - 调整断言条件或异常捕获逻辑\n   - 预计工时：0.5天\n\n3. **放宽LocalResponseNorm容差**（P2）\n   - 针对常数输入调整容差阈值\n   - 添加特殊情况的测试逻辑\n   - 预计工时：0.5天\n\n### 补充测试（P2-P3）\n4. **补充CrossMapLRN2d基础测试**（P3）\n   - 创建基础功能测试用例\n   - 验证与LocalResponseNorm的差异\n   - 预计工时：1天\n\n5. **添加训练/评估模式测试**（P3）\n   - 验证所有归一化层模式切换一致性\n   - 测试eval()和train()方法\n   - 预计工时：0.5天\n\n6. **扩展边界值测试**（P3）\n   - 添加极端数值稳定性测试\n   - 覆盖不同维度输入（2D/3D/4D）\n   - 预计工时：1天\n\n### 环境与优化（P4）\n7. **完善测试参数化**（P4）\n   - 优化设备/数据类型参数组合\n   - 添加CUDA环境测试\n   - 预计工时：1天\n\n8. **文档补充**（P4）\n   - 补充CrossMapLRN2d文档说明\n   - 记录测试发现的边界条件\n   - 预计工时：0.5天\n\n**优先级排序**:\n1. 修复现有失败用例（P1-P2）\n2. 补充CrossMapLRN2d测试（P3）\n3. 扩展边界值覆盖（P3）\n4. 环境优化与文档（P4）\n\n**预计总工时**: 5.5天\n**风险缓解**: 修复现有问题后可达到85%+通过率，补充测试后可达95%+覆盖率"
  },
  "stage_history": [
    {
      "stage": "understand_function",
      "status": "completed",
      "timestamp": "2026-01-18T23:18:58.127583",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "generate_requirements",
      "status": "completed",
      "timestamp": "2026-01-18T23:19:33.641342",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "design_test_plan",
      "status": "completed",
      "timestamp": "2026-01-18T23:21:14.403315",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-18T23:25:10.459627",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-18T23:25:10.773079",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-18T23:26:11.749346",
      "attempts": 1,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-18T23:32:12.573154",
      "attempts": 2,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-18T23:32:13.815884",
      "attempts": 2,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-18T23:33:09.593186",
      "attempts": 2,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-18T23:36:18.193294",
      "attempts": 3,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-18T23:36:19.129475",
      "attempts": 3,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-18T23:37:39.256425",
      "attempts": 3,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-18T23:44:09.997409",
      "attempts": 4,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-18T23:44:11.749622",
      "attempts": 4,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-18T23:45:51.589459",
      "attempts": 4,
      "error": null
    },
    {
      "stage": "generate_code",
      "status": "completed",
      "timestamp": "2026-01-18T23:50:58.645778",
      "attempts": 5,
      "error": null
    },
    {
      "stage": "execute_tests",
      "status": "completed",
      "timestamp": "2026-01-18T23:51:00.195775",
      "attempts": 5,
      "error": null
    },
    {
      "stage": "analyze_results",
      "status": "completed",
      "timestamp": "2026-01-18T23:52:19.613475",
      "attempts": 5,
      "error": null
    },
    {
      "stage": "generate_report",
      "status": "completed",
      "timestamp": "2026-01-18T23:53:20.789988",
      "attempts": 1,
      "error": null
    }
  ],
  "user_feedback": []
}