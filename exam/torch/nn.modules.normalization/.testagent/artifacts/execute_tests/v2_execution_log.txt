=== Run Tests ===
FF                                                                       [100%]
=================================== FAILURES ===================================
_ test_layernorm_forward_basic[normalized_shape0-1e-05-True-dtype0-cpu-shape0] _

normalized_shape = [8, 8], eps = 1e-05, elementwise_affine = True
dtype = torch.float32, device = 'cpu', shape = (2, 4, 8, 8)
set_random_seed = 42

    @pytest.mark.parametrize("normalized_shape,eps,elementwise_affine,dtype,device,shape", [
        # Base case from test plan: 2D normalized shape
        ([8, 8], 1e-5, True, torch.float32, "cpu", (2, 4, 8, 8)),
        # Parameter extension: 1D normalized shape, no affine, float64
        (8, 1e-6, False, torch.float64, "cpu", (2, 4, 8)),
    ])
    def test_layernorm_forward_basic(normalized_shape, eps, elementwise_affine, dtype, device, shape, set_random_seed):
        """Test basic forward pass of LayerNorm"""
        # Skip CUDA tests if device not available
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("CUDA not available")
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create LayerNorm layer
        ln = LayerNorm(
            normalized_shape=normalized_shape,
            eps=eps,
            elementwise_affine=elementwise_affine
        ).to(device=device, dtype=dtype)
    
        # Forward pass
        output = ln(input_tensor)
    
        # Weak assertions
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == dtype, \
            f"Output dtype {output.dtype} != expected {dtype}"
    
        # 3. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            "Output contains NaN or infinite values"
    
        # 4. Basic property: normalized dimensions should have mean ~0 and std ~1
        # Determine which dimensions to normalize over
        if isinstance(normalized_shape, int):
            normalized_dims = (-1,)
            normalized_size = normalized_shape
        else:
            normalized_dims = tuple(range(-len(normalized_shape), 0))
            normalized_size = math.prod(normalized_shape)
    
        # Reshape for statistics calculation
        # Flatten the normalized dimensions
        batch_dims = shape[:len(shape) - len(normalized_dims)]
        batch_size = math.prod(batch_dims) if batch_dims else 1
    
        output_reshaped = output.reshape(batch_size, normalized_size)
    
        # Check statistics for each batch element
        for i in range(batch_size):
            batch_output = output_reshaped[i]
    
            # Mean should be close to 0
            mean_abs = torch.abs(torch.mean(batch_output))
            assert mean_abs < 0.1, f"Batch element {i} mean too large: {mean_abs}"
    
            # Std should be close to 1 (with eps adjustment)
            std = torch.std(batch_output)
            # Allow some tolerance for numerical precision
            assert 0.9 < std < 1.1, f"Batch element {i} std out of range: {std}"
    
        # 5. Check affine parameters if enabled
        if elementwise_affine:
            assert hasattr(ln, 'weight'), "Elementwise affine enabled but weight parameter missing"
            assert hasattr(ln, 'bias'), "Elementwise affine enabled but bias parameter missing"
    
            # Weight and bias should have normalized_shape
            expected_shape = normalized_shape if isinstance(normalized_shape, tuple) else (normalized_shape,)
>           assert ln.weight.shape == expected_shape, \
                f"Weight shape {ln.weight.shape} != expected {expected_shape}"
E           AssertionError: Weight shape torch.Size([8, 8]) != expected ([8, 8],)
E           assert torch.Size([8, 8]) == ([8, 8],)
E             
E             At index 0 diff: 8 != [8, 8]
E             Left contains one more item: 8
E             Use -v to get more diff

tests/test_torch_nn_modules_normalization_g2.py:119: AssertionError
________ test_layernorm_forward_basic[8-1e-06-False-dtype1-cpu-shape1] _________

normalized_shape = 8, eps = 1e-06, elementwise_affine = False
dtype = torch.float64, device = 'cpu', shape = (2, 4, 8), set_random_seed = 42

    @pytest.mark.parametrize("normalized_shape,eps,elementwise_affine,dtype,device,shape", [
        # Base case from test plan: 2D normalized shape
        ([8, 8], 1e-5, True, torch.float32, "cpu", (2, 4, 8, 8)),
        # Parameter extension: 1D normalized shape, no affine, float64
        (8, 1e-6, False, torch.float64, "cpu", (2, 4, 8)),
    ])
    def test_layernorm_forward_basic(normalized_shape, eps, elementwise_affine, dtype, device, shape, set_random_seed):
        """Test basic forward pass of LayerNorm"""
        # Skip CUDA tests if device not available
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("CUDA not available")
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create LayerNorm layer
        ln = LayerNorm(
            normalized_shape=normalized_shape,
            eps=eps,
            elementwise_affine=elementwise_affine
        ).to(device=device, dtype=dtype)
    
        # Forward pass
        output = ln(input_tensor)
    
        # Weak assertions
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == dtype, \
            f"Output dtype {output.dtype} != expected {dtype}"
    
        # 3. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            "Output contains NaN or infinite values"
    
        # 4. Basic property: normalized dimensions should have mean ~0 and std ~1
        # Determine which dimensions to normalize over
        if isinstance(normalized_shape, int):
            normalized_dims = (-1,)
            normalized_size = normalized_shape
        else:
            normalized_dims = tuple(range(-len(normalized_shape), 0))
            normalized_size = math.prod(normalized_shape)
    
        # Reshape for statistics calculation
        # Flatten the normalized dimensions
        batch_dims = shape[:len(shape) - len(normalized_dims)]
        batch_size = math.prod(batch_dims) if batch_dims else 1
    
        output_reshaped = output.reshape(batch_size, normalized_size)
    
        # Check statistics for each batch element
        for i in range(batch_size):
            batch_output = output_reshaped[i]
    
            # Mean should be close to 0
            mean_abs = torch.abs(torch.mean(batch_output))
            assert mean_abs < 0.1, f"Batch element {i} mean too large: {mean_abs}"
    
            # Std should be close to 1 (with eps adjustment)
            std = torch.std(batch_output)
            # Allow some tolerance for numerical precision
            assert 0.9 < std < 1.1, f"Batch element {i} std out of range: {std}"
    
        # 5. Check affine parameters if enabled
        if elementwise_affine:
            assert hasattr(ln, 'weight'), "Elementwise affine enabled but weight parameter missing"
            assert hasattr(ln, 'bias'), "Elementwise affine enabled but bias parameter missing"
    
            # Weight and bias should have normalized_shape
            expected_shape = normalized_shape if isinstance(normalized_shape, tuple) else (normalized_shape,)
            assert ln.weight.shape == expected_shape, \
                f"Weight shape {ln.weight.shape} != expected {expected_shape}"
            assert ln.bias.shape == expected_shape, \
                f"Bias shape {ln.bias.shape} != expected {expected_shape}"
    
        # 6. Compare with functional implementation (weak comparison)
        if elementwise_affine:
            # When elementwise_affine=True, use weight and bias
            try:
                functional_output = F.layer_norm(
                    input_tensor, normalized_shape, ln.weight, ln.bias, eps
                )
                # Basic shape check
                assert functional_output.shape == output.shape
            except Exception as e:
                pytest.fail(f"Functional layer_norm failed: {e}")
        else:
            # When elementwise_affine=False, compare directly
>           functional_output = F.layer_norm(
                input_tensor, normalized_shape, None, None, eps
            )

tests/test_torch_nn_modules_normalization_g2.py:137: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[ 0.2996,  0.2424,  0.2832, -0.2329,  0.6712,  0.7818, -1.7903,
          -1.7484],
         [ 0.1735,  0.266...2],
         [ 1.4654,  0.9461, -1.1149,  1.1146,  0.0309, -1.3457,  1.1755,
          -2.6328]]], dtype=torch.float64)
normalized_shape = 8, weight = None, bias = None, eps = 1e-06

    def layer_norm(
        input: Tensor,
        normalized_shape: List[int],
        weight: Optional[Tensor] = None,
        bias: Optional[Tensor] = None,
        eps: float = 1e-5,
    ) -> Tensor:
        r"""Applies Layer Normalization for last certain number of dimensions.
    
        See :class:`~torch.nn.LayerNorm` for details.
        """
        if has_torch_function_variadic(input, weight, bias):
            return handle_torch_function(
                layer_norm, (input, weight, bias), input, normalized_shape, weight=weight, bias=bias, eps=eps
            )
>       return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
E       TypeError: layer_norm(): argument 'normalized_shape' (position 2) must be tuple of ints, not int

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/functional.py:2515: TypeError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                              Stmts   Miss Branch BrPart  Cover   Missing
---------------------------------------------------------------------------------------------
tests/test_torch_nn_modules_normalization_g2.py      69     26     20      3    58%   24-39, 54, 121, 127-134, 141-159, 173
---------------------------------------------------------------------------------------------
TOTAL                                                69     26     20      3    58%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_normalization_g2.py::test_layernorm_forward_basic[normalized_shape0-1e-05-True-dtype0-cpu-shape0]
FAILED tests/test_torch_nn_modules_normalization_g2.py::test_layernorm_forward_basic[8-1e-06-False-dtype1-cpu-shape1]
2 failed in 0.79s

Error: exit 1