=== Run Tests ===
..F....FFss........F.....FF..F.FFFFFFF                                   [100%]
=================================== FAILURES ===================================
____________________ test_groupnorm_divisibility_exception _____________________

    def test_groupnorm_divisibility_exception():
        """Test that GroupNorm raises ValueError when num_channels not divisible by num_groups"""
        # Test case from test plan: num_groups=3, num_channels=5 (not divisible)
        num_groups = 3
        num_channels = 5
        eps = 1e-5
        affine = True
    
        # Weak assertions: exception type and message
        with pytest.raises(ValueError) as exc_info:
            GroupNorm(
                num_groups=num_groups,
                num_channels=num_channels,
                eps=eps,
                affine=affine
            )
    
        # Check exception type
        exception = exc_info.value
        assert isinstance(exception, ValueError), \
            f"Expected ValueError, got {type(exception).__name__}"
    
        # Check exception message contains relevant information
        error_msg = str(exception).lower()
    
        # Should mention divisibility or groups
        assert any(keyword in error_msg for keyword in
                  ['divisible', 'group', 'channel', 'num_groups', 'num_channels']), \
            f"Error message doesn't mention divisibility: {error_msg}"
    
        # Should mention the actual numbers (3 and 5)
>       assert '3' in error_msg or '5' in error_msg, \
            f"Error message should mention group/channel counts: {error_msg}"
E       AssertionError: Error message should mention group/channel counts: num_channels must be divisible by num_groups
E       assert ('3' in 'num_channels must be divisible by num_groups' or '5' in 'num_channels must be divisible by num_groups')

tests/test_torch_nn_modules_normalization_g1.py:173: AssertionError
___________________ test_groupnorm_device_dtype[dtype0-cpu] ____________________

dtype = torch.float32, device = 'cpu', set_random_seed = 42

    @pytest.mark.parametrize("dtype,device", [
        # Test different data types on CPU
        (torch.float32, "cpu"),
        (torch.float64, "cpu"),
        # Test CUDA if available
        pytest.param(torch.float32, "cuda", marks=pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")),
        pytest.param(torch.float64, "cuda", marks=pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")),
    ])
    def test_groupnorm_device_dtype(dtype, device, set_random_seed):
        """Test GroupNorm with different devices and data types"""
        # Common parameters
        num_groups = 2
        num_channels = 8
        eps = 1e-5
        affine = True
        shape = (2, 8, 16, 16)
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create GroupNorm layer
        gn = GroupNorm(
            num_groups=num_groups,
            num_channels=num_channels,
            eps=eps,
            affine=affine
        ).to(device=device, dtype=dtype)
    
        # Forward pass
        output = gn(input_tensor)
    
        # Weak assertions
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == dtype, \
            f"Output dtype {output.dtype} != expected {dtype}"
    
        # 3. Device assertion
        assert output.device.type == device, \
            f"Output device {output.device.type} != expected {device}"
    
        # 4. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            "Output contains NaN or infinite values"
    
        # 5. Basic normalization property
        batch_size = shape[0]
        channels_per_group = num_channels // num_groups
        spatial_dims = shape[2:]
    
        # Reshape for group-wise statistics
        output_reshaped = output.view(batch_size, num_groups, channels_per_group, *spatial_dims)
    
        # Check that each group is normalized
        for b in range(batch_size):
            for g in range(num_groups):
                group_output = output_reshaped[b, g].flatten()
    
                # Mean should be close to 0
                mean_abs = torch.abs(torch.mean(group_output))
                assert mean_abs < 0.2, f"Group mean too large: {mean_abs}"
    
                # Std should be close to 1
                std = torch.std(group_output)
                assert 0.8 < std < 1.2, f"Group std out of range: {std}"
    
        # 6. Test parameter device/dtype consistency
        if affine:
            # Weight and bias should be on correct device and dtype
            assert gn.weight.device.type == device, \
                f"Weight device {gn.weight.device.type} != expected {device}"
            assert gn.weight.dtype == dtype, \
                f"Weight dtype {gn.weight.dtype} != expected {dtype}"
    
            assert gn.bias.device.type == device, \
                f"Bias device {gn.bias.device.type} != expected {device}"
            assert gn.bias.dtype == dtype, \
                f"Bias dtype {gn.bias.dtype} != expected {dtype}"
    
        # 7. Test with mixed precision (if applicable)
        # Create input with different dtype than layer
        if dtype == torch.float32:
            mixed_input = torch.randn(*shape, dtype=torch.float64, device=device)
        else:
            mixed_input = torch.randn(*shape, dtype=torch.float32, device=device)
    
        # This should work (PyTorch will cast)
>       mixed_output = gn(mixed_input)

tests/test_torch_nn_modules_normalization_g1.py:435: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/normalization.py:273: in forward
    return F.group_norm(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[ 1.1783e-01, -2.7464e-01,  4.6905e-01,  ..., -1.0862e+00,
           -1.5260e-01, -8.5328e-02],
          [...-5.3100e-01,  7.2015e-02, -2.4343e+00,  ..., -1.4235e+00,
            5.6463e-02, -6.9635e-01]]]], dtype=torch.float64)
num_groups = 2
weight = Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)
bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)
eps = 1e-05

    def group_norm(
        input: Tensor, num_groups: int, weight: Optional[Tensor] = None, bias: Optional[Tensor] = None, eps: float = 1e-5
    ) -> Tensor:
        r"""Applies Group Normalization for last certain number of dimensions.
    
        See :class:`~torch.nn.GroupNorm` for details.
        """
        if has_torch_function_variadic(input, weight, bias):
            return handle_torch_function(group_norm, (input, weight, bias,), input, num_groups, weight=weight, bias=bias, eps=eps)
        _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))
>       return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)
E       RuntimeError: expected scalar type Double but found Float

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/functional.py:2528: RuntimeError
___________________ test_groupnorm_device_dtype[dtype1-cpu] ____________________

dtype = torch.float64, device = 'cpu', set_random_seed = 42

    @pytest.mark.parametrize("dtype,device", [
        # Test different data types on CPU
        (torch.float32, "cpu"),
        (torch.float64, "cpu"),
        # Test CUDA if available
        pytest.param(torch.float32, "cuda", marks=pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")),
        pytest.param(torch.float64, "cuda", marks=pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")),
    ])
    def test_groupnorm_device_dtype(dtype, device, set_random_seed):
        """Test GroupNorm with different devices and data types"""
        # Common parameters
        num_groups = 2
        num_channels = 8
        eps = 1e-5
        affine = True
        shape = (2, 8, 16, 16)
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create GroupNorm layer
        gn = GroupNorm(
            num_groups=num_groups,
            num_channels=num_channels,
            eps=eps,
            affine=affine
        ).to(device=device, dtype=dtype)
    
        # Forward pass
        output = gn(input_tensor)
    
        # Weak assertions
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == dtype, \
            f"Output dtype {output.dtype} != expected {dtype}"
    
        # 3. Device assertion
        assert output.device.type == device, \
            f"Output device {output.device.type} != expected {device}"
    
        # 4. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            "Output contains NaN or infinite values"
    
        # 5. Basic normalization property
        batch_size = shape[0]
        channels_per_group = num_channels // num_groups
        spatial_dims = shape[2:]
    
        # Reshape for group-wise statistics
        output_reshaped = output.view(batch_size, num_groups, channels_per_group, *spatial_dims)
    
        # Check that each group is normalized
        for b in range(batch_size):
            for g in range(num_groups):
                group_output = output_reshaped[b, g].flatten()
    
                # Mean should be close to 0
                mean_abs = torch.abs(torch.mean(group_output))
                assert mean_abs < 0.2, f"Group mean too large: {mean_abs}"
    
                # Std should be close to 1
                std = torch.std(group_output)
                assert 0.8 < std < 1.2, f"Group std out of range: {std}"
    
        # 6. Test parameter device/dtype consistency
        if affine:
            # Weight and bias should be on correct device and dtype
            assert gn.weight.device.type == device, \
                f"Weight device {gn.weight.device.type} != expected {device}"
            assert gn.weight.dtype == dtype, \
                f"Weight dtype {gn.weight.dtype} != expected {dtype}"
    
            assert gn.bias.device.type == device, \
                f"Bias device {gn.bias.device.type} != expected {device}"
            assert gn.bias.dtype == dtype, \
                f"Bias dtype {gn.bias.dtype} != expected {dtype}"
    
        # 7. Test with mixed precision (if applicable)
        # Create input with different dtype than layer
        if dtype == torch.float32:
            mixed_input = torch.randn(*shape, dtype=torch.float64, device=device)
        else:
            mixed_input = torch.randn(*shape, dtype=torch.float32, device=device)
    
        # This should work (PyTorch will cast)
>       mixed_output = gn(mixed_input)

tests/test_torch_nn_modules_normalization_g1.py:435: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/normalization.py:273: in forward
    return F.group_norm(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[-0.7978,  1.0261,  1.1465,  ...,  0.6885, -0.5113,  0.8036],
          [ 1.5612, -0.5414, -0.9858,  ...,  1...,  1.0505,  ..., -0.7464,  1.3066,  0.4343],
          [-0.3333, -0.1604, -1.8263,  ..., -0.2558,  0.7870,  0.9924]]]])
num_groups = 2
weight = Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64,
       requires_grad=True)
bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64,
       requires_grad=True)
eps = 1e-05

    def group_norm(
        input: Tensor, num_groups: int, weight: Optional[Tensor] = None, bias: Optional[Tensor] = None, eps: float = 1e-5
    ) -> Tensor:
        r"""Applies Group Normalization for last certain number of dimensions.
    
        See :class:`~torch.nn.GroupNorm` for details.
        """
        if has_torch_function_variadic(input, weight, bias):
            return handle_torch_function(group_norm, (input, weight, bias,), input, num_groups, weight=weight, bias=bias, eps=eps)
        _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))
>       return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)
E       RuntimeError: expected scalar type Float but found Double

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/functional.py:2528: RuntimeError
_______________________ test_layernorm_exception_shapes ________________________

    def test_layernorm_exception_shapes():
        """Test LayerNorm with invalid shapes and parameters"""
        # Test 1: normalized_shape doesn't match input shape
        normalized_shape = [8, 8]
        eps = 1e-5
        elementwise_affine = True
    
        # Create LayerNorm layer
        ln = LayerNorm(
            normalized_shape=normalized_shape,
            eps=eps,
            elementwise_affine=elementwise_affine
        )
    
        # Test with input that has wrong last dimensions
        # LayerNorm expects last 2 dimensions to be 8x8, but we give 4x4
>       wrong_shape_input = torch.randn(2, 4, 4, 4)
E       UnboundLocalError: local variable 'torch' referenced before assignment

tests/test_torch_nn_modules_normalization_g2.py:354: UnboundLocalError
_ test_localresponsenorm_boundary_values[3-0.0001-0.75-1.0-dtype1-cpu-shape1-small_size] _

size = 3, alpha = 0.0001, beta = 0.75, k = 1.0, dtype = torch.float32
device = 'cpu', shape = (2, 8, 8, 8), test_type = 'small_size'
set_random_seed = 42

    @pytest.mark.parametrize("size,alpha,beta,k,dtype,device,shape,test_type", [
        # Test boundary values for size
        (1, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "min_size"),
        (3, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "small_size"),
        (15, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_size"),
        # Test boundary values for alpha
        (5, 0.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_alpha"),
        (5, 1e-10, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "tiny_alpha"),
        (5, 1.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_alpha"),
        # Test boundary values for beta
        (5, 1e-4, 0.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_beta"),
        (5, 1e-4, 0.5, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "small_beta"),
        (5, 1e-4, 2.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_beta"),
        # Test boundary values for k
        (5, 1e-4, 0.75, 0.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_k"),
        (5, 1e-4, 0.75, 0.1, torch.float32, "cpu", (2, 16, 8, 8), "small_k"),
        (5, 1e-4, 0.75, 10.0, torch.float32, "cpu", (2, 16, 8, 8), "large_k"),
        # Test with different data types
        (5, 1e-4, 0.75, 1.0, torch.float64, "cpu", (2, 16, 8, 8), "float64"),
        # Test with small batch size
        (5, 1e-4, 0.75, 1.0, torch.float32, "cpu", (1, 16, 8, 8), "batch_size_1"),
    ])
    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):
        """Test LocalResponseNorm with boundary values for parameters"""
        # Skip CUDA tests if device not available
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("CUDA not available")
    
        # Validate parameters based on test type
        if test_type == "min_size":
            assert size == 1, "Test requires size=1"
        elif test_type == "large_size":
            # Size should be odd and <= number of channels
            assert size % 2 == 1, "Size should be odd"
            assert size <= shape[1], f"Size {size} should not exceed channels {shape[1]}"
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create LocalResponseNorm layer
        lrn = LocalResponseNorm(
            size=size,
            alpha=alpha,
            beta=beta,
            k=k
        ).to(device=device)
    
        # Forward pass
        output = lrn(input_tensor)
    
        # Basic assertions for all boundary tests
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == input_tensor.dtype, \
            f"Output dtype {output.dtype} != input dtype {input_tensor.dtype}"
    
        # 3. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            f"Output contains NaN or infinite values for test_type={test_type}"
    
        # 4. Sign preservation
        sign_preserved = torch.all((input_tensor * output) >= 0)
        assert sign_preserved, f"Sign not preserved for test_type={test_type}"
    
        # Test-specific assertions
        if test_type == "zero_alpha":
            # With alpha=0, normalization should have minimal effect
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero alpha test failed: diff={diff}"
    
        elif test_type == "zero_beta":
            # With beta=0, output should be input / (k^0) = input / 1 = input
            expected = input_tensor
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero beta test failed: diff={diff}"
    
        elif test_type == "zero_k":
            # With k=0, need to handle division by zero carefully
            # PyTorch should handle this, but output might have large values
            # Just verify it doesn't crash and produces finite values
            assert torch.all(torch.isfinite(output)), "Zero k test produced non-finite values"
    
        elif test_type == "large_alpha":
            # With large alpha, normalization effect is stronger
            # Output magnitude should be significantly reduced
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
            reduction_ratio = output_norm / input_norm
    
            # With alpha=1.0, reduction should be noticeable
            assert reduction_ratio < 0.9, \
                f"Large alpha should reduce norm: reduction_ratio={reduction_ratio}"
    
        elif test_type == "large_beta":
            # With large beta (2.0), normalization is more aggressive
            # Output should have smaller magnitude
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
    
            # Compare with beta=0.75 case
            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)
            normal_output = lrn_normal(input_tensor)
            normal_norm = torch.norm(normal_output)
    
            # Larger beta should produce smaller output
            assert output_norm < normal_norm * 1.1, \
                f"Large beta should reduce output more: beta={beta} vs 0.75"
    
        elif test_type == "min_size":
            # With size=1, only self-channel is considered
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.1, f"Size=1 test failed: diff={diff}"
    
        # Test with all positive input
        positive_input = torch.abs(input_tensor) + 0.1
        positive_output = lrn(positive_input)
        assert torch.all(positive_output >= 0), \
            f"Positive input should produce non-negative output for test_type={test_type}"
    
        # Test with all negative input
        negative_input = -torch.abs(input_tensor) - 0.1
        negative_output = lrn(negative_input)
        assert torch.all(negative_output <= 0), \
            f"Negative input should produce non-positive output for test_type={test_type}"
    
        # Test with mixed positive/negative input
        # Should preserve signs
        mixed_input = input_tensor  # Already has mixed signs from randn
        mixed_output = lrn(mixed_input)
    
        # Check sign preservation element-wise
        sign_match = (mixed_input >= 0) == (mixed_output >= 0)
        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()
        assert sign_match_ratio > 0.99, \
            f"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}"
    
        # Test with constant input
        constant_value = 5.0
        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)
        constant_output = lrn(constant_input)
    
        # For constant input, output should be scaled version of input
        # All values should be equal
        output_std = torch.std(constant_output)
>       assert output_std < 1e-5, \
            f"Constant input should produce constant output for test_type={test_type}: std={output_std}"
E       AssertionError: Constant input should produce constant output for test_type=small_size: std=0.0013487441465258598
E       assert tensor(0.0013) < 1e-05

tests/test_torch_nn_modules_normalization_g3.py:438: AssertionError
_ test_localresponsenorm_boundary_values[15-0.0001-0.75-1.0-dtype2-cpu-shape2-large_size] _

size = 15, alpha = 0.0001, beta = 0.75, k = 1.0, dtype = torch.float32
device = 'cpu', shape = (2, 16, 8, 8), test_type = 'large_size'
set_random_seed = 42

    @pytest.mark.parametrize("size,alpha,beta,k,dtype,device,shape,test_type", [
        # Test boundary values for size
        (1, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "min_size"),
        (3, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "small_size"),
        (15, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_size"),
        # Test boundary values for alpha
        (5, 0.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_alpha"),
        (5, 1e-10, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "tiny_alpha"),
        (5, 1.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_alpha"),
        # Test boundary values for beta
        (5, 1e-4, 0.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_beta"),
        (5, 1e-4, 0.5, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "small_beta"),
        (5, 1e-4, 2.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_beta"),
        # Test boundary values for k
        (5, 1e-4, 0.75, 0.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_k"),
        (5, 1e-4, 0.75, 0.1, torch.float32, "cpu", (2, 16, 8, 8), "small_k"),
        (5, 1e-4, 0.75, 10.0, torch.float32, "cpu", (2, 16, 8, 8), "large_k"),
        # Test with different data types
        (5, 1e-4, 0.75, 1.0, torch.float64, "cpu", (2, 16, 8, 8), "float64"),
        # Test with small batch size
        (5, 1e-4, 0.75, 1.0, torch.float32, "cpu", (1, 16, 8, 8), "batch_size_1"),
    ])
    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):
        """Test LocalResponseNorm with boundary values for parameters"""
        # Skip CUDA tests if device not available
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("CUDA not available")
    
        # Validate parameters based on test type
        if test_type == "min_size":
            assert size == 1, "Test requires size=1"
        elif test_type == "large_size":
            # Size should be odd and <= number of channels
            assert size % 2 == 1, "Size should be odd"
            assert size <= shape[1], f"Size {size} should not exceed channels {shape[1]}"
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create LocalResponseNorm layer
        lrn = LocalResponseNorm(
            size=size,
            alpha=alpha,
            beta=beta,
            k=k
        ).to(device=device)
    
        # Forward pass
        output = lrn(input_tensor)
    
        # Basic assertions for all boundary tests
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == input_tensor.dtype, \
            f"Output dtype {output.dtype} != input dtype {input_tensor.dtype}"
    
        # 3. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            f"Output contains NaN or infinite values for test_type={test_type}"
    
        # 4. Sign preservation
        sign_preserved = torch.all((input_tensor * output) >= 0)
        assert sign_preserved, f"Sign not preserved for test_type={test_type}"
    
        # Test-specific assertions
        if test_type == "zero_alpha":
            # With alpha=0, normalization should have minimal effect
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero alpha test failed: diff={diff}"
    
        elif test_type == "zero_beta":
            # With beta=0, output should be input / (k^0) = input / 1 = input
            expected = input_tensor
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero beta test failed: diff={diff}"
    
        elif test_type == "zero_k":
            # With k=0, need to handle division by zero carefully
            # PyTorch should handle this, but output might have large values
            # Just verify it doesn't crash and produces finite values
            assert torch.all(torch.isfinite(output)), "Zero k test produced non-finite values"
    
        elif test_type == "large_alpha":
            # With large alpha, normalization effect is stronger
            # Output magnitude should be significantly reduced
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
            reduction_ratio = output_norm / input_norm
    
            # With alpha=1.0, reduction should be noticeable
            assert reduction_ratio < 0.9, \
                f"Large alpha should reduce norm: reduction_ratio={reduction_ratio}"
    
        elif test_type == "large_beta":
            # With large beta (2.0), normalization is more aggressive
            # Output should have smaller magnitude
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
    
            # Compare with beta=0.75 case
            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)
            normal_output = lrn_normal(input_tensor)
            normal_norm = torch.norm(normal_output)
    
            # Larger beta should produce smaller output
            assert output_norm < normal_norm * 1.1, \
                f"Large beta should reduce output more: beta={beta} vs 0.75"
    
        elif test_type == "min_size":
            # With size=1, only self-channel is considered
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.1, f"Size=1 test failed: diff={diff}"
    
        # Test with all positive input
        positive_input = torch.abs(input_tensor) + 0.1
        positive_output = lrn(positive_input)
        assert torch.all(positive_output >= 0), \
            f"Positive input should produce non-negative output for test_type={test_type}"
    
        # Test with all negative input
        negative_input = -torch.abs(input_tensor) - 0.1
        negative_output = lrn(negative_input)
        assert torch.all(negative_output <= 0), \
            f"Negative input should produce non-positive output for test_type={test_type}"
    
        # Test with mixed positive/negative input
        # Should preserve signs
        mixed_input = input_tensor  # Already has mixed signs from randn
        mixed_output = lrn(mixed_input)
    
        # Check sign preservation element-wise
        sign_match = (mixed_input >= 0) == (mixed_output >= 0)
        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()
        assert sign_match_ratio > 0.99, \
            f"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}"
    
        # Test with constant input
        constant_value = 5.0
        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)
        constant_output = lrn(constant_input)
    
        # For constant input, output should be scaled version of input
        # All values should be equal
        output_std = torch.std(constant_output)
>       assert output_std < 1e-5, \
            f"Constant input should produce constant output for test_type={test_type}: std={output_std}"
E       AssertionError: Constant input should produce constant output for test_type=large_size: std=0.0014275847934186459
E       assert tensor(0.0014) < 1e-05

tests/test_torch_nn_modules_normalization_g3.py:438: AssertionError
_ test_localresponsenorm_boundary_values[5-1.0-0.75-1.0-dtype5-cpu-shape5-large_alpha] _

size = 5, alpha = 1.0, beta = 0.75, k = 1.0, dtype = torch.float32
device = 'cpu', shape = (2, 16, 8, 8), test_type = 'large_alpha'
set_random_seed = 42

    @pytest.mark.parametrize("size,alpha,beta,k,dtype,device,shape,test_type", [
        # Test boundary values for size
        (1, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "min_size"),
        (3, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "small_size"),
        (15, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_size"),
        # Test boundary values for alpha
        (5, 0.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_alpha"),
        (5, 1e-10, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "tiny_alpha"),
        (5, 1.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_alpha"),
        # Test boundary values for beta
        (5, 1e-4, 0.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_beta"),
        (5, 1e-4, 0.5, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "small_beta"),
        (5, 1e-4, 2.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_beta"),
        # Test boundary values for k
        (5, 1e-4, 0.75, 0.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_k"),
        (5, 1e-4, 0.75, 0.1, torch.float32, "cpu", (2, 16, 8, 8), "small_k"),
        (5, 1e-4, 0.75, 10.0, torch.float32, "cpu", (2, 16, 8, 8), "large_k"),
        # Test with different data types
        (5, 1e-4, 0.75, 1.0, torch.float64, "cpu", (2, 16, 8, 8), "float64"),
        # Test with small batch size
        (5, 1e-4, 0.75, 1.0, torch.float32, "cpu", (1, 16, 8, 8), "batch_size_1"),
    ])
    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):
        """Test LocalResponseNorm with boundary values for parameters"""
        # Skip CUDA tests if device not available
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("CUDA not available")
    
        # Validate parameters based on test type
        if test_type == "min_size":
            assert size == 1, "Test requires size=1"
        elif test_type == "large_size":
            # Size should be odd and <= number of channels
            assert size % 2 == 1, "Size should be odd"
            assert size <= shape[1], f"Size {size} should not exceed channels {shape[1]}"
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create LocalResponseNorm layer
        lrn = LocalResponseNorm(
            size=size,
            alpha=alpha,
            beta=beta,
            k=k
        ).to(device=device)
    
        # Forward pass
        output = lrn(input_tensor)
    
        # Basic assertions for all boundary tests
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == input_tensor.dtype, \
            f"Output dtype {output.dtype} != input dtype {input_tensor.dtype}"
    
        # 3. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            f"Output contains NaN or infinite values for test_type={test_type}"
    
        # 4. Sign preservation
        sign_preserved = torch.all((input_tensor * output) >= 0)
        assert sign_preserved, f"Sign not preserved for test_type={test_type}"
    
        # Test-specific assertions
        if test_type == "zero_alpha":
            # With alpha=0, normalization should have minimal effect
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero alpha test failed: diff={diff}"
    
        elif test_type == "zero_beta":
            # With beta=0, output should be input / (k^0) = input / 1 = input
            expected = input_tensor
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero beta test failed: diff={diff}"
    
        elif test_type == "zero_k":
            # With k=0, need to handle division by zero carefully
            # PyTorch should handle this, but output might have large values
            # Just verify it doesn't crash and produces finite values
            assert torch.all(torch.isfinite(output)), "Zero k test produced non-finite values"
    
        elif test_type == "large_alpha":
            # With large alpha, normalization effect is stronger
            # Output magnitude should be significantly reduced
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
            reduction_ratio = output_norm / input_norm
    
            # With alpha=1.0, reduction should be noticeable
            assert reduction_ratio < 0.9, \
                f"Large alpha should reduce norm: reduction_ratio={reduction_ratio}"
    
        elif test_type == "large_beta":
            # With large beta (2.0), normalization is more aggressive
            # Output should have smaller magnitude
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
    
            # Compare with beta=0.75 case
            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)
            normal_output = lrn_normal(input_tensor)
            normal_norm = torch.norm(normal_output)
    
            # Larger beta should produce smaller output
            assert output_norm < normal_norm * 1.1, \
                f"Large beta should reduce output more: beta={beta} vs 0.75"
    
        elif test_type == "min_size":
            # With size=1, only self-channel is considered
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.1, f"Size=1 test failed: diff={diff}"
    
        # Test with all positive input
        positive_input = torch.abs(input_tensor) + 0.1
        positive_output = lrn(positive_input)
        assert torch.all(positive_output >= 0), \
            f"Positive input should produce non-negative output for test_type={test_type}"
    
        # Test with all negative input
        negative_input = -torch.abs(input_tensor) - 0.1
        negative_output = lrn(negative_input)
        assert torch.all(negative_output <= 0), \
            f"Negative input should produce non-positive output for test_type={test_type}"
    
        # Test with mixed positive/negative input
        # Should preserve signs
        mixed_input = input_tensor  # Already has mixed signs from randn
        mixed_output = lrn(mixed_input)
    
        # Check sign preservation element-wise
        sign_match = (mixed_input >= 0) == (mixed_output >= 0)
        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()
        assert sign_match_ratio > 0.99, \
            f"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}"
    
        # Test with constant input
        constant_value = 5.0
        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)
        constant_output = lrn(constant_input)
    
        # For constant input, output should be scaled version of input
        # All values should be equal
        output_std = torch.std(constant_output)
>       assert output_std < 1e-5, \
            f"Constant input should produce constant output for test_type={test_type}: std={output_std}"
E       AssertionError: Constant input should produce constant output for test_type=large_alpha: std=0.06445513665676117
E       assert tensor(0.0645) < 1e-05

tests/test_torch_nn_modules_normalization_g3.py:438: AssertionError
_ test_localresponsenorm_boundary_values[5-0.0001-0.5-1.0-dtype7-cpu-shape7-small_beta] _

size = 5, alpha = 0.0001, beta = 0.5, k = 1.0, dtype = torch.float32
device = 'cpu', shape = (2, 16, 8, 8), test_type = 'small_beta'
set_random_seed = 42

    @pytest.mark.parametrize("size,alpha,beta,k,dtype,device,shape,test_type", [
        # Test boundary values for size
        (1, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "min_size"),
        (3, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "small_size"),
        (15, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_size"),
        # Test boundary values for alpha
        (5, 0.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_alpha"),
        (5, 1e-10, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "tiny_alpha"),
        (5, 1.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_alpha"),
        # Test boundary values for beta
        (5, 1e-4, 0.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_beta"),
        (5, 1e-4, 0.5, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "small_beta"),
        (5, 1e-4, 2.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_beta"),
        # Test boundary values for k
        (5, 1e-4, 0.75, 0.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_k"),
        (5, 1e-4, 0.75, 0.1, torch.float32, "cpu", (2, 16, 8, 8), "small_k"),
        (5, 1e-4, 0.75, 10.0, torch.float32, "cpu", (2, 16, 8, 8), "large_k"),
        # Test with different data types
        (5, 1e-4, 0.75, 1.0, torch.float64, "cpu", (2, 16, 8, 8), "float64"),
        # Test with small batch size
        (5, 1e-4, 0.75, 1.0, torch.float32, "cpu", (1, 16, 8, 8), "batch_size_1"),
    ])
    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):
        """Test LocalResponseNorm with boundary values for parameters"""
        # Skip CUDA tests if device not available
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("CUDA not available")
    
        # Validate parameters based on test type
        if test_type == "min_size":
            assert size == 1, "Test requires size=1"
        elif test_type == "large_size":
            # Size should be odd and <= number of channels
            assert size % 2 == 1, "Size should be odd"
            assert size <= shape[1], f"Size {size} should not exceed channels {shape[1]}"
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create LocalResponseNorm layer
        lrn = LocalResponseNorm(
            size=size,
            alpha=alpha,
            beta=beta,
            k=k
        ).to(device=device)
    
        # Forward pass
        output = lrn(input_tensor)
    
        # Basic assertions for all boundary tests
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == input_tensor.dtype, \
            f"Output dtype {output.dtype} != input dtype {input_tensor.dtype}"
    
        # 3. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            f"Output contains NaN or infinite values for test_type={test_type}"
    
        # 4. Sign preservation
        sign_preserved = torch.all((input_tensor * output) >= 0)
        assert sign_preserved, f"Sign not preserved for test_type={test_type}"
    
        # Test-specific assertions
        if test_type == "zero_alpha":
            # With alpha=0, normalization should have minimal effect
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero alpha test failed: diff={diff}"
    
        elif test_type == "zero_beta":
            # With beta=0, output should be input / (k^0) = input / 1 = input
            expected = input_tensor
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero beta test failed: diff={diff}"
    
        elif test_type == "zero_k":
            # With k=0, need to handle division by zero carefully
            # PyTorch should handle this, but output might have large values
            # Just verify it doesn't crash and produces finite values
            assert torch.all(torch.isfinite(output)), "Zero k test produced non-finite values"
    
        elif test_type == "large_alpha":
            # With large alpha, normalization effect is stronger
            # Output magnitude should be significantly reduced
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
            reduction_ratio = output_norm / input_norm
    
            # With alpha=1.0, reduction should be noticeable
            assert reduction_ratio < 0.9, \
                f"Large alpha should reduce norm: reduction_ratio={reduction_ratio}"
    
        elif test_type == "large_beta":
            # With large beta (2.0), normalization is more aggressive
            # Output should have smaller magnitude
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
    
            # Compare with beta=0.75 case
            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)
            normal_output = lrn_normal(input_tensor)
            normal_norm = torch.norm(normal_output)
    
            # Larger beta should produce smaller output
            assert output_norm < normal_norm * 1.1, \
                f"Large beta should reduce output more: beta={beta} vs 0.75"
    
        elif test_type == "min_size":
            # With size=1, only self-channel is considered
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.1, f"Size=1 test failed: diff={diff}"
    
        # Test with all positive input
        positive_input = torch.abs(input_tensor) + 0.1
        positive_output = lrn(positive_input)
        assert torch.all(positive_output >= 0), \
            f"Positive input should produce non-negative output for test_type={test_type}"
    
        # Test with all negative input
        negative_input = -torch.abs(input_tensor) - 0.1
        negative_output = lrn(negative_input)
        assert torch.all(negative_output <= 0), \
            f"Negative input should produce non-positive output for test_type={test_type}"
    
        # Test with mixed positive/negative input
        # Should preserve signs
        mixed_input = input_tensor  # Already has mixed signs from randn
        mixed_output = lrn(mixed_input)
    
        # Check sign preservation element-wise
        sign_match = (mixed_input >= 0) == (mixed_output >= 0)
        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()
        assert sign_match_ratio > 0.99, \
            f"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}"
    
        # Test with constant input
        constant_value = 5.0
        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)
        constant_output = lrn(constant_input)
    
        # For constant input, output should be scaled version of input
        # All values should be equal
        output_std = torch.std(constant_output)
>       assert output_std < 1e-5, \
            f"Constant input should produce constant output for test_type={test_type}: std={output_std}"
E       AssertionError: Constant input should produce constant output for test_type=small_beta: std=0.0008674598066136241
E       assert tensor(0.0009) < 1e-05

tests/test_torch_nn_modules_normalization_g3.py:438: AssertionError
_ test_localresponsenorm_boundary_values[5-0.0001-2.0-1.0-dtype8-cpu-shape8-large_beta] _

size = 5, alpha = 0.0001, beta = 2.0, k = 1.0, dtype = torch.float32
device = 'cpu', shape = (2, 16, 8, 8), test_type = 'large_beta'
set_random_seed = 42

    @pytest.mark.parametrize("size,alpha,beta,k,dtype,device,shape,test_type", [
        # Test boundary values for size
        (1, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "min_size"),
        (3, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "small_size"),
        (15, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_size"),
        # Test boundary values for alpha
        (5, 0.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_alpha"),
        (5, 1e-10, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "tiny_alpha"),
        (5, 1.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_alpha"),
        # Test boundary values for beta
        (5, 1e-4, 0.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_beta"),
        (5, 1e-4, 0.5, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "small_beta"),
        (5, 1e-4, 2.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_beta"),
        # Test boundary values for k
        (5, 1e-4, 0.75, 0.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_k"),
        (5, 1e-4, 0.75, 0.1, torch.float32, "cpu", (2, 16, 8, 8), "small_k"),
        (5, 1e-4, 0.75, 10.0, torch.float32, "cpu", (2, 16, 8, 8), "large_k"),
        # Test with different data types
        (5, 1e-4, 0.75, 1.0, torch.float64, "cpu", (2, 16, 8, 8), "float64"),
        # Test with small batch size
        (5, 1e-4, 0.75, 1.0, torch.float32, "cpu", (1, 16, 8, 8), "batch_size_1"),
    ])
    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):
        """Test LocalResponseNorm with boundary values for parameters"""
        # Skip CUDA tests if device not available
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("CUDA not available")
    
        # Validate parameters based on test type
        if test_type == "min_size":
            assert size == 1, "Test requires size=1"
        elif test_type == "large_size":
            # Size should be odd and <= number of channels
            assert size % 2 == 1, "Size should be odd"
            assert size <= shape[1], f"Size {size} should not exceed channels {shape[1]}"
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create LocalResponseNorm layer
        lrn = LocalResponseNorm(
            size=size,
            alpha=alpha,
            beta=beta,
            k=k
        ).to(device=device)
    
        # Forward pass
        output = lrn(input_tensor)
    
        # Basic assertions for all boundary tests
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == input_tensor.dtype, \
            f"Output dtype {output.dtype} != input dtype {input_tensor.dtype}"
    
        # 3. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            f"Output contains NaN or infinite values for test_type={test_type}"
    
        # 4. Sign preservation
        sign_preserved = torch.all((input_tensor * output) >= 0)
        assert sign_preserved, f"Sign not preserved for test_type={test_type}"
    
        # Test-specific assertions
        if test_type == "zero_alpha":
            # With alpha=0, normalization should have minimal effect
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero alpha test failed: diff={diff}"
    
        elif test_type == "zero_beta":
            # With beta=0, output should be input / (k^0) = input / 1 = input
            expected = input_tensor
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero beta test failed: diff={diff}"
    
        elif test_type == "zero_k":
            # With k=0, need to handle division by zero carefully
            # PyTorch should handle this, but output might have large values
            # Just verify it doesn't crash and produces finite values
            assert torch.all(torch.isfinite(output)), "Zero k test produced non-finite values"
    
        elif test_type == "large_alpha":
            # With large alpha, normalization effect is stronger
            # Output magnitude should be significantly reduced
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
            reduction_ratio = output_norm / input_norm
    
            # With alpha=1.0, reduction should be noticeable
            assert reduction_ratio < 0.9, \
                f"Large alpha should reduce norm: reduction_ratio={reduction_ratio}"
    
        elif test_type == "large_beta":
            # With large beta (2.0), normalization is more aggressive
            # Output should have smaller magnitude
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
    
            # Compare with beta=0.75 case
            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)
            normal_output = lrn_normal(input_tensor)
            normal_norm = torch.norm(normal_output)
    
            # Larger beta should produce smaller output
            assert output_norm < normal_norm * 1.1, \
                f"Large beta should reduce output more: beta={beta} vs 0.75"
    
        elif test_type == "min_size":
            # With size=1, only self-channel is considered
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.1, f"Size=1 test failed: diff={diff}"
    
        # Test with all positive input
        positive_input = torch.abs(input_tensor) + 0.1
        positive_output = lrn(positive_input)
        assert torch.all(positive_output >= 0), \
            f"Positive input should produce non-negative output for test_type={test_type}"
    
        # Test with all negative input
        negative_input = -torch.abs(input_tensor) - 0.1
        negative_output = lrn(negative_input)
        assert torch.all(negative_output <= 0), \
            f"Negative input should produce non-positive output for test_type={test_type}"
    
        # Test with mixed positive/negative input
        # Should preserve signs
        mixed_input = input_tensor  # Already has mixed signs from randn
        mixed_output = lrn(mixed_input)
    
        # Check sign preservation element-wise
        sign_match = (mixed_input >= 0) == (mixed_output >= 0)
        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()
        assert sign_match_ratio > 0.99, \
            f"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}"
    
        # Test with constant input
        constant_value = 5.0
        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)
        constant_output = lrn(constant_input)
    
        # For constant input, output should be scaled version of input
        # All values should be equal
        output_std = torch.std(constant_output)
>       assert output_std < 1e-5, \
            f"Constant input should produce constant output for test_type={test_type}: std={output_std}"
E       AssertionError: Constant input should produce constant output for test_type=large_beta: std=0.0034596342593431473
E       assert tensor(0.0035) < 1e-05

tests/test_torch_nn_modules_normalization_g3.py:438: AssertionError
_ test_localresponsenorm_boundary_values[5-0.0001-0.75-0.0-dtype9-cpu-shape9-zero_k] _

size = 5, alpha = 0.0001, beta = 0.75, k = 0.0, dtype = torch.float32
device = 'cpu', shape = (2, 16, 8, 8), test_type = 'zero_k'
set_random_seed = 42

    @pytest.mark.parametrize("size,alpha,beta,k,dtype,device,shape,test_type", [
        # Test boundary values for size
        (1, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "min_size"),
        (3, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "small_size"),
        (15, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_size"),
        # Test boundary values for alpha
        (5, 0.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_alpha"),
        (5, 1e-10, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "tiny_alpha"),
        (5, 1.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_alpha"),
        # Test boundary values for beta
        (5, 1e-4, 0.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_beta"),
        (5, 1e-4, 0.5, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "small_beta"),
        (5, 1e-4, 2.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_beta"),
        # Test boundary values for k
        (5, 1e-4, 0.75, 0.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_k"),
        (5, 1e-4, 0.75, 0.1, torch.float32, "cpu", (2, 16, 8, 8), "small_k"),
        (5, 1e-4, 0.75, 10.0, torch.float32, "cpu", (2, 16, 8, 8), "large_k"),
        # Test with different data types
        (5, 1e-4, 0.75, 1.0, torch.float64, "cpu", (2, 16, 8, 8), "float64"),
        # Test with small batch size
        (5, 1e-4, 0.75, 1.0, torch.float32, "cpu", (1, 16, 8, 8), "batch_size_1"),
    ])
    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):
        """Test LocalResponseNorm with boundary values for parameters"""
        # Skip CUDA tests if device not available
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("CUDA not available")
    
        # Validate parameters based on test type
        if test_type == "min_size":
            assert size == 1, "Test requires size=1"
        elif test_type == "large_size":
            # Size should be odd and <= number of channels
            assert size % 2 == 1, "Size should be odd"
            assert size <= shape[1], f"Size {size} should not exceed channels {shape[1]}"
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create LocalResponseNorm layer
        lrn = LocalResponseNorm(
            size=size,
            alpha=alpha,
            beta=beta,
            k=k
        ).to(device=device)
    
        # Forward pass
        output = lrn(input_tensor)
    
        # Basic assertions for all boundary tests
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == input_tensor.dtype, \
            f"Output dtype {output.dtype} != input dtype {input_tensor.dtype}"
    
        # 3. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            f"Output contains NaN or infinite values for test_type={test_type}"
    
        # 4. Sign preservation
        sign_preserved = torch.all((input_tensor * output) >= 0)
        assert sign_preserved, f"Sign not preserved for test_type={test_type}"
    
        # Test-specific assertions
        if test_type == "zero_alpha":
            # With alpha=0, normalization should have minimal effect
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero alpha test failed: diff={diff}"
    
        elif test_type == "zero_beta":
            # With beta=0, output should be input / (k^0) = input / 1 = input
            expected = input_tensor
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero beta test failed: diff={diff}"
    
        elif test_type == "zero_k":
            # With k=0, need to handle division by zero carefully
            # PyTorch should handle this, but output might have large values
            # Just verify it doesn't crash and produces finite values
            assert torch.all(torch.isfinite(output)), "Zero k test produced non-finite values"
    
        elif test_type == "large_alpha":
            # With large alpha, normalization effect is stronger
            # Output magnitude should be significantly reduced
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
            reduction_ratio = output_norm / input_norm
    
            # With alpha=1.0, reduction should be noticeable
            assert reduction_ratio < 0.9, \
                f"Large alpha should reduce norm: reduction_ratio={reduction_ratio}"
    
        elif test_type == "large_beta":
            # With large beta (2.0), normalization is more aggressive
            # Output should have smaller magnitude
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
    
            # Compare with beta=0.75 case
            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)
            normal_output = lrn_normal(input_tensor)
            normal_norm = torch.norm(normal_output)
    
            # Larger beta should produce smaller output
            assert output_norm < normal_norm * 1.1, \
                f"Large beta should reduce output more: beta={beta} vs 0.75"
    
        elif test_type == "min_size":
            # With size=1, only self-channel is considered
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.1, f"Size=1 test failed: diff={diff}"
    
        # Test with all positive input
        positive_input = torch.abs(input_tensor) + 0.1
        positive_output = lrn(positive_input)
        assert torch.all(positive_output >= 0), \
            f"Positive input should produce non-negative output for test_type={test_type}"
    
        # Test with all negative input
        negative_input = -torch.abs(input_tensor) - 0.1
        negative_output = lrn(negative_input)
        assert torch.all(negative_output <= 0), \
            f"Negative input should produce non-positive output for test_type={test_type}"
    
        # Test with mixed positive/negative input
        # Should preserve signs
        mixed_input = input_tensor  # Already has mixed signs from randn
        mixed_output = lrn(mixed_input)
    
        # Check sign preservation element-wise
        sign_match = (mixed_input >= 0) == (mixed_output >= 0)
        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()
        assert sign_match_ratio > 0.99, \
            f"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}"
    
        # Test with constant input
        constant_value = 5.0
        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)
        constant_output = lrn(constant_input)
    
        # For constant input, output should be scaled version of input
        # All values should be equal
        output_std = torch.std(constant_output)
>       assert output_std < 1e-5, \
            f"Constant input should produce constant output for test_type={test_type}: std={output_std}"
E       AssertionError: Constant input should produce constant output for test_type=zero_k: std=70.45955657958984
E       assert tensor(70.4596) < 1e-05

tests/test_torch_nn_modules_normalization_g3.py:438: AssertionError
_ test_localresponsenorm_boundary_values[5-0.0001-0.75-0.1-dtype10-cpu-shape10-small_k] _

size = 5, alpha = 0.0001, beta = 0.75, k = 0.1, dtype = torch.float32
device = 'cpu', shape = (2, 16, 8, 8), test_type = 'small_k'
set_random_seed = 42

    @pytest.mark.parametrize("size,alpha,beta,k,dtype,device,shape,test_type", [
        # Test boundary values for size
        (1, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "min_size"),
        (3, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "small_size"),
        (15, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_size"),
        # Test boundary values for alpha
        (5, 0.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_alpha"),
        (5, 1e-10, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "tiny_alpha"),
        (5, 1.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_alpha"),
        # Test boundary values for beta
        (5, 1e-4, 0.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_beta"),
        (5, 1e-4, 0.5, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "small_beta"),
        (5, 1e-4, 2.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_beta"),
        # Test boundary values for k
        (5, 1e-4, 0.75, 0.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_k"),
        (5, 1e-4, 0.75, 0.1, torch.float32, "cpu", (2, 16, 8, 8), "small_k"),
        (5, 1e-4, 0.75, 10.0, torch.float32, "cpu", (2, 16, 8, 8), "large_k"),
        # Test with different data types
        (5, 1e-4, 0.75, 1.0, torch.float64, "cpu", (2, 16, 8, 8), "float64"),
        # Test with small batch size
        (5, 1e-4, 0.75, 1.0, torch.float32, "cpu", (1, 16, 8, 8), "batch_size_1"),
    ])
    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):
        """Test LocalResponseNorm with boundary values for parameters"""
        # Skip CUDA tests if device not available
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("CUDA not available")
    
        # Validate parameters based on test type
        if test_type == "min_size":
            assert size == 1, "Test requires size=1"
        elif test_type == "large_size":
            # Size should be odd and <= number of channels
            assert size % 2 == 1, "Size should be odd"
            assert size <= shape[1], f"Size {size} should not exceed channels {shape[1]}"
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create LocalResponseNorm layer
        lrn = LocalResponseNorm(
            size=size,
            alpha=alpha,
            beta=beta,
            k=k
        ).to(device=device)
    
        # Forward pass
        output = lrn(input_tensor)
    
        # Basic assertions for all boundary tests
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == input_tensor.dtype, \
            f"Output dtype {output.dtype} != input dtype {input_tensor.dtype}"
    
        # 3. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            f"Output contains NaN or infinite values for test_type={test_type}"
    
        # 4. Sign preservation
        sign_preserved = torch.all((input_tensor * output) >= 0)
        assert sign_preserved, f"Sign not preserved for test_type={test_type}"
    
        # Test-specific assertions
        if test_type == "zero_alpha":
            # With alpha=0, normalization should have minimal effect
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero alpha test failed: diff={diff}"
    
        elif test_type == "zero_beta":
            # With beta=0, output should be input / (k^0) = input / 1 = input
            expected = input_tensor
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero beta test failed: diff={diff}"
    
        elif test_type == "zero_k":
            # With k=0, need to handle division by zero carefully
            # PyTorch should handle this, but output might have large values
            # Just verify it doesn't crash and produces finite values
            assert torch.all(torch.isfinite(output)), "Zero k test produced non-finite values"
    
        elif test_type == "large_alpha":
            # With large alpha, normalization effect is stronger
            # Output magnitude should be significantly reduced
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
            reduction_ratio = output_norm / input_norm
    
            # With alpha=1.0, reduction should be noticeable
            assert reduction_ratio < 0.9, \
                f"Large alpha should reduce norm: reduction_ratio={reduction_ratio}"
    
        elif test_type == "large_beta":
            # With large beta (2.0), normalization is more aggressive
            # Output should have smaller magnitude
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
    
            # Compare with beta=0.75 case
            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)
            normal_output = lrn_normal(input_tensor)
            normal_norm = torch.norm(normal_output)
    
            # Larger beta should produce smaller output
            assert output_norm < normal_norm * 1.1, \
                f"Large beta should reduce output more: beta={beta} vs 0.75"
    
        elif test_type == "min_size":
            # With size=1, only self-channel is considered
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.1, f"Size=1 test failed: diff={diff}"
    
        # Test with all positive input
        positive_input = torch.abs(input_tensor) + 0.1
        positive_output = lrn(positive_input)
        assert torch.all(positive_output >= 0), \
            f"Positive input should produce non-negative output for test_type={test_type}"
    
        # Test with all negative input
        negative_input = -torch.abs(input_tensor) - 0.1
        negative_output = lrn(negative_input)
        assert torch.all(negative_output <= 0), \
            f"Negative input should produce non-positive output for test_type={test_type}"
    
        # Test with mixed positive/negative input
        # Should preserve signs
        mixed_input = input_tensor  # Already has mixed signs from randn
        mixed_output = lrn(mixed_input)
    
        # Check sign preservation element-wise
        sign_match = (mixed_input >= 0) == (mixed_output >= 0)
        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()
        assert sign_match_ratio > 0.99, \
            f"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}"
    
        # Test with constant input
        constant_value = 5.0
        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)
        constant_output = lrn(constant_input)
    
        # For constant input, output should be scaled version of input
        # All values should be equal
        output_std = torch.std(constant_output)
>       assert output_std < 1e-5, \
            f"Constant input should produce constant output for test_type={test_type}: std={output_std}"
E       AssertionError: Constant input should produce constant output for test_type=small_k: std=0.07085202634334564
E       assert tensor(0.0709) < 1e-05

tests/test_torch_nn_modules_normalization_g3.py:438: AssertionError
_ test_localresponsenorm_boundary_values[5-0.0001-0.75-10.0-dtype11-cpu-shape11-large_k] _

size = 5, alpha = 0.0001, beta = 0.75, k = 10.0, dtype = torch.float32
device = 'cpu', shape = (2, 16, 8, 8), test_type = 'large_k'
set_random_seed = 42

    @pytest.mark.parametrize("size,alpha,beta,k,dtype,device,shape,test_type", [
        # Test boundary values for size
        (1, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "min_size"),
        (3, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "small_size"),
        (15, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_size"),
        # Test boundary values for alpha
        (5, 0.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_alpha"),
        (5, 1e-10, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "tiny_alpha"),
        (5, 1.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_alpha"),
        # Test boundary values for beta
        (5, 1e-4, 0.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_beta"),
        (5, 1e-4, 0.5, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "small_beta"),
        (5, 1e-4, 2.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_beta"),
        # Test boundary values for k
        (5, 1e-4, 0.75, 0.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_k"),
        (5, 1e-4, 0.75, 0.1, torch.float32, "cpu", (2, 16, 8, 8), "small_k"),
        (5, 1e-4, 0.75, 10.0, torch.float32, "cpu", (2, 16, 8, 8), "large_k"),
        # Test with different data types
        (5, 1e-4, 0.75, 1.0, torch.float64, "cpu", (2, 16, 8, 8), "float64"),
        # Test with small batch size
        (5, 1e-4, 0.75, 1.0, torch.float32, "cpu", (1, 16, 8, 8), "batch_size_1"),
    ])
    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):
        """Test LocalResponseNorm with boundary values for parameters"""
        # Skip CUDA tests if device not available
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("CUDA not available")
    
        # Validate parameters based on test type
        if test_type == "min_size":
            assert size == 1, "Test requires size=1"
        elif test_type == "large_size":
            # Size should be odd and <= number of channels
            assert size % 2 == 1, "Size should be odd"
            assert size <= shape[1], f"Size {size} should not exceed channels {shape[1]}"
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create LocalResponseNorm layer
        lrn = LocalResponseNorm(
            size=size,
            alpha=alpha,
            beta=beta,
            k=k
        ).to(device=device)
    
        # Forward pass
        output = lrn(input_tensor)
    
        # Basic assertions for all boundary tests
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == input_tensor.dtype, \
            f"Output dtype {output.dtype} != input dtype {input_tensor.dtype}"
    
        # 3. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            f"Output contains NaN or infinite values for test_type={test_type}"
    
        # 4. Sign preservation
        sign_preserved = torch.all((input_tensor * output) >= 0)
        assert sign_preserved, f"Sign not preserved for test_type={test_type}"
    
        # Test-specific assertions
        if test_type == "zero_alpha":
            # With alpha=0, normalization should have minimal effect
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero alpha test failed: diff={diff}"
    
        elif test_type == "zero_beta":
            # With beta=0, output should be input / (k^0) = input / 1 = input
            expected = input_tensor
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero beta test failed: diff={diff}"
    
        elif test_type == "zero_k":
            # With k=0, need to handle division by zero carefully
            # PyTorch should handle this, but output might have large values
            # Just verify it doesn't crash and produces finite values
            assert torch.all(torch.isfinite(output)), "Zero k test produced non-finite values"
    
        elif test_type == "large_alpha":
            # With large alpha, normalization effect is stronger
            # Output magnitude should be significantly reduced
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
            reduction_ratio = output_norm / input_norm
    
            # With alpha=1.0, reduction should be noticeable
            assert reduction_ratio < 0.9, \
                f"Large alpha should reduce norm: reduction_ratio={reduction_ratio}"
    
        elif test_type == "large_beta":
            # With large beta (2.0), normalization is more aggressive
            # Output should have smaller magnitude
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
    
            # Compare with beta=0.75 case
            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)
            normal_output = lrn_normal(input_tensor)
            normal_norm = torch.norm(normal_output)
    
            # Larger beta should produce smaller output
            assert output_norm < normal_norm * 1.1, \
                f"Large beta should reduce output more: beta={beta} vs 0.75"
    
        elif test_type == "min_size":
            # With size=1, only self-channel is considered
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.1, f"Size=1 test failed: diff={diff}"
    
        # Test with all positive input
        positive_input = torch.abs(input_tensor) + 0.1
        positive_output = lrn(positive_input)
        assert torch.all(positive_output >= 0), \
            f"Positive input should produce non-negative output for test_type={test_type}"
    
        # Test with all negative input
        negative_input = -torch.abs(input_tensor) - 0.1
        negative_output = lrn(negative_input)
        assert torch.all(negative_output <= 0), \
            f"Negative input should produce non-positive output for test_type={test_type}"
    
        # Test with mixed positive/negative input
        # Should preserve signs
        mixed_input = input_tensor  # Already has mixed signs from randn
        mixed_output = lrn(mixed_input)
    
        # Check sign preservation element-wise
        sign_match = (mixed_input >= 0) == (mixed_output >= 0)
        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()
        assert sign_match_ratio > 0.99, \
            f"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}"
    
        # Test with constant input
        constant_value = 5.0
        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)
        constant_output = lrn(constant_input)
    
        # For constant input, output should be scaled version of input
        # All values should be equal
        output_std = torch.std(constant_output)
>       assert output_std < 1e-5, \
            f"Constant input should produce constant output for test_type={test_type}: std={output_std}"
E       AssertionError: Constant input should produce constant output for test_type=large_k: std=2.31948488362832e-05
E       assert tensor(2.3195e-05) < 1e-05

tests/test_torch_nn_modules_normalization_g3.py:438: AssertionError
_ test_localresponsenorm_boundary_values[5-0.0001-0.75-1.0-dtype12-cpu-shape12-float64] _

size = 5, alpha = 0.0001, beta = 0.75, k = 1.0, dtype = torch.float64
device = 'cpu', shape = (2, 16, 8, 8), test_type = 'float64'
set_random_seed = 42

    @pytest.mark.parametrize("size,alpha,beta,k,dtype,device,shape,test_type", [
        # Test boundary values for size
        (1, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "min_size"),
        (3, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "small_size"),
        (15, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_size"),
        # Test boundary values for alpha
        (5, 0.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_alpha"),
        (5, 1e-10, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "tiny_alpha"),
        (5, 1.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_alpha"),
        # Test boundary values for beta
        (5, 1e-4, 0.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_beta"),
        (5, 1e-4, 0.5, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "small_beta"),
        (5, 1e-4, 2.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_beta"),
        # Test boundary values for k
        (5, 1e-4, 0.75, 0.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_k"),
        (5, 1e-4, 0.75, 0.1, torch.float32, "cpu", (2, 16, 8, 8), "small_k"),
        (5, 1e-4, 0.75, 10.0, torch.float32, "cpu", (2, 16, 8, 8), "large_k"),
        # Test with different data types
        (5, 1e-4, 0.75, 1.0, torch.float64, "cpu", (2, 16, 8, 8), "float64"),
        # Test with small batch size
        (5, 1e-4, 0.75, 1.0, torch.float32, "cpu", (1, 16, 8, 8), "batch_size_1"),
    ])
    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):
        """Test LocalResponseNorm with boundary values for parameters"""
        # Skip CUDA tests if device not available
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("CUDA not available")
    
        # Validate parameters based on test type
        if test_type == "min_size":
            assert size == 1, "Test requires size=1"
        elif test_type == "large_size":
            # Size should be odd and <= number of channels
            assert size % 2 == 1, "Size should be odd"
            assert size <= shape[1], f"Size {size} should not exceed channels {shape[1]}"
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create LocalResponseNorm layer
        lrn = LocalResponseNorm(
            size=size,
            alpha=alpha,
            beta=beta,
            k=k
        ).to(device=device)
    
        # Forward pass
        output = lrn(input_tensor)
    
        # Basic assertions for all boundary tests
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == input_tensor.dtype, \
            f"Output dtype {output.dtype} != input dtype {input_tensor.dtype}"
    
        # 3. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            f"Output contains NaN or infinite values for test_type={test_type}"
    
        # 4. Sign preservation
        sign_preserved = torch.all((input_tensor * output) >= 0)
        assert sign_preserved, f"Sign not preserved for test_type={test_type}"
    
        # Test-specific assertions
        if test_type == "zero_alpha":
            # With alpha=0, normalization should have minimal effect
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero alpha test failed: diff={diff}"
    
        elif test_type == "zero_beta":
            # With beta=0, output should be input / (k^0) = input / 1 = input
            expected = input_tensor
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero beta test failed: diff={diff}"
    
        elif test_type == "zero_k":
            # With k=0, need to handle division by zero carefully
            # PyTorch should handle this, but output might have large values
            # Just verify it doesn't crash and produces finite values
            assert torch.all(torch.isfinite(output)), "Zero k test produced non-finite values"
    
        elif test_type == "large_alpha":
            # With large alpha, normalization effect is stronger
            # Output magnitude should be significantly reduced
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
            reduction_ratio = output_norm / input_norm
    
            # With alpha=1.0, reduction should be noticeable
            assert reduction_ratio < 0.9, \
                f"Large alpha should reduce norm: reduction_ratio={reduction_ratio}"
    
        elif test_type == "large_beta":
            # With large beta (2.0), normalization is more aggressive
            # Output should have smaller magnitude
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
    
            # Compare with beta=0.75 case
            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)
            normal_output = lrn_normal(input_tensor)
            normal_norm = torch.norm(normal_output)
    
            # Larger beta should produce smaller output
            assert output_norm < normal_norm * 1.1, \
                f"Large beta should reduce output more: beta={beta} vs 0.75"
    
        elif test_type == "min_size":
            # With size=1, only self-channel is considered
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.1, f"Size=1 test failed: diff={diff}"
    
        # Test with all positive input
        positive_input = torch.abs(input_tensor) + 0.1
        positive_output = lrn(positive_input)
        assert torch.all(positive_output >= 0), \
            f"Positive input should produce non-negative output for test_type={test_type}"
    
        # Test with all negative input
        negative_input = -torch.abs(input_tensor) - 0.1
        negative_output = lrn(negative_input)
        assert torch.all(negative_output <= 0), \
            f"Negative input should produce non-positive output for test_type={test_type}"
    
        # Test with mixed positive/negative input
        # Should preserve signs
        mixed_input = input_tensor  # Already has mixed signs from randn
        mixed_output = lrn(mixed_input)
    
        # Check sign preservation element-wise
        sign_match = (mixed_input >= 0) == (mixed_output >= 0)
        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()
        assert sign_match_ratio > 0.99, \
            f"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}"
    
        # Test with constant input
        constant_value = 5.0
        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)
        constant_output = lrn(constant_input)
    
        # For constant input, output should be scaled version of input
        # All values should be equal
        output_std = torch.std(constant_output)
>       assert output_std < 1e-5, \
            f"Constant input should produce constant output for test_type={test_type}: std={output_std}"
E       AssertionError: Constant input should produce constant output for test_type=float64: std=0.0013006162728712114
E       assert tensor(0.0013, dtype=torch.float64) < 1e-05

tests/test_torch_nn_modules_normalization_g3.py:438: AssertionError
_ test_localresponsenorm_boundary_values[5-0.0001-0.75-1.0-dtype13-cpu-shape13-batch_size_1] _

size = 5, alpha = 0.0001, beta = 0.75, k = 1.0, dtype = torch.float32
device = 'cpu', shape = (1, 16, 8, 8), test_type = 'batch_size_1'
set_random_seed = 42

    @pytest.mark.parametrize("size,alpha,beta,k,dtype,device,shape,test_type", [
        # Test boundary values for size
        (1, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "min_size"),
        (3, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 8, 8, 8), "small_size"),
        (15, 1e-4, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_size"),
        # Test boundary values for alpha
        (5, 0.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_alpha"),
        (5, 1e-10, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "tiny_alpha"),
        (5, 1.0, 0.75, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_alpha"),
        # Test boundary values for beta
        (5, 1e-4, 0.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_beta"),
        (5, 1e-4, 0.5, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "small_beta"),
        (5, 1e-4, 2.0, 1.0, torch.float32, "cpu", (2, 16, 8, 8), "large_beta"),
        # Test boundary values for k
        (5, 1e-4, 0.75, 0.0, torch.float32, "cpu", (2, 16, 8, 8), "zero_k"),
        (5, 1e-4, 0.75, 0.1, torch.float32, "cpu", (2, 16, 8, 8), "small_k"),
        (5, 1e-4, 0.75, 10.0, torch.float32, "cpu", (2, 16, 8, 8), "large_k"),
        # Test with different data types
        (5, 1e-4, 0.75, 1.0, torch.float64, "cpu", (2, 16, 8, 8), "float64"),
        # Test with small batch size
        (5, 1e-4, 0.75, 1.0, torch.float32, "cpu", (1, 16, 8, 8), "batch_size_1"),
    ])
    def test_localresponsenorm_boundary_values(size, alpha, beta, k, dtype, device, shape, test_type, set_random_seed):
        """Test LocalResponseNorm with boundary values for parameters"""
        # Skip CUDA tests if device not available
        if device == "cuda" and not torch.cuda.is_available():
            pytest.skip("CUDA not available")
    
        # Validate parameters based on test type
        if test_type == "min_size":
            assert size == 1, "Test requires size=1"
        elif test_type == "large_size":
            # Size should be odd and <= number of channels
            assert size % 2 == 1, "Size should be odd"
            assert size <= shape[1], f"Size {size} should not exceed channels {shape[1]}"
    
        # Create input tensor
        torch.manual_seed(42)
        input_tensor = torch.randn(*shape, dtype=dtype, device=device)
    
        # Create LocalResponseNorm layer
        lrn = LocalResponseNorm(
            size=size,
            alpha=alpha,
            beta=beta,
            k=k
        ).to(device=device)
    
        # Forward pass
        output = lrn(input_tensor)
    
        # Basic assertions for all boundary tests
        # 1. Shape assertion
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
    
        # 2. Dtype assertion
        assert output.dtype == input_tensor.dtype, \
            f"Output dtype {output.dtype} != input dtype {input_tensor.dtype}"
    
        # 3. Finite values assertion
        assert torch.all(torch.isfinite(output)), \
            f"Output contains NaN or infinite values for test_type={test_type}"
    
        # 4. Sign preservation
        sign_preserved = torch.all((input_tensor * output) >= 0)
        assert sign_preserved, f"Sign not preserved for test_type={test_type}"
    
        # Test-specific assertions
        if test_type == "zero_alpha":
            # With alpha=0, normalization should have minimal effect
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero alpha test failed: diff={diff}"
    
        elif test_type == "zero_beta":
            # With beta=0, output should be input / (k^0) = input / 1 = input
            expected = input_tensor
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.01, f"Zero beta test failed: diff={diff}"
    
        elif test_type == "zero_k":
            # With k=0, need to handle division by zero carefully
            # PyTorch should handle this, but output might have large values
            # Just verify it doesn't crash and produces finite values
            assert torch.all(torch.isfinite(output)), "Zero k test produced non-finite values"
    
        elif test_type == "large_alpha":
            # With large alpha, normalization effect is stronger
            # Output magnitude should be significantly reduced
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
            reduction_ratio = output_norm / input_norm
    
            # With alpha=1.0, reduction should be noticeable
            assert reduction_ratio < 0.9, \
                f"Large alpha should reduce norm: reduction_ratio={reduction_ratio}"
    
        elif test_type == "large_beta":
            # With large beta (2.0), normalization is more aggressive
            # Output should have smaller magnitude
            input_norm = torch.norm(input_tensor)
            output_norm = torch.norm(output)
    
            # Compare with beta=0.75 case
            lrn_normal = LocalResponseNorm(size=size, alpha=alpha, beta=0.75, k=k).to(device=device)
            normal_output = lrn_normal(input_tensor)
            normal_norm = torch.norm(normal_output)
    
            # Larger beta should produce smaller output
            assert output_norm < normal_norm * 1.1, \
                f"Large beta should reduce output more: beta={beta} vs 0.75"
    
        elif test_type == "min_size":
            # With size=1, only self-channel is considered
            # Output should be close to input / (k^beta)
            expected = input_tensor / (k**beta)
            diff = torch.norm(output - expected) / torch.norm(expected)
            assert diff < 0.1, f"Size=1 test failed: diff={diff}"
    
        # Test with all positive input
        positive_input = torch.abs(input_tensor) + 0.1
        positive_output = lrn(positive_input)
        assert torch.all(positive_output >= 0), \
            f"Positive input should produce non-negative output for test_type={test_type}"
    
        # Test with all negative input
        negative_input = -torch.abs(input_tensor) - 0.1
        negative_output = lrn(negative_input)
        assert torch.all(negative_output <= 0), \
            f"Negative input should produce non-positive output for test_type={test_type}"
    
        # Test with mixed positive/negative input
        # Should preserve signs
        mixed_input = input_tensor  # Already has mixed signs from randn
        mixed_output = lrn(mixed_input)
    
        # Check sign preservation element-wise
        sign_match = (mixed_input >= 0) == (mixed_output >= 0)
        sign_match_ratio = torch.sum(sign_match).item() / sign_match.numel()
        assert sign_match_ratio > 0.99, \
            f"Sign preservation failed for test_type={test_type}: ratio={sign_match_ratio}"
    
        # Test with constant input
        constant_value = 5.0
        constant_input = torch.full(shape, constant_value, dtype=dtype, device=device)
        constant_output = lrn(constant_input)
    
        # For constant input, output should be scaled version of input
        # All values should be equal
        output_std = torch.std(constant_output)
>       assert output_std < 1e-5, \
            f"Constant input should produce constant output for test_type={test_type}: std={output_std}"
E       AssertionError: Constant input should produce constant output for test_type=batch_size_1: std=0.0013009667163714767
E       assert tensor(0.0013) < 1e-05

tests/test_torch_nn_modules_normalization_g3.py:438: AssertionError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                              Stmts   Miss Branch BrPart  Cover   Missing
---------------------------------------------------------------------------------------------
tests/test_torch_nn_modules_normalization_g1.py     205     64     44      5    67%   24-39, 54, 101->96, 129-130, 177-231, 247, 415->429, 436-479, 485
tests/test_torch_nn_modules_normalization_g2.py     235     95     42      3    61%   24-39, 54, 143-144, 190, 358-538, 544
tests/test_torch_nn_modules_normalization_g3.py     204     19     50      6    87%   24-39, 52, 117-118, 128->143, 177, 239->254, 312, 465-471, 483
---------------------------------------------------------------------------------------------
TOTAL                                               644    178    136     14    72%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_normalization_g1.py::test_groupnorm_divisibility_exception
FAILED tests/test_torch_nn_modules_normalization_g1.py::test_groupnorm_device_dtype[dtype0-cpu]
FAILED tests/test_torch_nn_modules_normalization_g1.py::test_groupnorm_device_dtype[dtype1-cpu]
FAILED tests/test_torch_nn_modules_normalization_g2.py::test_layernorm_exception_shapes
FAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[3-0.0001-0.75-1.0-dtype1-cpu-shape1-small_size]
FAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[15-0.0001-0.75-1.0-dtype2-cpu-shape2-large_size]
FAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-1.0-0.75-1.0-dtype5-cpu-shape5-large_alpha]
FAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-0.0001-0.5-1.0-dtype7-cpu-shape7-small_beta]
FAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-0.0001-2.0-1.0-dtype8-cpu-shape8-large_beta]
FAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-0.0001-0.75-0.0-dtype9-cpu-shape9-zero_k]
FAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-0.0001-0.75-0.1-dtype10-cpu-shape10-small_k]
FAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-0.0001-0.75-10.0-dtype11-cpu-shape11-large_k]
FAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-0.0001-0.75-1.0-dtype12-cpu-shape12-float64]
FAILED tests/test_torch_nn_modules_normalization_g3.py::test_localresponsenorm_boundary_values[5-0.0001-0.75-1.0-dtype13-cpu-shape13-batch_size_1]
14 failed, 22 passed, 2 skipped in 1.26s

Error: exit 1