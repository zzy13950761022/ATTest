=== Run Tests ===
.FF..                                                                    [100%]
=================================== FAILURES ===================================
_ test_transformer_encoder_only[dtype0-cpu-False-2-8-1-1-32-0.0-relu-src_shape0-None] _

dtype = torch.float32, device = 'cpu', batch_first = False, nhead = 2
d_model = 8, num_encoder_layers = 1, num_decoder_layers = 1
dim_feedforward = 32, dropout = 0.0, activation = 'relu', src_shape = (5, 16, 8)
tgt = None, set_random_seed = None

    @pytest.mark.parametrize(
        "dtype,device,batch_first,nhead,d_model,num_encoder_layers,num_decoder_layers,"
        "dim_feedforward,dropout,activation,src_shape,tgt",
        [
            (
                torch.float32,
                "cpu",
                False,
                2,
                8,
                1,
                1,
                32,
                0.0,
                "relu",
                (5, 16, 8),
                None,
            ),
        ]
    )
    def test_transformer_encoder_only(
        dtype,
        device,
        batch_first,
        nhead,
        d_model,
        num_encoder_layers,
        num_decoder_layers,
        dim_feedforward,
        dropout,
        activation,
        src_shape,
        tgt,
        set_random_seed,
    ):
        """TC-02: Transformer 仅编码器模式"""
        # Create model
        model = Transformer(
            d_model=d_model,
            nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation=activation,
            batch_first=batch_first,
        )
        model.to(device)
        model.eval()  # Disable dropout for deterministic output
    
        # Create source tensor
        src = create_test_tensor(src_shape, dtype=dtype, device=device)
    
        # Forward pass with tgt=None (encoder-only mode)
        with torch.no_grad():
>           output = model(src, tgt)

tests/test_torch_nn_modules_transformer_g1.py:189: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (s...ut3): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
  )
)
src = tensor([[[ 3.3737e-01, -1.7778e-01, -3.0353e-01, -5.8801e-01,  3.4861e-01,
           6.6034e-01, -2.1964e-01, -3.7917...  [ 5.5391e-01,  1.9170e+00, -6.0736e-01, -7.2208e-01,  2.5328e+00,
          -1.2669e+00,  1.1882e+00,  1.2210e-01]]])
tgt = None, src_mask = None, tgt_mask = None, memory_mask = None
src_key_padding_mask = None, tgt_key_padding_mask = None
memory_key_padding_mask = None

    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None,
                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,
                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:
        r"""Take in and process masked source/target sequences.
    
        Args:
            src: the sequence to the encoder (required).
            tgt: the sequence to the decoder (required).
            src_mask: the additive mask for the src sequence (optional).
            tgt_mask: the additive mask for the tgt sequence (optional).
            memory_mask: the additive mask for the encoder output (optional).
            src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).
            tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).
            memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).
    
        Shape:
            - src: :math:`(S, E)` for unbatched input, :math:`(S, N, E)` if `batch_first=False` or
              `(N, S, E)` if `batch_first=True`.
            - tgt: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or
              `(N, T, E)` if `batch_first=True`.
            - src_mask: :math:`(S, S)` or :math:`(N\cdot\text{num\_heads}, S, S)`.
            - tgt_mask: :math:`(T, T)` or :math:`(N\cdot\text{num\_heads}, T, T)`.
            - memory_mask: :math:`(T, S)`.
            - src_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.
            - tgt_key_padding_mask: :math:`(T)` for unbatched input otherwise :math:`(N, T)`.
            - memory_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.
    
            Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked
            positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``
            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
            is provided, it will be added to the attention weight.
            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by
            the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero
            positions will be unchanged. If a BoolTensor is provided, the positions with the
            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
    
            - output: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or
              `(N, T, E)` if `batch_first=True`.
    
            Note: Due to the multi-head attention architecture in the transformer model,
            the output sequence length of a transformer is same as the input sequence
            (i.e. target) length of the decoder.
    
            where S is the source sequence length, T is the target sequence length, N is the
            batch size, E is the feature number
    
        Examples:
            >>> # xdoctest: +SKIP
            >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)
        """
    
        is_batched = src.dim() == 3
>       if not self.batch_first and src.size(1) != tgt.size(1) and is_batched:
E       AttributeError: 'NoneType' object has no attribute 'size'

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/transformer.py:138: AttributeError
_______ test_transformer_parameter_validation[dtype0-cpu-3-8-ValueError] _______

dtype = torch.float32, device = 'cpu', nhead = 3, d_model = 8
expected_error = <class 'ValueError'>

    @pytest.mark.parametrize(
        "dtype,device,nhead,d_model,expected_error",
        [
            (
                torch.float32,
                "cpu",
                3,  # nhead=3
                8,  # d_model=8 (not divisible by 3)
                ValueError,
            ),
        ]
    )
    def test_transformer_parameter_validation(
        dtype,
        device,
        nhead,
        d_model,
        expected_error,
    ):
        """TC-04: Transformer 参数验证异常"""
        # Test that ValueError is raised when d_model is not divisible by nhead
        with pytest.raises(expected_error) as exc_info:
>           Transformer(
                d_model=d_model,
                nhead=nhead,
                num_encoder_layers=2,
                num_decoder_layers=2,
                dim_feedforward=32,
                dropout=0.0,
                activation="relu",
            )

tests/test_torch_nn_modules_transformer_g1.py:241: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/transformer.py:63: in __init__
    encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/transformer.py:404: in __init__
    self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = MultiheadAttention(), embed_dim = 8, num_heads = 3, dropout = 0.0
bias = True, add_bias_kv = False, add_zero_attn = False, kdim = None
vdim = None, batch_first = False, device = None, dtype = None

    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,
                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:
        factory_kwargs = {'device': device, 'dtype': dtype}
        super(MultiheadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim
    
        self.num_heads = num_heads
        self.dropout = dropout
        self.batch_first = batch_first
        self.head_dim = embed_dim // num_heads
>       assert self.head_dim * num_heads == self.embed_dim, "embed_dim must be divisible by num_heads"
E       AssertionError: embed_dim must be divisible by num_heads

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/activation.py:960: AssertionError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                            Stmts   Miss Branch BrPart  Cover   Missing
-------------------------------------------------------------------------------------------
tests/test_torch_nn_modules_transformer_g1.py     104     29     18      3    64%   20, 25, 35-46, 111, 193-211, 253-267, 414-418
-------------------------------------------------------------------------------------------
TOTAL                                             104     29     18      3    64%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_transformer_g1.py::test_transformer_encoder_only[dtype0-cpu-False-2-8-1-1-32-0.0-relu-src_shape0-None]
FAILED tests/test_torch_nn_modules_transformer_g1.py::test_transformer_parameter_validation[dtype0-cpu-3-8-ValueError]
2 failed, 3 passed in 0.84s

Error: exit 1