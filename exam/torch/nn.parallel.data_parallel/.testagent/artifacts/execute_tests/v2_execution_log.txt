=== Run Tests ===
FFFFsFFFFFFFF                                                            [100%]
=================================== FAILURES ===================================
___________________ test_cpu_as_output_device[test_config0] ____________________

test_config = {'device_ids': [0], 'dim': 0, 'input_shape': [8, 16], 'module_kwargs': {}, ...}

    @pytest.mark.parametrize("test_config", [
        # 基础配置
        {
            "module_type": "ReLU",
            "input_shape": [8, 16],
            "device_ids": [0],
            "output_device": -1,
            "dim": 0,
            "module_kwargs": {}
        },
        # 扩展配置：单样本输入测试
        {
            "module_type": "Tanh",
            "input_shape": [1, 10],
            "device_ids": [0],
            "output_device": -1,
            "dim": 0,
            "module_kwargs": {}
        }
    ])
    def test_cpu_as_output_device(test_config):
        """测试CPU作为输出设备"""
        # 解包测试配置
        module_type = test_config["module_type"]
        input_shape = test_config["input_shape"]
        device_ids = test_config["device_ids"]
        output_device = test_config["output_device"]
        dim = test_config["dim"]
        module_kwargs = test_config["module_kwargs"]
    
        # 检查CUDA可用性，如果不可用则使用CPU
        cuda_available = torch.cuda.is_available()
    
        # 调整device_ids：如果CUDA不可用，使用CPU（device_ids为空表示使用所有可用设备）
        if not cuda_available:
            # 对于CPU环境，device_ids应该为空或None，让data_parallel使用默认行为
            device_ids = None
    
        # 创建模块
        module = create_module(module_type, input_shape, device_id=0 if cuda_available else -1)
    
        # 创建输入张量
        inputs = create_input_tensor(input_shape, device_id=0 if cuda_available else -1)
    
        # 计算参考输出（单设备执行后移动到CPU）
        with torch.no_grad():
            if cuda_available:
                expected_output = module(inputs, **module_kwargs).cpu()
            else:
                # 在CPU环境下，模块已经在CPU上
                expected_output = module(inputs, **module_kwargs)
    
        # 使用data_parallel执行，输出到CPU
        with torch.no_grad():
>           actual_output = torch.nn.parallel.data_parallel(
                module=module,
                inputs=inputs,
                device_ids=device_ids,
                output_device=output_device,
                dim=dim,
                module_kwargs=module_kwargs
            )

tests/test_torch_nn_parallel_data_parallel_g2.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = ReLU()
inputs = (tensor([[ 3.3737e-01, -1.7778e-01, -3.0353e-01, -5.8801e-01,  3.4861e-01,
          6.6034e-01, -2.1964e-01, -3.7917e...6e+00, -9.9491e-01,
         -1.5822e-03,  1.2471e+00, -7.7105e-02,  1.2774e+00, -1.4596e+00,
         -2.1595e+00]]),)
device_ids = None, output_device = -1, dim = 0, module_kwargs = {}

    def data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None):
        r"""Evaluates module(input) in parallel across the GPUs given in device_ids.
    
        This is the functional version of the DataParallel module.
    
        Args:
            module (Module): the module to evaluate in parallel
            inputs (Tensor): inputs to the module
            device_ids (list of int or torch.device): GPU ids on which to replicate module
            output_device (list of int or torch.device): GPU location of the output  Use -1 to indicate the CPU.
                (default: device_ids[0])
        Returns:
            a Tensor containing the result of module(input) located on
            output_device
        """
        if not isinstance(inputs, tuple):
            inputs = (inputs,) if inputs is not None else ()
    
        device_type = _get_available_device_type()
    
        if device_ids is None:
            device_ids = _get_all_device_indices()
    
        if output_device is None:
            output_device = device_ids[0]
    
>       device_ids = [_get_device_index(x, True) for x in device_ids]
E       TypeError: 'NoneType' object is not iterable

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:213: TypeError
___________________ test_cpu_as_output_device[test_config1] ____________________

test_config = {'device_ids': [0], 'dim': 0, 'input_shape': [1, 10], 'module_kwargs': {}, ...}

    @pytest.mark.parametrize("test_config", [
        # 基础配置
        {
            "module_type": "ReLU",
            "input_shape": [8, 16],
            "device_ids": [0],
            "output_device": -1,
            "dim": 0,
            "module_kwargs": {}
        },
        # 扩展配置：单样本输入测试
        {
            "module_type": "Tanh",
            "input_shape": [1, 10],
            "device_ids": [0],
            "output_device": -1,
            "dim": 0,
            "module_kwargs": {}
        }
    ])
    def test_cpu_as_output_device(test_config):
        """测试CPU作为输出设备"""
        # 解包测试配置
        module_type = test_config["module_type"]
        input_shape = test_config["input_shape"]
        device_ids = test_config["device_ids"]
        output_device = test_config["output_device"]
        dim = test_config["dim"]
        module_kwargs = test_config["module_kwargs"]
    
        # 检查CUDA可用性，如果不可用则使用CPU
        cuda_available = torch.cuda.is_available()
    
        # 调整device_ids：如果CUDA不可用，使用CPU（device_ids为空表示使用所有可用设备）
        if not cuda_available:
            # 对于CPU环境，device_ids应该为空或None，让data_parallel使用默认行为
            device_ids = None
    
        # 创建模块
        module = create_module(module_type, input_shape, device_id=0 if cuda_available else -1)
    
        # 创建输入张量
        inputs = create_input_tensor(input_shape, device_id=0 if cuda_available else -1)
    
        # 计算参考输出（单设备执行后移动到CPU）
        with torch.no_grad():
            if cuda_available:
                expected_output = module(inputs, **module_kwargs).cpu()
            else:
                # 在CPU环境下，模块已经在CPU上
                expected_output = module(inputs, **module_kwargs)
    
        # 使用data_parallel执行，输出到CPU
        with torch.no_grad():
>           actual_output = torch.nn.parallel.data_parallel(
                module=module,
                inputs=inputs,
                device_ids=device_ids,
                output_device=output_device,
                dim=dim,
                module_kwargs=module_kwargs
            )

tests/test_torch_nn_parallel_data_parallel_g2.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = Tanh()
inputs = (tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
          0.3239, -0.1085]]),)
device_ids = None, output_device = -1, dim = 0, module_kwargs = {}

    def data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None):
        r"""Evaluates module(input) in parallel across the GPUs given in device_ids.
    
        This is the functional version of the DataParallel module.
    
        Args:
            module (Module): the module to evaluate in parallel
            inputs (Tensor): inputs to the module
            device_ids (list of int or torch.device): GPU ids on which to replicate module
            output_device (list of int or torch.device): GPU location of the output  Use -1 to indicate the CPU.
                (default: device_ids[0])
        Returns:
            a Tensor containing the result of module(input) located on
            output_device
        """
        if not isinstance(inputs, tuple):
            inputs = (inputs,) if inputs is not None else ()
    
        device_type = _get_available_device_type()
    
        if device_ids is None:
            device_ids = _get_all_device_indices()
    
        if output_device is None:
            output_device = device_ids[0]
    
>       device_ids = [_get_device_index(x, True) for x in device_ids]
E       TypeError: 'NoneType' object is not iterable

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:213: TypeError
_____________ test_parameter_validation_and_exceptions[test_case0] _____________

test_case = {'device_ids': [0], 'dim': 0, 'error_pattern': 'module.*Module', 'expected_error': <class 'TypeError'>, ...}

    @pytest.mark.parametrize("test_case", [
        # 测试1: module参数类型错误
        {
            "name": "module非Module类型",
            "module": "not_a_module",  # 字符串而不是Module
            "inputs": torch.randn(2, 10),
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_error": TypeError,
            "error_pattern": "module.*Module"
        },
        # 测试2: inputs参数类型错误
        {
            "name": "inputs非Tensor类型",
            "module": nn.Linear(10, 5),
            "inputs": "not_a_tensor",  # 字符串而不是Tensor
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_error": TypeError,
            "error_pattern": "inputs.*Tensor"
        },
        # 测试3: device_ids包含无效GPU ID（仅在CUDA可用时测试）
        {
            "name": "device_ids包含无效GPU ID",
            "module": nn.Linear(10, 5),
            "inputs": torch.randn(2, 10),
            "device_ids": [999],  # 无效的GPU ID
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_error": RuntimeError,
            "error_pattern": "device|invalid|cuda",
            "skip_if_no_cuda": True
        },
        # 测试4: dim超出输入维度范围
        {
            "name": "dim超出输入维度范围",
            "module": nn.Linear(10, 5),
            "inputs": torch.randn(2, 10),  # 2维张量
            "device_ids": [0],
            "output_device": 0,
            "dim": 5,  # 超出有效范围(0-1)
            "module_kwargs": {},
            "expected_error": IndexError,
            "error_pattern": "dim|index|out of range"
        },
        # 测试5: module_kwargs非字典类型
        {
            "name": "module_kwargs非字典类型",
            "module": nn.Linear(10, 5),
            "inputs": torch.randn(2, 10),
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": "not_a_dict",  # 字符串而不是字典
            "expected_error": TypeError,
            "error_pattern": "module_kwargs.*dict"
        }
    ])
    def test_parameter_validation_and_exceptions(test_case):
        """测试参数验证与异常场景"""
        # 检查是否需要跳过（如需要CUDA但不可用）
        if test_case.get("skip_if_no_cuda", False) and not torch.cuda.is_available():
            pytest.skip("需要CUDA设备来测试此场景")
    
        # 准备模块
        module = test_case["module"]
        if isinstance(module, nn.Module):
            # 如果是真实的Module，确保它在正确的设备上
            if torch.cuda.is_available() and test_case["device_ids"] and test_case["device_ids"][0] >= 0:
                device = torch.device(f"cuda:{test_case['device_ids'][0]}")
                module = module.to(device)
    
        # 准备输入
        inputs = test_case["inputs"]
        if isinstance(inputs, torch.Tensor):
            # 如果是真实的Tensor，确保它在正确的设备上
            if torch.cuda.is_available() and test_case["device_ids"] and test_case["device_ids"][0] >= 0:
                device = torch.device(f"cuda:{test_case['device_ids'][0]}")
                inputs = inputs.to(device)
    
        # 准备其他参数
        device_ids = test_case["device_ids"]
        output_device = test_case["output_device"]
        dim = test_case["dim"]
        module_kwargs = test_case["module_kwargs"]
    
        # 验证异常
        with pytest.raises(test_case["expected_error"]) as exc_info:
            torch.nn.parallel.data_parallel(
                module=module,
                inputs=inputs,
                device_ids=device_ids,
                output_device=output_device,
                dim=dim,
                module_kwargs=module_kwargs
            )
    
        # 检查错误消息是否包含预期模式
        if "error_pattern" in test_case:
            error_msg = str(exc_info.value).lower()
            pattern = test_case["error_pattern"].lower()
>           assert pattern in error_msg, \
                f"错误消息中应包含'{pattern}'，但得到: {error_msg}"
E           AssertionError: 错误消息中应包含'module.*module'，但得到: device(): argument 'type' (position 1) must be str, not nonetype
E           assert 'module.*module' in "device(): argument 'type' (position 1) must be str, not nonetype"

tests/test_torch_nn_parallel_data_parallel_g2.py:286: AssertionError
_____________ test_parameter_validation_and_exceptions[test_case1] _____________

test_case = {'device_ids': [0], 'dim': 0, 'error_pattern': 'inputs.*Tensor', 'expected_error': <class 'TypeError'>, ...}

    @pytest.mark.parametrize("test_case", [
        # 测试1: module参数类型错误
        {
            "name": "module非Module类型",
            "module": "not_a_module",  # 字符串而不是Module
            "inputs": torch.randn(2, 10),
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_error": TypeError,
            "error_pattern": "module.*Module"
        },
        # 测试2: inputs参数类型错误
        {
            "name": "inputs非Tensor类型",
            "module": nn.Linear(10, 5),
            "inputs": "not_a_tensor",  # 字符串而不是Tensor
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_error": TypeError,
            "error_pattern": "inputs.*Tensor"
        },
        # 测试3: device_ids包含无效GPU ID（仅在CUDA可用时测试）
        {
            "name": "device_ids包含无效GPU ID",
            "module": nn.Linear(10, 5),
            "inputs": torch.randn(2, 10),
            "device_ids": [999],  # 无效的GPU ID
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_error": RuntimeError,
            "error_pattern": "device|invalid|cuda",
            "skip_if_no_cuda": True
        },
        # 测试4: dim超出输入维度范围
        {
            "name": "dim超出输入维度范围",
            "module": nn.Linear(10, 5),
            "inputs": torch.randn(2, 10),  # 2维张量
            "device_ids": [0],
            "output_device": 0,
            "dim": 5,  # 超出有效范围(0-1)
            "module_kwargs": {},
            "expected_error": IndexError,
            "error_pattern": "dim|index|out of range"
        },
        # 测试5: module_kwargs非字典类型
        {
            "name": "module_kwargs非字典类型",
            "module": nn.Linear(10, 5),
            "inputs": torch.randn(2, 10),
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": "not_a_dict",  # 字符串而不是字典
            "expected_error": TypeError,
            "error_pattern": "module_kwargs.*dict"
        }
    ])
    def test_parameter_validation_and_exceptions(test_case):
        """测试参数验证与异常场景"""
        # 检查是否需要跳过（如需要CUDA但不可用）
        if test_case.get("skip_if_no_cuda", False) and not torch.cuda.is_available():
            pytest.skip("需要CUDA设备来测试此场景")
    
        # 准备模块
        module = test_case["module"]
        if isinstance(module, nn.Module):
            # 如果是真实的Module，确保它在正确的设备上
            if torch.cuda.is_available() and test_case["device_ids"] and test_case["device_ids"][0] >= 0:
                device = torch.device(f"cuda:{test_case['device_ids'][0]}")
                module = module.to(device)
    
        # 准备输入
        inputs = test_case["inputs"]
        if isinstance(inputs, torch.Tensor):
            # 如果是真实的Tensor，确保它在正确的设备上
            if torch.cuda.is_available() and test_case["device_ids"] and test_case["device_ids"][0] >= 0:
                device = torch.device(f"cuda:{test_case['device_ids'][0]}")
                inputs = inputs.to(device)
    
        # 准备其他参数
        device_ids = test_case["device_ids"]
        output_device = test_case["output_device"]
        dim = test_case["dim"]
        module_kwargs = test_case["module_kwargs"]
    
        # 验证异常
        with pytest.raises(test_case["expected_error"]) as exc_info:
            torch.nn.parallel.data_parallel(
                module=module,
                inputs=inputs,
                device_ids=device_ids,
                output_device=output_device,
                dim=dim,
                module_kwargs=module_kwargs
            )
    
        # 检查错误消息是否包含预期模式
        if "error_pattern" in test_case:
            error_msg = str(exc_info.value).lower()
            pattern = test_case["error_pattern"].lower()
>           assert pattern in error_msg, \
                f"错误消息中应包含'{pattern}'，但得到: {error_msg}"
E           AssertionError: 错误消息中应包含'inputs.*tensor'，但得到: device(): argument 'type' (position 1) must be str, not nonetype
E           assert 'inputs.*tensor' in "device(): argument 'type' (position 1) must be str, not nonetype"

tests/test_torch_nn_parallel_data_parallel_g2.py:286: AssertionError
_____________ test_parameter_validation_and_exceptions[test_case3] _____________

test_case = {'device_ids': [0], 'dim': 5, 'error_pattern': 'dim|index|out of range', 'expected_error': <class 'IndexError'>, ...}

    @pytest.mark.parametrize("test_case", [
        # 测试1: module参数类型错误
        {
            "name": "module非Module类型",
            "module": "not_a_module",  # 字符串而不是Module
            "inputs": torch.randn(2, 10),
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_error": TypeError,
            "error_pattern": "module.*Module"
        },
        # 测试2: inputs参数类型错误
        {
            "name": "inputs非Tensor类型",
            "module": nn.Linear(10, 5),
            "inputs": "not_a_tensor",  # 字符串而不是Tensor
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_error": TypeError,
            "error_pattern": "inputs.*Tensor"
        },
        # 测试3: device_ids包含无效GPU ID（仅在CUDA可用时测试）
        {
            "name": "device_ids包含无效GPU ID",
            "module": nn.Linear(10, 5),
            "inputs": torch.randn(2, 10),
            "device_ids": [999],  # 无效的GPU ID
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_error": RuntimeError,
            "error_pattern": "device|invalid|cuda",
            "skip_if_no_cuda": True
        },
        # 测试4: dim超出输入维度范围
        {
            "name": "dim超出输入维度范围",
            "module": nn.Linear(10, 5),
            "inputs": torch.randn(2, 10),  # 2维张量
            "device_ids": [0],
            "output_device": 0,
            "dim": 5,  # 超出有效范围(0-1)
            "module_kwargs": {},
            "expected_error": IndexError,
            "error_pattern": "dim|index|out of range"
        },
        # 测试5: module_kwargs非字典类型
        {
            "name": "module_kwargs非字典类型",
            "module": nn.Linear(10, 5),
            "inputs": torch.randn(2, 10),
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": "not_a_dict",  # 字符串而不是字典
            "expected_error": TypeError,
            "error_pattern": "module_kwargs.*dict"
        }
    ])
    def test_parameter_validation_and_exceptions(test_case):
        """测试参数验证与异常场景"""
        # 检查是否需要跳过（如需要CUDA但不可用）
        if test_case.get("skip_if_no_cuda", False) and not torch.cuda.is_available():
            pytest.skip("需要CUDA设备来测试此场景")
    
        # 准备模块
        module = test_case["module"]
        if isinstance(module, nn.Module):
            # 如果是真实的Module，确保它在正确的设备上
            if torch.cuda.is_available() and test_case["device_ids"] and test_case["device_ids"][0] >= 0:
                device = torch.device(f"cuda:{test_case['device_ids'][0]}")
                module = module.to(device)
    
        # 准备输入
        inputs = test_case["inputs"]
        if isinstance(inputs, torch.Tensor):
            # 如果是真实的Tensor，确保它在正确的设备上
            if torch.cuda.is_available() and test_case["device_ids"] and test_case["device_ids"][0] >= 0:
                device = torch.device(f"cuda:{test_case['device_ids'][0]}")
                inputs = inputs.to(device)
    
        # 准备其他参数
        device_ids = test_case["device_ids"]
        output_device = test_case["output_device"]
        dim = test_case["dim"]
        module_kwargs = test_case["module_kwargs"]
    
        # 验证异常
        with pytest.raises(test_case["expected_error"]) as exc_info:
>           torch.nn.parallel.data_parallel(
                module=module,
                inputs=inputs,
                device_ids=device_ids,
                output_device=output_device,
                dim=dim,
                module_kwargs=module_kwargs
            )

tests/test_torch_nn_parallel_data_parallel_g2.py:273: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = Linear(in_features=10, out_features=5, bias=True)
inputs = (tensor([[-0.0352,  1.4856,  1.2461,  0.2920, -0.7115, -1.1287,  0.8450, -0.8523,
          1.2711, -1.6028],
        [ 1.1518, -0.8913,  0.9754,  1.5406, -0.4506, -0.1123,  0.5386,  0.6552,
         -0.5334,  0.4318]]),)
device_ids = [0], output_device = 0, dim = 5, module_kwargs = {}

    def data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None):
        r"""Evaluates module(input) in parallel across the GPUs given in device_ids.
    
        This is the functional version of the DataParallel module.
    
        Args:
            module (Module): the module to evaluate in parallel
            inputs (Tensor): inputs to the module
            device_ids (list of int or torch.device): GPU ids on which to replicate module
            output_device (list of int or torch.device): GPU location of the output  Use -1 to indicate the CPU.
                (default: device_ids[0])
        Returns:
            a Tensor containing the result of module(input) located on
            output_device
        """
        if not isinstance(inputs, tuple):
            inputs = (inputs,) if inputs is not None else ()
    
        device_type = _get_available_device_type()
    
        if device_ids is None:
            device_ids = _get_all_device_indices()
    
        if output_device is None:
            output_device = device_ids[0]
    
        device_ids = [_get_device_index(x, True) for x in device_ids]
        output_device = _get_device_index(output_device, True)
>       src_device_obj = torch.device(device_type, device_ids[0])
E       TypeError: Device(): argument 'type' (position 1) must be str, not NoneType

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:215: TypeError
_____________ test_parameter_validation_and_exceptions[test_case4] _____________

test_case = {'device_ids': [0], 'dim': 0, 'error_pattern': 'module_kwargs.*dict', 'expected_error': <class 'TypeError'>, ...}

    @pytest.mark.parametrize("test_case", [
        # 测试1: module参数类型错误
        {
            "name": "module非Module类型",
            "module": "not_a_module",  # 字符串而不是Module
            "inputs": torch.randn(2, 10),
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_error": TypeError,
            "error_pattern": "module.*Module"
        },
        # 测试2: inputs参数类型错误
        {
            "name": "inputs非Tensor类型",
            "module": nn.Linear(10, 5),
            "inputs": "not_a_tensor",  # 字符串而不是Tensor
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_error": TypeError,
            "error_pattern": "inputs.*Tensor"
        },
        # 测试3: device_ids包含无效GPU ID（仅在CUDA可用时测试）
        {
            "name": "device_ids包含无效GPU ID",
            "module": nn.Linear(10, 5),
            "inputs": torch.randn(2, 10),
            "device_ids": [999],  # 无效的GPU ID
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_error": RuntimeError,
            "error_pattern": "device|invalid|cuda",
            "skip_if_no_cuda": True
        },
        # 测试4: dim超出输入维度范围
        {
            "name": "dim超出输入维度范围",
            "module": nn.Linear(10, 5),
            "inputs": torch.randn(2, 10),  # 2维张量
            "device_ids": [0],
            "output_device": 0,
            "dim": 5,  # 超出有效范围(0-1)
            "module_kwargs": {},
            "expected_error": IndexError,
            "error_pattern": "dim|index|out of range"
        },
        # 测试5: module_kwargs非字典类型
        {
            "name": "module_kwargs非字典类型",
            "module": nn.Linear(10, 5),
            "inputs": torch.randn(2, 10),
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": "not_a_dict",  # 字符串而不是字典
            "expected_error": TypeError,
            "error_pattern": "module_kwargs.*dict"
        }
    ])
    def test_parameter_validation_and_exceptions(test_case):
        """测试参数验证与异常场景"""
        # 检查是否需要跳过（如需要CUDA但不可用）
        if test_case.get("skip_if_no_cuda", False) and not torch.cuda.is_available():
            pytest.skip("需要CUDA设备来测试此场景")
    
        # 准备模块
        module = test_case["module"]
        if isinstance(module, nn.Module):
            # 如果是真实的Module，确保它在正确的设备上
            if torch.cuda.is_available() and test_case["device_ids"] and test_case["device_ids"][0] >= 0:
                device = torch.device(f"cuda:{test_case['device_ids'][0]}")
                module = module.to(device)
    
        # 准备输入
        inputs = test_case["inputs"]
        if isinstance(inputs, torch.Tensor):
            # 如果是真实的Tensor，确保它在正确的设备上
            if torch.cuda.is_available() and test_case["device_ids"] and test_case["device_ids"][0] >= 0:
                device = torch.device(f"cuda:{test_case['device_ids'][0]}")
                inputs = inputs.to(device)
    
        # 准备其他参数
        device_ids = test_case["device_ids"]
        output_device = test_case["output_device"]
        dim = test_case["dim"]
        module_kwargs = test_case["module_kwargs"]
    
        # 验证异常
        with pytest.raises(test_case["expected_error"]) as exc_info:
            torch.nn.parallel.data_parallel(
                module=module,
                inputs=inputs,
                device_ids=device_ids,
                output_device=output_device,
                dim=dim,
                module_kwargs=module_kwargs
            )
    
        # 检查错误消息是否包含预期模式
        if "error_pattern" in test_case:
            error_msg = str(exc_info.value).lower()
            pattern = test_case["error_pattern"].lower()
>           assert pattern in error_msg, \
                f"错误消息中应包含'{pattern}'，但得到: {error_msg}"
E           AssertionError: 错误消息中应包含'module_kwargs.*dict'，但得到: device(): argument 'type' (position 1) must be str, not nonetype
E           assert 'module_kwargs.*dict' in "device(): argument 'type' (position 1) must be str, not nonetype"

tests/test_torch_nn_parallel_data_parallel_g2.py:286: AssertionError
_____________________ test_edge_case_handling[test_case0] ______________________

test_case = {'device_ids': [], 'dim': 0, 'expected_behavior': 'use_all_available', 'input_shape': [4, 10], ...}

    @pytest.mark.parametrize("test_case", [
        # 测试1: 空device_ids列表（应该使用所有可用设备）
        {
            "name": "空device_ids列表",
            "module_type": "Linear",
            "input_shape": [4, 10],
            "device_ids": [],  # 空列表
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "use_all_available"
        },
        # 测试2: device_ids为None（应该使用所有可用设备）
        {
            "name": "device_ids为None",
            "module_type": "Linear",
            "input_shape": [4, 10],
            "device_ids": None,  # None值
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "use_all_available"
        },
        # 测试3: 空输入张量
        {
            "name": "空输入张量",
            "module_type": "Linear",
            "input_shape": [0, 10],  # batch size为0
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_empty_input"
        },
        # 测试4: 单元素batch size
        {
            "name": "单元素batch size",
            "module_type": "Linear",
            "input_shape": [1, 10],  # batch size为1
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_single_batch"
        },
        # 测试5: 极端大形状输入（内存允许的情况下）
        {
            "name": "中等大形状输入",
            "module_type": "Linear",
            "input_shape": [100, 50],  # 中等大小，避免内存问题
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_large_input"
        },
        # 测试6: 不同dim值
        {
            "name": "dim=1的分散维度",
            "module_type": "Linear",
            "input_shape": [8, 10],
            "device_ids": [0],
            "output_device": 0,
            "dim": 1,  # 在特征维度上分散
            "module_kwargs": {},
            "expected_behavior": "scatter_along_dim"
        }
    ])
    def test_edge_case_handling(test_case):
        """测试边界条件处理"""
        # 解包测试配置
        name = test_case["name"]
        module_type = test_case["module_type"]
        input_shape = test_case["input_shape"]
        device_ids = test_case["device_ids"]
        output_device = test_case["output_device"]
        dim = test_case["dim"]
        module_kwargs = test_case["module_kwargs"]
        expected_behavior = test_case["expected_behavior"]
    
        # 检查CUDA可用性
        cuda_available = torch.cuda.is_available()
    
        # 对于需要CUDA的测试，如果CUDA不可用则跳过
        if device_ids and device_ids != [] and device_ids != [0] and not cuda_available:
            pytest.skip("需要CUDA设备来测试此场景")
    
        # 调整device_ids：如果CUDA不可用，使用CPU
        if not cuda_available:
            if device_ids is not None and device_ids != []:
                # 对于CPU环境，device_ids应该为空或None
                device_ids = None
    
        # 创建模块
        module = create_module(module_type, input_shape, device_id=0 if cuda_available else -1)
    
        # 创建输入张量
        inputs = create_input_tensor(input_shape, device_id=0 if cuda_available else -1)
    
        # 计算参考输出（单设备执行）
        with torch.no_grad():
            if cuda_available:
                expected_output = module(inputs, **module_kwargs)
            else:
                expected_output = module(inputs, **module_kwargs)
    
        # 使用data_parallel执行
        with torch.no_grad():
>           actual_output = torch.nn.parallel.data_parallel(
                module=module,
                inputs=inputs,
                device_ids=device_ids,
                output_device=output_device,
                dim=dim,
                module_kwargs=module_kwargs
            )

tests/test_torch_nn_parallel_data_parallel_g2.py:399: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = Linear(in_features=10, out_features=5, bias=True)
inputs = (tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,
          0.7671, -1.1925],
        ....4210],
        [-0.9620,  1.2825, -0.3430, -0.6821, -0.9887, -1.7018, -0.7498, -1.1285,
          0.4135,  0.2892]]),)
device_ids = [], output_device = 0, dim = 0, module_kwargs = {}

    def data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None):
        r"""Evaluates module(input) in parallel across the GPUs given in device_ids.
    
        This is the functional version of the DataParallel module.
    
        Args:
            module (Module): the module to evaluate in parallel
            inputs (Tensor): inputs to the module
            device_ids (list of int or torch.device): GPU ids on which to replicate module
            output_device (list of int or torch.device): GPU location of the output  Use -1 to indicate the CPU.
                (default: device_ids[0])
        Returns:
            a Tensor containing the result of module(input) located on
            output_device
        """
        if not isinstance(inputs, tuple):
            inputs = (inputs,) if inputs is not None else ()
    
        device_type = _get_available_device_type()
    
        if device_ids is None:
            device_ids = _get_all_device_indices()
    
        if output_device is None:
            output_device = device_ids[0]
    
        device_ids = [_get_device_index(x, True) for x in device_ids]
        output_device = _get_device_index(output_device, True)
>       src_device_obj = torch.device(device_type, device_ids[0])
E       IndexError: list index out of range

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:215: IndexError
_____________________ test_edge_case_handling[test_case1] ______________________

test_case = {'device_ids': None, 'dim': 0, 'expected_behavior': 'use_all_available', 'input_shape': [4, 10], ...}

    @pytest.mark.parametrize("test_case", [
        # 测试1: 空device_ids列表（应该使用所有可用设备）
        {
            "name": "空device_ids列表",
            "module_type": "Linear",
            "input_shape": [4, 10],
            "device_ids": [],  # 空列表
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "use_all_available"
        },
        # 测试2: device_ids为None（应该使用所有可用设备）
        {
            "name": "device_ids为None",
            "module_type": "Linear",
            "input_shape": [4, 10],
            "device_ids": None,  # None值
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "use_all_available"
        },
        # 测试3: 空输入张量
        {
            "name": "空输入张量",
            "module_type": "Linear",
            "input_shape": [0, 10],  # batch size为0
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_empty_input"
        },
        # 测试4: 单元素batch size
        {
            "name": "单元素batch size",
            "module_type": "Linear",
            "input_shape": [1, 10],  # batch size为1
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_single_batch"
        },
        # 测试5: 极端大形状输入（内存允许的情况下）
        {
            "name": "中等大形状输入",
            "module_type": "Linear",
            "input_shape": [100, 50],  # 中等大小，避免内存问题
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_large_input"
        },
        # 测试6: 不同dim值
        {
            "name": "dim=1的分散维度",
            "module_type": "Linear",
            "input_shape": [8, 10],
            "device_ids": [0],
            "output_device": 0,
            "dim": 1,  # 在特征维度上分散
            "module_kwargs": {},
            "expected_behavior": "scatter_along_dim"
        }
    ])
    def test_edge_case_handling(test_case):
        """测试边界条件处理"""
        # 解包测试配置
        name = test_case["name"]
        module_type = test_case["module_type"]
        input_shape = test_case["input_shape"]
        device_ids = test_case["device_ids"]
        output_device = test_case["output_device"]
        dim = test_case["dim"]
        module_kwargs = test_case["module_kwargs"]
        expected_behavior = test_case["expected_behavior"]
    
        # 检查CUDA可用性
        cuda_available = torch.cuda.is_available()
    
        # 对于需要CUDA的测试，如果CUDA不可用则跳过
        if device_ids and device_ids != [] and device_ids != [0] and not cuda_available:
            pytest.skip("需要CUDA设备来测试此场景")
    
        # 调整device_ids：如果CUDA不可用，使用CPU
        if not cuda_available:
            if device_ids is not None and device_ids != []:
                # 对于CPU环境，device_ids应该为空或None
                device_ids = None
    
        # 创建模块
        module = create_module(module_type, input_shape, device_id=0 if cuda_available else -1)
    
        # 创建输入张量
        inputs = create_input_tensor(input_shape, device_id=0 if cuda_available else -1)
    
        # 计算参考输出（单设备执行）
        with torch.no_grad():
            if cuda_available:
                expected_output = module(inputs, **module_kwargs)
            else:
                expected_output = module(inputs, **module_kwargs)
    
        # 使用data_parallel执行
        with torch.no_grad():
>           actual_output = torch.nn.parallel.data_parallel(
                module=module,
                inputs=inputs,
                device_ids=device_ids,
                output_device=output_device,
                dim=dim,
                module_kwargs=module_kwargs
            )

tests/test_torch_nn_parallel_data_parallel_g2.py:399: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = Linear(in_features=10, out_features=5, bias=True)
inputs = (tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,
          0.7671, -1.1925],
        ....4210],
        [-0.9620,  1.2825, -0.3430, -0.6821, -0.9887, -1.7018, -0.7498, -1.1285,
          0.4135,  0.2892]]),)
device_ids = None, output_device = 0, dim = 0, module_kwargs = {}

    def data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None):
        r"""Evaluates module(input) in parallel across the GPUs given in device_ids.
    
        This is the functional version of the DataParallel module.
    
        Args:
            module (Module): the module to evaluate in parallel
            inputs (Tensor): inputs to the module
            device_ids (list of int or torch.device): GPU ids on which to replicate module
            output_device (list of int or torch.device): GPU location of the output  Use -1 to indicate the CPU.
                (default: device_ids[0])
        Returns:
            a Tensor containing the result of module(input) located on
            output_device
        """
        if not isinstance(inputs, tuple):
            inputs = (inputs,) if inputs is not None else ()
    
        device_type = _get_available_device_type()
    
        if device_ids is None:
            device_ids = _get_all_device_indices()
    
        if output_device is None:
            output_device = device_ids[0]
    
>       device_ids = [_get_device_index(x, True) for x in device_ids]
E       TypeError: 'NoneType' object is not iterable

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:213: TypeError
_____________________ test_edge_case_handling[test_case2] ______________________

test_case = {'device_ids': [0], 'dim': 0, 'expected_behavior': 'handle_empty_input', 'input_shape': [0, 10], ...}

    @pytest.mark.parametrize("test_case", [
        # 测试1: 空device_ids列表（应该使用所有可用设备）
        {
            "name": "空device_ids列表",
            "module_type": "Linear",
            "input_shape": [4, 10],
            "device_ids": [],  # 空列表
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "use_all_available"
        },
        # 测试2: device_ids为None（应该使用所有可用设备）
        {
            "name": "device_ids为None",
            "module_type": "Linear",
            "input_shape": [4, 10],
            "device_ids": None,  # None值
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "use_all_available"
        },
        # 测试3: 空输入张量
        {
            "name": "空输入张量",
            "module_type": "Linear",
            "input_shape": [0, 10],  # batch size为0
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_empty_input"
        },
        # 测试4: 单元素batch size
        {
            "name": "单元素batch size",
            "module_type": "Linear",
            "input_shape": [1, 10],  # batch size为1
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_single_batch"
        },
        # 测试5: 极端大形状输入（内存允许的情况下）
        {
            "name": "中等大形状输入",
            "module_type": "Linear",
            "input_shape": [100, 50],  # 中等大小，避免内存问题
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_large_input"
        },
        # 测试6: 不同dim值
        {
            "name": "dim=1的分散维度",
            "module_type": "Linear",
            "input_shape": [8, 10],
            "device_ids": [0],
            "output_device": 0,
            "dim": 1,  # 在特征维度上分散
            "module_kwargs": {},
            "expected_behavior": "scatter_along_dim"
        }
    ])
    def test_edge_case_handling(test_case):
        """测试边界条件处理"""
        # 解包测试配置
        name = test_case["name"]
        module_type = test_case["module_type"]
        input_shape = test_case["input_shape"]
        device_ids = test_case["device_ids"]
        output_device = test_case["output_device"]
        dim = test_case["dim"]
        module_kwargs = test_case["module_kwargs"]
        expected_behavior = test_case["expected_behavior"]
    
        # 检查CUDA可用性
        cuda_available = torch.cuda.is_available()
    
        # 对于需要CUDA的测试，如果CUDA不可用则跳过
        if device_ids and device_ids != [] and device_ids != [0] and not cuda_available:
            pytest.skip("需要CUDA设备来测试此场景")
    
        # 调整device_ids：如果CUDA不可用，使用CPU
        if not cuda_available:
            if device_ids is not None and device_ids != []:
                # 对于CPU环境，device_ids应该为空或None
                device_ids = None
    
        # 创建模块
        module = create_module(module_type, input_shape, device_id=0 if cuda_available else -1)
    
        # 创建输入张量
        inputs = create_input_tensor(input_shape, device_id=0 if cuda_available else -1)
    
        # 计算参考输出（单设备执行）
        with torch.no_grad():
            if cuda_available:
                expected_output = module(inputs, **module_kwargs)
            else:
                expected_output = module(inputs, **module_kwargs)
    
        # 使用data_parallel执行
        with torch.no_grad():
>           actual_output = torch.nn.parallel.data_parallel(
                module=module,
                inputs=inputs,
                device_ids=device_ids,
                output_device=output_device,
                dim=dim,
                module_kwargs=module_kwargs
            )

tests/test_torch_nn_parallel_data_parallel_g2.py:399: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = Linear(in_features=10, out_features=5, bias=True)
inputs = (tensor([], size=(0, 10)),), device_ids = None, output_device = 0
dim = 0, module_kwargs = {}

    def data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None):
        r"""Evaluates module(input) in parallel across the GPUs given in device_ids.
    
        This is the functional version of the DataParallel module.
    
        Args:
            module (Module): the module to evaluate in parallel
            inputs (Tensor): inputs to the module
            device_ids (list of int or torch.device): GPU ids on which to replicate module
            output_device (list of int or torch.device): GPU location of the output  Use -1 to indicate the CPU.
                (default: device_ids[0])
        Returns:
            a Tensor containing the result of module(input) located on
            output_device
        """
        if not isinstance(inputs, tuple):
            inputs = (inputs,) if inputs is not None else ()
    
        device_type = _get_available_device_type()
    
        if device_ids is None:
            device_ids = _get_all_device_indices()
    
        if output_device is None:
            output_device = device_ids[0]
    
>       device_ids = [_get_device_index(x, True) for x in device_ids]
E       TypeError: 'NoneType' object is not iterable

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:213: TypeError
_____________________ test_edge_case_handling[test_case3] ______________________

test_case = {'device_ids': [0], 'dim': 0, 'expected_behavior': 'handle_single_batch', 'input_shape': [1, 10], ...}

    @pytest.mark.parametrize("test_case", [
        # 测试1: 空device_ids列表（应该使用所有可用设备）
        {
            "name": "空device_ids列表",
            "module_type": "Linear",
            "input_shape": [4, 10],
            "device_ids": [],  # 空列表
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "use_all_available"
        },
        # 测试2: device_ids为None（应该使用所有可用设备）
        {
            "name": "device_ids为None",
            "module_type": "Linear",
            "input_shape": [4, 10],
            "device_ids": None,  # None值
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "use_all_available"
        },
        # 测试3: 空输入张量
        {
            "name": "空输入张量",
            "module_type": "Linear",
            "input_shape": [0, 10],  # batch size为0
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_empty_input"
        },
        # 测试4: 单元素batch size
        {
            "name": "单元素batch size",
            "module_type": "Linear",
            "input_shape": [1, 10],  # batch size为1
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_single_batch"
        },
        # 测试5: 极端大形状输入（内存允许的情况下）
        {
            "name": "中等大形状输入",
            "module_type": "Linear",
            "input_shape": [100, 50],  # 中等大小，避免内存问题
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_large_input"
        },
        # 测试6: 不同dim值
        {
            "name": "dim=1的分散维度",
            "module_type": "Linear",
            "input_shape": [8, 10],
            "device_ids": [0],
            "output_device": 0,
            "dim": 1,  # 在特征维度上分散
            "module_kwargs": {},
            "expected_behavior": "scatter_along_dim"
        }
    ])
    def test_edge_case_handling(test_case):
        """测试边界条件处理"""
        # 解包测试配置
        name = test_case["name"]
        module_type = test_case["module_type"]
        input_shape = test_case["input_shape"]
        device_ids = test_case["device_ids"]
        output_device = test_case["output_device"]
        dim = test_case["dim"]
        module_kwargs = test_case["module_kwargs"]
        expected_behavior = test_case["expected_behavior"]
    
        # 检查CUDA可用性
        cuda_available = torch.cuda.is_available()
    
        # 对于需要CUDA的测试，如果CUDA不可用则跳过
        if device_ids and device_ids != [] and device_ids != [0] and not cuda_available:
            pytest.skip("需要CUDA设备来测试此场景")
    
        # 调整device_ids：如果CUDA不可用，使用CPU
        if not cuda_available:
            if device_ids is not None and device_ids != []:
                # 对于CPU环境，device_ids应该为空或None
                device_ids = None
    
        # 创建模块
        module = create_module(module_type, input_shape, device_id=0 if cuda_available else -1)
    
        # 创建输入张量
        inputs = create_input_tensor(input_shape, device_id=0 if cuda_available else -1)
    
        # 计算参考输出（单设备执行）
        with torch.no_grad():
            if cuda_available:
                expected_output = module(inputs, **module_kwargs)
            else:
                expected_output = module(inputs, **module_kwargs)
    
        # 使用data_parallel执行
        with torch.no_grad():
>           actual_output = torch.nn.parallel.data_parallel(
                module=module,
                inputs=inputs,
                device_ids=device_ids,
                output_device=output_device,
                dim=dim,
                module_kwargs=module_kwargs
            )

tests/test_torch_nn_parallel_data_parallel_g2.py:399: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = Linear(in_features=10, out_features=5, bias=True)
inputs = (tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
          0.3239, -0.1085]]),)
device_ids = None, output_device = 0, dim = 0, module_kwargs = {}

    def data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None):
        r"""Evaluates module(input) in parallel across the GPUs given in device_ids.
    
        This is the functional version of the DataParallel module.
    
        Args:
            module (Module): the module to evaluate in parallel
            inputs (Tensor): inputs to the module
            device_ids (list of int or torch.device): GPU ids on which to replicate module
            output_device (list of int or torch.device): GPU location of the output  Use -1 to indicate the CPU.
                (default: device_ids[0])
        Returns:
            a Tensor containing the result of module(input) located on
            output_device
        """
        if not isinstance(inputs, tuple):
            inputs = (inputs,) if inputs is not None else ()
    
        device_type = _get_available_device_type()
    
        if device_ids is None:
            device_ids = _get_all_device_indices()
    
        if output_device is None:
            output_device = device_ids[0]
    
>       device_ids = [_get_device_index(x, True) for x in device_ids]
E       TypeError: 'NoneType' object is not iterable

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:213: TypeError
_____________________ test_edge_case_handling[test_case4] ______________________

test_case = {'device_ids': [0], 'dim': 0, 'expected_behavior': 'handle_large_input', 'input_shape': [100, 50], ...}

    @pytest.mark.parametrize("test_case", [
        # 测试1: 空device_ids列表（应该使用所有可用设备）
        {
            "name": "空device_ids列表",
            "module_type": "Linear",
            "input_shape": [4, 10],
            "device_ids": [],  # 空列表
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "use_all_available"
        },
        # 测试2: device_ids为None（应该使用所有可用设备）
        {
            "name": "device_ids为None",
            "module_type": "Linear",
            "input_shape": [4, 10],
            "device_ids": None,  # None值
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "use_all_available"
        },
        # 测试3: 空输入张量
        {
            "name": "空输入张量",
            "module_type": "Linear",
            "input_shape": [0, 10],  # batch size为0
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_empty_input"
        },
        # 测试4: 单元素batch size
        {
            "name": "单元素batch size",
            "module_type": "Linear",
            "input_shape": [1, 10],  # batch size为1
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_single_batch"
        },
        # 测试5: 极端大形状输入（内存允许的情况下）
        {
            "name": "中等大形状输入",
            "module_type": "Linear",
            "input_shape": [100, 50],  # 中等大小，避免内存问题
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_large_input"
        },
        # 测试6: 不同dim值
        {
            "name": "dim=1的分散维度",
            "module_type": "Linear",
            "input_shape": [8, 10],
            "device_ids": [0],
            "output_device": 0,
            "dim": 1,  # 在特征维度上分散
            "module_kwargs": {},
            "expected_behavior": "scatter_along_dim"
        }
    ])
    def test_edge_case_handling(test_case):
        """测试边界条件处理"""
        # 解包测试配置
        name = test_case["name"]
        module_type = test_case["module_type"]
        input_shape = test_case["input_shape"]
        device_ids = test_case["device_ids"]
        output_device = test_case["output_device"]
        dim = test_case["dim"]
        module_kwargs = test_case["module_kwargs"]
        expected_behavior = test_case["expected_behavior"]
    
        # 检查CUDA可用性
        cuda_available = torch.cuda.is_available()
    
        # 对于需要CUDA的测试，如果CUDA不可用则跳过
        if device_ids and device_ids != [] and device_ids != [0] and not cuda_available:
            pytest.skip("需要CUDA设备来测试此场景")
    
        # 调整device_ids：如果CUDA不可用，使用CPU
        if not cuda_available:
            if device_ids is not None and device_ids != []:
                # 对于CPU环境，device_ids应该为空或None
                device_ids = None
    
        # 创建模块
        module = create_module(module_type, input_shape, device_id=0 if cuda_available else -1)
    
        # 创建输入张量
        inputs = create_input_tensor(input_shape, device_id=0 if cuda_available else -1)
    
        # 计算参考输出（单设备执行）
        with torch.no_grad():
            if cuda_available:
                expected_output = module(inputs, **module_kwargs)
            else:
                expected_output = module(inputs, **module_kwargs)
    
        # 使用data_parallel执行
        with torch.no_grad():
>           actual_output = torch.nn.parallel.data_parallel(
                module=module,
                inputs=inputs,
                device_ids=device_ids,
                output_device=output_device,
                dim=dim,
                module_kwargs=module_kwargs
            )

tests/test_torch_nn_parallel_data_parallel_g2.py:399: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = Linear(in_features=50, out_features=5, bias=True)
inputs = (tensor([[ 0.3374, -0.1778, -0.3035,  ..., -0.5850,  0.8768,  1.6221],
        [-1.4779,  1.1331, -1.2203,  ..., -1.31...74, -0.3299,  ...,  0.9435,  1.0918,  1.0149],
        [-1.0725, -0.1296, -0.3116,  ..., -1.7791,  0.9917,  0.0354]]),)
device_ids = None, output_device = 0, dim = 0, module_kwargs = {}

    def data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None):
        r"""Evaluates module(input) in parallel across the GPUs given in device_ids.
    
        This is the functional version of the DataParallel module.
    
        Args:
            module (Module): the module to evaluate in parallel
            inputs (Tensor): inputs to the module
            device_ids (list of int or torch.device): GPU ids on which to replicate module
            output_device (list of int or torch.device): GPU location of the output  Use -1 to indicate the CPU.
                (default: device_ids[0])
        Returns:
            a Tensor containing the result of module(input) located on
            output_device
        """
        if not isinstance(inputs, tuple):
            inputs = (inputs,) if inputs is not None else ()
    
        device_type = _get_available_device_type()
    
        if device_ids is None:
            device_ids = _get_all_device_indices()
    
        if output_device is None:
            output_device = device_ids[0]
    
>       device_ids = [_get_device_index(x, True) for x in device_ids]
E       TypeError: 'NoneType' object is not iterable

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:213: TypeError
_____________________ test_edge_case_handling[test_case5] ______________________

test_case = {'device_ids': [0], 'dim': 1, 'expected_behavior': 'scatter_along_dim', 'input_shape': [8, 10], ...}

    @pytest.mark.parametrize("test_case", [
        # 测试1: 空device_ids列表（应该使用所有可用设备）
        {
            "name": "空device_ids列表",
            "module_type": "Linear",
            "input_shape": [4, 10],
            "device_ids": [],  # 空列表
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "use_all_available"
        },
        # 测试2: device_ids为None（应该使用所有可用设备）
        {
            "name": "device_ids为None",
            "module_type": "Linear",
            "input_shape": [4, 10],
            "device_ids": None,  # None值
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "use_all_available"
        },
        # 测试3: 空输入张量
        {
            "name": "空输入张量",
            "module_type": "Linear",
            "input_shape": [0, 10],  # batch size为0
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_empty_input"
        },
        # 测试4: 单元素batch size
        {
            "name": "单元素batch size",
            "module_type": "Linear",
            "input_shape": [1, 10],  # batch size为1
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_single_batch"
        },
        # 测试5: 极端大形状输入（内存允许的情况下）
        {
            "name": "中等大形状输入",
            "module_type": "Linear",
            "input_shape": [100, 50],  # 中等大小，避免内存问题
            "device_ids": [0],
            "output_device": 0,
            "dim": 0,
            "module_kwargs": {},
            "expected_behavior": "handle_large_input"
        },
        # 测试6: 不同dim值
        {
            "name": "dim=1的分散维度",
            "module_type": "Linear",
            "input_shape": [8, 10],
            "device_ids": [0],
            "output_device": 0,
            "dim": 1,  # 在特征维度上分散
            "module_kwargs": {},
            "expected_behavior": "scatter_along_dim"
        }
    ])
    def test_edge_case_handling(test_case):
        """测试边界条件处理"""
        # 解包测试配置
        name = test_case["name"]
        module_type = test_case["module_type"]
        input_shape = test_case["input_shape"]
        device_ids = test_case["device_ids"]
        output_device = test_case["output_device"]
        dim = test_case["dim"]
        module_kwargs = test_case["module_kwargs"]
        expected_behavior = test_case["expected_behavior"]
    
        # 检查CUDA可用性
        cuda_available = torch.cuda.is_available()
    
        # 对于需要CUDA的测试，如果CUDA不可用则跳过
        if device_ids and device_ids != [] and device_ids != [0] and not cuda_available:
            pytest.skip("需要CUDA设备来测试此场景")
    
        # 调整device_ids：如果CUDA不可用，使用CPU
        if not cuda_available:
            if device_ids is not None and device_ids != []:
                # 对于CPU环境，device_ids应该为空或None
                device_ids = None
    
        # 创建模块
        module = create_module(module_type, input_shape, device_id=0 if cuda_available else -1)
    
        # 创建输入张量
        inputs = create_input_tensor(input_shape, device_id=0 if cuda_available else -1)
    
        # 计算参考输出（单设备执行）
        with torch.no_grad():
            if cuda_available:
                expected_output = module(inputs, **module_kwargs)
            else:
                expected_output = module(inputs, **module_kwargs)
    
        # 使用data_parallel执行
        with torch.no_grad():
>           actual_output = torch.nn.parallel.data_parallel(
                module=module,
                inputs=inputs,
                device_ids=device_ids,
                output_device=output_device,
                dim=dim,
                module_kwargs=module_kwargs
            )

tests/test_torch_nn_parallel_data_parallel_g2.py:399: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

module = Linear(in_features=10, out_features=5, bias=True)
inputs = (tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,
          0.7671, -1.1925],
        ....6236],
        [-2.3229,  1.0878,  0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,
          0.4403, -1.4465]]),)
device_ids = None, output_device = 0, dim = 1, module_kwargs = {}

    def data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None):
        r"""Evaluates module(input) in parallel across the GPUs given in device_ids.
    
        This is the functional version of the DataParallel module.
    
        Args:
            module (Module): the module to evaluate in parallel
            inputs (Tensor): inputs to the module
            device_ids (list of int or torch.device): GPU ids on which to replicate module
            output_device (list of int or torch.device): GPU location of the output  Use -1 to indicate the CPU.
                (default: device_ids[0])
        Returns:
            a Tensor containing the result of module(input) located on
            output_device
        """
        if not isinstance(inputs, tuple):
            inputs = (inputs,) if inputs is not None else ()
    
        device_type = _get_available_device_type()
    
        if device_ids is None:
            device_ids = _get_all_device_indices()
    
        if output_device is None:
            output_device = device_ids[0]
    
>       device_ids = [_get_device_index(x, True) for x in device_ids]
E       TypeError: 'NoneType' object is not iterable

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:213: TypeError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                               Stmts   Miss Branch BrPart  Cover   Missing
----------------------------------------------------------------------------------------------
tests/test_torch_nn_parallel_data_parallel_g2.py     152     61     66     16    55%   15, 27-30, 34, 38, 42-43, 55-59, 63-64, 71-72, 77-80, 124->129, 137, 155-169, 254-255, 262-263, 283->exit, 376, 379->385, 393, 409-453, 458
----------------------------------------------------------------------------------------------
TOTAL                                                152     61     66     16    55%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_parallel_data_parallel_g2.py::test_cpu_as_output_device[test_config0]
FAILED tests/test_torch_nn_parallel_data_parallel_g2.py::test_cpu_as_output_device[test_config1]
FAILED tests/test_torch_nn_parallel_data_parallel_g2.py::test_parameter_validation_and_exceptions[test_case0]
FAILED tests/test_torch_nn_parallel_data_parallel_g2.py::test_parameter_validation_and_exceptions[test_case1]
FAILED tests/test_torch_nn_parallel_data_parallel_g2.py::test_parameter_validation_and_exceptions[test_case3]
FAILED tests/test_torch_nn_parallel_data_parallel_g2.py::test_parameter_validation_and_exceptions[test_case4]
FAILED tests/test_torch_nn_parallel_data_parallel_g2.py::test_edge_case_handling[test_case0]
FAILED tests/test_torch_nn_parallel_data_parallel_g2.py::test_edge_case_handling[test_case1]
FAILED tests/test_torch_nn_parallel_data_parallel_g2.py::test_edge_case_handling[test_case2]
FAILED tests/test_torch_nn_parallel_data_parallel_g2.py::test_edge_case_handling[test_case3]
FAILED tests/test_torch_nn_parallel_data_parallel_g2.py::test_edge_case_handling[test_case4]
FAILED tests/test_torch_nn_parallel_data_parallel_g2.py::test_edge_case_handling[test_case5]
12 failed, 1 skipped in 0.62s

Error: exit 1