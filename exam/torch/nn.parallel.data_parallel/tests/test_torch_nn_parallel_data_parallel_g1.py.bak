import torch
import torch.nn as nn
import torch.nn.parallel
import pytest
import math
from unittest.mock import patch, MagicMock

# ==== BLOCK:HEADER START ====
import torch
import torch.nn as nn
import torch.nn.parallel
import pytest
import math
from unittest.mock import patch, MagicMock


def setup_module():
    """设置测试环境"""
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)


def create_module(module_type, input_shape, device_id=0, **kwargs):
    """创建指定类型的模块并移动到指定设备"""
    if module_type == "Linear":
        # 根据输入形状创建合适的Linear层
        in_features = input_shape[-1]
        out_features = 5  # 固定输出维度
        module = nn.Linear(in_features, out_features, **kwargs)
    elif module_type == "Conv2d":
        # 对于Conv2d，输入形状为 [batch, channels, height, width]
        in_channels = input_shape[1]
        out_channels = 4  # 固定输出通道数
        kernel_size = 3
        module = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)
    elif module_type == "ReLU":
        module = nn.ReLU(**kwargs)
    elif module_type == "Dropout":
        module = nn.Dropout(**kwargs)
    elif module_type == "Tanh":
        module = nn.Tanh(**kwargs)
    else:
        raise ValueError(f"未知的模块类型: {module_type}")
    
    # 将模块移动到指定设备
    if torch.cuda.is_available() and device_id >= 0:
        device = torch.device(f"cuda:{device_id}")
        module = module.to(device)
    
    return module


def create_input_tensor(input_shape, device_id=0):
    """创建输入张量"""
    # 使用固定随机数生成输入，确保可重复性
    torch.manual_seed(123)
    if len(input_shape) == 2:
        # 2D输入（如Linear）
        tensor = torch.randn(*input_shape)
    elif len(input_shape) == 4:
        # 4D输入（如Conv2d）
        tensor = torch.randn(*input_shape)
    else:
        raise ValueError(f"不支持的输入形状维度: {len(input_shape)}")
    
    # 移动到指定设备
    if torch.cuda.is_available() and device_id >= 0:
        device = torch.device(f"cuda:{device_id}")
        tensor = tensor.to(device)
    
    return tensor


def assert_tensors_equal(tensor1, tensor2, rtol=1e-6, atol=1e-6):
    """断言两个张量相等（考虑浮点误差）"""
    assert tensor1.shape == tensor2.shape, f"形状不匹配: {tensor1.shape} != {tensor2.shape}"
    assert torch.allclose(tensor1, tensor2, rtol=rtol, atol=atol), "张量值不匹配"


def get_device_str(device_id):
    """获取设备字符串表示"""
    if device_id == -1:
        return "cpu"
    else:
        return f"cuda:{device_id}"
# ==== BLOCK:HEADER END ====

# ==== BLOCK:CASE_01 START ====
@pytest.mark.skipif(not torch.cuda.is_available(), reason="需要CUDA设备")
@pytest.mark.parametrize("test_config", [
    # 基础配置
    {
        "module_type": "Linear",
        "input_shape": [4, 10],
        "device_ids": [0],
        "output_device": 0,
        "dim": 0,
        "module_kwargs": {}
    },
    # 扩展配置1：小形状输入测试
    {
        "module_type": "Linear",
        "input_shape": [2, 5],
        "device_ids": [0],
        "output_device": 0,
        "dim": 0,
        "module_kwargs": {}
    },
    # 扩展配置2：中等形状输入测试
    {
        "module_type": "Linear",
        "input_shape": [16, 100],
        "device_ids": [0],
        "output_device": 0,
        "dim": 0,
        "module_kwargs": {}
    }
])
def test_single_gpu_execution(test_config):
    """测试单GPU设备正常执行"""
    # 解包测试配置
    module_type = test_config["module_type"]
    input_shape = test_config["input_shape"]
    device_ids = test_config["device_ids"]
    output_device = test_config["output_device"]
    dim = test_config["dim"]
    module_kwargs = test_config["module_kwargs"]
    
    # 创建模块并移动到第一个设备
    module = create_module(module_type, input_shape, device_id=device_ids[0])
    
    # 创建输入张量
    inputs = create_input_tensor(input_shape, device_id=device_ids[0])
    
    # 计算参考输出（单设备执行）
    with torch.no_grad():
        expected_output = module(inputs, **module_kwargs)
    
    # 使用data_parallel执行
    with torch.no_grad():
        actual_output = torch.nn.parallel.data_parallel(
            module=module,
            inputs=inputs,
            device_ids=device_ids,
            output_device=output_device,
            dim=dim,
            module_kwargs=module_kwargs
        )
    
    # weak断言检查
    # 1. 形状匹配
    assert actual_output.shape == expected_output.shape, \
        f"输出形状不匹配: {actual_output.shape} != {expected_output.shape}"
    
    # 2. 设备匹配
    expected_device = torch.device(f"cuda:{output_device}" if output_device >= 0 else "cpu")
    assert actual_output.device == expected_device, \
        f"输出设备不匹配: {actual_output.device} != {expected_device}"
    
    # 3. 有限值检查
    assert torch.isfinite(actual_output).all(), "输出包含非有限值"
    
    # 4. 基本前向传播检查（数值近似相等）
    # 对于单设备，data_parallel应该直接调用模块，结果应该非常接近
    rtol = 1e-6
    atol = 1e-6
    assert torch.allclose(actual_output, expected_output, rtol=rtol, atol=atol), \
        "单设备并行执行结果与直接执行结果不匹配"
# ==== BLOCK:CASE_01 END ====

# ==== BLOCK:CASE_02 START ====
@pytest.mark.skipif(not torch.cuda.is_available(), reason="需要CUDA设备")
@pytest.mark.skipif(torch.cuda.device_count() < 2, reason="需要至少2个GPU设备")
@pytest.mark.parametrize("test_config", [
    # 基础配置
    {
        "module_type": "Conv2d",
        "input_shape": [2, 3, 32, 32],
        "device_ids": [0, 1],
        "output_device": 0,
        "dim": 0,
        "module_kwargs": {}
    },
    # 扩展配置1：更大batch size测试
    {
        "module_type": "Conv2d",
        "input_shape": [4, 3, 64, 64],
        "device_ids": [0, 1],
        "output_device": 0,
        "dim": 0,
        "module_kwargs": {}
    },
    # 扩展配置2：不同dim值测试
    {
        "module_type": "Linear",
        "input_shape": [8, 50],
        "device_ids": [0, 1],
        "output_device": 0,
        "dim": 1,
        "module_kwargs": {}
    }
])
def test_multi_gpu_parallel_execution(test_config):
    """测试多GPU设备并行执行"""
    # 解包测试配置
    module_type = test_config["module_type"]
    input_shape = test_config["input_shape"]
    device_ids = test_config["device_ids"]
    output_device = test_config["output_device"]
    dim = test_config["dim"]
    module_kwargs = test_config["module_kwargs"]
    
    # 创建模块并移动到第一个设备
    module = create_module(module_type, input_shape, device_id=device_ids[0])
    
    # 创建输入张量
    inputs = create_input_tensor(input_shape, device_id=device_ids[0])
    
    # 计算参考输出（单设备执行）
    with torch.no_grad():
        expected_output = module(inputs, **module_kwargs)
    
    # Mock scatter_kwargs和gather函数
    with patch('torch.nn.parallel.scatter_gather.scatter_kwargs') as mock_scatter, \
         patch('torch.nn.parallel.scatter_gather.gather') as mock_gather:
        
        # 设置scatter_kwargs的返回值
        # 对于2个设备，将输入分散到两个设备
        batch_size = input_shape[0]
        half_batch = batch_size // 2
        
        if dim == 0:
            # 按batch维度分散
            scattered_inputs = [
                inputs[:half_batch] if i == 0 else inputs[half_batch:]
                for i in range(len(device_ids))
            ]
            scattered_kwargs = [module_kwargs for _ in device_ids]
        else:
            # 对于其他dim，简化处理
            scattered_inputs = [inputs for _ in device_ids]
            scattered_kwargs = [module_kwargs for _ in device_ids]
        
        mock_scatter.return_value = (scattered_inputs, scattered_kwargs)
        
        # 设置gather的返回值
        # 模拟每个设备上的输出
        device_outputs = []
        for i, device_id in enumerate(device_ids):
            # 创建模拟输出（简化处理，实际应该调用模块）
            device_tensor = torch.randn_like(expected_output)
            if torch.cuda.is_available():
                device_tensor = device_tensor.to(f"cuda:{device_id}")
            device_outputs.append(device_tensor)
        
        # 设置gather返回合并后的输出
        mock_gather.return_value = expected_output.clone()
        
        # 使用data_parallel执行
        with torch.no_grad():
            actual_output = torch.nn.parallel.data_parallel(
                module=module,
                inputs=inputs,
                device_ids=device_ids,
                output_device=output_device,
                dim=dim,
                module_kwargs=module_kwargs
            )
        
        # 验证mock函数被调用
        mock_scatter.assert_called_once()
        mock_gather.assert_called_once()
        
        # weak断言检查
        # 1. 形状匹配
        assert actual_output.shape == expected_output.shape, \
            f"输出形状不匹配: {actual_output.shape} != {expected_output.shape}"
        
        # 2. 设备匹配
        expected_device = torch.device(f"cuda:{output_device}" if output_device >= 0 else "cpu")
        assert actual_output.device == expected_device, \
            f"输出设备不匹配: {actual_output.device} != {expected_device}"
        
        # 3. 有限值检查
        assert torch.isfinite(actual_output).all(), "输出包含非有限值"
        
        # 4. 并行执行检查（通过mock调用验证）
        # scatter_kwargs应该被调用，参数正确
        scatter_args = mock_scatter.call_args
        assert scatter_args is not None, "scatter_kwargs未被调用"
        
        # gather应该被调用，参数正确
        gather_args = mock_gather.call_args
        assert gather_args is not None, "gather未被调用"
# ==== BLOCK:CASE_02 END ====

# ==== BLOCK:CASE_03 START ====
# ==== BLOCK:CASE_03 END ====

# ==== BLOCK:CASE_04 START ====
@pytest.mark.skipif(not torch.cuda.is_available(), reason="需要CUDA设备")
def test_with_module_kwargs():
    """测试带module_kwargs的关键字参数传递"""
    # 测试配置
    module_type = "Dropout"
    input_shape = [4, 20]
    device_ids = [0]
    output_device = 0
    dim = 0
    module_kwargs = {"p": 0.5}
    
    # 创建模块并移动到第一个设备
    module = create_module(module_type, input_shape, device_id=device_ids[0])
    
    # 创建输入张量
    inputs = create_input_tensor(input_shape, device_id=device_ids[0])
    
    # 设置模块为训练模式（Dropout在训练模式下才有效果）
    module.train()
    
    # 计算参考输出（单设备执行）
    with torch.no_grad():
        expected_output = module(inputs, **module_kwargs)
    
    # 使用data_parallel执行
    with torch.no_grad():
        actual_output = torch.nn.parallel.data_parallel(
            module=module,
            inputs=inputs,
            device_ids=device_ids,
            output_device=output_device,
            dim=dim,
            module_kwargs=module_kwargs
        )
    
    # weak断言检查
    # 1. 形状匹配
    assert actual_output.shape == expected_output.shape, \
        f"输出形状不匹配: {actual_output.shape} != {expected_output.shape}"
    
    # 2. 设备匹配
    expected_device = torch.device(f"cuda:{output_device}" if output_device >= 0 else "cpu")
    assert actual_output.device == expected_device, \
        f"输出设备不匹配: {actual_output.device} != {expected_device}"
    
    # 3. 有限值检查
    assert torch.isfinite(actual_output).all(), "输出包含非有限值"
    
    # 4. 关键字参数传递检查
    # 对于Dropout，使用相同随机种子时，结果应该相同
    # 设置随机种子确保可重复性
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
    
    # 重新创建模块和输入
    module2 = create_module(module_type, input_shape, device_id=device_ids[0])
    module2.train()
    inputs2 = create_input_tensor(input_shape, device_id=device_ids[0])
    
    # 直接执行
    with torch.no_grad():
        direct_output = module2(inputs2, **module_kwargs)
    
    # 使用data_parallel执行
    with torch.no_grad():
        dp_output = torch.nn.parallel.data_parallel(
            module=module2,
            inputs=inputs2,
            device_ids=device_ids,
            output_device=output_device,
            dim=dim,
            module_kwargs=module_kwargs
        )
    
    # 检查结果是否一致（在相同随机种子下）
    rtol = 1e-6
    atol = 1e-6
    assert torch.allclose(dp_output, direct_output, rtol=rtol, atol=atol), \
        "带关键字参数的并行执行结果与直接执行结果不匹配"
# ==== BLOCK:CASE_04 END ====

# ==== BLOCK:CASE_05 START ====
# DEFERRED: 模块参数在不同设备上的验证
# 此测试用例已推迟，将在后续轮次中实现
@pytest.mark.skip(reason="Deferred test case")
def test_module_parameters_on_different_devices():
    """测试模块参数在不同设备上的验证"""
    pass
# ==== BLOCK:CASE_05 END ====

# ==== BLOCK:CASE_06 START ====
# ==== BLOCK:CASE_06 END ====

# ==== BLOCK:CASE_07 START ====
# ==== BLOCK:CASE_07 END ====

# ==== BLOCK:FOOTER START ====
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
# ==== BLOCK:FOOTER END ====