=== Run Tests ===
FFF.                                                                     [100%]
=================================== FAILURES ===================================
_ TestGradcheckAdvanced.test_sparse_tensor_gradient_check[dtype0-shape0-True-1e-06-1e-05-0.001] _

self = <test_torch_autograd_gradcheck_advanced.TestGradcheckAdvanced object at 0x112389bd0>
dtype = torch.float32, shape = (4, 4), sparse = True, eps = 1e-06, atol = 1e-05
rtol = 0.001

    @pytest.mark.parametrize("dtype,shape,sparse,eps,atol,rtol", [
        (torch.float32, (4, 4), True, 1e-6, 1e-5, 0.001),  # Base case from param_matrix
        (torch.float64, (6, 6), True, 1e-6, 1e-5, 0.001),  # Extension: large sparse tensor
    ])
    def test_sparse_tensor_gradient_check(self, dtype, shape, sparse, eps, atol, rtol):
        """
        TC-03: Sparse tensor gradient check
        Test gradcheck with sparse tensor inputs.
    
        Weak assertions:
        - returns_bool: gradcheck returns boolean
        - no_exception: no exception raised during execution
        - sparse_gradient_check: gradient check passes for sparse tensor
        """
        # Set random seed for reproducibility
        torch.manual_seed(42)
    
        # Create sparse tensor
        dense_tensor = torch.randn(*shape, dtype=dtype)
        # Create sparse tensor with some zeros
        indices = torch.tensor([[0, 1, 2], [0, 1, 2]], dtype=torch.long)
        values = torch.tensor([1.0, 2.0, 3.0], dtype=dtype)
        sparse_tensor = torch.sparse_coo_tensor(indices, values, shape, dtype=dtype)
        sparse_tensor.requires_grad_(True)
    
        # Test with sparse tensor
>       result = torch.autograd.gradcheck(
            func=simple_polynomial,
            inputs=(sparse_tensor,),
            eps=eps,
            atol=atol,
            rtol=rtol,
            raise_exception=True,
            check_sparse_nnz=True,
            check_backward_ad=True
        )

tests/test_torch_autograd_gradcheck_advanced.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/autograd/gradcheck.py:1418: in gradcheck
    return _gradcheck_helper(**args)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/autograd/gradcheck.py:1427: in _gradcheck_helper
    func_out = func(*tupled_inputs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = tensor(indices=tensor([[0, 1, 2],
                       [0, 1, 2]]),
       values=tensor([1., 2., 3.]),
       size=(4, 4), nnz=3, layout=torch.sparse_coo, requires_grad=True)

    def simple_polynomial(x: torch.Tensor) -> torch.Tensor:
        """Simple polynomial function: f(x) = x^3 + 2x^2 + 3x + 4"""
>       return x.pow(3) + 2 * x.pow(2) + 3 * x + 4
E       RuntimeError: add(sparse, dense) is not supported. Use add(dense, sparse) instead.

tests/test_torch_autograd_gradcheck_advanced.py:17: RuntimeError
_ TestGradcheckAdvanced.test_sparse_tensor_gradient_check[dtype1-shape1-True-1e-06-1e-05-0.001] _

self = <test_torch_autograd_gradcheck_advanced.TestGradcheckAdvanced object at 0x112389c60>
dtype = torch.float64, shape = (6, 6), sparse = True, eps = 1e-06, atol = 1e-05
rtol = 0.001

    @pytest.mark.parametrize("dtype,shape,sparse,eps,atol,rtol", [
        (torch.float32, (4, 4), True, 1e-6, 1e-5, 0.001),  # Base case from param_matrix
        (torch.float64, (6, 6), True, 1e-6, 1e-5, 0.001),  # Extension: large sparse tensor
    ])
    def test_sparse_tensor_gradient_check(self, dtype, shape, sparse, eps, atol, rtol):
        """
        TC-03: Sparse tensor gradient check
        Test gradcheck with sparse tensor inputs.
    
        Weak assertions:
        - returns_bool: gradcheck returns boolean
        - no_exception: no exception raised during execution
        - sparse_gradient_check: gradient check passes for sparse tensor
        """
        # Set random seed for reproducibility
        torch.manual_seed(42)
    
        # Create sparse tensor
        dense_tensor = torch.randn(*shape, dtype=dtype)
        # Create sparse tensor with some zeros
        indices = torch.tensor([[0, 1, 2], [0, 1, 2]], dtype=torch.long)
        values = torch.tensor([1.0, 2.0, 3.0], dtype=dtype)
        sparse_tensor = torch.sparse_coo_tensor(indices, values, shape, dtype=dtype)
        sparse_tensor.requires_grad_(True)
    
        # Test with sparse tensor
>       result = torch.autograd.gradcheck(
            func=simple_polynomial,
            inputs=(sparse_tensor,),
            eps=eps,
            atol=atol,
            rtol=rtol,
            raise_exception=True,
            check_sparse_nnz=True,
            check_backward_ad=True
        )

tests/test_torch_autograd_gradcheck_advanced.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/autograd/gradcheck.py:1418: in gradcheck
    return _gradcheck_helper(**args)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/autograd/gradcheck.py:1427: in _gradcheck_helper
    func_out = func(*tupled_inputs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = tensor(indices=tensor([[0, 1, 2],
                       [0, 1, 2]]),
       values=tensor([1., 2., 3.]),
       size=(6, 6), nnz=3, dtype=torch.float64, layout=torch.sparse_coo,
       requires_grad=True)

    def simple_polynomial(x: torch.Tensor) -> torch.Tensor:
        """Simple polynomial function: f(x) = x^3 + 2x^2 + 3x + 4"""
>       return x.pow(3) + 2 * x.pow(2) + 3 * x + 4
E       RuntimeError: add(sparse, dense) is not supported. Use add(dense, sparse) instead.

tests/test_torch_autograd_gradcheck_advanced.py:17: RuntimeError
_ TestGradcheckAdvanced.test_exception_handling_raise_exception_behavior[dtype0-shape0-1e-06-1e-05-0.001] _

self = <test_torch_autograd_gradcheck_advanced.TestGradcheckAdvanced object at 0x11238a050>
dtype = torch.float32, shape = (2, 2), eps = 1e-06, atol = 1e-05, rtol = 0.001

    @pytest.mark.parametrize("dtype,shape,eps,atol,rtol", [
        (torch.float32, (2, 2), 1e-6, 1e-5, 0.001),  # Base case from param_matrix
    ])
    def test_exception_handling_raise_exception_behavior(self, dtype, shape, eps, atol, rtol):
        """
        TC-05: Exception handling: raise_exception behavior
        Test gradcheck with raise_exception=False when gradient check fails.
    
        Weak assertions:
        - returns_false: gradcheck returns False when gradient check fails
        - no_exception_raised: no exception raised when raise_exception=False
        - exception_handling: proper handling of gradient check failures
        """
        # Set random seed for reproducibility
        torch.manual_seed(42)
    
        # Create input tensor
        x = torch.randn(*shape, dtype=dtype)
        x.requires_grad_(True)
    
        # Test with raise_exception=False when gradient check should fail
        result = torch.autograd.gradcheck(
            func=failing_function,
            inputs=(x,),
            eps=eps,
            atol=atol,
            rtol=rtol,
            raise_exception=False,  # Should not raise exception
            check_backward_ad=True
        )
    
        # Weak assertion: returns False when gradient check fails
        assert isinstance(result, bool), f"gradcheck should return bool, got {type(result)}"
        assert result is False, f"gradcheck should return False for failing function. dtype={dtype}, shape={shape}"
    
        # Weak assertion: no exception raised (implied by successful execution)
        # Weak assertion: exception handling works correctly
    
        # Additional test: verify that raise_exception=True would raise an exception
>       with pytest.raises(torch.autograd.gradcheck.GradcheckError) as exc_info:
E       AttributeError: 'function' object has no attribute 'GradcheckError'

tests/test_torch_autograd_gradcheck_advanced.py:114: AttributeError
=============================== warnings summary ===============================
exam/torch_group/autograd.gradcheck/tests/test_torch_autograd_gradcheck_advanced.py::TestGradcheckAdvanced::test_sparse_tensor_gradient_check[dtype0-shape0-True-1e-06-1e-05-0.001]
exam/torch_group/autograd.gradcheck/tests/test_torch_autograd_gradcheck_advanced.py::TestGradcheckAdvanced::test_exception_handling_raise_exception_behavior[dtype0-shape0-1e-06-1e-05-0.001]
  /opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/autograd/gradcheck.py:652: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                              Stmts   Miss Branch BrPart  Cover   Missing
---------------------------------------------------------------------------------------------
tests/test_torch_autograd_gradcheck_advanced.py      47      6      4      2    84%   67-71, 115-126, 175, 194
---------------------------------------------------------------------------------------------
TOTAL                                                47      6      4      2    84%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_autograd_gradcheck_advanced.py::TestGradcheckAdvanced::test_sparse_tensor_gradient_check[dtype0-shape0-True-1e-06-1e-05-0.001]
FAILED tests/test_torch_autograd_gradcheck_advanced.py::TestGradcheckAdvanced::test_sparse_tensor_gradient_check[dtype1-shape1-True-1e-06-1e-05-0.001]
FAILED tests/test_torch_autograd_gradcheck_advanced.py::TestGradcheckAdvanced::test_exception_handling_raise_exception_behavior[dtype0-shape0-1e-06-1e-05-0.001]
3 failed, 1 passed, 2 warnings in 0.84s

Error: exit 1