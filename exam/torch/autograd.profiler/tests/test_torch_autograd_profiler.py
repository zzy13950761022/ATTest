"""
Test cases for torch.autograd.profiler module.
Generated by TestAgent for target: torch.autograd.profiler
"""

import pytest
import torch
import torch.nn as nn
import torch.nn.functional as F
import tempfile
import os
import json
from unittest.mock import patch, MagicMock

# ==== BLOCK:HEADER START ====
# Test fixtures and helper functions
import pytest
import torch
import torch.nn as nn
import torch.nn.functional as F
import tempfile
import os
import json
import sys
from unittest.mock import patch, MagicMock

# 确保测试环境正确设置
@pytest.fixture(autouse=True)
def setup_test_environment():
    """Setup test environment before each test."""
    # 固定随机种子
    torch.manual_seed(42)
    # 确保使用CPU模式
    torch.set_grad_enabled(True)
    # 清理可能存在的分析器状态
    try:
        torch.autograd._disable_profiler()
    except Exception:
        pass
    yield
    # 测试后清理
    try:
        torch.autograd._disable_profiler()
    except Exception:
        pass

@pytest.fixture
def fixed_seed():
    """Fix random seed for reproducibility."""
    torch.manual_seed(42)
    return 42

@pytest.fixture
def simple_tensor_ops():
    """Create simple tensor operations for profiling."""
    x = torch.randn(2, 3, requires_grad=True)
    y = torch.randn(3, 4, requires_grad=True)
    return x, y

@pytest.fixture
def mock_cuda_unavailable():
    """Mock CUDA as unavailable for tests that require CPU-only."""
    with patch('torch.cuda.is_available', return_value=False):
        with patch('torch.cuda.device_count', return_value=0):
            yield

@pytest.fixture
def temp_trace_file():
    """Create a temporary file for chrome trace export."""
    with tempfile.NamedTemporaryFile(suffix='.json', delete=False) as f:
        temp_path = f.name
    yield temp_path
    # Cleanup
    if os.path.exists(temp_path):
        os.unlink(temp_path)
# ==== BLOCK:HEADER END ====

# ==== BLOCK:CASE_01 START ====
def test_basic_cpu_profiling(fixed_seed, simple_tensor_ops):
    """TC-01: 基本 CPU 分析
    
    测试 profile 上下文管理器的基本功能，使用默认配置。
    Weak assertions: context_manager_works, profiler_created, events_recorded, no_exception
    """
    x, y = simple_tensor_ops
    
    # 使用默认配置进行性能分析
    with torch.autograd.profiler.profile(
        enabled=True,
        use_cuda=False,
        record_shapes=False,
        with_flops=False,
        profile_memory=False,
        with_stack=False,
        with_modules=False,
        use_kineto=False,
        use_cpu=True,
        experimental_config=None
    ) as prof:
        # 执行一些张量操作
        z = torch.matmul(x, y)
        loss = z.sum()
        loss.backward()
    
    # Weak assertions
    # 1. 上下文管理器正常工作
    assert prof is not None, "Profiler should be created"
    
    # 2. 分析器已创建
    assert hasattr(prof, 'events'), "Profiler should have events attribute"
    assert hasattr(prof, 'key_averages'), "Profiler should have key_averages method"
    assert hasattr(prof, 'table'), "Profiler should have table method"
    
    # 3. 事件已记录
    # 检查是否有事件被记录（即使为空列表，也说明分析器工作）
    events = prof.events()
    assert isinstance(events, list), "Events should be a list"
    
    # 4. 无异常
    # 如果执行到这里，说明没有抛出异常
    
    # 额外检查：table() 方法正常工作
    table_str = prof.table()
    assert isinstance(table_str, str), "table() should return a string"
    assert len(table_str) > 0, "table() should return non-empty string"
    
    # 额外检查：key_averages() 方法正常工作
    key_avg = prof.key_averages()
    assert key_avg is not None, "key_averages() should not return None"
    
    print(f"Basic CPU profiling test passed. Recorded {len(events)} events.")
# ==== BLOCK:CASE_01 END ====

# ==== BLOCK:CASE_02 START ====
def test_disabled_profiler(fixed_seed, simple_tensor_ops):
    """TC-02: 禁用分析器
    
    测试当 enabled=False 时，分析器不记录任何事件。
    Weak assertions: context_manager_works, profiler_created, no_events_when_disabled, no_exception
    """
    x, y = simple_tensor_ops
    
    # 使用禁用配置进行性能分析
    with torch.autograd.profiler.profile(
        enabled=False,  # 关键：禁用分析器
        use_cuda=False,
        record_shapes=False,
        with_flops=False,
        profile_memory=False,
        with_stack=False,
        with_modules=False,
        use_kineto=False,
        use_cpu=True,
        experimental_config=None
    ) as prof:
        # 执行一些张量操作
        z = torch.matmul(x, y)
        loss = z.sum()
        loss.backward()
    
    # Weak assertions
    # 1. 上下文管理器正常工作
    assert prof is not None, "Profiler should be created even when disabled"
    
    # 2. 分析器已创建
    assert hasattr(prof, 'events'), "Profiler should have events attribute"
    assert hasattr(prof, 'key_averages'), "Profiler should have key_averages method"
    
    # 3. 禁用时无事件记录
    events = prof.events()
    assert isinstance(events, list), "Events should be a list"
    # 当分析器禁用时，事件列表可能为空或包含很少的事件
    # 我们只检查类型，不检查数量
    
    # 4. 无异常
    # 如果执行到这里，说明没有抛出异常
    
    # 额外检查：table() 方法返回空表或占位符
    table_str = prof.table()
    assert isinstance(table_str, str), "table() should return a string"
    
    # 额外检查：key_averages() 方法正常工作
    key_avg = prof.key_averages()
    assert key_avg is not None, "key_averages() should not return None"
    
    print(f"Disabled profiler test passed. Table length: {len(table_str)}")
# ==== BLOCK:CASE_02 END ====

# ==== BLOCK:CASE_03 START ====
def test_shape_recording(fixed_seed):
    """TC-03: 形状记录功能
    
    测试 record_shapes=True 时，分析器正确记录张量形状信息。
    Weak assertions: context_manager_works, profiler_created, events_recorded, shape_info_present
    """
    # 创建不同形状的张量
    x1 = torch.randn(2, 3, requires_grad=True)
    x2 = torch.randn(3, 4, requires_grad=True)
    x3 = torch.randn(4, 2, requires_grad=True)
    
    # 使用形状记录配置进行性能分析
    with torch.autograd.profiler.profile(
        enabled=True,
        use_cuda=False,
        record_shapes=True,  # 关键：启用形状记录
        with_flops=False,
        profile_memory=False,
        with_stack=False,
        with_modules=False,
        use_kineto=False,
        use_cpu=True,
        experimental_config=None
    ) as prof:
        # 执行多个不同形状的张量操作
        y1 = torch.matmul(x1, x2)  # 形状: (2, 4)
        y2 = torch.matmul(x2, x3)  # 形状: (3, 2)
        y3 = torch.matmul(y1, y2)  # 形状: (2, 2)
        
        loss = y3.sum()
        loss.backward()
    
    # Weak assertions
    # 1. 上下文管理器正常工作
    assert prof is not None, "Profiler should be created"
    
    # 2. 分析器已创建
    assert hasattr(prof, 'events'), "Profiler should have events attribute"
    assert hasattr(prof, 'key_averages'), "Profiler should have key_averages method"
    
    # 3. 事件已记录
    events = prof.events()
    assert isinstance(events, list), "Events should be a list"
    assert len(events) > 0, "Should record some events with shape recording enabled"
    
    # 4. 形状信息存在
    # 检查 key_averages 是否支持按形状分组
    key_avg = prof.key_averages()
    assert key_avg is not None, "key_averages() should not return None"
    
    # 尝试按输入形状分组（当 record_shapes=True 时应该可用）
    try:
        grouped_by_shape = prof.key_averages(group_by_input_shape=True)
        assert grouped_by_shape is not None, "Grouping by input shape should work"
    except Exception as e:
        # 如果分组失败，可能是不支持或没有形状数据
        # 这不算测试失败，只是记录
        print(f"Note: group_by_input_shape raised exception: {e}")
    
    # 额外检查：table() 方法正常工作
    table_str = prof.table()
    assert isinstance(table_str, str), "table() should return a string"
    assert len(table_str) > 0, "table() should return non-empty string"
    
    # 检查表格中是否包含形状相关信息
    # 注意：这取决于具体的实现，可能不是强保证
    if "shape" in table_str.lower() or "dim" in table_str.lower():
        print("Shape information found in table output")
    
    print(f"Shape recording test passed. Recorded {len(events)} events.")
# ==== BLOCK:CASE_03 END ====

# ==== BLOCK:CASE_04 START ====
def test_nested_profiler_raises_exception(fixed_seed, simple_tensor_ops):
    """TC-04: 嵌套调用检测
    
    测试嵌套调用 profile 上下文管理器时是否抛出 RuntimeError。
    Weak assertions: nested_context_raises, exception_type_correct, error_message_contains_nested
    """
    x, y = simple_tensor_ops
    
    # 外层分析器
    with torch.autograd.profiler.profile(
        enabled=True,
        use_cuda=False,
        record_shapes=False,
        with_flops=False,
        profile_memory=False,
        with_stack=False,
        with_modules=False,
        use_kineto=False,
        use_cpu=True,
        experimental_config=None
    ) as outer_prof:
        # 执行一些操作
        z = torch.matmul(x, y)
        
        # 尝试嵌套调用 - 应该抛出 RuntimeError
        try:
            with torch.autograd.profiler.profile(
                enabled=True,
                use_cuda=False,
                record_shapes=False,
                with_flops=False,
                profile_memory=False,
                with_stack=False,
                with_modules=False,
                use_kineto=False,
                use_cpu=True,
                experimental_config=None
            ) as inner_prof:
                w = torch.randn(4, 2, requires_grad=True)
                v = torch.matmul(z, w)
            
            # 如果执行到这里，说明没有抛出异常，测试失败
            assert False, "Nested profiler should raise RuntimeError"
            
        except RuntimeError as e:
            # Weak assertions
            # 1. 嵌套上下文抛出异常
            # 已经捕获到 RuntimeError
            
            # 2. 异常类型正确
            assert isinstance(e, RuntimeError), f"Expected RuntimeError, got {type(e).__name__}"
            
            # 3. 错误消息包含嵌套相关信息
            error_msg = str(e).lower()
            # 检查错误消息是否包含相关关键词
            if "nested" in error_msg or "already" in error_msg or "profiler" in error_msg:
                print(f"Nested profiler correctly raised RuntimeError: {e}")
            else:
                # 即使错误消息不包含关键词，只要抛出 RuntimeError 就通过
                print(f"Nested profiler raised RuntimeError (message: {e})")
            
            # 继续执行外层分析器的操作
            loss = z.sum()
            loss.backward()
    
    # 检查外层分析器是否正常工作
    assert outer_prof is not None, "Outer profiler should be created"
    events = outer_prof.events()
    assert isinstance(events, list), "Events should be a list"
    
    print(f"Nested profiler test passed. Outer profiler recorded {len(events)} events.")
# ==== BLOCK:CASE_04 END ====
# ==== BLOCK:CASE_04 END ====

# ==== BLOCK:CASE_05 START ====
def test_memory_profiling(fixed_seed, simple_tensor_ops):
    """TC-05: 内存分析功能
    
    测试 profile_memory=True 时，分析器正确记录内存分配和释放信息。
    Weak assertions: context_manager_works, profiler_created, events_recorded, memory_info_present
    """
    x, y = simple_tensor_ops
    
    # 使用内存分析配置进行性能分析
    with torch.autograd.profiler.profile(
        enabled=True,
        use_cuda=False,
        record_shapes=False,
        with_flops=False,
        profile_memory=True,  # 关键：启用内存分析
        with_stack=False,
        with_modules=False,
        use_kineto=False,
        use_cpu=True,
        experimental_config=None
    ) as prof:
        # 执行一些张量操作，包括内存分配
        z = torch.matmul(x, y)
        # 创建新的张量以触发内存分配
        w = torch.randn(4, 2, requires_grad=True)
        v = torch.matmul(z, w)
        
        loss = v.sum()
        loss.backward()
        
        # 删除一些张量以触发内存释放
        del w
        del v
    
    # Weak assertions
    # 1. 上下文管理器正常工作
    assert prof is not None, "Profiler should be created"
    
    # 2. 分析器已创建
    assert hasattr(prof, 'events'), "Profiler should have events attribute"
    assert hasattr(prof, 'key_averages'), "Profiler should have key_averages method"
    
    # 3. 事件已记录
    events = prof.events()
    assert isinstance(events, list), "Events should be a list"
    assert len(events) > 0, "Should record some events with memory profiling enabled"
    
    # 4. 内存信息存在
    # 检查是否有内存相关的事件
    memory_events = []
    for event in events:
        # 检查事件名称或属性是否包含内存相关信息
        event_name = getattr(event, 'name', '')
        if 'memory' in event_name.lower() or 'alloc' in event_name.lower() or 'free' in event_name.lower():
            memory_events.append(event)
    
    # 当启用内存分析时，应该有一些内存相关事件
    # 注意：具体实现可能不同，我们只检查分析器正常工作
    print(f"Found {len(memory_events)} memory-related events out of {len(events)} total events")
    
    # 额外检查：key_averages() 方法正常工作
    key_avg = prof.key_averages()
    assert key_avg is not None, "key_averages() should not return None"
    
    # 尝试按内存使用分组（当 profile_memory=True 时可能可用）
    try:
        # 检查是否有内存相关的分组选项
        grouped_by_memory = prof.key_averages(group_by_input_shape=False)
        assert grouped_by_memory is not None, "key_averages should work with memory profiling"
    except Exception as e:
        # 如果分组失败，可能是不支持或没有内存数据
        # 这不算测试失败，只是记录
        print(f"Note: key_averages raised exception (might be normal): {e}")
    
    # 额外检查：table() 方法正常工作
    table_str = prof.table()
    assert isinstance(table_str, str), "table() should return a string"
    assert len(table_str) > 0, "table() should return non-empty string"
    
    # 检查表格中是否包含内存相关信息
    # 注意：这取决于具体的实现，可能不是强保证
    if "memory" in table_str.lower() or "alloc" in table_str.lower():
        print("Memory information found in table output")
    
    print(f"Memory profiling test passed. Recorded {len(events)} events.")
# ==== BLOCK:CASE_05 END ====

# ==== BLOCK:FOOTER START ====
# ==== BLOCK:CASE_07 START ====
def test_record_function_basic(fixed_seed, simple_tensor_ops):
    """G3-01: record_function 基本功能测试
    
    测试 record_function 上下文管理器在性能分析中的标签功能。
    """
    x, y = simple_tensor_ops
    
    # 使用性能分析器
    with torch.autograd.profiler.profile(
        enabled=True,
        use_cuda=False,
        record_shapes=False,
        with_flops=False,
        profile_memory=False,
        with_stack=False,
        with_modules=False,
        use_kineto=False,
        use_cpu=True,
        experimental_config=None
    ) as prof:
        # 执行一些张量操作
        z = torch.matmul(x, y)
        
        # 使用 record_function 添加标签
        with torch.autograd.profiler.record_function("custom_label"):
            w = torch.randn(4, 2, requires_grad=True)
            v = torch.matmul(z, w)
        
        loss = v.sum()
        loss.backward()
    
    # 基本断言
    assert prof is not None, "Profiler should be created"
    
    # 检查事件列表
    events = prof.events()
    assert isinstance(events, list), "Events should be a list"
    
    # 检查 key_averages 输出
    key_avg = prof.key_averages()
    assert key_avg is not None, "key_averages() should not return None"
    
    # 检查表格输出中是否包含自定义标签
    table_str = prof.table()
    assert isinstance(table_str, str), "table() should return a string"
    
    # 检查是否包含自定义标签（可能以小写形式出现）
    if "custom_label" in table_str or "custom" in table_str.lower():
        print("Custom label found in table output")
    else:
        # 如果没有找到，可能是标签被记录但不在表格中，或者表格格式不同
        print("Note: custom label not found in table output (may be normal)")
    
    print(f"Record function test passed. Recorded {len(events)} events.")
# ==== BLOCK:CASE_07 END ====

# ==== BLOCK:CASE_08 START ====
def test_record_function_disabled_profiler(fixed_seed, simple_tensor_ops):
    """G3-02: 禁用分析器时的 record_function
    
    测试当性能分析器禁用时，record_function 是否正常工作（应无异常）。
    """
    x, y = simple_tensor_ops
    
    # 使用禁用的性能分析器
    with torch.autograd.profiler.profile(
        enabled=False,  # 禁用分析器
        use_cuda=False,
        record_shapes=False,
        with_flops=False,
        profile_memory=False,
        with_stack=False,
        with_modules=False,
        use_kineto=False,
        use_cpu=True,
        experimental_config=None
    ) as prof:
        # 执行一些张量操作
        z = torch.matmul(x, y)
        
        # 即使分析器禁用，record_function 也应该正常工作
        with torch.autograd.profiler.record_function("disabled_profiler_label"):
            w = torch.randn(4, 2, requires_grad=True)
            v = torch.matmul(z, w)
        
        loss = v.sum()
        loss.backward()
    
    # 基本断言 - 即使分析器禁用，也不应抛出异常
    assert prof is not None, "Profiler should be created even when disabled"
    
    # 检查事件列表
    events = prof.events()
    assert isinstance(events, list), "Events should be a list"
    
    print(f"Record function with disabled profiler test passed.")
# ==== BLOCK:CASE_08 END ====

# ==== BLOCK:CASE_09 START ====
def test_emit_nvtx_basic(mock_cuda_unavailable, fixed_seed, simple_tensor_ops):
    """G3-03: emit_nvtx 基本功能测试
    
    测试 emit_nvtx 上下文管理器的基本功能。
    注意：由于测试环境通常没有CUDA，我们主要测试它不会崩溃。
    """
    x, y = simple_tensor_ops
    
    # 使用 emit_nvtx（通常用于CUDA分析，但在CPU上也不应崩溃）
    with torch.autograd.profiler.emit_nvtx(enabled=True, record_shapes=False):
        # 执行一些张量操作
        z = torch.matmul(x, y)
        loss = z.sum()
        loss.backward()
    
    # 基本断言 - 不应抛出异常
    # 如果执行到这里，说明 emit_nvtx 在CPU模式下正常工作
    
    # 测试禁用模式
    with torch.autograd.profiler.emit_nvtx(enabled=False, record_shapes=False):
        # 执行一些张量操作
        z = torch.matmul(x, y)
        loss = z.sum()
        loss.backward()
    
    # 测试启用形状记录
    with torch.autograd.profiler.emit_nvtx(enabled=True, record_shapes=True):
        # 执行一些张量操作
        z = torch.matmul(x, y)
        loss = z.sum()
        loss.backward()
    
    print("Emit NVTX test passed (no exceptions thrown).")
# ==== BLOCK:CASE_09 END ====

# Additional test cases and cleanup
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
# ==== BLOCK:FOOTER END ====