=== Run Tests ===
..F............F..s..                                                    [100%]
=================================== FAILURES ===================================
__________ test_linear_different_dtypes[8-4-True-dtype0-input_shape0] __________

in_features = 8, out_features = 4, bias = True, dtype = torch.float64
input_shape = (32, 8)

    @pytest.mark.parametrize("in_features,out_features,bias,dtype,input_shape", [
        (8, 4, True, torch.float64, (32, 8)),  # High precision case from test plan
    ])
    def test_linear_different_dtypes(in_features, out_features, bias, dtype, input_shape):
        """Test Linear layer with different data types."""
        # Create Linear layer with specified dtype
        linear = nn.Linear(
            in_features=in_features,
            out_features=out_features,
            bias=bias,
            dtype=dtype
        )
    
        # Verify layer dtype matches expected
        # Note: Linear layer dtype affects parameter dtypes
        assert linear.weight.dtype == dtype, f"Weight dtype mismatch: {linear.weight.dtype} != {dtype}"
        if bias:
            assert linear.bias.dtype == dtype, f"Bias dtype mismatch: {linear.bias.dtype} != {dtype}"
    
        # Create test input with matching dtype
        x = create_test_input(input_shape, dtype=dtype)
    
        # Forward pass
        y = linear(x)
    
        # Check output shape
        expected_shape = (*input_shape[:-1], out_features)
        assert y.shape == expected_shape, f"Output shape mismatch: {y.shape} != {expected_shape}"
    
        # Check output dtype matches input dtype
        assert y.dtype == dtype, f"Output dtype mismatch: {y.dtype} != {dtype}"
    
        # Check finite values
        check_finite(y, "Linear output")
    
        # Compare with functional.linear for verification
        with torch.no_grad():
            weight = linear.weight
            bias_tensor = linear.bias if bias else None
    
            # Compute expected output using functional linear
            y_expected = F.linear(x, weight, bias_tensor)
    
            # Use appropriate tolerance based on dtype
            if dtype == torch.float64:
                rtol, atol = 1e-10, 1e-12  # Higher precision for float64
            elif dtype == torch.float32:
                rtol, atol = 1e-5, 1e-8    # Standard precision for float32
            elif dtype == torch.float16:
                rtol, atol = 1e-3, 1e-5    # Lower precision for float16
            else:
                rtol, atol = 1e-5, 1e-8    # Default
    
            assert_tensors_close(y, y_expected, rtol=rtol, atol=atol,
                               msg=f"Linear output doesn't match functional.linear for dtype={dtype}")
    
        # Test dtype consistency across operations
        # Create another linear layer with same parameters but different dtype for comparison
        if dtype == torch.float64:
            # Compare with float32 version
            linear_f32 = nn.Linear(in_features, out_features, bias=bias, dtype=torch.float32)
    
            # Copy parameters (with dtype conversion)
            with torch.no_grad():
                linear_f32.weight.copy_(linear.weight.to(torch.float32))
                if bias:
                    linear_f32.bias.copy_(linear.bias.to(torch.float32))
    
            # Forward with float32 input
            x_f32 = x.to(torch.float32)
            y_f32 = linear_f32(x_f32)
    
            # Convert back to original dtype for comparison
            y_f32_in_original_dtype = y_f32.to(dtype)
    
            # Compare with original output (should be close but not exact due to precision loss)
            # Use relaxed tolerance for cross-dtype comparison
            assert_tensors_close(y, y_f32_in_original_dtype, rtol=1e-4, atol=1e-6,
                               msg="Float64 and float32 outputs differ significantly")
    
        # Test that parameters have correct dtype after reset_parameters
        # Store initial parameters
        initial_weight = linear.weight.clone()
        if bias:
            initial_bias = linear.bias.clone()
    
        # Reset parameters
        linear.reset_parameters()
    
        # Check dtype preserved after reset
        assert linear.weight.dtype == dtype, f"Weight dtype changed after reset: {linear.weight.dtype} != {dtype}"
        if bias:
            assert linear.bias.dtype == dtype, f"Bias dtype changed after reset: {linear.bias.dtype} != {dtype}"
    
        # Check parameters changed (not identical)
        assert not torch.allclose(linear.weight, initial_weight, rtol=1e-5, atol=1e-8), \
            "Weight unchanged after reset_parameters"
        if bias:
            assert not torch.allclose(linear.bias, initial_bias, rtol=1e-5, atol=1e-8), \
                "Bias unchanged after reset_parameters"
    
        # Test with mixed precision (if applicable)
        # Note: PyTorch Linear layer with dtype parameter requires input to have matching dtype
        # or be upcastable. float64 layer can accept float32 input (it will be upcast to float64).
        # However, we need to be careful about the test logic.
    
        # Remove the problematic mixed precision test that was causing RuntimeError
        # Instead, test that the layer works correctly with its own dtype
    
        # Test parameter dtype consistency in state_dict
        state_dict = linear.state_dict()
    
        assert state_dict['weight'].dtype == dtype, \
            f"State dict weight dtype mismatch: {state_dict['weight'].dtype} != {dtype}"
    
        if bias:
            assert state_dict['bias'].dtype == dtype, \
                f"State dict bias dtype mismatch: {state_dict['bias'].dtype} != {dtype}"
    
        # Test loading from state_dict preserves dtype
        new_linear = nn.Linear(in_features, out_features, bias=bias, dtype=dtype)
        new_linear.load_state_dict(state_dict)
    
        assert new_linear.weight.dtype == dtype, \
            f"Loaded weight dtype mismatch: {new_linear.weight.dtype} != {dtype}"
    
        if bias:
            assert new_linear.bias.dtype == dtype, \
                f"Loaded bias dtype mismatch: {new_linear.bias.dtype} != {dtype}"
    
        # Forward pass with loaded parameters should give same result
        y_loaded = new_linear(x)
>       assert_tensors_close(y, y_loaded, rtol=1e-5, atol=1e-8,
                           msg="Output mismatch after loading state_dict")

tests/test_torch_nn_modules_linear_g1.py:302: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

actual = tensor([[-0.2458, -0.1078, -0.4920,  0.2580],
        [-0.0880,  0.3163, -0.0108, -0.3664],
        [-0.2167,  0.5350,...-0.1250, -0.3325],
        [ 0.6405, -0.5467, -0.3015,  0.9402]], dtype=torch.float64,
       grad_fn=<AddmmBackward0>)
expected = tensor([[ 4.9869e-01,  3.9314e-01, -6.6461e-01,  9.5020e-01],
        [ 1.7870e-01, -5.1214e-01,  2.8473e-01,  7.6640e...],
        [-1.9374e-01,  3.7539e-01,  3.9954e-01, -5.9472e-02]],
       dtype=torch.float64, grad_fn=<AddmmBackward0>)
rtol = 1e-05, atol = 1e-08, msg = 'Output mismatch after loading state_dict'

    def assert_tensors_close(actual, expected, rtol=1e-5, atol=1e-8, msg=""):
        """Assert two tensors are close within tolerance."""
        assert actual.shape == expected.shape, f"Shape mismatch: {actual.shape} != {expected.shape}"
        assert actual.dtype == expected.dtype, f"Dtype mismatch: {actual.dtype} != {expected.dtype}"
    
        diff = torch.abs(actual - expected)
        max_diff = torch.max(diff).item()
        max_relative_diff = torch.max(diff / (torch.abs(expected) + 1e-8)).item()
    
        if max_diff > atol and max_relative_diff > rtol:
>           pytest.fail(f"{msg} Tensors not close: max_diff={max_diff:.2e}, max_relative_diff={max_relative_diff:.2e}")
E           Failed: Output mismatch after loading state_dict Tensors not close: max_diff=1.97e+00, max_relative_diff=3.31e+02

tests/test_torch_nn_modules_linear_g1.py:35: Failed
___________ test_linear_invalid_inputs[5-3-True-dtype0-input_shape0] ___________

in_features = 5, out_features = 3, bias = True, dtype = torch.float32
input_shape = (10, 4)

    @pytest.mark.parametrize("in_features,out_features,bias,dtype,input_shape", [
        (5, 3, True, torch.float32, (10, 4)),  # Wrong dimension case from test plan
    ])
    def test_linear_invalid_inputs(in_features, out_features, bias, dtype, input_shape):
        """Test Linear layer with invalid inputs (wrong dimension)."""
        # Create Linear layer
        linear = nn.Linear(
            in_features=in_features,
            out_features=out_features,
            bias=bias,
            dtype=dtype
        )
    
        # Create test input with wrong feature dimension
        # input_shape[-1] should be 4, but in_features is 5
        x = create_test_input(input_shape, dtype=dtype)
    
        # Verify that input has wrong dimension
        assert x.shape[-1] != in_features, \
            f"Test setup error: input feature dimension {x.shape[-1]} should not equal in_features {in_features}"
    
        # Forward pass should raise RuntimeError
        with pytest.raises(RuntimeError) as exc_info:
            linear(x)
    
        # Check error message contains relevant information
        error_msg = str(exc_info.value).lower()
    
        # The error should mention something about shape, size, or dimension mismatch
        assert any(keyword in error_msg for keyword in ["shape", "size", "dimension", "match", "inconsistent"]), \
            f"Error message should mention shape/size/dimension, got: {error_msg}"
    
        # Check that the error message includes the actual and expected dimensions
        # PyTorch error messages typically include something like:
        # "mat1 and mat2 shapes cannot be multiplied (axb and cxd)"
        # or "size mismatch"
        assert str(in_features) in error_msg or str(x.shape[-1]) in error_msg, \
            f"Error message should include feature dimensions, got: {error_msg}"
    
        # Test that layer state is preserved after error
        # Parameters should not be corrupted
        if bias:
            assert linear.bias is not None, "Bias should still exist after error"
            assert linear.bias.shape == (out_features,), \
                f"Bias shape corrupted: {linear.bias.shape} != ({out_features},)"
    
        assert linear.weight.shape == (out_features, in_features), \
            f"Weight shape corrupted: {linear.weight.shape} != ({out_features}, {in_features})"
    
        # Test with correct input dimension - should work
        x_correct = create_test_input((10, in_features), dtype=dtype)
        y_correct = linear(x_correct)
    
        assert y_correct.shape == (10, out_features), \
            f"Correct input output shape mismatch: {y_correct.shape} != (10, {out_features})"
    
        # Test other invalid input scenarios
    
        # Test with 0-dimensional input (scalar)
        x_scalar = torch.tensor(1.0, dtype=dtype)
        with pytest.raises(RuntimeError) as exc_info2:
            linear(x_scalar)
    
        # Test with 1D input but wrong dimension
        x_1d_wrong = create_test_input((in_features + 2,), dtype=dtype)
        with pytest.raises(RuntimeError) as exc_info3:
            linear(x_1d_wrong)
    
        # Test with 1D input correct dimension
        x_1d_correct = create_test_input((in_features,), dtype=dtype)
        y_1d = linear(x_1d_correct)
        assert y_1d.shape == (out_features,), \
            f"1D input output shape mismatch: {y_1d.shape} != ({out_features},)"
    
        # Test with 3D input wrong dimension
        x_3d_wrong = create_test_input((4, 5, in_features - 1), dtype=dtype)
        with pytest.raises(RuntimeError) as exc_info4:
            linear(x_3d_wrong)
    
        # Test with 3D input correct dimension
        x_3d_correct = create_test_input((4, 5, in_features), dtype=dtype)
        y_3d = linear(x_3d_correct)
        assert y_3d.shape == (4, 5, out_features), \
            f"3D input output shape mismatch: {y_3d.shape} != (4, 5, {out_features})"
    
        # Test with NaN/inf values in input
        # Linear should propagate NaN/inf values
        # IMPORTANT: Use correct input dimension for NaN/inf tests
        x_nan = torch.full((10, in_features), float('nan'), dtype=dtype)
        y_nan = linear(x_nan)
    
        # Output should contain NaN values
        assert torch.isnan(y_nan).any(), "NaN input should produce NaN output"
    
        # Check shape is still correct even with NaN input
        assert y_nan.shape == (10, out_features), \
            f"NaN input output shape mismatch: {y_nan.shape} != (10, {out_features})"
    
        # Test with inf values
        x_inf = torch.full((10, in_features), float('inf'), dtype=dtype)
        y_inf = linear(x_inf)
    
        # Output should contain inf or NaN values
        # Note: inf * weight could produce:
        # 1. inf if weight != 0
        # 2. NaN if weight = 0 (0 * inf = NaN in floating point)
        # 3. Very large finite values if weight is very small
    
        # Check that output contains either inf or NaN
        # This is more accurate than the previous assertion
        has_inf = torch.isinf(y_inf).any()
        has_nan = torch.isnan(y_inf).any()
    
        assert has_inf or has_nan, \
            f"Inf input should produce inf or NaN output, got min={y_inf.min().item()}, max={y_inf.max().item()}"
    
        # Also check that the output magnitude is extremely large if not inf/nan
        # (though this case should be rare with random weights)
        if not (has_inf or has_nan):
            max_abs = torch.abs(y_inf).max().item()
            assert max_abs > 1e10, \
                f"Inf input produced unexpectedly small output: max_abs={max_abs}"
    
        # Test with -inf values
        x_neg_inf = torch.full((10, in_features), float('-inf'), dtype=dtype)
        y_neg_inf = linear(x_neg_inf)
    
        # Similar logic for -inf
        has_neg_inf = torch.isinf(y_neg_inf).any() and (y_neg_inf < 0).any()
        has_nan_neg = torch.isnan(y_neg_inf).any()
    
        assert has_neg_inf or has_nan_neg, \
            f"-Inf input should produce -inf or NaN output, got min={y_neg_inf.min().item()}, max={y_neg_inf.max().item()}"
    
        # Test with empty batch dimension (0 samples)
        x_empty = create_test_input((0, in_features), dtype=dtype)
        y_empty = linear(x_empty)
    
        assert y_empty.shape == (0, out_features), \
            f"Empty batch output shape mismatch: {y_empty.shape} != (0, {out_features})"
    
        # Output should be empty tensor
        assert y_empty.numel() == 0, "Empty batch should produce empty output"
    
        # Test that error is raised for invalid constructor arguments
    
        # Invalid in_features (non-positive)
>       with pytest.raises(ValueError) as exc_info5:
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_torch_nn_modules_linear_g3.py:332: Failed
=============================== warnings summary ===============================
exam/torch_group/nn.modules.linear/tests/test_torch_nn_modules_linear_g3.py::test_lazy_linear_delayed_initialization[15-True-dtype0-input_shape0]
exam/torch_group/nn.modules.linear/tests/test_torch_nn_modules_linear_g3.py::TestLazyLinearEdgeCasesG3::test_lazy_linear_no_bias_extension
exam/torch_group/nn.modules.linear/tests/test_torch_nn_modules_linear_g3.py::TestLazyLinearEdgeCasesG3::test_lazy_linear_different_dtypes
  /opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
    warnings.warn('Lazy modules are a new feature under heavy development '

exam/torch_group/nn.modules.linear/tests/test_torch_nn_modules_linear_g3.py::test_linear_invalid_inputs[5-3-True-dtype0-input_shape0]
  /opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op
    warnings.warn("Initializing zero-element tensors is a no-op")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                       Stmts   Miss Branch BrPart  Cover   Missing
--------------------------------------------------------------------------------------
tests/test_torch_nn_modules_linear_g1.py     206      8     34     12    90%   186->190, 216-221, 228->252, 235->239, 253->257, 261->265, 267->280, 285->290, 296->301, 440->448, 469->exit, 537, 546-547
tests/test_torch_nn_modules_linear_g2.py     208      9     18      6    92%   35, 112, 167, 178->193, 253->261, 479, 489-491, 495-496
tests/test_torch_nn_modules_linear_g3.py     287     28     34     17    86%   35, 82, 119, 141->145, 226->231, 304-305, 335-363, 394, 398->404, 426->450, 460->465, 466->470, 474->480, 499->504, 516->522, 593->exit, 598, 603, 659, 667-670, 674-675
--------------------------------------------------------------------------------------
TOTAL                                        701     45     86     35    89%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_linear_g1.py::test_linear_different_dtypes[8-4-True-dtype0-input_shape0]
FAILED tests/test_torch_nn_modules_linear_g3.py::test_linear_invalid_inputs[5-3-True-dtype0-input_shape0]
2 failed, 18 passed, 1 skipped, 4 warnings in 1.05s

Error: exit 1