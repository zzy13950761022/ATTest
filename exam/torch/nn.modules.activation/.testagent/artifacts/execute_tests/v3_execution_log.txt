=== Run Tests ===
.F....F...                                                               [100%]
=================================== FAILURES ===================================
______________________ test_hardtanh_parameter_validation ______________________

    def test_hardtanh_parameter_validation():
        """Test Hardtanh parameter validation."""
        # Test with valid parameters
        hardtanh = nn.Hardtanh(min_val=-1.0, max_val=1.0)
        assert hardtanh.min_val == -1.0
        assert hardtanh.max_val == 1.0
    
        # Test with default parameters
        hardtanh_default = nn.Hardtanh()
        assert hardtanh_default.min_val == -1.0
        assert hardtanh_default.max_val == 1.0
    
        # Test with custom parameters
        hardtanh_custom = nn.Hardtanh(min_val=-2.5, max_val=3.0)
        assert hardtanh_custom.min_val == -2.5
        assert hardtanh_custom.max_val == 3.0
    
        # Test that max_val > min_val is required
        with pytest.raises(ValueError):
>           nn.Hardtanh(min_val=1.0, max_val=-1.0)  # max_val < min_val

tests/test_torch_nn_modules_activation_g3.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Hardtanh(min_val=1.0, max_val=-1.0), min_val = 1.0, max_val = -1.0
inplace = False, min_value = None, max_value = None

    def __init__(
        self,
        min_val: float = -1.,
        max_val: float = 1.,
        inplace: bool = False,
        min_value: Optional[float] = None,
        max_value: Optional[float] = None
    ) -> None:
        super(Hardtanh, self).__init__()
        if min_value is not None:
            warnings.warn("keyword argument min_value is deprecated and rename to min_val")
            min_val = min_value
        if max_value is not None:
            warnings.warn("keyword argument max_value is deprecated and rename to max_val")
            max_val = max_value
    
        self.min_val = min_val
        self.max_val = max_val
        self.inplace = inplace
>       assert self.max_val > self.min_val
E       AssertionError

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/activation.py:231: AssertionError
_______________________________ test_gelu_basic ________________________________

    def test_gelu_basic():
        """Test GELU (Gaussian Error Linear Unit) basic functionality."""
        # GELU(x) = x * Φ(x) where Φ is the CDF of standard Gaussian
        # Approx: 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x³)))
        gelu = nn.GELU()
    
        # Test at x = 0
        x_zero = torch.tensor([0.0])
        output_zero = gelu(x_zero)
        assert torch.allclose(output_zero, torch.tensor([0.0]), rtol=1e-5, atol=1e-8), \
            "GELU(0) should be 0"
    
        # Test positive values
        x_pos = torch.tensor([1.0, 2.0])
        output_pos = gelu(x_pos)
        # GELU should be monotonic increasing
        assert output_pos[0] < output_pos[1], "GELU should be monotonic increasing"
    
        # Test negative values
        x_neg = torch.tensor([-1.0, -2.0])
        output_neg = gelu(x_neg)
        # GELU should output negative values for negative inputs
        assert torch.all(output_neg < 0), "GELU should output negative values for negative inputs"
    
        # Test symmetry property: GELU is not symmetric, but check specific values
        # GELU(-x) ≠ -GELU(x) in general
    
        # Compare with approximate formula
        x_test = torch.tensor([0.5, -0.5, 1.5, -1.5])
        output_test = gelu(x_test)
    
        # Approximate GELU formula
        sqrt_2_over_pi = math.sqrt(2.0 / math.pi)
        approx_gelu = lambda x: 0.5 * x * (1.0 + torch.tanh(sqrt_2_over_pi * (x + 0.044715 * torch.pow(x, 3))))
        expected_test = approx_gelu(x_test)
    
>       assert torch.allclose(output_test, expected_test, rtol=1e-4, atol=1e-6), \
            "GELU should match approximate formula"
E       AssertionError: GELU should match approximate formula
E       assert False
E        +  where False = <built-in method allclose of type object at 0x1131aa320>(tensor([ 0.3457, -0.1543,  1.3998, -0.1002]), tensor([ 0.3457, -0.1543,  1.3996, -0.1004]), rtol=0.0001, atol=1e-06)
E        +    where <built-in method allclose of type object at 0x1131aa320> = torch.allclose

tests/test_torch_nn_modules_activation_g3.py:549: AssertionError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                           Stmts   Miss Branch BrPart  Cover   Missing
------------------------------------------------------------------------------------------
tests/test_torch_nn_modules_activation_g3.py     275     58     34      4    73%   41, 45-68, 83-86, 99-120, 126, 131, 136, 141-143, 148-150, 155-157, 162-164, 219, 249-255, 377-378
------------------------------------------------------------------------------------------
TOTAL                                            275     58     34      4    73%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_activation_g3.py::test_hardtanh_parameter_validation
FAILED tests/test_torch_nn_modules_activation_g3.py::test_gelu_basic - Assert...
2 failed, 8 passed in 0.76s

Error: exit 1