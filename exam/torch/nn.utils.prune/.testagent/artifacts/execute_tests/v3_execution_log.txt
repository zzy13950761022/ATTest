=== Run Tests ===
.F.                                                                      [100%]
=================================== FAILURES ===================================
_____________ TestPruneBasic.test_l1_unstructured_validation[5-5] ______________

self = <test_torch_nn_utils_prune.TestPruneBasic object at 0x13e100670>
conv2d_module = Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))
set_random_seed = 42, amount = 5, expected_pruned = 5

    @pytest.mark.parametrize("amount,expected_pruned", [
        (5, 5),  # 绝对数量5个参数
    ])
    def test_l1_unstructured_validation(self, conv2d_module, set_random_seed, amount, expected_pruned):
        """Test L1 unstructured pruning with absolute amount."""
        # Arrange
        module = conv2d_module
        param_name = "weight"
        original_weight = module.weight.clone()
    
        # We need to set weights with predictable absolute values
        # L1剪枝基于绝对值，所以我们需要确保绝对值有明确的顺序
        # 注意：我们使用循环值0.0-0.9，这会导致重复值
        # 当有重复值时，torch.topk选择哪些索引是不确定的
        with torch.no_grad():
            # Create weights with predictable absolute values
            # We'll use positive values only to avoid sign confusion
            flat_weights = original_weight.view(-1)
            for i in range(len(flat_weights)):
                # Use values from 0.0 to 0.9 in order (循环)
                # 这会产生重复值：0.0, 0.1, ..., 0.9, 0.0, 0.1, ...
                flat_weights[i] = (i % 10) * 0.1  # Values: 0.0, 0.1, 0.2, ..., 0.9, 0.0, 0.1, ...
    
        # Act
        pruned_module = prune.l1_unstructured(module, param_name, amount)
    
        # Assert - weak assertions
        # 1. mask_shape: 掩码形状正确
        mask_name = f"{param_name}_mask"
        assert hasattr(pruned_module, mask_name), f"Mask buffer {mask_name} should exist"
        mask = getattr(pruned_module, mask_name)
        assert mask.shape == original_weight.shape, f"Mask shape {mask.shape} should match weight shape {original_weight.shape}"
    
        # 2. mask_dtype: 掩码数据类型正确
        assert mask.dtype == torch.float32, f"Mask dtype should be float32, got {mask.dtype}"
    
        # 3. prune_count_correct: 剪枝数量正确
        pruned_count = torch.sum(mask == 0).item()
        assert pruned_count == expected_pruned, \
            f"Pruned count {pruned_count} should equal expected {expected_pruned}"
    
        # 4. module_modified: 模块被修改
        assert pruned_module is module, "Function should return the modified module"
    
        # 5. l1_order: 检查L1顺序 - 修改断言以适应实际实现
        # L1剪枝应该剪掉绝对值最小的参数
        # 在我们的设置中，绝对值就是值本身（因为都是正数）
        # 注意：由于我们的值有重复（0.0, 0.1, ..., 0.9, 0.0, ...），
        # 当有重复值时，torch.topk选择哪些索引是不确定的
        # 所以我们只检查被剪枝的数量正确，不检查具体索引
        mask_flat = mask.view(-1)
        zero_indices = torch.where(mask_flat == 0)[0]
        # 只检查剪枝数量，不检查具体索引
        # 这是弱断言，符合test_plan的要求
    
        # 6. Check original parameter is stored
        orig_name = f"{param_name}_orig"
        assert hasattr(pruned_module, orig_name), f"Original parameter {orig_name} should be stored"
        orig_param = getattr(pruned_module, orig_name)
>       assert torch.allclose(orig_param, original_weight), "Original parameter should be preserved"
E       AssertionError: Original parameter should be preserved
E       assert False
E        +  where False = <built-in method allclose of type object at 0x108dfe320>(Parameter containing:\ntensor([[[[-0.0272,  0.1484,  0.0284],\n          [-0.0898,  0.0491, -0.0887],\n          [-0.0226... 0.0233, -0.0582],\n          [ 0.1399, -0.0050,  0.1502],\n          [ 0.1850, -0.0938, -0.1404]]]], requires_grad=True), tensor([[[[0.0000, 0.1000, 0.2000],\n          [0.3000, 0.4000, 0.5000],\n          [0.6000, 0.7000, 0.8000]],\n\n        ..., 0.4000, 0.5000],\n          [0.6000, 0.7000, 0.8000],\n          [0.9000, 0.0000, 0.1000]]]], grad_fn=<CloneBackward0>))
E        +    where <built-in method allclose of type object at 0x108dfe320> = torch.allclose

tests/test_torch_nn_utils_prune.py:159: AssertionError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                 Stmts   Miss Branch BrPart  Cover   Missing
--------------------------------------------------------------------------------
tests/test_torch_nn_utils_prune.py     122     14     26      8    81%   92->96, 93->92, 162-174, 195, 219-221, 229, 252->256, 253->252, 311
--------------------------------------------------------------------------------
TOTAL                                  122     14     26      8    81%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_utils_prune.py::TestPruneBasic::test_l1_unstructured_validation[5-5]
1 failed, 2 passed in 0.75s

Error: exit 1