=== Run Tests ===
..F......x..                                                             [100%]
=================================== FAILURES ===================================
_______________________ test_quantized_input_validation ________________________

    def test_quantized_input_validation():
        """Test that non-quantized inputs raise ValueError."""
        # Create a regular (non-quantized) tensor
        regular_tensor = torch.randn(1, 3, 5, 5)
    
        # Try to use it with quantized conv2d - should raise ValueError
        with pytest.raises(ValueError, match="must be quantized"):
            # Create a quantized weight tensor
            weight = create_quantized_weight([2, 3, 3, 3])
            # conv2d requires bias as third positional argument, even if None
>           qF.conv2d(regular_tensor, weight, None)

tests/test_torch_ao_nn_quantized_functional_g1.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[ 0.2123, -0.7252, -0.9528, -0.8428, -1.6413],
          [-0.7181, -0.1090, -1.8300,  0.0933, -1.9153],
    ...],
          [ 0.8050, -1.1134,  0.4982, -1.2000,  0.1271],
          [ 0.4404,  0.6378,  0.1598,  1.7698,  0.6268]]]])
weight = tensor([[[[-2.,  2., -1.],
          [ 1.,  0., -2.],
          [ 1.,  0., -1.]],

         [[ 2.,  2.,  0.],
        ...]]], size=(2, 3, 3, 3), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=1.0, zero_point=0)
bias = None, stride = 1, padding = 0, dilation = 1, groups = 1
padding_mode = 'zeros', scale = 1.0, zero_point = 0, dtype = torch.quint8

    def conv2d(input, weight, bias,
               stride=1, padding=0, dilation=1, groups=1,
               padding_mode='zeros',
               scale=1.0, zero_point=0,
               dtype=torch.quint8):
        r"""
        Applies a 2D convolution over a quantized 2D input composed of several input
        planes.
    
        See :class:`~torch.ao.nn.quantized.Conv2d` for details and output shape.
    
        Args:
            input: quantized input tensor of shape :math:`(\text{minibatch} , \text{in\_channels} , iH , iW)`
            weight: quantized filters of shape :math:`(\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kH , kW)`
            bias: **non-quantized** bias tensor of shape :math:`(\text{out\_channels})`. The tensor type must be `torch.float`.
            stride: the stride of the convolving kernel. Can be a single number or a
              tuple `(sH, sW)`. Default: 1
            padding: implicit paddings on both sides of the input. Can be a
              single number or a tuple `(padH, padW)`. Default: 0
            dilation: the spacing between kernel elements. Can be a single number or
              a tuple `(dH, dW)`. Default: 1
            groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the
              number of groups. Default: 1
            padding_mode: the padding mode to use. Only "zeros" is supported for quantized convolution at the moment. Default: "zeros"
            scale: quantization scale for the output. Default: 1.0
            zero_point: quantization zero_point for the output. Default: 0
            dtype: quantization data type to use. Default: ``torch.quint8``
    
        Examples::
    
            >>> from torch.ao.nn.quantized import functional as qF
            >>> filters = torch.randn(8, 4, 3, 3, dtype=torch.float)
            >>> inputs = torch.randn(1, 4, 5, 5, dtype=torch.float)
            >>> bias = torch.randn(8, dtype=torch.float)
            >>>
            >>> scale, zero_point = 1.0, 0
            >>> dtype_inputs = torch.quint8
            >>> dtype_filters = torch.qint8
            >>>
            >>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)
            >>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)
            >>> qF.conv2d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)
        """  # noqa: E501
        if padding_mode != 'zeros':
            raise NotImplementedError("Only zero-padding is supported!")
        if input.dtype != torch.quint8:
>           raise NotImplementedError("Only torch.quint8 is supported for activation tensor!")
E           NotImplementedError: Only torch.quint8 is supported for activation tensor!

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/ao/nn/quantized/functional.py:217: NotImplementedError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                                Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------------------
tests/test_torch_ao_nn_quantized_functional_g1.py     119     13     18      3    82%   29, 49-69, 137->143, 231
tests/test_torch_ao_nn_quantized_functional_g2.py     195     27     42      9    81%   29, 40-43, 49-69, 73-83, 129->136, 228-229, 285->284, 301, 303->284, 371-372, 434->429, 437->421
tests/test_torch_ao_nn_quantized_functional_g3.py      94     20     22      5    70%   22-29, 37-57, 104, 127->135, 141->156, 156->exit, 187-200
-----------------------------------------------------------------------------------------------
TOTAL                                                 408     60     82     17    79%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_ao_nn_quantized_functional_g1.py::test_quantized_input_validation
1 failed, 10 passed, 1 xfailed in 0.70s

Error: exit 1