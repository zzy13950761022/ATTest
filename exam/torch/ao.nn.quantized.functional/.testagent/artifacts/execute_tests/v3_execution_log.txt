=== Run Tests ===
FF                                                                       [100%]
=================================== FAILURES ===================================
_________________ test_relu_quantized_activation[test_params0] _________________

test_params = {'inplace': False, 'input_dtype': torch.quint8, 'input_shape': [2, 3, 4, 4], 'scale': 1.0, ...}

    @pytest.mark.parametrize("test_params", [
        {
            "input_shape": [2, 3, 4, 4],
            "input_dtype": torch.quint8,
            "scale": 1.0,
            "zero_point": 0,
            "inplace": False
        }
    ])
    def test_relu_quantized_activation(test_params):
        """Test quantized ReLU activation function."""
        # Unpack parameters
        input_shape = test_params["input_shape"]
        input_dtype = test_params["input_dtype"]
        scale = test_params["scale"]
        zero_point = test_params["zero_point"]
        inplace = test_params["inplace"]
    
        # Create quantized input tensor with both positive and negative values
        # First create float data with both positive and negative values
        float_data = torch.randn(input_shape)
        # Ensure we have both positive and negative values for ReLU test
        float_data = float_data * 2.0  # Scale to get wider range
    
        # Quantize the data
        input_tensor = torch.quantize_per_tensor(
            float_data, scale=scale, zero_point=zero_point, dtype=input_dtype
        )
    
        # Store original tensor for comparison if not inplace
        original_tensor = input_tensor.clone() if not inplace else None
    
        # Perform quantized ReLU operation
>       output = qF.relu(
            input=input_tensor,
            inplace=inplace
        )
E       AttributeError: module 'torch.ao.nn.quantized.functional' has no attribute 'relu'. Did you mean: 'celu'?

tests/test_torch_ao_nn_quantized_functional_g3.py:99: AttributeError
________________________ test_relu_inplace_vs_outplace _________________________

    def test_relu_inplace_vs_outplace():
        """Compare inplace and outplace ReLU operations."""
        # Create two identical tensors
        input1 = create_quantized_tensor([2, 3, 4, 4], scale=1.0, zero_point=0)
        input2 = input1.clone()
    
        # Apply ReLU inplace
>       output_inplace = qF.relu(input1, inplace=True)
E       AttributeError: module 'torch.ao.nn.quantized.functional' has no attribute 'relu'. Did you mean: 'celu'?

tests/test_torch_ao_nn_quantized_functional_g3.py:177: AttributeError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                                Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------------------
tests/test_torch_ao_nn_quantized_functional_g3.py      74     41     20      1    36%   22-29, 37-57, 106-155, 180-193
-----------------------------------------------------------------------------------------------
TOTAL                                                  74     41     20      1    36%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_ao_nn_quantized_functional_g3.py::test_relu_quantized_activation[test_params0]
FAILED tests/test_torch_ao_nn_quantized_functional_g3.py::test_relu_inplace_vs_outplace
2 failed in 0.74s

Error: exit 1