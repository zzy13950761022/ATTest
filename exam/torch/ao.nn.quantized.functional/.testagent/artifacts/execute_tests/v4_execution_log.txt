=== Run Tests ===
..FF.F...F..                                                             [100%]
=================================== FAILURES ===================================
_______________________ test_quantized_input_validation ________________________

    def test_quantized_input_validation():
        """Test that non-quantized inputs raise ValueError."""
        # Create a regular (non-quantized) tensor
        regular_tensor = torch.randn(1, 3, 5, 5)
    
        # Try to use it with quantized conv2d - should raise ValueError
        with pytest.raises(ValueError, match="must be quantized"):
            # Create a quantized weight tensor
            weight = create_quantized_weight([2, 3, 3, 3])
>           qF.conv2d(regular_tensor, weight)
E           TypeError: conv2d() missing 1 required positional argument: 'bias'

tests/test_torch_ao_nn_quantized_functional_g1.py:303: TypeError
________________ test_conv2d_with_different_quantization_params ________________

    def test_conv2d_with_different_quantization_params():
        """Test conv2d with different input and output quantization parameters."""
        # Create input with one set of parameters
        input_tensor = create_quantized_tensor(
            [1, 3, 5, 5], scale=0.5, zero_point=64
        )
    
        # Create weight
        weight_tensor = create_quantized_weight([2, 3, 3, 3], scale=0.5, zero_point=0)
    
        # Specify different output quantization parameters
        output_scale = 0.25
        output_zero_point = 128
    
>       output = qF.conv2d(
            input_tensor, weight_tensor,
            scale=output_scale, zero_point=output_zero_point
        )
E       TypeError: conv2d() missing 1 required positional argument: 'bias'

tests/test_torch_ao_nn_quantized_functional_g1.py:319: TypeError
______________ test_avg_pool2d_quantized_operation[test_params0] _______________

test_params = {'ceil_mode': False, 'count_include_pad': True, 'divisor_override': None, 'input_dtype': torch.quint8, ...}

    @pytest.mark.parametrize("test_params", [
        {
            "input_shape": [1, 3, 6, 6],
            "input_dtype": torch.quint8,
            "kernel_size": 2,
            "stride": 2,
            "padding": 0,
            "ceil_mode": False,
            "count_include_pad": True,
            "divisor_override": None,
            "scale": 1.0,
            "zero_point": 0
        }
    ])
    def test_avg_pool2d_quantized_operation(test_params):
        """Test quantized avg_pool2d operation."""
        # Unpack parameters
        input_shape = test_params["input_shape"]
        input_dtype = test_params["input_dtype"]
        kernel_size = test_params["kernel_size"]
        stride = test_params["stride"]
        padding = test_params["padding"]
        ceil_mode = test_params["ceil_mode"]
        count_include_pad = test_params["count_include_pad"]
        divisor_override = test_params["divisor_override"]
        scale = test_params["scale"]
        zero_point = test_params["zero_point"]
    
        # Create quantized input tensor
        input_tensor = create_quantized_tensor(
            input_shape, dtype=input_dtype, scale=scale, zero_point=zero_point
        )
    
        # Calculate expected output shape
        # For avg_pool2d with kernel_size=2, stride=2, padding=0
        # Input: [1, 3, 6, 6] -> Output: [1, 3, 3, 3]
        N, C, H, W = input_shape
        kernel_h, kernel_w = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size
        stride_h, stride_w = (stride, stride) if isinstance(stride, int) else stride
        padding_h, padding_w = (padding, padding) if isinstance(padding, int) else padding
    
        # Calculate output dimensions
        if ceil_mode:
            H_out = math.ceil((H + 2 * padding_h - kernel_h) / stride_h + 1)
            W_out = math.ceil((W + 2 * padding_w - kernel_w) / stride_w + 1)
        else:
            H_out = math.floor((H + 2 * padding_h - kernel_h) / stride_h + 1)
            W_out = math.floor((W + 2 * padding_w - kernel_w) / stride_w + 1)
    
        expected_shape = (N, C, H_out, W_out)
    
        # Perform quantized avg_pool2d operation
        output = qF.avg_pool2d(
            input=input_tensor,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            ceil_mode=ceil_mode,
            count_include_pad=count_include_pad,
            divisor_override=divisor_override
        )
    
        # Weak assertions (first round)
        # 1. Output is quantized
        assert output.is_quantized, "Output should be quantized"
    
        # 2. Output shape is correct
        assert output.shape == torch.Size(expected_shape), \
            f"Expected shape {expected_shape}, got {output.shape}"
    
        # 3. Output dtype is correct
        assert output.dtype == input_dtype, \
            f"Expected dtype {input_dtype}, got {output.dtype}"
    
        # 4. No NaN or Inf values
        assert not torch.any(torch.isnan(output.dequantize())), "Output contains NaN values"
        assert not torch.any(torch.isinf(output.dequantize())), "Output contains Inf values"
    
        # Additional checks for avg_pool2d
        # Check that quantization parameters are preserved
        # Note: avg_pool2d propagates input quantization parameters to output
        assert math.isclose(output.q_scale(), scale, rel_tol=1e-6), \
            f"Expected scale {scale}, got {output.q_scale()}"
        assert output.q_zero_point() == zero_point, \
            f"Expected zero_point {zero_point}, got {output.q_zero_point()}"
    
        # Verify pooling effect
        # For kernel_size=2, stride=2, each 2x2 block should be averaged
        dequantized_input = input_tensor.dequantize()
        dequantized_output = output.dequantize()
    
        # Check a few sample positions (not all to avoid too strict assertions)
        # For quantized operations, we need larger tolerance due to integer arithmetic
        sample_positions = [
            (0, 0, 0),  # top-left corner
            (0, 1, 1),  # center
            (1, 2, 2),  # bottom-right corner of channel 1
        ]
    
        for c, h_out, w_out in sample_positions:
            if c < C and h_out < H_out and w_out < W_out:
                # Calculate input region
                h_start = h_out * stride_h
                w_start = w_out * stride_w
                h_end = min(h_start + kernel_h, H)
                w_end = min(w_start + kernel_w, W)
    
                # Extract input region
                input_region = dequantized_input[0, c, h_start:h_end, w_start:w_end]
    
                # Calculate expected average
                if count_include_pad:
                    # Include padding in average calculation
                    region_size = kernel_h * kernel_w
                else:
                    # Only include actual input values
                    region_size = (h_end - h_start) * (w_end - w_start)
    
                if region_size > 0:
                    expected_value = input_region.sum().item() / region_size
                    actual_value = dequantized_output[0, c, h_out, w_out].item()
    
                    # For quantized avg_pool2d, use larger tolerance
                    # Integer arithmetic can cause larger errors
                    tolerance = 0.5 * scale  # Scale-dependent tolerance
>                   assert abs(actual_value - expected_value) < tolerance, \
                        f"Pooling mismatch at position (c={c}, h={h_out}, w={w_out}): " \
                        f"expected {expected_value:.4f}, got {actual_value:.4f}, " \
                        f"diff={abs(actual_value - expected_value):.4f}, tolerance={tolerance:.4f}"
E                   AssertionError: Pooling mismatch at position (c=0, h=0, w=0): expected 0.5000, got 0.0000, diff=0.5000, tolerance=0.5000
E                   assert 0.5 < 0.5
E                    +  where 0.5 = abs((0.0 - 0.5))

tests/test_torch_ao_nn_quantized_functional_g2.py:310: AssertionError
________________________ test_relu_inplace_vs_outplace _________________________

    def test_relu_inplace_vs_outplace():
        """Compare inplace and outplace ReLU operations using clamp."""
        # Create two identical tensors
        input1 = create_quantized_tensor([2, 3, 4, 4], scale=1.0, zero_point=0)
        input2 = input1.clone()
    
        # Apply ReLU inplace using torch.clamp_
        # Note: qF.clamp doesn't have inplace parameter, so we use torch.clamp_
>       output_inplace = torch.clamp_(input1, min=0)
E       NotImplementedError: Could not run 'aten::clamp.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::clamp.out' is only available for these backends: [CPU, MPS, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].
E       
E       CPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]
E       MPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMPS.cpp:20632 [kernel]
E       Meta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]
E       BackendSelect: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
E       Python: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]
E       FuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]
E       Functionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:19962 [kernel]
E       Named: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]
E       Conjugate: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]
E       Negative: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]
E       ZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]
E       ADInplaceOrView: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]
E       AutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       AutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]
E       Tracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]
E       AutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]
E       AutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]
E       FuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]
E       FuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]
E       Batched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]
E       VmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
E       FuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]
E       PythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]
E       FuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]
E       PythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]

tests/test_torch_ao_nn_quantized_functional_g3.py:183: NotImplementedError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                                Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------------------
tests/test_torch_ao_nn_quantized_functional_g1.py     119     16     18      3    80%   29, 49-69, 137->143, 231, 325-327
tests/test_torch_ao_nn_quantized_functional_g2.py     195     31     42     10    78%   29, 40-43, 49-69, 73-83, 129->136, 228-229, 285->284, 301, 303->284, 316-319, 370-371, 433->428, 436->420
tests/test_torch_ao_nn_quantized_functional_g3.py      93     20     22      5    70%   22-29, 37-57, 104, 127->135, 141->156, 156->exit, 186-199
-----------------------------------------------------------------------------------------------
TOTAL                                                 407     67     82     18    77%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_ao_nn_quantized_functional_g1.py::test_quantized_input_validation
FAILED tests/test_torch_ao_nn_quantized_functional_g1.py::test_conv2d_with_different_quantization_params
FAILED tests/test_torch_ao_nn_quantized_functional_g2.py::test_avg_pool2d_quantized_operation[test_params0]
FAILED tests/test_torch_ao_nn_quantized_functional_g3.py::test_relu_inplace_vs_outplace
4 failed, 8 passed in 0.88s

Error: exit 1