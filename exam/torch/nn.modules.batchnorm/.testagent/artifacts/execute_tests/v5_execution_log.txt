=== Run Tests ===
......FF.....FFFF.....FFFF                                               [100%]
=================================== FAILURES ===================================
_________________ test_lazy_batchnorm1d_delayed_initialization _________________

    def test_lazy_batchnorm1d_delayed_initialization():
        """Test lazy initialization for LazyBatchNorm1d.
    
        TC-04: 懒加载类延迟初始化
        Priority: Medium
        Assertion level: weak
        Group: G1
        """
        # Set random seed for reproducibility
        torch.manual_seed(42)
    
        # Create LazyBatchNorm1d instance with num_features to be inferred
        lazy_bn = LazyBatchNorm1d(
            eps=1e-5,
            momentum=0.1,
            affine=True,
            track_running_stats=True
        )
    
        # Weak assertions from test plan
        # 1. lazy_initialization_works
        # Initially, num_features should be 0 (uninitialized)
        assert lazy_bn.num_features == 0, \
            f"Initial num_features should be 0, got {lazy_bn.num_features}"
    
        # Initially, weight and bias should be UninitializedParameter
        assert isinstance(lazy_bn.weight, torch.nn.parameter.UninitializedParameter), \
            "weight should be UninitializedParameter before first forward"
        assert isinstance(lazy_bn.bias, torch.nn.parameter.UninitializedParameter), \
            "bias should be UninitializedParameter before first forward"
    
        # Initially, running stats should be UninitializedBuffer
        assert isinstance(lazy_bn.running_mean, torch.nn.parameter.UninitializedBuffer), \
            "running_mean should be UninitializedBuffer before first forward"
        assert isinstance(lazy_bn.running_var, torch.nn.parameter.UninitializedBuffer), \
            "running_var should be UninitializedBuffer before first forward"
    
        # 2. num_features_inferred
        # Create test input with shape [4, 12] from test plan
        input_tensor = torch.randn(4, 12, dtype=torch.float32)
    
        # 3. first_forward_succeeds
        # First forward pass should succeed and initialize parameters
        output = lazy_bn(input_tensor)
    
        # After first forward, num_features should be inferred from input
        assert lazy_bn.num_features == 12, \
            f"num_features should be inferred as 12, got {lazy_bn.num_features}"
    
        # Weight and bias should now be regular Parameters
        assert isinstance(lazy_bn.weight, torch.nn.Parameter), \
            "weight should be Parameter after first forward"
        assert isinstance(lazy_bn.bias, torch.nn.Parameter), \
            "bias should be Parameter after first forward"
    
        # Running stats should now be regular tensors
        assert isinstance(lazy_bn.running_mean, torch.Tensor), \
            "running_mean should be Tensor after first forward"
        assert isinstance(lazy_bn.running_var, torch.Tensor), \
            "running_var should be Tensor after first forward"
    
        # Check parameter shapes
        assert lazy_bn.weight.shape == (12,), f"weight shape {lazy_bn.weight.shape} != (12,)"
        assert lazy_bn.bias.shape == (12,), f"bias shape {lazy_bn.bias.shape} != (12,)"
        assert lazy_bn.running_mean.shape == (12,), f"running_mean shape {lazy_bn.running_mean.shape} != (12,)"
        assert lazy_bn.running_var.shape == (12,), f"running_var shape {lazy_bn.running_var.shape} != (12,)"
    
        # Check output properties
        assert output.shape == input_tensor.shape, \
            f"Output shape {output.shape} != input shape {input_tensor.shape}"
        assert output.dtype == torch.float32, \
            f"Output dtype {output.dtype} != expected float32"
        assert torch.isfinite(output).all(), \
            "Output contains non-finite values"
    
        # Test that subsequent forwards work correctly
        second_input = torch.randn(2, 12, dtype=torch.float32)
        second_output = lazy_bn(second_input)
    
        assert second_output.shape == second_input.shape, \
            f"Second output shape {second_output.shape} != input shape {second_input.shape}"
    
        # Test with different batch size but same feature dimension
        third_input = torch.randn(8, 12, dtype=torch.float32)
        third_output = lazy_bn(third_input)
    
        assert third_output.shape == third_input.shape, \
            f"Third output shape {third_output.shape} != input shape {third_input.shape}"
    
        # Test that trying to use wrong feature dimension raises error
        wrong_input = torch.randn(4, 10, dtype=torch.float32)
        try:
            wrong_output = lazy_bn(wrong_input)
            # If no error is raised, at least check the output shape
            assert wrong_output.shape == wrong_input.shape, \
                "Output shape should match input shape even with wrong feature dimension"
        except (RuntimeError, ValueError) as e:
            # It's acceptable for this to raise an error
            pass
    
        # Test eval mode
        lazy_bn.eval()
        eval_output = lazy_bn(input_tensor)
    
        assert eval_output.shape == input_tensor.shape, \
            f"Eval output shape {eval_output.shape} != input shape {input_tensor.shape}"
    
        # Test reset_parameters
        original_weight = lazy_bn.weight.clone()
        original_bias = lazy_bn.bias.clone()
    
        lazy_bn.reset_parameters()
    
        # After reset, parameters should be reinitialized (different from before)
>       assert not torch.allclose(lazy_bn.weight, original_weight), \
            "weight should be reinitialized after reset_parameters"
E       AssertionError: weight should be reinitialized after reset_parameters
E       assert not True
E        +  where True = <built-in method allclose of type object at 0x1073b6320>(Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       grad_fn=<CloneBackward0>))
E        +    where <built-in method allclose of type object at 0x1073b6320> = torch.allclose
E        +    and   Parameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True) = BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True).weight

tests/test_torch_nn_modules_batchnorm_g1.py:445: AssertionError
__________________________ test_invalid_num_features ___________________________

    def test_invalid_num_features():
        """Test that num_features <= 0 raises appropriate error.
    
        Note: BatchNorm constructors validate num_features > 0.
        """
        # Test with num_features=0 - this should raise RuntimeError
>       with pytest.raises(RuntimeError, match="Trying to create tensor with negative dimension"):
E       Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_torch_nn_modules_batchnorm_g1.py:461: Failed
__________________________ test_invalid_num_features ___________________________

    def test_invalid_num_features():
        """Test that num_features <= 0 raises ValueError."""
>       with pytest.raises(ValueError):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_torch_nn_modules_batchnorm_g2.py:200: Failed
_______________________________ test_invalid_eps _______________________________

    def test_invalid_eps():
        """Test that eps <= 0 raises ValueError."""
>       with pytest.raises(ValueError):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_torch_nn_modules_batchnorm_g2.py:209: Failed
____________________________ test_invalid_momentum _____________________________

    def test_invalid_momentum():
        """Test that momentum outside [0, 1] raises ValueError."""
>       with pytest.raises(ValueError):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_torch_nn_modules_batchnorm_g2.py:218: Failed
_______________________ test_input_dimension_validation ________________________

    def test_input_dimension_validation():
        """Test that input dimensions are validated."""
        bn1d = BatchNorm1d(num_features=10)
        bn2d = BatchNorm2d(num_features=10)
        bn3d = BatchNorm3d(num_features=10)
    
        # BatchNorm1d should accept 2D or 3D input
        input_2d = torch.randn(4, 10)
        input_3d = torch.randn(4, 10, 32)
        output_2d = bn1d(input_2d)
        output_3d = bn1d(input_3d)
        assert output_2d.shape == input_2d.shape
        assert output_3d.shape == input_3d.shape
    
        # BatchNorm2d should reject 2D input
        with pytest.raises(RuntimeError):
>           bn2d(input_2d)

tests/test_torch_nn_modules_batchnorm_g2.py:241: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:138: in forward
    self._check_input_dim(input)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
input = tensor([[-0.5197,  1.8524,  1.8365,  2.0741, -0.7373, -0.7687, -0.0512,  1.5986,
          0.2123,  1.1060],
        [...-0.2296],
        [ 1.7335, -1.1168,  0.1869,  1.0334, -1.2218, -0.4378,  1.4290,  0.2306,
         -1.8913,  0.7457]])

    def _check_input_dim(self, input):
        if input.dim() != 4:
>           raise ValueError("expected 4D input (got {}D input)".format(input.dim()))
E           ValueError: expected 4D input (got 2D input)

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:410: ValueError
__________________________ test_invalid_num_features ___________________________

    def test_invalid_num_features():
        """Test that num_features <= 0 raises ValueError."""
>       with pytest.raises(ValueError):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_torch_nn_modules_batchnorm_new.py:265: Failed
_______________________________ test_invalid_eps _______________________________

    def test_invalid_eps():
        """Test that eps <= 0 raises ValueError."""
>       with pytest.raises(ValueError):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_torch_nn_modules_batchnorm_new.py:274: Failed
____________________________ test_invalid_momentum _____________________________

    def test_invalid_momentum():
        """Test that momentum outside [0, 1] raises ValueError."""
>       with pytest.raises(ValueError):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_torch_nn_modules_batchnorm_new.py:283: Failed
_______________________ test_input_dimension_validation ________________________

    def test_input_dimension_validation():
        """Test that input dimensions are validated."""
        bn1d = BatchNorm1d(num_features=10)
        bn2d = BatchNorm2d(num_features=10)
        bn3d = BatchNorm3d(num_features=10)
    
        # BatchNorm1d should accept 2D or 3D input
        input_2d = torch.randn(4, 10)
        input_3d = torch.randn(4, 10, 32)
        output_2d = bn1d(input_2d)
        output_3d = bn1d(input_3d)
        assert output_2d.shape == input_2d.shape
        assert output_3d.shape == input_3d.shape
    
        # BatchNorm2d should reject 2D input
        with pytest.raises(RuntimeError):
>           bn2d(input_2d)

tests/test_torch_nn_modules_batchnorm_new.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/module.py:1190: in _call_impl
    return forward_call(*input, **kwargs)
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:138: in forward
    self._check_input_dim(input)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
input = tensor([[-0.5197,  1.8524,  1.8365,  2.0741, -0.7373, -0.7687, -0.0512,  1.5986,
          0.2123,  1.1060],
        [...-0.2296],
        [ 1.7335, -1.1168,  0.1869,  1.0334, -1.2218, -0.4378,  1.4290,  0.2306,
         -1.8913,  0.7457]])

    def _check_input_dim(self, input):
        if input.dim() != 4:
>           raise ValueError("expected 4D input (got {}D input)".format(input.dim()))
E           ValueError: expected 4D input (got 2D input)

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:410: ValueError
=============================== warnings summary ===============================
exam/torch_group/nn.modules.batchnorm/tests/test_torch_nn_modules_batchnorm_g1.py::test_lazy_batchnorm1d_delayed_initialization
  /opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
    warnings.warn('Lazy modules are a new feature under heavy development '

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                           Stmts   Miss Branch BrPart  Cover   Missing
------------------------------------------------------------------------------------------
cleanup.py                                         7      7      4      0     0%   1-16
cleanup_temp.py                                   10     10      4      0     0%   1-21
direct_test.py                                    67     67      0      0     0%   1-84
execute_test.py                                    8      8      2      0     0%   1-12
final_cleanup.py                                  13     13      4      0     0%   1-26
run_test.py                                        8      8      0      0     0%   1-10
test_num_features.py                              32     32      0      0     0%   1-41
tests/test_torch_nn_modules_batchnorm_g1.py      191     20     12      2    88%   19-20, 25-26, 31-33, 38-52, 119->130, 184->189, 425, 447, 465-476
tests/test_torch_nn_modules_batchnorm_g2.py       84     18     10      0    79%   19-20, 25-26, 31-33, 38-52, 203-204, 212-213, 221-222, 244-245
tests/test_torch_nn_modules_batchnorm_new.py      97     18     10      2    79%   19-20, 25-26, 31-33, 38-52, 118->129, 182->187, 268-269, 277-278, 286-287, 309-310
------------------------------------------------------------------------------------------
TOTAL                                            517    201     46      4    60%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_nn_modules_batchnorm_g1.py::test_lazy_batchnorm1d_delayed_initialization
FAILED tests/test_torch_nn_modules_batchnorm_g1.py::test_invalid_num_features
FAILED tests/test_torch_nn_modules_batchnorm_g2.py::test_invalid_num_features
FAILED tests/test_torch_nn_modules_batchnorm_g2.py::test_invalid_eps - Failed...
FAILED tests/test_torch_nn_modules_batchnorm_g2.py::test_invalid_momentum - F...
FAILED tests/test_torch_nn_modules_batchnorm_g2.py::test_input_dimension_validation
FAILED tests/test_torch_nn_modules_batchnorm_new.py::test_invalid_num_features
FAILED tests/test_torch_nn_modules_batchnorm_new.py::test_invalid_eps - Faile...
FAILED tests/test_torch_nn_modules_batchnorm_new.py::test_invalid_momentum - ...
FAILED tests/test_torch_nn_modules_batchnorm_new.py::test_input_dimension_validation
10 failed, 16 passed, 1 warning in 0.90s

Error: exit 1