=== Run Tests ===
....F.FF..                                                               [100%]
=================================== FAILURES ===================================
_____________ test_random_split_integer_split[100-lengths0-False] ______________

dataset_size = 100, lengths = [70, 30], use_generator = False

    @pytest.mark.parametrize("dataset_size,lengths,use_generator", [
        (100, [70, 30], False),
        (100, [0.7, 0.3], True),
    ])
    def test_random_split_integer_split(dataset_size, lengths, use_generator):
        """random_split 整数分割"""
        # 创建模拟数据集
        class MockDataset(Dataset):
            def __init__(self, size):
                self.size = size
    
            def __len__(self):
                return self.size
    
            def __getitem__(self, idx):
                return idx  # 返回索引作为样本
    
        dataset = MockDataset(dataset_size)
    
        # 设置随机种子
        generator = torch.Generator().manual_seed(42) if use_generator else None
    
        # Mock torch.randperm 以控制随机性
        with patch('torch.randperm') as mock_randperm:
            # 创建固定的随机排列
            if isinstance(lengths[0], float):
                # 比例分割：转换为整数长度
                int_lengths = [int(dataset_size * length) for length in lengths]
                # 调整最后一个长度以确保总和正确
                int_lengths[-1] = dataset_size - sum(int_lengths[:-1])
                expected_perm = torch.arange(dataset_size)
            else:
                # 整数分割
                int_lengths = lengths
                expected_perm = torch.arange(dataset_size)
    
            mock_randperm.return_value = expected_perm
    
            # 执行 random_split
            subsets = random_split(dataset, lengths, generator=generator)
    
        # 验证返回结果
        assert isinstance(subsets, list), "应返回列表"
        assert len(subsets) == len(lengths), "返回列表长度应与指定长度数一致"
    
        # 验证每个子集
        total_samples = 0
        all_indices = set()
    
        for i, subset in enumerate(subsets):
            # 验证子集类型
            assert isinstance(subset, Subset), f"子集{i}应为Subset类型"
    
            # 验证子集长度
            if isinstance(lengths[0], float):
                expected_len = int_lengths[i]
            else:
                expected_len = lengths[i]
    
            assert len(subset) == expected_len, f"子集{i}大小与指定长度不一致"
    
            # 收集所有样本索引
            for j in range(len(subset)):
                idx = subset[j]
                all_indices.add(idx)
                total_samples += 1
    
        # 验证所有样本被分配且不重复
        assert total_samples == dataset_size, "所有样本应被分配"
        assert len(all_indices) == dataset_size, "样本分配不应重复"
    
        # 验证原数据集未被修改
        assert len(dataset) == dataset_size, "原数据集长度不应改变"
    
        # 随机性可控制（使用固定种子） - strong断言
        # 使用相同的种子和参数，结果应可重现
        generator1 = torch.Generator().manual_seed(42) if use_generator else None
        generator2 = torch.Generator().manual_seed(42) if use_generator else None
    
        with patch('torch.randperm') as mock_randperm1:
            mock_randperm1.return_value = expected_perm
            subsets1 = random_split(dataset, lengths, generator=generator1)
    
        with patch('torch.randperm') as mock_randperm2:
            mock_randperm2.return_value = expected_perm
            subsets2 = random_split(dataset, lengths, generator=generator2)
    
        # 验证两次分割结果相同
        assert len(subsets1) == len(subsets2), "相同参数下分割结果长度应相同"
    
        for i, (subset1, subset2) in enumerate(zip(subsets1, subsets2)):
            assert len(subset1) == len(subset2), f"子集{i}长度应相同"
    
            # 验证子集内容相同
            for j in range(len(subset1)):
>               assert subset1[j] == subset2[j], f"子集{i}的第{j}个样本应相同"
E               AssertionError: 子集0的第0个样本应相同
E               assert 91 == 17

tests/test_torch_utils_data_dataset.py:312: AssertionError
_____ test_concat_dataset_multiple_datasets[dataset_sizes0-index_to_test0] _____

dataset_sizes = [20, 30, 50], index_to_test = [0, 25, 80, -1]

    @pytest.mark.parametrize("dataset_sizes,index_to_test", [
        ([20, 30, 50], [0, 25, 80, -1]),
        ([0, 50, 0], [0, 25, 49]),
    ])
    def test_concat_dataset_multiple_datasets(dataset_sizes, index_to_test):
        """ConcatDataset 多数据集拼接"""
        # 创建多个模拟数据集
        datasets = []
        for size in dataset_sizes:
            dataset = MockDataset(size)
            datasets.append(dataset)
    
        # 创建 ConcatDataset
        concat_dataset = ConcatDataset(datasets)
    
        # 验证总长度等于各数据集长度之和
        expected_total_size = sum(dataset_sizes)
        assert len(concat_dataset) == expected_total_size, \
            f"总长度应为{expected_total_size}，实际为{len(concat_dataset)}"
    
        # 测试索引映射
        for idx in index_to_test:
            # 跳过空数据集的测试
            if expected_total_size == 0:
                with pytest.raises(IndexError):
                    _ = concat_dataset[idx]
                continue
    
            # 计算实际索引
            actual_idx = idx if idx >= 0 else expected_total_size + idx
    
            # 验证索引在有效范围内
            if 0 <= actual_idx < expected_total_size:
                # 获取样本
                sample = concat_dataset[idx]
    
                # 验证样本值正确
                # 计算样本来自哪个数据集
                cumulative_sizes = [0]
                for size in dataset_sizes:
                    cumulative_sizes.append(cumulative_sizes[-1] + size)
    
                # 使用二分查找确定数据集索引
                dataset_idx = bisect.bisect_right(cumulative_sizes, actual_idx) - 1
    
                # 计算在数据集内的局部索引
                local_idx = actual_idx - cumulative_sizes[dataset_idx]
    
                # 验证样本值
                expected_sample = datasets[dataset_idx][local_idx]
                assert sample == expected_sample, \
                    f"索引{idx}的样本值不正确，应为{expected_sample}，实际为{sample}"
    
                # 验证二分查找逻辑正确（strong断言）
                # 手动计算数据集索引进行验证
                manual_dataset_idx = -1
                cumulative = 0
                for i, size in enumerate(dataset_sizes):
                    if actual_idx < cumulative + size:
                        manual_dataset_idx = i
                        break
                    cumulative += size
    
                assert dataset_idx == manual_dataset_idx, \
                    f"二分查找结果{dataset_idx}与手动计算{manual_dataset_idx}不一致"
    
                # 验证局部索引计算正确
                manual_local_idx = actual_idx - cumulative
                assert local_idx == manual_local_idx, \
                    f"局部索引计算{local_idx}与手动计算{manual_local_idx}不一致"
            else:
                # 索引超出范围应抛出IndexError
                with pytest.raises(IndexError):
                    _ = concat_dataset[idx]
    
        # 测试空数据集拼接的特殊情况
        if all(size == 0 for size in dataset_sizes):
            # 所有数据集都为空
            assert len(concat_dataset) == 0, "空数据集拼接后长度应为0"
            with pytest.raises(IndexError):
                _ = concat_dataset[0]
    
        # 验证支持负索引访问
        if expected_total_size > 0:
            # 测试最后一个元素
            last_sample = concat_dataset[-1]
            first_sample = concat_dataset[0]
    
            # 验证负索引与正索引对应关系
            assert concat_dataset[-1] == concat_dataset[expected_total_size - 1], \
                "负索引-1应与最后一个正索引对应"
    
            # 验证边界条件
            if expected_total_size > 1:
                assert concat_dataset[-2] == concat_dataset[expected_total_size - 2], \
                    "负索引-2应与倒数第二个正索引对应"
    
        # 边界条件处理正确（strong断言）
        # 测试边界索引
        if expected_total_size > 0:
            # 测试第一个数据集的第一个元素
            if dataset_sizes[0] > 0:
                sample = concat_dataset[0]
                expected = datasets[0][0]
                assert sample == expected, "第一个数据集的第一个元素访问不正确"
    
            # 测试第一个数据集的最后一个元素
            if dataset_sizes[0] > 0:
                last_idx = dataset_sizes[0] - 1
                sample = concat_dataset[last_idx]
                expected = datasets[0][-1]
                assert sample == expected, "第一个数据集的最后一个元素访问不正确"
    
            # 测试最后一个数据集的第一个元素
            if dataset_sizes[-1] > 0:
                first_idx = sum(dataset_sizes[:-1])
                sample = concat_dataset[first_idx]
                expected = datasets[-1][0]
                assert sample == expected, "最后一个数据集的第一个元素访问不正确"
    
            # 测试最后一个数据集的最后一个元素
            if dataset_sizes[-1] > 0:
                last_idx = expected_total_size - 1
                sample = concat_dataset[last_idx]
                expected = datasets[-1][-1]
                assert sample == expected, "最后一个数据集的最后一个元素访问不正确"
    
        # 空数据集拼接处理（strong断言）
        # 测试包含空数据集的混合情况
        mixed_sizes = []
        mixed_datasets = []
        for size in dataset_sizes:
            if size == 0:
                # 空数据集
                mixed_datasets.append(MockDataset(0))
            else:
                # 非空数据集
                mixed_datasets.append(MockDataset(size))
            mixed_sizes.append(size)
    
        mixed_concat = ConcatDataset(mixed_datasets)
        assert len(mixed_concat) == sum(mixed_sizes), "混合空/非空数据集拼接长度应正确"
    
        # 验证空数据集不影响索引映射
        if sum(mixed_sizes) > 0:
            # 找到第一个非空数据集
            first_nonempty_idx = next(i for i, size in enumerate(mixed_sizes) if size > 0)
            cumulative_before = sum(mixed_sizes[:first_nonempty_idx])
    
            # 测试第一个非空数据集的第一个元素
            sample = mixed_concat[cumulative_before]
            expected = mixed_datasets[first_nonempty_idx][0]
            assert sample == expected, "空数据集后的第一个元素访问不正确"
    
        # 测试单个数据集的情况（边界情况）
        single_dataset = [MockDataset(10)]
        single_concat = ConcatDataset(single_dataset)
        assert len(single_concat) == 10, "单个数据集拼接长度应正确"
        assert single_concat[0] == single_dataset[0][0], "单个数据集元素访问应正确"
        assert single_concat[-1] == single_dataset[0][-1], "单个数据集负索引访问应正确"
    
        # 测试零个数据集的情况（边界情况）
>       empty_concat = ConcatDataset([])

tests/test_torch_utils_data_dataset.py:516: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.utils.data.dataset.ConcatDataset object at 0x122394d60>
datasets = []

    def __init__(self, datasets: Iterable[Dataset]) -> None:
        super(ConcatDataset, self).__init__()
        self.datasets = list(datasets)
>       assert len(self.datasets) > 0, 'datasets should not be an empty iterable'  # type: ignore[arg-type]
E       AssertionError: datasets should not be an empty iterable

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/utils/data/dataset.py:222: AssertionError
_____ test_concat_dataset_multiple_datasets[dataset_sizes1-index_to_test1] _____

dataset_sizes = [0, 50, 0], index_to_test = [0, 25, 49]

    @pytest.mark.parametrize("dataset_sizes,index_to_test", [
        ([20, 30, 50], [0, 25, 80, -1]),
        ([0, 50, 0], [0, 25, 49]),
    ])
    def test_concat_dataset_multiple_datasets(dataset_sizes, index_to_test):
        """ConcatDataset 多数据集拼接"""
        # 创建多个模拟数据集
        datasets = []
        for size in dataset_sizes:
            dataset = MockDataset(size)
            datasets.append(dataset)
    
        # 创建 ConcatDataset
        concat_dataset = ConcatDataset(datasets)
    
        # 验证总长度等于各数据集长度之和
        expected_total_size = sum(dataset_sizes)
        assert len(concat_dataset) == expected_total_size, \
            f"总长度应为{expected_total_size}，实际为{len(concat_dataset)}"
    
        # 测试索引映射
        for idx in index_to_test:
            # 跳过空数据集的测试
            if expected_total_size == 0:
                with pytest.raises(IndexError):
                    _ = concat_dataset[idx]
                continue
    
            # 计算实际索引
            actual_idx = idx if idx >= 0 else expected_total_size + idx
    
            # 验证索引在有效范围内
            if 0 <= actual_idx < expected_total_size:
                # 获取样本
                sample = concat_dataset[idx]
    
                # 验证样本值正确
                # 计算样本来自哪个数据集
                cumulative_sizes = [0]
                for size in dataset_sizes:
                    cumulative_sizes.append(cumulative_sizes[-1] + size)
    
                # 使用二分查找确定数据集索引
                dataset_idx = bisect.bisect_right(cumulative_sizes, actual_idx) - 1
    
                # 计算在数据集内的局部索引
                local_idx = actual_idx - cumulative_sizes[dataset_idx]
    
                # 验证样本值
                expected_sample = datasets[dataset_idx][local_idx]
                assert sample == expected_sample, \
                    f"索引{idx}的样本值不正确，应为{expected_sample}，实际为{sample}"
    
                # 验证二分查找逻辑正确（strong断言）
                # 手动计算数据集索引进行验证
                manual_dataset_idx = -1
                cumulative = 0
                for i, size in enumerate(dataset_sizes):
                    if actual_idx < cumulative + size:
                        manual_dataset_idx = i
                        break
                    cumulative += size
    
                assert dataset_idx == manual_dataset_idx, \
                    f"二分查找结果{dataset_idx}与手动计算{manual_dataset_idx}不一致"
    
                # 验证局部索引计算正确
                manual_local_idx = actual_idx - cumulative
                assert local_idx == manual_local_idx, \
                    f"局部索引计算{local_idx}与手动计算{manual_local_idx}不一致"
            else:
                # 索引超出范围应抛出IndexError
                with pytest.raises(IndexError):
                    _ = concat_dataset[idx]
    
        # 测试空数据集拼接的特殊情况
        if all(size == 0 for size in dataset_sizes):
            # 所有数据集都为空
            assert len(concat_dataset) == 0, "空数据集拼接后长度应为0"
            with pytest.raises(IndexError):
                _ = concat_dataset[0]
    
        # 验证支持负索引访问
        if expected_total_size > 0:
            # 测试最后一个元素
            last_sample = concat_dataset[-1]
            first_sample = concat_dataset[0]
    
            # 验证负索引与正索引对应关系
            assert concat_dataset[-1] == concat_dataset[expected_total_size - 1], \
                "负索引-1应与最后一个正索引对应"
    
            # 验证边界条件
            if expected_total_size > 1:
                assert concat_dataset[-2] == concat_dataset[expected_total_size - 2], \
                    "负索引-2应与倒数第二个正索引对应"
    
        # 边界条件处理正确（strong断言）
        # 测试边界索引
        if expected_total_size > 0:
            # 测试第一个数据集的第一个元素
            if dataset_sizes[0] > 0:
                sample = concat_dataset[0]
                expected = datasets[0][0]
                assert sample == expected, "第一个数据集的第一个元素访问不正确"
    
            # 测试第一个数据集的最后一个元素
            if dataset_sizes[0] > 0:
                last_idx = dataset_sizes[0] - 1
                sample = concat_dataset[last_idx]
                expected = datasets[0][-1]
                assert sample == expected, "第一个数据集的最后一个元素访问不正确"
    
            # 测试最后一个数据集的第一个元素
            if dataset_sizes[-1] > 0:
                first_idx = sum(dataset_sizes[:-1])
                sample = concat_dataset[first_idx]
                expected = datasets[-1][0]
                assert sample == expected, "最后一个数据集的第一个元素访问不正确"
    
            # 测试最后一个数据集的最后一个元素
            if dataset_sizes[-1] > 0:
                last_idx = expected_total_size - 1
                sample = concat_dataset[last_idx]
                expected = datasets[-1][-1]
                assert sample == expected, "最后一个数据集的最后一个元素访问不正确"
    
        # 空数据集拼接处理（strong断言）
        # 测试包含空数据集的混合情况
        mixed_sizes = []
        mixed_datasets = []
        for size in dataset_sizes:
            if size == 0:
                # 空数据集
                mixed_datasets.append(MockDataset(0))
            else:
                # 非空数据集
                mixed_datasets.append(MockDataset(size))
            mixed_sizes.append(size)
    
        mixed_concat = ConcatDataset(mixed_datasets)
        assert len(mixed_concat) == sum(mixed_sizes), "混合空/非空数据集拼接长度应正确"
    
        # 验证空数据集不影响索引映射
        if sum(mixed_sizes) > 0:
            # 找到第一个非空数据集
            first_nonempty_idx = next(i for i, size in enumerate(mixed_sizes) if size > 0)
            cumulative_before = sum(mixed_sizes[:first_nonempty_idx])
    
            # 测试第一个非空数据集的第一个元素
            sample = mixed_concat[cumulative_before]
            expected = mixed_datasets[first_nonempty_idx][0]
            assert sample == expected, "空数据集后的第一个元素访问不正确"
    
        # 测试单个数据集的情况（边界情况）
        single_dataset = [MockDataset(10)]
        single_concat = ConcatDataset(single_dataset)
        assert len(single_concat) == 10, "单个数据集拼接长度应正确"
        assert single_concat[0] == single_dataset[0][0], "单个数据集元素访问应正确"
        assert single_concat[-1] == single_dataset[0][-1], "单个数据集负索引访问应正确"
    
        # 测试零个数据集的情况（边界情况）
>       empty_concat = ConcatDataset([])

tests/test_torch_utils_data_dataset.py:516: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch.utils.data.dataset.ConcatDataset object at 0x122337b20>
datasets = []

    def __init__(self, datasets: Iterable[Dataset]) -> None:
        super(ConcatDataset, self).__init__()
        self.datasets = list(datasets)
>       assert len(self.datasets) > 0, 'datasets should not be an empty iterable'  # type: ignore[arg-type]
E       AssertionError: datasets should not be an empty iterable

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/utils/data/dataset.py:222: AssertionError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                                     Stmts   Miss Branch BrPart  Cover   Missing
------------------------------------------------------------------------------------
tests/test_torch_utils_data_dataset.py     388     40     94     14    87%   13-17, 21-32, 48, 69, 122->exit, 190-191, 212-213, 318-326, 329->exit, 378-380, 411->417, 426-427, 432-434, 437->453, 447->453, 453->483, 498->509, 517-519, 571
------------------------------------------------------------------------------------
TOTAL                                      388     40     94     14    87%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_utils_data_dataset.py::test_random_split_integer_split[100-lengths0-False]
FAILED tests/test_torch_utils_data_dataset.py::test_concat_dataset_multiple_datasets[dataset_sizes0-index_to_test0]
FAILED tests/test_torch_utils_data_dataset.py::test_concat_dataset_multiple_datasets[dataset_sizes1-index_to_test1]
3 failed, 7 passed in 0.79s

Error: exit 1