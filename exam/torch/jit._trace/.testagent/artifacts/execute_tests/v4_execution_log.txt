=== Run Tests ===
.FF....                                                                  [100%]
=================================== FAILURES ===================================
_________________________ test_invalid_input_handling __________________________

    def test_invalid_input_handling():
        """Test exception handling for invalid inputs."""
        set_random_seed(42)
    
        # Define a simple function
        def add_multiply(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
            """Simple function: (x + y) * 2"""
            return (x + y) * 2
    
        # Create valid test tensors
        shape = (2, 2)
        x = torch.randn(shape, dtype=torch.float32)
        y = torch.randn(shape, dtype=torch.float32)
    
        # Test 1: Non-tensor input should raise an error during tracing
        print("Test 1: Testing with non-tensor input...")
        with pytest.raises((RuntimeError, TypeError)) as exc_info:
            # Try to trace with a non-tensor input
            traced_fn = torch.jit.trace(
                add_multiply,
                (x, 42),  # 42 is not a tensor
                strict=True,
                check_trace=True
            )
    
        # Check error message contains expected keywords
        error_msg = str(exc_info.value).lower()
        assert any(keyword in error_msg for keyword in ['tensor', 'type', 'expected']), \
            f"Expected error about tensor type, got: {error_msg}"
    
        # Test 2: Mismatched shape inputs should work during tracing
        # but might fail during execution with wrong shapes
        print("Test 2: Testing with mismatched shapes during execution...")
    
        # First trace with correct shapes
        traced_fn = torch.jit.trace(
            add_multiply,
            (x, y),
            strict=True,
            check_trace=True
        )
    
        # Now try to call with mismatched shapes
        y_wrong_shape = torch.randn(3, 3, dtype=torch.float32)
    
        # This should raise a runtime error when executing the traced function
        with pytest.raises(RuntimeError) as exc_info:
            traced_fn(x, y_wrong_shape)
    
        error_msg = str(exc_info.value).lower()
        # The error might be about shape mismatch or broadcasting
        assert any(keyword in error_msg for keyword in ['shape', 'size', 'broadcast', 'inconsistent']), \
            f"Expected error about shape mismatch, got: {error_msg}"
    
        # Test 3: None input should raise an error
        print("Test 3: Testing with None input...")
        with pytest.raises((RuntimeError, TypeError)) as exc_info:
            traced_fn = torch.jit.trace(
                add_multiply,
                (x, None),  # None is not a tensor
                strict=True,
                check_trace=True
            )
    
        # Test 4: Wrong dtype input to traced function
        print("Test 4: Testing with wrong dtype...")
        y_wrong_dtype = torch.randn(shape, dtype=torch.float64)
    
        # This might work (dtype conversion) or fail depending on PyTorch version
        try:
            result = traced_fn(x, y_wrong_dtype)
            # If it works, check the result dtype
            assert result.dtype == torch.float32 or result.dtype == torch.float64, \
                f"Unexpected dtype: {result.dtype}"
        except RuntimeError as e:
            # It's also acceptable if it fails with a dtype error
            error_msg = str(e).lower()
            assert any(keyword in error_msg for keyword in ['dtype', 'type', 'scalar type']), \
                f"Expected error about dtype, got: {error_msg}"
    
        # Test 5: Test with invalid function (not callable)
        print("Test 5: Testing with non-callable func...")
        with pytest.raises(TypeError) as exc_info:
>           traced_fn = torch.jit.trace(
                "not a function",  # Not callable
                (x, y),
                strict=True,
                check_trace=True
            )

tests/test_torch_jit_trace_g4.py:173: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/jit/_trace.py:803: in trace
    name = _qualified_name(func)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = 'not a function', mangle_name = True

    def _qualified_name(obj, mangle_name=True) -> str:
        # This special case allows us to override the qualified name on a type.
        # It's currently used in conjunction with tracing, where we create a
        # fake module to filter only supported attributes. However, since this
        # new type is defined as a local class, we need a mechanism to override
        # its qualname so it appears correctly in the TorchScript system. This,
        # we set '_jit_override_qualname' with the original traced module's
        # qualified name, which is picked up here
        if hasattr(obj, "_jit_override_qualname"):
            return obj._jit_override_qualname
        # short-circuit in cases where the object already has a known qualified name
        if isinstance(obj, torch._C.ScriptFunction):
            return obj.qualified_name
    
        if getattr(obj, "__name__", None):
            name = obj.__name__
        # Enum classes do not have `__name__` attr, instead they have `name`.
        elif isinstance(obj, enum.Enum):
            name = obj.name
        else:
>           raise RuntimeError("Could not get name of python class object")
E           RuntimeError: Could not get name of python class object

/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/torch/_jit_internal.py:1125: RuntimeError
----------------------------- Captured stdout call -----------------------------
Test 1: Testing with non-tensor input...
Test 2: Testing with mismatched shapes during execution...
Test 3: Testing with None input...
Test 4: Testing with wrong dtype...
Test 5: Testing with non-callable func...
_____________________ test_dynamic_control_flow_rejection ______________________

    def test_dynamic_control_flow_rejection():
        """Test tracing with dynamic control flow (should generate warnings)."""
        set_random_seed(42)
    
        # Define a function with dynamic control flow based on tensor values
        def function_with_control_flow(x: torch.Tensor) -> torch.Tensor:
            """
            Function with data-dependent control flow.
            This should generate TracerWarnings.
            """
            result = torch.zeros_like(x)
    
            # Dynamic control flow based on tensor values
            for i in range(x.shape[0]):
                if x[i].item() > 0:  # .item() converts to Python scalar
                    result[i] = x[i] * 2
                else:
                    result[i] = x[i] * 0.5
    
            return result
    
        # Create test tensor
        shape = (3,)
        x = torch.tensor([1.0, -2.0, 3.0], dtype=torch.float32)
    
        # Weak assertions:
        # 1. Warning should be generated during tracing
        # 2. Basic equality should still work for the specific input
    
        # Capture warnings during tracing
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
    
            # Trace the function - this should generate TracerWarnings
            traced_fn = torch.jit.trace(
                function_with_control_flow,
                (x,),
                strict=True,
                check_trace=True
            )
    
            # Check that at least one warning was generated
            assert len(w) > 0, "Expected TracerWarnings for dynamic control flow"
    
            # Check that it's a TracerWarning
            tracer_warnings = [warning for warning in w if 'TracerWarning' in str(warning.category)]
            assert len(tracer_warnings) > 0, "Expected TracerWarning category"
    
            # Check warning message contains relevant keywords
            warning_msg = str(tracer_warnings[0].message).lower()
            assert any(keyword in warning_msg for keyword in [
                'tensor', 'python', 'boolean', 'constant', 'trace', 'incorrect',
                'control flow', 'data flow'
            ]), f"Expected TracerWarning about tensor to Python conversion, got: {warning_msg}"
    
        # Test the traced function with the original input
        result = traced_fn(x)
        expected = function_with_control_flow(x)
    
        # Basic equality check - should work for this specific input
        assert torch.allclose(result, expected, rtol=1e-5, atol=1e-5), \
            "Traced output doesn't match original function for the traced input"
    
        # Test with a different input to show the limitation
        # The control flow becomes constant in the trace
        x2 = torch.tensor([-1.0, 4.0, -3.0], dtype=torch.float32)
    
        result2 = traced_fn(x2)
        # The traced function will use the control flow decisions from the trace time
        # which were based on x = [1.0, -2.0, 3.0]
        # So for x2, it will apply the same operations as for x
    
        # Manually compute what the traced function should produce
        # Based on the trace-time decisions:
        # x[0] = 1.0 > 0 -> multiply by 2
        # x[1] = -2.0 <= 0 -> multiply by 0.5
        # x[2] = 3.0 > 0 -> multiply by 2
        # So the traced function always does: [*2, *0.5, *2]
        expected_traced = x2 * torch.tensor([2.0, 0.5, 2.0])
    
        assert torch.allclose(result2, expected_traced, rtol=1e-5, atol=1e-5), \
            "Traced function doesn't behave as expected with different input"
    
        # Compare with what the original function would do
        expected_original = function_with_control_flow(x2)
        # For x2 = [-1.0, 4.0, -3.0]:
        # -1.0 <= 0 -> *0.5 = -0.5
        # 4.0 > 0 -> *2 = 8.0
        # -3.0 <= 0 -> *0.5 = -1.5
    
        # They should be different because the trace fixed the control flow
        assert not torch.allclose(result2, expected_original, rtol=1e-5, atol=1e-5), \
            "Traced function shouldn't match original for different input with dynamic control flow"
    
        # Test another function with loop control flow
        def function_with_loop(x: torch.Tensor) -> torch.Tensor:
            """Function with Python loop."""
            result = torch.zeros_like(x)
            n = x.shape[0]  # This is a Python int
    
            # Python for loop
            for i in range(n):
                result[i] = x[i] * i  # i is Python int
    
            return result
    
        # This should also generate warnings
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
    
            traced_fn2 = torch.jit.trace(
                function_with_loop,
                (x,),
                strict=True,
                check_trace=True
            )
    
>           assert len(w) > 0, "Expected warnings for Python loop"
E           AssertionError: Expected warnings for Python loop
E           assert 0 > 0
E            +  where 0 = len([])

tests/test_torch_jit_trace_g4.py:309: AssertionError
=============================== warnings summary ===============================
exam/torch_group/jit._trace/tests/test_torch_jit_trace_g4.py::test_trace_module_with_edge_cases
  /Users/zzf1sh/Documents/Project/TestAgent-CLI-main/exam/torch_group/jit._trace/tests/test_torch_jit_trace_g4.py:371: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    if x.shape[0] == 0:

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.10.19-final-0 _______________

Name                               Stmts   Miss Branch BrPart  Cover   Missing
------------------------------------------------------------------------------
run_tests.py                          35     35     10      0     0%   4-62
tests/test_torch_jit_trace_g4.py     168     21     16      2    85%   21, 164-167, 181-183, 312, 372, 403-423
------------------------------------------------------------------------------
TOTAL                                203     56     26      2    69%
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/test_torch_jit_trace_g4.py::test_invalid_input_handling - Runtim...
FAILED tests/test_torch_jit_trace_g4.py::test_dynamic_control_flow_rejection
2 failed, 5 passed, 1 warning in 0.63s

Error: exit 1