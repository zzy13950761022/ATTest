"""
Test cases for tensorflow.python.ops.nn_ops module.
Generated by TestAgent for target: tensorflow.python.ops.nn_ops
"""

import math
import numpy as np
import pytest
import tensorflow as tf
from tensorflow.python.ops import nn_ops

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# ==== BLOCK:HEADER START ====
# Helper functions
def create_random_tensor(shape, dtype=np.float32):
    """Create random tensor with given shape and dtype."""
    if dtype == np.float32:
        return tf.constant(np.random.randn(*shape).astype(np.float32))
    elif dtype == np.float64:
        return tf.constant(np.random.randn(*shape).astype(np.float64))
    else:
        raise ValueError(f"Unsupported dtype: {dtype}")

def assert_shapes_match(actual, expected, msg=""):
    """Assert that tensor shapes match."""
    assert actual.shape == expected.shape, f"{msg} Shape mismatch: {actual.shape} != {expected.shape}"

def assert_dtypes_match(actual, expected, msg=""):
    """Assert that tensor dtypes match."""
    assert actual.dtype == expected.dtype, f"{msg} Dtype mismatch: {actual.dtype} != {expected.dtype}"

def assert_no_nan_inf(tensor, msg=""):
    """Assert tensor contains no NaN or Inf values."""
    tensor_np = tensor.numpy()
    assert not np.any(np.isnan(tensor_np)), f"{msg} Tensor contains NaN values"
    assert not np.any(np.isinf(tensor_np)), f"{msg} Tensor contains Inf values"

def compute_conv2d_output_shape(input_shape, filter_shape, strides, padding):
    """Compute expected output shape for conv2d operation."""
    batch, in_height, in_width, in_channels = input_shape
    filter_height, filter_width, _, out_channels = filter_shape
    
    if isinstance(padding, str):
        if padding == 'VALID':
            out_height = math.ceil((in_height - filter_height + 1) / strides[1])
            out_width = math.ceil((in_width - filter_width + 1) / strides[2])
        elif padding == 'SAME':
            out_height = math.ceil(in_height / strides[1])
            out_width = math.ceil(in_width / strides[2])
        else:
            raise ValueError(f"Unknown padding: {padding}")
    elif isinstance(padding, (list, tuple)):
        # Explicit padding: [[0,0], [pad_top, pad_bottom], [pad_left, pad_right], [0,0]]
        pad_top = padding[1][0]
        pad_bottom = padding[1][1]
        pad_left = padding[2][0]
        pad_right = padding[2][1]
        out_height = math.ceil((in_height + pad_top + pad_bottom - filter_height + 1) / strides[1])
        out_width = math.ceil((in_width + pad_left + pad_right - filter_width + 1) / strides[2])
    else:
        raise ValueError(f"Invalid padding type: {type(padding)}")
    
    return (batch, out_height, out_width, out_channels)

def compute_max_pool_output_shape(input_shape, ksize, strides, padding):
    """Compute expected output shape for max_pool operation."""
    batch, in_height, in_width, channels = input_shape
    
    if isinstance(padding, str):
        if padding == 'VALID':
            out_height = math.ceil((in_height - ksize[1] + 1) / strides[1])
            out_width = math.ceil((in_width - ksize[2] + 1) / strides[2])
        elif padding == 'SAME':
            out_height = math.ceil(in_height / strides[1])
            out_width = math.ceil(in_width / strides[2])
        else:
            raise ValueError(f"Unknown padding: {padding}")
    else:
        raise ValueError(f"Invalid padding type for max_pool: {type(padding)}")
    
    return (batch, out_height, out_width, channels)
# ==== BLOCK:HEADER END ====

# ==== BLOCK:CASE_01 START ====
@pytest.mark.parametrize("test_params", [
    {
        "function": "conv2d",
        "input_shape": [1, 4, 4, 3],
        "filter_shape": [2, 2, 3, 4],
        "strides": [1, 1, 1, 1],
        "padding": "VALID",
        "dtype": "float32"
    },
    {
        "function": "conv2d",
        "input_shape": [2, 8, 8, 4],
        "filter_shape": [3, 3, 4, 8],
        "strides": [1, 2, 2, 1],
        "padding": "VALID",
        "dtype": "float64"
    }
])
def test_conv2d_basic_validation(test_params):
    """TC-01: 卷积操作基本验证"""
    # Extract parameters
    input_shape = test_params["input_shape"]
    filter_shape = test_params["filter_shape"]
    strides = test_params["strides"]
    padding = test_params["padding"]
    dtype = test_params["dtype"]
    
    # Convert dtype string to numpy dtype
    np_dtype = np.float32 if dtype == "float32" else np.float64
    
    # Create input tensor
    input_tensor = create_random_tensor(input_shape, np_dtype)
    
    # Create filter tensor
    filter_tensor = create_random_tensor(filter_shape, np_dtype)
    
    # Call target function
    result = nn_ops.conv2d(
        input=input_tensor,
        filter=filter_tensor,
        strides=strides,
        padding=padding
    )
    
    # Weak assertions (round 1)
    # 1. Shape match
    expected_shape = compute_conv2d_output_shape(input_shape, filter_shape, strides, padding)
    assert_shapes_match(result, tf.constant(np.zeros(expected_shape, dtype=np_dtype)), 
                       "Output shape mismatch")
    
    # 2. Dtype match
    assert_dtypes_match(result, input_tensor, "Output dtype mismatch")
    
    # 3. No NaN/Inf values
    assert_no_nan_inf(result, "Output contains NaN/Inf values")
    
    # 4. Basic property: output values should be finite
    result_np = result.numpy()
    assert np.all(np.isfinite(result_np)), "Output contains non-finite values"
    
    # 5. Compare with oracle (tensorflow.nn.conv2d)
    oracle_result = tf.nn.conv2d(
        input=input_tensor,
        filters=filter_tensor,
        strides=strides,
        padding=padding
    )
    
    # Check shape consistency with oracle
    assert_shapes_match(result, oracle_result, "Shape mismatch with oracle")
    
    # Check dtype consistency with oracle
    assert_dtypes_match(result, oracle_result, "Dtype mismatch with oracle")
    
    # For float32, use appropriate tolerance
    if dtype == "float32":
        tolerance = 1e-6
    else:  # float64
        tolerance = 1e-12
    
    # Check numerical consistency with oracle (within tolerance)
    diff = np.abs(result.numpy() - oracle_result.numpy())
    max_diff = np.max(diff)
    assert max_diff <= tolerance, f"Numerical mismatch with oracle: max_diff={max_diff} > tolerance={tolerance}"
# ==== BLOCK:CASE_01 END ====

# ==== BLOCK:CASE_02 START ====
@pytest.mark.parametrize("test_params", [
    {
        "function": "conv2d",
        "input_shape": [1, 5, 5, 3],
        "filter_shape": [3, 3, 3, 4],
        "strides": [1, 2, 2, 1],
        "padding": "SAME",
        "dtype": "float32"
    },
    {
        "function": "conv2d",
        "input_shape": [1, 7, 7, 2],
        "filter_shape": [2, 2, 2, 4],
        "strides": [1, 1, 1, 1],
        "padding": "SAME",
        "dtype": "float32"
    }
])
def test_conv2d_same_padding_validation(test_params):
    """TC-02: SAME填充卷积验证"""
    # Extract parameters
    input_shape = test_params["input_shape"]
    filter_shape = test_params["filter_shape"]
    strides = test_params["strides"]
    padding = test_params["padding"]
    dtype = test_params["dtype"]
    
    # Convert dtype string to numpy dtype
    np_dtype = np.float32 if dtype == "float32" else np.float64
    
    # Create input tensor
    input_tensor = create_random_tensor(input_shape, np_dtype)
    
    # Create filter tensor
    filter_tensor = create_random_tensor(filter_shape, np_dtype)
    
    # Call target function
    result = nn_ops.conv2d(
        input=input_tensor,
        filter=filter_tensor,
        strides=strides,
        padding=padding
    )
    
    # Weak assertions (round 1)
    # 1. Shape match
    expected_shape = compute_conv2d_output_shape(input_shape, filter_shape, strides, padding)
    assert_shapes_match(result, tf.constant(np.zeros(expected_shape, dtype=np_dtype)), 
                       "Output shape mismatch")
    
    # 2. Dtype match
    assert_dtypes_match(result, input_tensor, "Output dtype mismatch")
    
    # 3. Padding correctness check for SAME padding
    # For SAME padding with stride > 1, output size should be ceil(input_size / stride)
    batch, in_height, in_width, in_channels = input_shape
    _, filter_height, filter_width, out_channels = filter_shape
    
    # Compute expected output dimensions
    expected_height = math.ceil(in_height / strides[1])
    expected_width = math.ceil(in_width / strides[2])
    
    assert result.shape[1] == expected_height, \
        f"SAME padding height mismatch: {result.shape[1]} != {expected_height}"
    assert result.shape[2] == expected_width, \
        f"SAME padding width mismatch: {result.shape[2]} != {expected_width}"
    
    # 4. Output size validation
    # For SAME padding, output size should be consistent with formula
    # Check that output dimensions are correct
    assert result.shape[0] == batch, f"Batch size mismatch: {result.shape[0]} != {batch}"
    assert result.shape[3] == out_channels, f"Output channels mismatch: {result.shape[3]} != {out_channels}"
    
    # 5. Compare with oracle (tensorflow.nn.conv2d)
    oracle_result = tf.nn.conv2d(
        input=input_tensor,
        filters=filter_tensor,
        strides=strides,
        padding=padding
    )
    
    # Check shape consistency with oracle
    assert_shapes_match(result, oracle_result, "Shape mismatch with oracle")
    
    # Check dtype consistency with oracle
    assert_dtypes_match(result, oracle_result, "Dtype mismatch with oracle")
    
    # For float32, use appropriate tolerance
    if dtype == "float32":
        tolerance = 1e-6
    else:  # float64
        tolerance = 1e-12
    
    # Check numerical consistency with oracle (within tolerance)
    diff = np.abs(result.numpy() - oracle_result.numpy())
    max_diff = np.max(diff)
    assert max_diff <= tolerance, f"Numerical mismatch with oracle: max_diff={max_diff} > tolerance={tolerance}"
    
    # Additional check: no NaN/Inf values
    assert_no_nan_inf(result, "Output contains NaN/Inf values")
# ==== BLOCK:CASE_02 END ====

# ==== BLOCK:CASE_03 START ====
@pytest.mark.parametrize("test_params", [
    {
        "function": "max_pool",
        "input_shape": [1, 6, 6, 3],
        "ksize": [1, 2, 2, 1],
        "strides": [1, 2, 2, 1],
        "padding": "VALID",
        "dtype": "float32"
    },
    {
        "function": "max_pool",
        "input_shape": [2, 8, 8, 4],
        "ksize": [1, 3, 3, 1],
        "strides": [1, 3, 3, 1],
        "padding": "SAME",
        "dtype": "float64"
    }
])
def test_max_pool_validation(test_params):
    """TC-03: 最大池化操作验证"""
    # Extract parameters
    input_shape = test_params["input_shape"]
    ksize = test_params["ksize"]
    strides = test_params["strides"]
    padding = test_params["padding"]
    dtype = test_params["dtype"]
    
    # Convert dtype string to numpy dtype
    np_dtype = np.float32 if dtype == "float32" else np.float64
    
    # Create input tensor with positive values for max pool testing
    # Use absolute values to ensure we have positive numbers for meaningful max pool
    input_data = np.abs(np.random.randn(*input_shape).astype(np_dtype))
    input_tensor = tf.constant(input_data)
    
    # Call target function
    result = nn_ops.max_pool(
        value=input_tensor,
        ksize=ksize,
        strides=strides,
        padding=padding
    )
    
    # Weak assertions (round 1)
    # 1. Shape match
    expected_shape = compute_max_pool_output_shape(input_shape, ksize, strides, padding)
    assert_shapes_match(result, tf.constant(np.zeros(expected_shape, dtype=np_dtype)), 
                       "Output shape mismatch")
    
    # 2. Dtype match
    assert_dtypes_match(result, input_tensor, "Output dtype mismatch")
    
    # 3. Pooling correctness check
    # For max pool, each output value should be the maximum of the corresponding input window
    result_np = result.numpy()
    input_np = input_data
    
    batch, in_height, in_width, channels = input_shape
    _, ksize_h, ksize_w, _ = ksize
    stride_h, stride_w = strides[1], strides[2]
    
    out_height, out_width = result.shape[1], result.shape[2]
    
    # Verify max pooling property for a few sample positions
    for b in range(min(batch, 2)):  # Check first 2 batches
        for h in range(min(out_height, 3)):  # Check first 3 rows
            for w in range(min(out_width, 3)):  # Check first 3 columns
                for c in range(min(channels, 2)):  # Check first 2 channels
                    # Compute input window bounds
                    h_start = h * stride_h
                    h_end = min(h_start + ksize_h, in_height)
                    w_start = w * stride_w
                    w_end = min(w_start + ksize_w, in_width)
                    
                    # Extract input window
                    if padding == 'VALID':
                        # For VALID padding, window must be fully inside input
                        if h_end <= in_height and w_end <= in_width:
                            window = input_np[b, h_start:h_end, w_start:w_end, c]
                            expected_max = np.max(window)
                            actual_max = result_np[b, h, w, c]
                            
                            # Allow small tolerance for floating point comparison
                            if dtype == "float32":
                                tolerance = 1e-6
                            else:
                                tolerance = 1e-12
                            
                            assert abs(actual_max - expected_max) <= tolerance, \
                                f"Max pool incorrect at position ({b},{h},{w},{c}): " \
                                f"{actual_max} != {expected_max}"
    
    # 4. No NaN/Inf values
    assert_no_nan_inf(result, "Output contains NaN/Inf values")
    
    # 5. Compare with oracle (tensorflow.nn.max_pool)
    oracle_result = tf.nn.max_pool(
        input=input_tensor,
        ksize=ksize,
        strides=strides,
        padding=padding
    )
    
    # Check shape consistency with oracle
    assert_shapes_match(result, oracle_result, "Shape mismatch with oracle")
    
    # Check dtype consistency with oracle
    assert_dtypes_match(result, oracle_result, "Dtype mismatch with oracle")
    
    # For float32, use appropriate tolerance
    if dtype == "float32":
        tolerance = 1e-6
    else:  # float64
        tolerance = 1e-12
    
    # Check numerical consistency with oracle (within tolerance)
    diff = np.abs(result.numpy() - oracle_result.numpy())
    max_diff = np.max(diff)
    assert max_diff <= tolerance, f"Numerical mismatch with oracle: max_diff={max_diff} > tolerance={tolerance}"
    
    # Additional property: max pool output should be <= input max
    input_max = np.max(input_np)
    output_max = np.max(result_np)
    assert output_max <= input_max + tolerance, \
        f"Max pool output max ({output_max}) > input max ({input_max})"
# ==== BLOCK:CASE_03 END ====

# ==== BLOCK:CASE_04 START ====
@pytest.mark.parametrize("test_params", [
    {
        "function": "relu",
        "input_shape": [2, 3, 4],
        "dtype": "float32",
        "test_type": "positive_negative"
    }
])
def test_relu_activation_function_verification(test_params):
    """TC-04: ReLU激活函数验证"""
    # Extract parameters
    input_shape = test_params["input_shape"]
    dtype = test_params["dtype"]
    test_type = test_params["test_type"]
    
    # Convert dtype string to numpy dtype
    np_dtype = np.float32 if dtype == "float32" else np.float64
    
    # Create input tensor based on test type
    if test_type == "positive_negative":
        # Create input with both positive and negative values
        # Generate random numbers centered around 0
        input_data = np.random.randn(*input_shape).astype(np_dtype)
    else:
        # Default: random positive values
        input_data = np.random.rand(*input_shape).astype(np_dtype)
    
    input_tensor = tf.constant(input_data)
    
    # Call target function
    result = nn_ops.relu(input_tensor)
    
    # Weak assertions (round 1)
    # 1. Shape match
    assert_shapes_match(result, input_tensor, "Output shape mismatch")
    
    # 2. Dtype match
    assert_dtypes_match(result, input_tensor, "Output dtype mismatch")
    
    # 3. ReLU property: max(features, 0)
    result_np = result.numpy()
    expected_np = np.maximum(input_data, 0)
    
    # For float32, use appropriate tolerance
    if dtype == "float32":
        tolerance = 1e-6
    else:  # float64
        tolerance = 1e-12
    
    # Check numerical correctness
    diff = np.abs(result_np - expected_np)
    max_diff = np.max(diff)
    assert max_diff <= tolerance, \
        f"ReLU property violated: max_diff={max_diff} > tolerance={tolerance}"
    
    # 4. Non-negative output
    assert np.all(result_np >= -tolerance), "ReLU output contains negative values"
    
    # 5. Compare with oracle (tensorflow.nn.relu)
    oracle_result = tf.nn.relu(input_tensor)
    
    # Check shape consistency with oracle
    assert_shapes_match(result, oracle_result, "Shape mismatch with oracle")
    
    # Check dtype consistency with oracle
    assert_dtypes_match(result, oracle_result, "Dtype mismatch with oracle")
    
    # Check numerical consistency with oracle
    diff_oracle = np.abs(result.numpy() - oracle_result.numpy())
    max_diff_oracle = np.max(diff_oracle)
    assert max_diff_oracle <= tolerance, \
        f"Numerical mismatch with oracle: max_diff={max_diff_oracle} > tolerance={tolerance}"
    
    # Additional checks based on test type
    if test_type == "positive_negative":
        # Check that positive inputs are unchanged
        positive_mask = input_data > 0
        if np.any(positive_mask):
            positive_input = input_data[positive_mask]
            positive_output = result_np[positive_mask]
            positive_diff = np.abs(positive_output - positive_input)
            max_positive_diff = np.max(positive_diff)
            assert max_positive_diff <= tolerance, \
                f"Positive inputs changed: max_diff={max_positive_diff} > tolerance={tolerance}"
        
        # Check that negative inputs produce zero
        negative_mask = input_data < 0
        if np.any(negative_mask):
            negative_output = result_np[negative_mask]
            assert np.all(np.abs(negative_output) <= tolerance), \
                "Negative inputs should produce zero output"
    
    # 6. No NaN/Inf values
    assert_no_nan_inf(result, "Output contains NaN/Inf values")
# ==== BLOCK:CASE_04 END ====

# ==== BLOCK:CASE_05 START ====
@pytest.mark.parametrize("test_params", [
    {
        "function": "conv2d",
        "input_shape": [1, 4, 4, 3],
        "filter_shape": [2, 2, 3, 4],
        "strides": [1, 1, 1, 1],
        "padding": [[0, 0], [1, 1], [1, 1], [0, 0]],
        "dtype": "float32"
    }
])
def test_conv2d_explicit_padding_validation(test_params):
    """TC-05: 显式填充卷积验证"""
    # Extract parameters
    input_shape = test_params["input_shape"]
    filter_shape = test_params["filter_shape"]
    strides = test_params["strides"]
    padding = test_params["padding"]
    dtype = test_params["dtype"]
    
    # Convert dtype string to numpy dtype
    np_dtype = np.float32 if dtype == "float32" else np.float64
    
    # Create input tensor
    input_tensor = create_random_tensor(input_shape, np_dtype)
    
    # Create filter tensor
    filter_tensor = create_random_tensor(filter_shape, np_dtype)
    
    # Call target function
    result = nn_ops.conv2d(
        input=input_tensor,
        filter=filter_tensor,
        strides=strides,
        padding=padding
    )
    
    # Weak assertions (round 1)
    # 1. Shape match
    expected_shape = compute_conv2d_output_shape(input_shape, filter_shape, strides, padding)
    assert_shapes_match(result, tf.constant(np.zeros(expected_shape, dtype=np_dtype)), 
                       "Output shape mismatch")
    
    # 2. Dtype match
    assert_dtypes_match(result, input_tensor, "Output dtype mismatch")
    
    # 3. Explicit padding verification
    # For explicit padding [[0,0], [1,1], [1,1], [0,0]]:
    # - Top padding: 1, Bottom padding: 1
    # - Left padding: 1, Right padding: 1
    # Input size: 4x4, Filter size: 2x2, Stride: 1x1
    # Output size should be: (4 + 1 + 1 - 2 + 1) = 5
    batch, in_height, in_width, in_channels = input_shape
    filter_height, filter_width, _, out_channels = filter_shape
    
    # Extract padding values
    pad_top = padding[1][0]
    pad_bottom = padding[1][1]
    pad_left = padding[2][0]
    pad_right = padding[2][1]
    
    # Compute expected output dimensions
    expected_height = (in_height + pad_top + pad_bottom - filter_height) // strides[1] + 1
    expected_width = (in_width + pad_left + pad_right - filter_width) // strides[2] + 1
    
    assert result.shape[1] == expected_height, \
        f"Explicit padding height mismatch: {result.shape[1]} != {expected_height}"
    assert result.shape[2] == expected_width, \
        f"Explicit padding width mismatch: {result.shape[2]} != {expected_width}"
    
    # 4. Output size validation
    assert result.shape[0] == batch, f"Batch size mismatch: {result.shape[0]} != {batch}"
    assert result.shape[3] == out_channels, f"Output channels mismatch: {result.shape[3]} != {out_channels}"
    
    # 5. Compare with oracle (tensorflow.nn.conv2d)
    oracle_result = tf.nn.conv2d(
        input=input_tensor,
        filters=filter_tensor,
        strides=strides,
        padding=padding
    )
    
    # Check shape consistency with oracle
    assert_shapes_match(result, oracle_result, "Shape mismatch with oracle")
    
    # Check dtype consistency with oracle
    assert_dtypes_match(result, oracle_result, "Dtype mismatch with oracle")
    
    # For float32, use appropriate tolerance
    if dtype == "float32":
        tolerance = 1e-6
    else:  # float64
        tolerance = 1e-12
    
    # Check numerical consistency with oracle (within tolerance)
    diff = np.abs(result.numpy() - oracle_result.numpy())
    max_diff = np.max(diff)
    assert max_diff <= tolerance, f"Numerical mismatch with oracle: max_diff={max_diff} > tolerance={tolerance}"
    
    # 6. No NaN/Inf values
    assert_no_nan_inf(result, "Output contains NaN/Inf values")
    
    # 7. Basic property: output values should be finite
    result_np = result.numpy()
    assert np.all(np.isfinite(result_np)), "Output contains non-finite values"
# ==== BLOCK:CASE_05 END ====

# ==== BLOCK:FOOTER START ====
# Additional test cases for edge cases and error scenarios

def test_conv2d_invalid_inputs():
    """Test conv2d with invalid inputs."""
    # Valid input for comparison
    valid_input = tf.constant(np.ones([1, 4, 4, 3], dtype=np.float32))
    valid_filter = tf.constant(np.ones([2, 2, 3, 4], dtype=np.float32))
    
    # Test 1: Input and filter dtype mismatch
    input_f32 = tf.constant(np.ones([1, 4, 4, 3], dtype=np.float32))
    filter_f64 = tf.constant(np.ones([2, 2, 3, 4], dtype=np.float64))
    
    # TensorFlow should raise an error for dtype mismatch
    # Note: This might be caught at graph construction or runtime
    
    # Test 2: Invalid padding string
    with pytest.raises(Exception):
        nn_ops.conv2d(
            input=valid_input,
            filter=valid_filter,
            strides=[1, 1, 1, 1],
            padding="INVALID_PADDING"  # Invalid padding
        )
    
    # Test 3: Zero stride (should fail)
    with pytest.raises(Exception):
        nn_ops.conv2d(
            input=valid_input,
            filter=valid_filter,
            strides=[1, 0, 0, 1],  # Zero stride
            padding="VALID"
        )

def test_max_pool_invalid_inputs():
    """Test max_pool with invalid inputs."""
    # Valid input for comparison
    valid_input = tf.constant(np.ones([1, 4, 4, 3], dtype=np.float32))
    
    # Test 1: Invalid padding string
    with pytest.raises(Exception):
        nn_ops.max_pool(
            value=valid_input,
            ksize=[1, 2, 2, 1],
            strides=[1, 2, 2, 1],
            padding="INVALID_PADDING"  # Invalid padding
        )
    
    # Test 2: Zero ksize (should fail)
    with pytest.raises(Exception):
        nn_ops.max_pool(
            value=valid_input,
            ksize=[1, 0, 0, 1],  # Zero ksize
            strides=[1, 2, 2, 1],
            padding="VALID"
        )

def test_relu_basic():
    """Basic ReLU test (simplified version for round 1)."""
    # Test with mixed positive and negative values
    input_tensor = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=tf.float32)
    
    # Call target function
    result = nn_ops.relu(input_tensor)
    
    # Basic assertions
    assert result.shape == input_tensor.shape, "Shape mismatch"
    assert result.dtype == input_tensor.dtype, "Dtype mismatch"
    
    # Check ReLU property: max(features, 0)
    result_np = result.numpy()
    expected = np.maximum([-2.0, -1.0, 0.0, 1.0, 2.0], 0.0)
    
    np.testing.assert_array_almost_equal(result_np, expected, decimal=6)
    
    # Check no negative values
    assert np.all(result_np >= 0), "ReLU output contains negative values"

if __name__ == "__main__":
    # Simple test runner for debugging
    import sys
    sys.exit(pytest.main([__file__, "-v"]))
# ==== BLOCK:FOOTER END ====