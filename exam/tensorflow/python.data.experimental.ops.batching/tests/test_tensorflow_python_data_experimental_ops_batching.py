"""
Test cases for tensorflow.python.data.experimental.ops.batching module.
Generated by TestAgent.
"""
import numpy as np
import pytest
import tensorflow as tf
from tensorflow.python.data.experimental.ops import batching

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# ==== BLOCK:HEADER START ====
"""
Test cases for tensorflow.python.data.experimental.ops.batching module.
Generated by TestAgent.

Note: Tests are organized into group files:
- G1: tests/test_tensorflow_python_data_experimental_ops_batching_g1.py
- G2: tests/test_tensorflow_python_data_experimental_ops_batching_g2.py

This file contains all tests for backward compatibility.
"""
import numpy as np
import pytest
import tensorflow as tf
from tensorflow.python.data.experimental.ops import batching

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Test class for batching operations
class TestBatchingOps:
    """Test cases for batching experimental operations."""
    
    def setup_method(self):
        """Setup method for each test."""
        pass
    
    def teardown_method(self):
        """Teardown method for each test."""
        pass
# ==== BLOCK:HEADER END ====

# ==== BLOCK:CASE_01 START ====
    @pytest.mark.parametrize("batch_size,drop_remainder,row_splits_dtype,input_shape,data_type", [
        (2, False, tf.int64, "varying", tf.float32),
        (1, False, tf.int64, "single_element", tf.float32),  # param extension
        (5, False, tf.int32, "large_variation", tf.int32),   # param extension
    ])
    def test_dense_to_ragged_batch_basic(self, batch_size, drop_remainder, 
                                        row_splits_dtype, input_shape, data_type,
                                        random_tensor_generator):
        """Test basic functionality of dense_to_ragged_batch (CASE_01)."""
        # Generate test data
        tensors = random_tensor_generator(shape_type=input_shape, dtype=data_type)
        dataset = create_dataset_from_tensors(tensors)
        
        # Apply dense_to_ragged_batch transformation
        batch_transform = batching.dense_to_ragged_batch(
            batch_size=batch_size,
            drop_remainder=drop_remainder,
            row_splits_dtype=row_splits_dtype
        )
        batched_dataset = dataset.apply(batch_transform)
        
        # Collect batches
        batches = list(batched_dataset.as_numpy_iterator())
        
        # Weak assertions
        assert len(batches) > 0, "Should produce at least one batch"
        
        # Check each batch
        for batch in batches:
            # Output type assertion
            assert isinstance(batch, (np.ndarray, tf.RaggedTensor)), \
                f"Batch should be ndarray or RaggedTensor, got {type(batch)}"
            
            # Batch size assertion (for full batches)
            if isinstance(batch, tf.RaggedTensor):
                batch_dim = batch.shape[0]
                if batch_dim is not None:
                    # For the last batch with drop_remainder=False, size may be smaller
                    if not drop_remainder and batch is batches[-1]:
                        # Last batch can be smaller
                        assert batch_dim <= batch_size, \
                            f"Last batch size {batch_dim} should be ≤ {batch_size}"
                    else:
                        assert batch_dim == batch_size, \
                            f"Batch size {batch_dim} should equal {batch_size}"
                
                # Row splits dtype assertion
                if hasattr(batch, 'row_splits'):
                    assert batch.row_splits.dtype == row_splits_dtype, \
                        f"Row splits dtype {batch.row_splits.dtype} should be {row_splits_dtype}"
            
            # Element shape consistency
            if isinstance(batch, tf.RaggedTensor) and batch.flat_values is not None:
                # All values should have same dtype as input
                assert batch.flat_values.dtype == data_type, \
                    f"Values dtype {batch.flat_values.dtype} should match input {data_type}"
        
        # Additional validation: total elements should be preserved
        total_input_elements = sum(tf.size(t).numpy() for t in tensors)
        total_batched_elements = 0
        for batch in batches:
            if isinstance(batch, tf.RaggedTensor):
                total_batched_elements += tf.size(batch.flat_values).numpy()
            else:
                total_batched_elements += tf.size(batch).numpy()
        
        assert total_batched_elements == total_input_elements, \
            f"Total elements mismatch: {total_batched_elements} != {total_input_elements}"
# ==== BLOCK:CASE_01 END ====

# ==== BLOCK:CASE_02 START ====
    def test_dense_to_ragged_batch_drop_remainder(self, random_tensor_generator):
        """Test drop_remainder parameter behavior (CASE_02)."""
        # Test parameters from test_plan
        batch_size = 3
        drop_remainder = True
        row_splits_dtype = tf.int32
        input_shape = "fixed"
        data_type = tf.int32
        
        # Generate test data with 7 elements (2 full batches + 1 remainder)
        tensors = random_tensor_generator(shape_type=input_shape, dtype=data_type)
        # Ensure we have exactly 7 elements for testing
        while len(tensors) < 7:
            tensors.append(tensors[0])
        tensors = tensors[:7]
        
        dataset = create_dataset_from_tensors(tensors)
        
        # Apply dense_to_ragged_batch with drop_remainder=True
        batch_transform = batching.dense_to_ragged_batch(
            batch_size=batch_size,
            drop_remainder=drop_remainder,
            row_splits_dtype=row_splits_dtype
        )
        batched_dataset = dataset.apply(batch_transform)
        
        # Collect batches
        batches = list(batched_dataset.as_numpy_iterator())
        
        # Weak assertions
        # Batch count assertion: with 7 elements and batch_size=3, drop_remainder=True
        # should produce 2 batches (drops the 1 remaining element)
        expected_batch_count = len(tensors) // batch_size  # 7 // 3 = 2
        assert len(batches) == expected_batch_count, \
            f"With drop_remainder=True, expected {expected_batch_count} batches, got {len(batches)}"
        
        # Remainder handling assertion: all batches should have exact batch_size
        for i, batch in enumerate(batches):
            if isinstance(batch, tf.RaggedTensor):
                batch_dim = batch.shape[0]
                assert batch_dim == batch_size, \
                    f"Batch {i} size {batch_dim} should equal {batch_size} with drop_remainder=True"
                
                # Row splits dtype assertion
                assert batch.row_splits.dtype == row_splits_dtype, \
                    f"Row splits dtype {batch.row_splits.dtype} should be {row_splits_dtype}"
        
        # Output shape consistency
        if len(batches) > 0:
            first_batch = batches[0]
            for batch in batches[1:]:
                # All batches should have same rank
                if isinstance(first_batch, tf.RaggedTensor) and isinstance(batch, tf.RaggedTensor):
                    assert len(first_batch.shape) == len(batch.shape), \
                        "All batches should have same rank"
        
        # Test with drop_remainder=False for comparison
        batch_transform_no_drop = batching.dense_to_ragged_batch(
            batch_size=batch_size,
            drop_remainder=False,
            row_splits_dtype=row_splits_dtype
        )
        batched_dataset_no_drop = dataset.apply(batch_transform_no_drop)
        batches_no_drop = list(batched_dataset_no_drop.as_numpy_iterator())
        
        # With drop_remainder=False, should have 3 batches (2 full + 1 partial)
        assert len(batches_no_drop) == expected_batch_count + 1, \
            f"With drop_remainder=False, expected {expected_batch_count + 1} batches, got {len(batches_no_drop)}"
        
        # Last batch should be smaller
        if len(batches_no_drop) > 0:
            last_batch = batches_no_drop[-1]
            if isinstance(last_batch, tf.RaggedTensor):
                last_batch_size = last_batch.shape[0]
                expected_last_size = len(tensors) % batch_size  # 7 % 3 = 1
                if last_batch_size is not None:
                    assert last_batch_size == expected_last_size, \
                        f"Last batch size {last_batch_size} should be {expected_last_size}"
# ==== BLOCK:CASE_02 END ====

# ==== BLOCK:CASE_03 START ====
    @pytest.mark.parametrize("batch_size,row_shape,input_shape,data_type", [
        (2, [4], "compatible", tf.float64),
        (3, [2, 2, 2], "multi_dimension", tf.float32),  # param extension
    ])
    def test_dense_to_sparse_batch_basic(self, batch_size, row_shape, 
                                        input_shape, data_type):
        """Test basic functionality of dense_to_sparse_batch (CASE_03)."""
        # Create test data with compatible shapes
        if input_shape == "compatible":
            # Create tensors with shape compatible with row_shape=[4]
            # Each tensor is 1D with length <= 4
            tensors = [
                tf.constant([1.0, 2.0], dtype=data_type),           # length 2
                tf.constant([3.0, 4.0, 5.0], dtype=data_type),      # length 3
                tf.constant([6.0], dtype=data_type),                # length 1
                tf.constant([7.0, 8.0, 9.0, 10.0], dtype=data_type), # length 4
                tf.constant([11.0, 12.0], dtype=data_type),         # length 2
                tf.constant([13.0, 14.0, 15.0], dtype=data_type),   # length 3
            ]
        elif input_shape == "multi_dimension":
            # Create 3D tensors compatible with row_shape=[2, 2, 2]
            # Each tensor is 3D with shape <= [2, 2, 2]
            tensors = [
                tf.constant([[[1.0, 2.0]]], dtype=data_type),  # shape [1, 1, 2]
                tf.constant([[[3.0], [4.0]]], dtype=data_type), # shape [1, 2, 1]
                tf.constant([[[5.0, 6.0], [7.0, 8.0]]], dtype=data_type), # shape [1, 2, 2]
                tf.constant([[[9.0]]], dtype=data_type),       # shape [1, 1, 1]
                tf.constant([[[10.0, 11.0]]], dtype=data_type), # shape [1, 1, 2]
                tf.constant([[[12.0], [13.0]]], dtype=data_type), # shape [1, 2, 1]
            ]
        else:
            # Default case
            tensors = [
                tf.constant([1.0, 2.0], dtype=data_type),
                tf.constant([3.0, 4.0, 5.0], dtype=data_type),
            ]
        
        dataset = create_dataset_from_tensors(tensors)
        
        # Apply dense_to_sparse_batch transformation
        batch_transform = batching.dense_to_sparse_batch(
            batch_size=batch_size,
            row_shape=row_shape
        )
        batched_dataset = dataset.apply(batch_transform)
        
        # Collect batches using tf.data.Dataset iteration
        batches = []
        for batch in batched_dataset:
            # Convert each component to numpy
            indices = batch[0].numpy()
            values = batch[1].numpy()
            dense_shape = batch[2].numpy()
            batches.append((indices, values, dense_shape))
        
        # Weak assertions
        assert len(batches) > 0, "Should produce at least one batch"
        
        # Check each batch
        for batch_idx, batch in enumerate(batches):
            # Output type assertion - should be tuple of (indices, values, dense_shape)
            assert isinstance(batch, tuple), "Batch should be a tuple"
            assert len(batch) == 3, "Batch tuple should have 3 elements"
            
            indices, values, dense_shape = batch
            
            # Convert to SparseTensor for easier validation
            sparse_tensor = tf.SparseTensor(
                indices=indices,
                values=values,
                dense_shape=dense_shape
            )
            
            # Sparse format assertion
            assert isinstance(sparse_tensor, tf.SparseTensor), \
                "Should be convertible to SparseTensor"
            
            # Row shape compliance assertion
            # dense_shape should be [batch_dim, ...row_shape]
            expected_dense_shape = tf.TensorShape([None] + list(row_shape))
            assert len(sparse_tensor.dense_shape) == len(expected_dense_shape), \
                f"Dense shape rank mismatch: {len(sparse_tensor.dense_shape)} != {len(expected_dense_shape)}"
            
            # Check row shape dimensions (excluding batch dimension)
            for i in range(1, len(sparse_tensor.dense_shape)):
                assert sparse_tensor.dense_shape[i] == row_shape[i-1], \
                    f"Row shape dimension {i-1} mismatch: {sparse_tensor.dense_shape[i]} != {row_shape[i-1]}"
            
            # Batch dimension assertion
            batch_dim = sparse_tensor.dense_shape[0]
            if batch_idx < len(batches) - 1:
                # Full batches should have exact batch_size
                assert batch_dim == batch_size, \
                    f"Full batch {batch_idx} size {batch_dim} should equal {batch_size}"
            else:
                # Last batch can be smaller or equal to batch_size
                assert batch_dim <= batch_size, \
                    f"Last batch size {batch_dim} should be ≤ {batch_size}"
            
            # Values dtype should match input
            assert values.dtype == data_type.as_numpy_dtype, \
                f"Values dtype {values.dtype} should match input {data_type.as_numpy_dtype}"
            
            # Indices should be within bounds
            if len(indices) > 0:
                for idx in indices:
                    # Check batch index
                    assert 0 <= idx[0] < batch_dim, \
                        f"Batch index {idx[0]} out of bounds [0, {batch_dim})"
                    # Check other indices
                    for i in range(1, len(idx)):
                        assert 0 <= idx[i] < row_shape[i-1], \
                            f"Index {idx[i]} out of bounds [0, {row_shape[i-1]})"
        
        # Validate that all input values are preserved
        total_input_values = sum(tf.size(t).numpy() for t in tensors)
        total_sparse_values = sum(len(batch[1]) for batch in batches)
        
        assert total_sparse_values == total_input_values, \
            f"Values count mismatch: {total_sparse_values} != {total_input_values}"
# ==== BLOCK:CASE_03 END ====

# ==== BLOCK:CASE_04 START ====
    def test_dense_to_sparse_batch_row_shape_constraints(self):
        """Test row_shape constraints validation (CASE_04)."""
        # Test parameters from test_plan
        batch_size = 2
        row_shape = [3, 3]  # 2D row shape
        input_shape = "smaller_than_row_shape"
        data_type = tf.int64
        
        # Create test data with shapes smaller than row_shape
        # Each tensor is 2D with shape <= [3, 3]
        tensors = [
            tf.constant([[1, 2], [3, 4]], dtype=data_type),           # shape [2, 2]
            tf.constant([[5, 6, 7]], dtype=data_type),                # shape [1, 3]
            tf.constant([[8], [9], [10]], dtype=data_type),           # shape [3, 1]
            tf.constant([[11, 12, 13], [14, 15, 16]], dtype=data_type), # shape [2, 3]
            tf.constant([[17, 18], [19, 20], [21, 22]], dtype=data_type), # shape [3, 2]
            tf.constant([[23, 24, 25], [26, 27, 28], [29, 30, 31]], dtype=data_type), # shape [3, 3]
        ]
        
        dataset = create_dataset_from_tensors(tensors)
        
        # Apply dense_to_sparse_batch transformation
        batch_transform = batching.dense_to_sparse_batch(
            batch_size=batch_size,
            row_shape=row_shape
        )
        batched_dataset = dataset.apply(batch_transform)
        
        # Collect batches using tf.data.Dataset iteration
        batches = []
        for batch in batched_dataset:
            # Convert each component to numpy
            indices = batch[0].numpy()
            values = batch[1].numpy()
            dense_shape = batch[2].numpy()
            batches.append((indices, values, dense_shape))
        
        # Weak assertions
        # Shape compatibility assertion: no errors should be raised
        assert len(batches) > 0, "Should produce batches without errors"
        
        # Check each batch
        for batch_idx, batch in enumerate(batches):
            indices, values, dense_shape = batch
            
            # Output structure assertion
            assert isinstance(batch, tuple), "Batch should be a tuple"
            assert len(batch) == 3, "Batch tuple should have 3 elements"
            assert len(indices.shape) == 2, "Indices should be 2D"
            assert len(values.shape) == 1, "Values should be 1D"
            assert len(dense_shape) == len(row_shape) + 1, \
                f"Dense shape should have rank {len(row_shape) + 1}"
            
            # No errors assertion: indices should be valid
            sparse_tensor = tf.SparseTensor(
                indices=indices,
                values=values,
                dense_shape=dense_shape
            )
            
            # Validate indices are within bounds
            if len(indices) > 0:
                for idx in indices:
                    # Check batch dimension
                    assert 0 <= idx[0] < dense_shape[0], \
                        f"Batch index {idx[0]} out of bounds [0, {dense_shape[0]})"
                    # Check row shape dimensions
                    for i in range(1, len(idx)):
                        assert 0 <= idx[i] < row_shape[i-1], \
                            f"Index {idx[i]} out of bounds [0, {row_shape[i-1]})"
            
            # Values should have correct dtype
            assert values.dtype == data_type.as_numpy_dtype, \
                f"Values dtype {values.dtype} should be {data_type.as_numpy_dtype}"
        
        # Test with incompatible shape (should raise error)
        # Create a tensor with shape larger than row_shape
        incompatible_tensor = tf.constant(
            [[[1, 2, 3, 4], [5, 6, 7, 8]]],  # shape [1, 2, 4] but row_shape is [3, 3]
            dtype=data_type
        )
        
        # This should work because we're creating a 3D tensor but row_shape is 2D
        # The error would be rank mismatch, not size mismatch
        incompatible_dataset = tf.data.Dataset.from_tensor_slices([incompatible_tensor])
        
        # This should raise an error because rank doesn't match
        try:
            incompatible_batched = incompatible_dataset.apply(batch_transform)
            # Try to iterate - should fail
            for _ in incompatible_batched:
                pass
            # If we get here, the test should note this
            print("Note: Rank mismatch error was not raised as expected")
        except Exception as e:
            # Expected to fail due to rank mismatch
            assert "rank" in str(e).lower() or "shape" in str(e).lower(), \
                f"Expected shape/rank error, got: {e}"
        
        # Test with correct rank but larger dimension
        larger_tensor = tf.constant(
            [[1, 2, 3, 4]],  # shape [1, 4] but row_shape[1] = 3
            dtype=data_type
        )
        larger_dataset = tf.data.Dataset.from_tensor_slices([larger_tensor])
        
        try:
            larger_batched = larger_dataset.apply(batch_transform)
            for _ in larger_batched:
                pass
            # If we get here, maybe the error happens during iteration
            print("Note: Dimension size error handling may vary")
        except Exception as e:
            # May fail due to dimension size
            pass
# ==== BLOCK:CASE_04 END ====

# ==== BLOCK:CASE_05 START ====
# Placeholder for CASE_05: DEFERRED SET - dense_to_ragged_batch 边界情况
# ==== BLOCK:CASE_05 END ====

# ==== BLOCK:CASE_06 START ====
# Placeholder for CASE_06: DEFERRED SET - dense_to_sparse_batch 边界情况
# ==== BLOCK:CASE_06 END ====

# ==== BLOCK:FOOTER START ====
# Additional helper functions and fixtures

@pytest.fixture
def random_tensor_generator():
    """Generate random tensors with varying shapes."""
    def _generator(shape_type="varying", dtype=tf.float32):
        if shape_type == "varying":
            # Generate tensors with varying shapes
            shapes = [
                (2,),      # 1D vector
                (3, 4),    # 2D matrix
                (1, 5, 2), # 3D tensor
                (4,),      # Another 1D vector
                (2, 3),    # Another 2D matrix
            ]
        elif shape_type == "fixed":
            # All tensors have same shape
            shapes = [(3, 4)] * 5
        elif shape_type == "single_element":
            # Single element tensors
            shapes = [(1,)] * 5
        elif shape_type == "large_variation":
            # Large variation in shapes
            shapes = [
                (1,), (10,), (2, 5), (3, 3, 3), (1, 2, 3, 4)
            ]
        else:
            shapes = [(2, 3)] * 5
            
        tensors = []
        for shape in shapes:
            if dtype == tf.float32:
                data = np.random.randn(*shape).astype(np.float32)
            elif dtype == tf.int32:
                data = np.random.randint(0, 10, shape, dtype=np.int32)
            elif dtype == tf.int64:
                data = np.random.randint(0, 10, shape, dtype=np.int64)
            elif dtype == tf.float64:
                data = np.random.randn(*shape).astype(np.float64)
            else:
                data = np.random.randn(*shape).astype(np.float32)
            tensors.append(tf.constant(data, dtype=dtype))
        return tensors
    return _generator

def create_dataset_from_tensors(tensors):
    """Create a tf.data.Dataset from a list of tensors."""
    return tf.data.Dataset.from_tensor_slices(tensors)

def assert_ragged_tensor_properties(rt, expected_batch_size=None, 
                                   expected_row_splits_dtype=tf.int64):
    """Assert basic properties of a RaggedTensor."""
    assert isinstance(rt, tf.RaggedTensor), "Output should be a RaggedTensor"
    if expected_batch_size is not None:
        assert rt.shape[0] == expected_batch_size, f"Batch size mismatch: {rt.shape[0]} != {expected_batch_size}"
    assert rt.row_splits.dtype == expected_row_splits_dtype, f"Row splits dtype mismatch: {rt.row_splits.dtype} != {expected_row_splits_dtype}"
    return True

def assert_sparse_tensor_properties(st, expected_batch_size=None, 
                                   expected_row_shape=None):
    """Assert basic properties of a SparseTensor."""
    assert isinstance(st, tf.SparseTensor), "Output should be a SparseTensor"
    if expected_batch_size is not None:
        assert st.dense_shape[0] == expected_batch_size, f"Batch size mismatch: {st.dense_shape[0]} != {expected_batch_size}"
    if expected_row_shape is not None:
        # Check that row shape matches (excluding batch dimension)
        assert st.dense_shape[1:] == expected_row_shape, f"Row shape mismatch: {st.dense_shape[1:]} != {expected_row_shape}"
    return True
# ==== BLOCK:FOOTER END ====