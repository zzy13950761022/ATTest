"""
Test cases for tensorflow.python.data.experimental.ops.batching module - Group G2.
Generated by TestAgent.
"""
import numpy as np
import pytest
import tensorflow as tf
from tensorflow.python.data.experimental.ops import batching

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# ==== BLOCK:HEADER START ====
# Test class for dense_to_sparse_batch operations
class TestDenseToSparseBatch:
    """Test cases for dense_to_sparse_batch operation."""
    
    def setup_method(self):
        """Setup method for each test."""
        pass
    
    def teardown_method(self):
        """Teardown method for each test."""
        pass
# ==== BLOCK:HEADER END ====

# ==== BLOCK:CASE_03 START ====
    @pytest.mark.parametrize("batch_size,row_shape,input_shape,data_type", [
        (2, [4], "compatible", tf.float64),
        (3, [2, 2, 2], "multi_dimension", tf.float32),  # param extension
    ])
    def test_dense_to_sparse_batch_basic(self, batch_size, row_shape, 
                                        input_shape, data_type):
        """Test basic functionality of dense_to_sparse_batch (CASE_03)."""
        # Create test data with compatible shapes
        if input_shape == "compatible":
            # Create tensors with shape compatible with row_shape=[4]
            # Each tensor is 1D with length <= 4
            tensors = [
                tf.constant([1.0, 2.0], dtype=data_type),           # length 2
                tf.constant([3.0, 4.0, 5.0], dtype=data_type),      # length 3
                tf.constant([6.0], dtype=data_type),                # length 1
                tf.constant([7.0, 8.0, 9.0, 10.0], dtype=data_type), # length 4
                tf.constant([11.0, 12.0], dtype=data_type),         # length 2
                tf.constant([13.0, 14.0, 15.0], dtype=data_type),   # length 3
            ]
        elif input_shape == "multi_dimension":
            # Create 3D tensors compatible with row_shape=[2, 2, 2]
            # Each tensor is 3D with shape <= [2, 2, 2]
            tensors = [
                tf.constant([[[1.0, 2.0]]], dtype=data_type),  # shape [1, 1, 2]
                tf.constant([[[3.0], [4.0]]], dtype=data_type), # shape [1, 2, 1]
                tf.constant([[[5.0, 6.0], [7.0, 8.0]]], dtype=data_type), # shape [1, 2, 2]
                tf.constant([[[9.0]]], dtype=data_type),       # shape [1, 1, 1]
                tf.constant([[[10.0, 11.0]]], dtype=data_type), # shape [1, 1, 2]
                tf.constant([[[12.0], [13.0]]], dtype=data_type), # shape [1, 2, 1]
            ]
        else:
            # Default case
            tensors = [
                tf.constant([1.0, 2.0], dtype=data_type),
                tf.constant([3.0, 4.0, 5.0], dtype=data_type),
            ]
        
        # Create dataset using from_generator to handle varying shapes
        def tensor_generator():
            for tensor in tensors:
                yield tensor
        
        # Get output signature from first tensor
        output_signature = tf.TensorSpec(shape=None, dtype=data_type)
        
        dataset = tf.data.Dataset.from_generator(
            tensor_generator,
            output_signature=output_signature
        )
        
        # Apply dense_to_sparse_batch transformation
        batch_transform = batching.dense_to_sparse_batch(
            batch_size=batch_size,
            row_shape=row_shape
        )
        batched_dataset = dataset.apply(batch_transform)
        
        # Collect batches
        batches = list(batched_dataset.as_numpy_iterator())
        
        # Weak assertions
        assert len(batches) > 0, "Should produce at least one batch"
        
        # Check each batch
        for batch_idx, batch in enumerate(batches):
            # Output type assertion - should be tuple of (indices, values, dense_shape)
            assert isinstance(batch, tuple), "Batch should be a tuple"
            assert len(batch) == 3, "Batch tuple should have 3 elements"
            
            indices, values, dense_shape = batch
            
            # Convert to SparseTensor for easier validation
            sparse_tensor = tf.SparseTensor(
                indices=indices,
                values=values,
                dense_shape=dense_shape
            )
            
            # Sparse format assertion
            assert isinstance(sparse_tensor, tf.SparseTensor), \
                "Should be convertible to SparseTensor"
            
            # Row shape compliance assertion
            # dense_shape should be [batch_dim, ...row_shape]
            expected_dense_shape = tf.TensorShape([None] + list(row_shape))
            assert len(sparse_tensor.dense_shape) == len(expected_dense_shape), \
                f"Dense shape rank mismatch: {len(sparse_tensor.dense_shape)} != {len(expected_dense_shape)}"
            
            # Check row shape dimensions (excluding batch dimension)
            for i in range(1, len(sparse_tensor.dense_shape)):
                assert sparse_tensor.dense_shape[i] == row_shape[i-1], \
                    f"Row shape dimension {i-1} mismatch: {sparse_tensor.dense_shape[i]} != {row_shape[i-1]}"
            
            # Batch dimension assertion
            batch_dim = sparse_tensor.dense_shape[0]
            if batch_idx < len(batches) - 1:
                # Full batches should have exact batch_size
                assert batch_dim == batch_size, \
                    f"Full batch {batch_idx} size {batch_dim} should equal {batch_size}"
            else:
                # Last batch can be smaller or equal to batch_size
                assert batch_dim <= batch_size, \
                    f"Last batch size {batch_dim} should be ≤ {batch_size}"
            
            # Values dtype should match input
            assert values.dtype == data_type, \
                f"Values dtype {values.dtype} should match input {data_type}"
            
            # Indices should be within bounds
            if len(indices) > 0:
                for idx in indices:
                    # Check batch index
                    assert 0 <= idx[0] < batch_dim, \
                        f"Batch index {idx[0]} out of bounds [0, {batch_dim})"
                    # Check other indices
                    for i in range(1, len(idx)):
                        assert 0 <= idx[i] < row_shape[i-1], \
                            f"Index {idx[i]} out of bounds [0, {row_shape[i-1]})"
        
        # Validate that all input values are preserved
        total_input_values = sum(tf.size(t).numpy() for t in tensors)
        total_sparse_values = sum(len(batch[1]) for batch in batches)
        
        assert total_sparse_values == total_input_values, \
            f"Values count mismatch: {total_sparse_values} != {total_input_values}"
# ==== BLOCK:CASE_03 END ====

# ==== BLOCK:CASE_04 START ====
    def test_dense_to_sparse_batch_row_shape_constraints(self):
        """Test row_shape constraints validation (CASE_04)."""
        # Test parameters from test_plan
        batch_size = 2
        row_shape = [3, 3]  # 2D row shape
        input_shape = "smaller_than_row_shape"
        data_type = tf.int64
        
        # Create test data with shapes smaller than row_shape
        # Each tensor is 2D with shape <= [3, 3]
        tensors = [
            tf.constant([[1, 2], [3, 4]], dtype=data_type),           # shape [2, 2]
            tf.constant([[5, 6, 7]], dtype=data_type),                # shape [1, 3]
            tf.constant([[8], [9], [10]], dtype=data_type),           # shape [3, 1]
            tf.constant([[11, 12, 13], [14, 15, 16]], dtype=data_type), # shape [2, 3]
            tf.constant([[17, 18], [19, 20], [21, 22]], dtype=data_type), # shape [3, 2]
            tf.constant([[23, 24, 25], [26, 27, 28], [29, 30, 31]], dtype=data_type), # shape [3, 3]
        ]
        
        dataset = tf.data.Dataset.from_tensor_slices(tensors)
        
        # Apply dense_to_sparse_batch transformation
        batch_transform = batching.dense_to_sparse_batch(
            batch_size=batch_size,
            row_shape=row_shape
        )
        batched_dataset = dataset.apply(batch_transform)
        
        # Collect batches - should not raise errors
        batches = list(batched_dataset.as_numpy_iterator())
        
        # Weak assertions
        # Shape compatibility assertion: no errors should be raised
        assert len(batches) > 0, "Should produce batches without errors"
        
        # Check each batch
        for batch_idx, batch in enumerate(batches):
            indices, values, dense_shape = batch
            
            # Output structure assertion
            assert isinstance(batch, tuple), "Batch should be a tuple"
            assert len(batch) == 3, "Batch tuple should have 3 elements"
            assert len(indices.shape) == 2, "Indices should be 2D"
            assert len(values.shape) == 1, "Values should be 1D"
            assert len(dense_shape) == len(row_shape) + 1, \
                f"Dense shape should have rank {len(row_shape) + 1}"
            
            # No errors assertion: indices should be valid
            sparse_tensor = tf.SparseTensor(
                indices=indices,
                values=values,
                dense_shape=dense_shape
            )
            
            # Validate indices are within bounds
            if len(indices) > 0:
                for idx in indices:
                    # Check batch dimension
                    assert 0 <= idx[0] < dense_shape[0], \
                        f"Batch index {idx[0]} out of bounds [0, {dense_shape[0]})"
                    # Check row shape dimensions
                    for i in range(1, len(idx)):
                        assert 0 <= idx[i] < row_shape[i-1], \
                            f"Index {idx[i]} out of bounds [0, {row_shape[i-1]})"
            
            # Values should have correct dtype
            assert values.dtype == data_type, \
                f"Values dtype {values.dtype} should be {data_type}"
        
        # Test with incompatible shape (should raise error)
        # Create a tensor with shape larger than row_shape
        incompatible_tensor = tf.constant(
            [[[1, 2, 3, 4], [5, 6, 7, 8]]],  # shape [1, 2, 4] but row_shape is [3, 3]
            dtype=data_type
        )
        
        # This should work because we're creating a 3D tensor but row_shape is 2D
        # The error would be rank mismatch, not size mismatch
        incompatible_dataset = tf.data.Dataset.from_tensor_slices([incompatible_tensor])
        
        # This should raise an error because rank doesn't match
        try:
            incompatible_batched = incompatible_dataset.apply(batch_transform)
            # Try to iterate - should fail
            list(incompatible_batched.as_numpy_iterator())
            # If we get here, the test should note this
            print("Note: Rank mismatch error was not raised as expected")
        except Exception as e:
            # Expected to fail due to rank mismatch
            assert "rank" in str(e).lower() or "shape" in str(e).lower(), \
                f"Expected shape/rank error, got: {e}"
        
        # Test with correct rank but larger dimension
        larger_tensor = tf.constant(
            [[1, 2, 3, 4]],  # shape [1, 4] but row_shape[1] = 3
            dtype=data_type
        )
        larger_dataset = tf.data.Dataset.from_tensor_slices([larger_tensor])
        
        try:
            larger_batched = larger_dataset.apply(batch_transform)
            list(larger_batched.as_numpy_iterator())
            # If we get here, maybe the error happens during iteration
            print("Note: Dimension size error handling may vary")
        except Exception as e:
            # May fail due to dimension size
            pass
# ==== BLOCK:CASE_04 END ====

# ==== BLOCK:CASE_06 START ====
# Placeholder for CASE_06: DEFERRED SET - dense_to_sparse_batch 边界情况
# ==== BLOCK:CASE_06 END ====

# ==== BLOCK:FOOTER START ====
# Additional helper functions and fixtures

@pytest.fixture
def random_tensor_generator():
    """Generate random tensors with varying shapes."""
    def _generator(shape_type="varying", dtype=tf.float32):
        if shape_type == "varying":
            # Generate tensors with varying shapes
            shapes = [
                (2,),      # 1D vector
                (3, 4),    # 2D matrix
                (1, 5, 2), # 3D tensor
                (4,),      # Another 1D vector
                (2, 3),    # Another 2D matrix
            ]
        elif shape_type == "fixed":
            # All tensors have same shape
            shapes = [(3, 4)] * 5
        elif shape_type == "single_element":
            # Single element tensors
            shapes = [(1,)] * 5
        elif shape_type == "large_variation":
            # Large variation in shapes
            shapes = [
                (1,), (10,), (2, 5), (3, 3, 3), (1, 2, 3, 4)
            ]
        else:
            shapes = [(2, 3)] * 5
            
        tensors = []
        for shape in shapes:
            if dtype == tf.float32:
                data = np.random.randn(*shape).astype(np.float32)
            elif dtype == tf.int32:
                data = np.random.randint(0, 10, shape, dtype=np.int32)
            elif dtype == tf.int64:
                data = np.random.randint(0, 10, shape, dtype=np.int64)
            elif dtype == tf.float64:
                data = np.random.randn(*shape).astype(np.float64)
            else:
                data = np.random.randn(*shape).astype(np.float32)
            tensors.append(tf.constant(data, dtype=dtype))
        return tensors
    return _generator

def assert_sparse_tensor_properties(st, expected_batch_size=None, 
                                   expected_row_shape=None):
    """Assert basic properties of a SparseTensor."""
    assert isinstance(st, tf.SparseTensor), "Output should be a SparseTensor"
    if expected_batch_size is not None:
        assert st.dense_shape[0] == expected_batch_size, f"Batch size mismatch: {st.dense_shape[0]} != {expected_batch_size}"
    if expected_row_shape is not None:
        # Check that row shape matches (excluding batch dimension)
        assert st.dense_shape[1:] == expected_row_shape, f"Row shape mismatch: {st.dense_shape[1:]} != {expected_row_shape}"
    return True
# ==== BLOCK:FOOTER END ====