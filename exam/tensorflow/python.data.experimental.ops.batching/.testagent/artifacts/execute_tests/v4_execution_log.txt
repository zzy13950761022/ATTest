=== Run Tests ===
FFF                                                                      [100%]
================================== FAILURES ===================================
_ TestDenseToSparseBatch.test_dense_to_sparse_batch_basic[2-row_shape0-compatible-data_type0] _

self = <test_tensorflow_python_data_experimental_ops_batching_g2.TestDenseToSparseBatch object at 0x000001F0E0D8EE80>
batch_size = 2, row_shape = [4], input_shape = 'compatible'
data_type = tf.float64

    @pytest.mark.parametrize("batch_size,row_shape,input_shape,data_type", [
        (2, [4], "compatible", tf.float64),
        (3, [2, 2, 2], "multi_dimension", tf.float32),  # param extension
    ])
    def test_dense_to_sparse_batch_basic(self, batch_size, row_shape,
                                        input_shape, data_type):
        """Test basic functionality of dense_to_sparse_batch (CASE_03)."""
        # Create test data with compatible shapes
        if input_shape == "compatible":
            # Create tensors with shape compatible with row_shape=[4]
            # Each tensor is 1D with length <= 4
            tensors = [
                tf.constant([1.0, 2.0], dtype=data_type),           # length 2
                tf.constant([3.0, 4.0, 5.0], dtype=data_type),      # length 3
                tf.constant([6.0], dtype=data_type),                # length 1
                tf.constant([7.0, 8.0, 9.0, 10.0], dtype=data_type), # length 4
                tf.constant([11.0, 12.0], dtype=data_type),         # length 2
                tf.constant([13.0, 14.0, 15.0], dtype=data_type),   # length 3
            ]
        elif input_shape == "multi_dimension":
            # Create 3D tensors compatible with row_shape=[2, 2, 2]
            # Each tensor is 3D with shape <= [2, 2, 2]
            tensors = [
                tf.constant([[[1.0, 2.0]]], dtype=data_type),  # shape [1, 1, 2]
                tf.constant([[[3.0], [4.0]]], dtype=data_type), # shape [1, 2, 1]
                tf.constant([[[5.0, 6.0], [7.0, 8.0]]], dtype=data_type), # shape [1, 2, 2]
                tf.constant([[[9.0]]], dtype=data_type),       # shape [1, 1, 1]
                tf.constant([[[10.0, 11.0]]], dtype=data_type), # shape [1, 1, 2]
                tf.constant([[[12.0], [13.0]]], dtype=data_type), # shape [1, 2, 1]
            ]
        else:
            # Default case
            tensors = [
                tf.constant([1.0, 2.0], dtype=data_type),
                tf.constant([3.0, 4.0, 5.0], dtype=data_type),
            ]
    
        # Create dataset using from_generator to handle varying shapes
        def tensor_generator():
            for tensor in tensors:
                yield tensor
    
        # Get output signature from first tensor
        output_signature = tf.TensorSpec(shape=None, dtype=data_type)
    
        dataset = tf.data.Dataset.from_generator(
            tensor_generator,
            output_signature=output_signature
        )
    
        # Apply dense_to_sparse_batch transformation
        batch_transform = batching.dense_to_sparse_batch(
            batch_size=batch_size,
            row_shape=row_shape
        )
        batched_dataset = dataset.apply(batch_transform)
    
        # Collect batches
>       batches = list(batched_dataset.as_numpy_iterator())

tests\test_tensorflow_python_data_experimental_ops_batching_g2.py:87: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <_DenseToSparseBatchDataset element_spec=SparseTensorSpec(TensorShape([None, 4]), tf.float64)>

    def as_numpy_iterator(self):
      """Returns an iterator which converts all elements of the dataset to numpy.
    
      Use `as_numpy_iterator` to inspect the content of your dataset. To see
      element shapes and types, print dataset elements directly instead of using
      `as_numpy_iterator`.
    
      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
      >>> for element in dataset:
      ...   print(element)
      tf.Tensor(1, shape=(), dtype=int32)
      tf.Tensor(2, shape=(), dtype=int32)
      tf.Tensor(3, shape=(), dtype=int32)
    
      This method requires that you are running in eager mode and the dataset's
      element_spec contains only `TensorSpec` components.
    
      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
      >>> for element in dataset.as_numpy_iterator():
      ...   print(element)
      1
      2
      3
    
      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
      >>> print(list(dataset.as_numpy_iterator()))
      [1, 2, 3]
    
      `as_numpy_iterator()` will preserve the nested structure of dataset
      elements.
    
      >>> dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),
      ...                                               'b': [5, 6]})
      >>> list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},
      ...                                       {'a': (2, 4), 'b': 6}]
      True
    
      Returns:
        An iterable over the elements of the dataset, with their tensors converted
        to numpy arrays.
    
      Raises:
        TypeError: if an element contains a non-`Tensor` value.
        RuntimeError: if eager execution is not enabled.
      """
      if not context.executing_eagerly():
        raise RuntimeError("`tf.data.Dataset.as_numpy_iterator()` is only "
                           "supported in eager mode.")
      for component_spec in nest.flatten(self.element_spec):
        if not isinstance(
            component_spec,
            (tensor_spec.TensorSpec, ragged_tensor.RaggedTensorSpec)):
>         raise TypeError(
              f"`tf.data.Dataset.as_numpy_iterator()` is not supported for "
              f"datasets that produce values of type {component_spec.value_type}")
E         TypeError: `tf.data.Dataset.as_numpy_iterator()` is not supported for datasets that produce values of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>

D:\Coding\Anaconda\envs\testagent-experiment\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py:608: TypeError
---------------------------- Captured stderr call -----------------------------
2026-01-18 16:09:55.888042: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
_ TestDenseToSparseBatch.test_dense_to_sparse_batch_basic[3-row_shape1-multi_dimension-data_type1] _

self = <test_tensorflow_python_data_experimental_ops_batching_g2.TestDenseToSparseBatch object at 0x000001F0E0D8EEE0>
batch_size = 3, row_shape = [2, 2, 2], input_shape = 'multi_dimension'
data_type = tf.float32

    @pytest.mark.parametrize("batch_size,row_shape,input_shape,data_type", [
        (2, [4], "compatible", tf.float64),
        (3, [2, 2, 2], "multi_dimension", tf.float32),  # param extension
    ])
    def test_dense_to_sparse_batch_basic(self, batch_size, row_shape,
                                        input_shape, data_type):
        """Test basic functionality of dense_to_sparse_batch (CASE_03)."""
        # Create test data with compatible shapes
        if input_shape == "compatible":
            # Create tensors with shape compatible with row_shape=[4]
            # Each tensor is 1D with length <= 4
            tensors = [
                tf.constant([1.0, 2.0], dtype=data_type),           # length 2
                tf.constant([3.0, 4.0, 5.0], dtype=data_type),      # length 3
                tf.constant([6.0], dtype=data_type),                # length 1
                tf.constant([7.0, 8.0, 9.0, 10.0], dtype=data_type), # length 4
                tf.constant([11.0, 12.0], dtype=data_type),         # length 2
                tf.constant([13.0, 14.0, 15.0], dtype=data_type),   # length 3
            ]
        elif input_shape == "multi_dimension":
            # Create 3D tensors compatible with row_shape=[2, 2, 2]
            # Each tensor is 3D with shape <= [2, 2, 2]
            tensors = [
                tf.constant([[[1.0, 2.0]]], dtype=data_type),  # shape [1, 1, 2]
                tf.constant([[[3.0], [4.0]]], dtype=data_type), # shape [1, 2, 1]
                tf.constant([[[5.0, 6.0], [7.0, 8.0]]], dtype=data_type), # shape [1, 2, 2]
                tf.constant([[[9.0]]], dtype=data_type),       # shape [1, 1, 1]
                tf.constant([[[10.0, 11.0]]], dtype=data_type), # shape [1, 1, 2]
                tf.constant([[[12.0], [13.0]]], dtype=data_type), # shape [1, 2, 1]
            ]
        else:
            # Default case
            tensors = [
                tf.constant([1.0, 2.0], dtype=data_type),
                tf.constant([3.0, 4.0, 5.0], dtype=data_type),
            ]
    
        # Create dataset using from_generator to handle varying shapes
        def tensor_generator():
            for tensor in tensors:
                yield tensor
    
        # Get output signature from first tensor
        output_signature = tf.TensorSpec(shape=None, dtype=data_type)
    
        dataset = tf.data.Dataset.from_generator(
            tensor_generator,
            output_signature=output_signature
        )
    
        # Apply dense_to_sparse_batch transformation
        batch_transform = batching.dense_to_sparse_batch(
            batch_size=batch_size,
            row_shape=row_shape
        )
        batched_dataset = dataset.apply(batch_transform)
    
        # Collect batches
>       batches = list(batched_dataset.as_numpy_iterator())

tests\test_tensorflow_python_data_experimental_ops_batching_g2.py:87: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <_DenseToSparseBatchDataset element_spec=SparseTensorSpec(TensorShape([None, 2, 2, 2]), tf.float32)>

    def as_numpy_iterator(self):
      """Returns an iterator which converts all elements of the dataset to numpy.
    
      Use `as_numpy_iterator` to inspect the content of your dataset. To see
      element shapes and types, print dataset elements directly instead of using
      `as_numpy_iterator`.
    
      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
      >>> for element in dataset:
      ...   print(element)
      tf.Tensor(1, shape=(), dtype=int32)
      tf.Tensor(2, shape=(), dtype=int32)
      tf.Tensor(3, shape=(), dtype=int32)
    
      This method requires that you are running in eager mode and the dataset's
      element_spec contains only `TensorSpec` components.
    
      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
      >>> for element in dataset.as_numpy_iterator():
      ...   print(element)
      1
      2
      3
    
      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
      >>> print(list(dataset.as_numpy_iterator()))
      [1, 2, 3]
    
      `as_numpy_iterator()` will preserve the nested structure of dataset
      elements.
    
      >>> dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),
      ...                                               'b': [5, 6]})
      >>> list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},
      ...                                       {'a': (2, 4), 'b': 6}]
      True
    
      Returns:
        An iterable over the elements of the dataset, with their tensors converted
        to numpy arrays.
    
      Raises:
        TypeError: if an element contains a non-`Tensor` value.
        RuntimeError: if eager execution is not enabled.
      """
      if not context.executing_eagerly():
        raise RuntimeError("`tf.data.Dataset.as_numpy_iterator()` is only "
                           "supported in eager mode.")
      for component_spec in nest.flatten(self.element_spec):
        if not isinstance(
            component_spec,
            (tensor_spec.TensorSpec, ragged_tensor.RaggedTensorSpec)):
>         raise TypeError(
              f"`tf.data.Dataset.as_numpy_iterator()` is not supported for "
              f"datasets that produce values of type {component_spec.value_type}")
E         TypeError: `tf.data.Dataset.as_numpy_iterator()` is not supported for datasets that produce values of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>

D:\Coding\Anaconda\envs\testagent-experiment\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py:608: TypeError
___ TestDenseToSparseBatch.test_dense_to_sparse_batch_row_shape_constraints ___

self = <test_tensorflow_python_data_experimental_ops_batching_g2.TestDenseToSparseBatch object at 0x000001F0E0DAE160>

    def test_dense_to_sparse_batch_row_shape_constraints(self):
        """Test row_shape constraints validation (CASE_04)."""
        # Test parameters from test_plan
        batch_size = 2
        row_shape = [3, 3]  # 2D row shape
        input_shape = "smaller_than_row_shape"
        data_type = tf.int64
    
        # Create test data with shapes smaller than row_shape
        # Each tensor is 2D with shape <= [3, 3]
        tensors = [
            tf.constant([[1, 2], [3, 4]], dtype=data_type),           # shape [2, 2]
            tf.constant([[5, 6, 7]], dtype=data_type),                # shape [1, 3]
            tf.constant([[8], [9], [10]], dtype=data_type),           # shape [3, 1]
            tf.constant([[11, 12, 13], [14, 15, 16]], dtype=data_type), # shape [2, 3]
            tf.constant([[17, 18], [19, 20], [21, 22]], dtype=data_type), # shape [3, 2]
            tf.constant([[23, 24, 25], [26, 27, 28], [29, 30, 31]], dtype=data_type), # shape [3, 3]
        ]
    
        # Create dataset using from_generator to handle varying shapes
        def tensor_generator():
            for tensor in tensors:
                yield tensor
    
        # Get output signature from first tensor
        output_signature = tf.TensorSpec(shape=None, dtype=data_type)
    
        dataset = tf.data.Dataset.from_generator(
            tensor_generator,
            output_signature=output_signature
        )
    
        # Apply dense_to_sparse_batch transformation
        batch_transform = batching.dense_to_sparse_batch(
            batch_size=batch_size,
            row_shape=row_shape
        )
        batched_dataset = dataset.apply(batch_transform)
    
        # Collect batches - should not raise errors
>       batches = list(batched_dataset.as_numpy_iterator())

tests\test_tensorflow_python_data_experimental_ops_batching_g2.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <_DenseToSparseBatchDataset element_spec=SparseTensorSpec(TensorShape([None, 3, 3]), tf.int64)>

    def as_numpy_iterator(self):
      """Returns an iterator which converts all elements of the dataset to numpy.
    
      Use `as_numpy_iterator` to inspect the content of your dataset. To see
      element shapes and types, print dataset elements directly instead of using
      `as_numpy_iterator`.
    
      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
      >>> for element in dataset:
      ...   print(element)
      tf.Tensor(1, shape=(), dtype=int32)
      tf.Tensor(2, shape=(), dtype=int32)
      tf.Tensor(3, shape=(), dtype=int32)
    
      This method requires that you are running in eager mode and the dataset's
      element_spec contains only `TensorSpec` components.
    
      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
      >>> for element in dataset.as_numpy_iterator():
      ...   print(element)
      1
      2
      3
    
      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
      >>> print(list(dataset.as_numpy_iterator()))
      [1, 2, 3]
    
      `as_numpy_iterator()` will preserve the nested structure of dataset
      elements.
    
      >>> dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),
      ...                                               'b': [5, 6]})
      >>> list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},
      ...                                       {'a': (2, 4), 'b': 6}]
      True
    
      Returns:
        An iterable over the elements of the dataset, with their tensors converted
        to numpy arrays.
    
      Raises:
        TypeError: if an element contains a non-`Tensor` value.
        RuntimeError: if eager execution is not enabled.
      """
      if not context.executing_eagerly():
        raise RuntimeError("`tf.data.Dataset.as_numpy_iterator()` is only "
                           "supported in eager mode.")
      for component_spec in nest.flatten(self.element_spec):
        if not isinstance(
            component_spec,
            (tensor_spec.TensorSpec, ragged_tensor.RaggedTensorSpec)):
>         raise TypeError(
              f"`tf.data.Dataset.as_numpy_iterator()` is not supported for "
              f"datasets that produce values of type {component_spec.value_type}")
E         TypeError: `tf.data.Dataset.as_numpy_iterator()` is not supported for datasets that produce values of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>

D:\Coding\Anaconda\envs\testagent-experiment\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py:608: TypeError

---------- coverage: platform win32, python 3.9.25-final-0 -----------
Name                                                                Stmts   Miss Branch BrPart  Cover   Missing
---------------------------------------------------------------------------------------------------------------
test_import.py                                                         27     27      8      0     0%   4-43
tests\test_tensorflow_python_data_experimental_ops_batching_g2.py     132     94     50      1    23%   61, 68-69, 90-152, 178-179, 201-287, 300-338, 343-349
---------------------------------------------------------------------------------------------------------------
TOTAL                                                                 159    121     58      1    19%
Coverage XML written to file coverage.xml

=========================== short test summary info ===========================
FAILED tests\test_tensorflow_python_data_experimental_ops_batching_g2.py::TestDenseToSparseBatch::test_dense_to_sparse_batch_basic[2-row_shape0-compatible-data_type0]
FAILED tests\test_tensorflow_python_data_experimental_ops_batching_g2.py::TestDenseToSparseBatch::test_dense_to_sparse_batch_basic[3-row_shape1-multi_dimension-data_type1]
FAILED tests\test_tensorflow_python_data_experimental_ops_batching_g2.py::TestDenseToSparseBatch::test_dense_to_sparse_batch_row_shape_constraints
3 failed in 2.33s

Error: exit 1