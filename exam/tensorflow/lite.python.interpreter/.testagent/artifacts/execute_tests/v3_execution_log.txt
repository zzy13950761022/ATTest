=== Run Tests ===
FFF....                                                                  [100%]
================================== FAILURES ===================================
___ test_model_loading_and_tensor_allocation[model_path-simple_add-1-False] ___

model_source = 'model_path', model_type = 'simple_add', num_threads = 1
preserve_tensors = False
simple_add_model_path = 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmppes5_u3v.tflite'
simple_add_model_content = b'TFL3\x00\x00\x00\x00'
mock_interpreter = <Mock name='Interpreter()' id='1464037406800'>

    @pytest.mark.parametrize("model_source,model_type,num_threads,preserve_tensors", [
        ("model_path", "simple_add", 1, False),
    ])
    def test_model_loading_and_tensor_allocation(
        model_source, model_type, num_threads, preserve_tensors,
        simple_add_model_path, simple_add_model_content, mock_interpreter
    ):
        """
        Test model loading and tensor allocation functionality.
    
        Weak assertions:
        - interpreter_created: Interpreter instance is created successfully
        - tensors_allocated: allocate_tensors() can be called without error
        - input_details_valid: get_input_details() returns valid structure
        - output_details_valid: get_output_details() returns valid structure
        """
        # Skip if tensorflow not available and we're not using mock
        if not TFLITE_AVAILABLE and model_source != "mock":
            pytest.skip("TensorFlow Lite not available")
    
        # Prepare model source based on parameter
        if model_source == "model_path":
            model_path = simple_add_model_path
            model_content = None
        elif model_source == "model_content":
            model_path = None
            model_content = simple_add_model_content
        else:
            model_path = None
            model_content = None
    
        # Create interpreter
>       interpreter = Interpreter(
            model_path=model_path,
            model_content=model_content,
            num_threads=num_threads,
            experimental_preserve_all_tensors=preserve_tensors
        )

tests\test_tensorflow_lite_python_interpreter_g1.py:175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tensorflow.lite.python.interpreter.Interpreter object at 0x00000154DF6DFC70>
model_path = 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmppes5_u3v.tflite'
model_content = None, experimental_delegates = None, num_threads = 1
experimental_op_resolver_type = <OpResolverType.AUTO: 0>
experimental_preserve_all_tensors = False

    def __init__(self,
                 model_path=None,
                 model_content=None,
                 experimental_delegates=None,
                 num_threads=None,
                 experimental_op_resolver_type=OpResolverType.AUTO,
                 experimental_preserve_all_tensors=False):
      """Constructor.
    
      Args:
        model_path: Path to TF-Lite Flatbuffer file.
        model_content: Content of model.
        experimental_delegates: Experimental. Subject to change. List of
          [TfLiteDelegate](https://www.tensorflow.org/lite/performance/delegates)
            objects returned by lite.load_delegate().
        num_threads: Sets the number of threads used by the interpreter and
          available to CPU kernels. If not set, the interpreter will use an
          implementation-dependent default number of threads. Currently, only a
          subset of kernels, such as conv, support multi-threading. num_threads
          should be >= -1. Setting num_threads to 0 has the effect to disable
          multithreading, which is equivalent to setting num_threads to 1. If set
          to the value -1, the number of threads used will be
          implementation-defined and platform-dependent.
        experimental_op_resolver_type: The op resolver used by the interpreter. It
          must be an instance of OpResolverType. By default, we use the built-in
          op resolver which corresponds to tflite::ops::builtin::BuiltinOpResolver
          in C++.
        experimental_preserve_all_tensors: If true, then intermediate tensors used
          during computation are preserved for inspection, and if the passed op
          resolver type is AUTO or BUILTIN, the type will be changed to
          BUILTIN_WITHOUT_DEFAULT_DELEGATES so that no Tensorflow Lite default
          delegates are applied. If false, getting intermediate tensors could
          result in undefined values or None, especially when the graph is
          successfully modified by the Tensorflow Lite default delegate.
    
      Raises:
        ValueError: If the interpreter was unable to create.
      """
      if not hasattr(self, '_custom_op_registerers'):
        self._custom_op_registerers = []
    
      actual_resolver_type = experimental_op_resolver_type
      if experimental_preserve_all_tensors and (
          experimental_op_resolver_type == OpResolverType.AUTO or
          experimental_op_resolver_type == OpResolverType.BUILTIN):
        actual_resolver_type = OpResolverType.BUILTIN_WITHOUT_DEFAULT_DELEGATES
      op_resolver_id = _get_op_resolver_id(actual_resolver_type)
      if op_resolver_id is None:
        raise ValueError('Unrecognized passed in op resolver type: {}'.format(
            experimental_op_resolver_type))
    
      if model_path and not model_content:
        custom_op_registerers_by_name = [
            x for x in self._custom_op_registerers if isinstance(x, str)
        ]
        custom_op_registerers_by_func = [
            x for x in self._custom_op_registerers if not isinstance(x, str)
        ]
        self._interpreter = (
>           _interpreter_wrapper.CreateWrapperFromFile(
                model_path, op_resolver_id, custom_op_registerers_by_name,
                custom_op_registerers_by_func, experimental_preserve_all_tensors))
E       ValueError: Model provided has model identifier '

D:\Coding\Anaconda\envs\testagent-experiment\lib\site-packages\tensorflow\lite\python\interpreter.py:456: ValueError
_____ test_complete_inference_workflow[simple_add-input_shape0-float32-1] _____

model_type = 'simple_add', input_shape = [2, 2], dtype = 'float32'
num_threads = 1
simple_add_model_path = 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp39ueakhi.tflite'
mock_interpreter = <Mock name='Interpreter()' id='1464056105136'>

    @pytest.mark.parametrize("model_type,input_shape,dtype,num_threads", [
        ("simple_add", [2, 2], "float32", 1),
    ])
    def test_complete_inference_workflow(
        model_type, input_shape, dtype, num_threads,
        simple_add_model_path, mock_interpreter
    ):
        """
        Test complete inference workflow: allocate_tensors → set_tensor → invoke → get_tensor.
    
        Weak assertions:
        - invoke_success: invoke() completes without error
        - output_shape_match: output tensor shape matches expected
        - output_dtype_match: output tensor dtype matches expected
        - result_finite: output values are finite (not NaN or inf)
        """
        # Skip if tensorflow not available
        if not TFLITE_AVAILABLE:
            pytest.skip("TensorFlow Lite not available")
    
        # Set random seed for reproducibility
        np.random.seed(42)
    
        # Create interpreter
>       interpreter = Interpreter(
            model_path=simple_add_model_path,
            num_threads=num_threads
        )

tests\test_tensorflow_lite_python_interpreter_g1.py:248: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tensorflow.lite.python.interpreter.Interpreter object at 0x00000154D81A92B0>
model_path = 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp39ueakhi.tflite'
model_content = None, experimental_delegates = None, num_threads = 1
experimental_op_resolver_type = <OpResolverType.AUTO: 0>
experimental_preserve_all_tensors = False

    def __init__(self,
                 model_path=None,
                 model_content=None,
                 experimental_delegates=None,
                 num_threads=None,
                 experimental_op_resolver_type=OpResolverType.AUTO,
                 experimental_preserve_all_tensors=False):
      """Constructor.
    
      Args:
        model_path: Path to TF-Lite Flatbuffer file.
        model_content: Content of model.
        experimental_delegates: Experimental. Subject to change. List of
          [TfLiteDelegate](https://www.tensorflow.org/lite/performance/delegates)
            objects returned by lite.load_delegate().
        num_threads: Sets the number of threads used by the interpreter and
          available to CPU kernels. If not set, the interpreter will use an
          implementation-dependent default number of threads. Currently, only a
          subset of kernels, such as conv, support multi-threading. num_threads
          should be >= -1. Setting num_threads to 0 has the effect to disable
          multithreading, which is equivalent to setting num_threads to 1. If set
          to the value -1, the number of threads used will be
          implementation-defined and platform-dependent.
        experimental_op_resolver_type: The op resolver used by the interpreter. It
          must be an instance of OpResolverType. By default, we use the built-in
          op resolver which corresponds to tflite::ops::builtin::BuiltinOpResolver
          in C++.
        experimental_preserve_all_tensors: If true, then intermediate tensors used
          during computation are preserved for inspection, and if the passed op
          resolver type is AUTO or BUILTIN, the type will be changed to
          BUILTIN_WITHOUT_DEFAULT_DELEGATES so that no Tensorflow Lite default
          delegates are applied. If false, getting intermediate tensors could
          result in undefined values or None, especially when the graph is
          successfully modified by the Tensorflow Lite default delegate.
    
      Raises:
        ValueError: If the interpreter was unable to create.
      """
      if not hasattr(self, '_custom_op_registerers'):
        self._custom_op_registerers = []
    
      actual_resolver_type = experimental_op_resolver_type
      if experimental_preserve_all_tensors and (
          experimental_op_resolver_type == OpResolverType.AUTO or
          experimental_op_resolver_type == OpResolverType.BUILTIN):
        actual_resolver_type = OpResolverType.BUILTIN_WITHOUT_DEFAULT_DELEGATES
      op_resolver_id = _get_op_resolver_id(actual_resolver_type)
      if op_resolver_id is None:
        raise ValueError('Unrecognized passed in op resolver type: {}'.format(
            experimental_op_resolver_type))
    
      if model_path and not model_content:
        custom_op_registerers_by_name = [
            x for x in self._custom_op_registerers if isinstance(x, str)
        ]
        custom_op_registerers_by_func = [
            x for x in self._custom_op_registerers if not isinstance(x, str)
        ]
        self._interpreter = (
>           _interpreter_wrapper.CreateWrapperFromFile(
                model_path, op_resolver_id, custom_op_registerers_by_name,
                custom_op_registerers_by_func, experimental_preserve_all_tensors))
E       ValueError: Model provided has model identifier '

D:\Coding\Anaconda\envs\testagent-experiment\lib\site-packages\tensorflow\lite\python\interpreter.py:456: ValueError
___________ test_model_content_loading[model_content-simple_add--1] ___________

model_source = 'model_content', model_type = 'simple_add', num_threads = -1
simple_add_model_content = b'TFL3\x00\x00\x00\x00'
simple_add_model_path = 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmplz9crjar.tflite'
mock_interpreter = <Mock name='Interpreter()' id='1464055741360'>

    @pytest.mark.parametrize("model_source,model_type,num_threads", [
        ("model_content", "simple_add", -1),
    ])
    def test_model_content_loading(
        model_source, model_type, num_threads,
        simple_add_model_content, simple_add_model_path, mock_interpreter
    ):
        """
        Test loading model from binary content instead of file path.
    
        Weak assertions:
        - interpreter_created: Interpreter instance is created successfully from content
        - tensors_allocated: allocate_tensors() works with content-loaded model
        - functionality_identical: Content-loaded model behaves identically to file-loaded model
        """
        # Skip if tensorflow not available
        if not TFLITE_AVAILABLE:
            pytest.skip("TensorFlow Lite not available")
    
        # Create interpreter from model content
>       interpreter_from_content = Interpreter(
            model_content=simple_add_model_content,
            num_threads=num_threads
        )

tests\test_tensorflow_lite_python_interpreter_g1.py:348: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tensorflow.lite.python.interpreter.Interpreter object at 0x00000154E087DBE0>
model_path = None, model_content = b'TFL3\x00\x00\x00\x00'
experimental_delegates = None, num_threads = -1
experimental_op_resolver_type = <OpResolverType.AUTO: 0>
experimental_preserve_all_tensors = False

    def __init__(self,
                 model_path=None,
                 model_content=None,
                 experimental_delegates=None,
                 num_threads=None,
                 experimental_op_resolver_type=OpResolverType.AUTO,
                 experimental_preserve_all_tensors=False):
      """Constructor.
    
      Args:
        model_path: Path to TF-Lite Flatbuffer file.
        model_content: Content of model.
        experimental_delegates: Experimental. Subject to change. List of
          [TfLiteDelegate](https://www.tensorflow.org/lite/performance/delegates)
            objects returned by lite.load_delegate().
        num_threads: Sets the number of threads used by the interpreter and
          available to CPU kernels. If not set, the interpreter will use an
          implementation-dependent default number of threads. Currently, only a
          subset of kernels, such as conv, support multi-threading. num_threads
          should be >= -1. Setting num_threads to 0 has the effect to disable
          multithreading, which is equivalent to setting num_threads to 1. If set
          to the value -1, the number of threads used will be
          implementation-defined and platform-dependent.
        experimental_op_resolver_type: The op resolver used by the interpreter. It
          must be an instance of OpResolverType. By default, we use the built-in
          op resolver which corresponds to tflite::ops::builtin::BuiltinOpResolver
          in C++.
        experimental_preserve_all_tensors: If true, then intermediate tensors used
          during computation are preserved for inspection, and if the passed op
          resolver type is AUTO or BUILTIN, the type will be changed to
          BUILTIN_WITHOUT_DEFAULT_DELEGATES so that no Tensorflow Lite default
          delegates are applied. If false, getting intermediate tensors could
          result in undefined values or None, especially when the graph is
          successfully modified by the Tensorflow Lite default delegate.
    
      Raises:
        ValueError: If the interpreter was unable to create.
      """
      if not hasattr(self, '_custom_op_registerers'):
        self._custom_op_registerers = []
    
      actual_resolver_type = experimental_op_resolver_type
      if experimental_preserve_all_tensors and (
          experimental_op_resolver_type == OpResolverType.AUTO or
          experimental_op_resolver_type == OpResolverType.BUILTIN):
        actual_resolver_type = OpResolverType.BUILTIN_WITHOUT_DEFAULT_DELEGATES
      op_resolver_id = _get_op_resolver_id(actual_resolver_type)
      if op_resolver_id is None:
        raise ValueError('Unrecognized passed in op resolver type: {}'.format(
            experimental_op_resolver_type))
    
      if model_path and not model_content:
        custom_op_registerers_by_name = [
            x for x in self._custom_op_registerers if isinstance(x, str)
        ]
        custom_op_registerers_by_func = [
            x for x in self._custom_op_registerers if not isinstance(x, str)
        ]
        self._interpreter = (
            _interpreter_wrapper.CreateWrapperFromFile(
                model_path, op_resolver_id, custom_op_registerers_by_name,
                custom_op_registerers_by_func, experimental_preserve_all_tensors))
        if not self._interpreter:
          raise ValueError('Failed to open {}'.format(model_path))
      elif model_content and not model_path:
        custom_op_registerers_by_name = [
            x for x in self._custom_op_registerers if isinstance(x, str)
        ]
        custom_op_registerers_by_func = [
            x for x in self._custom_op_registerers if not isinstance(x, str)
        ]
        # Take a reference, so the pointer remains valid.
        # Since python strings are immutable then PyString_XX functions
        # will always return the same pointer.
        self._model_content = model_content
        self._interpreter = (
>           _interpreter_wrapper.CreateWrapperFromBuffer(
                model_content, op_resolver_id, custom_op_registerers_by_name,
E               ValueError: Model provided has model identifier '

D:\Coding\Anaconda\envs\testagent-experiment\lib\site-packages\tensorflow\lite\python\interpreter.py:473: ValueError

---------- coverage: platform win32, python 3.9.25-final-0 -----------
Name                                                  Stmts   Miss Branch BrPart  Cover   Missing
-------------------------------------------------------------------------------------------------
tests\test_tensorflow_lite_python_interpreter_g1.py     224    157     46      5    27%   17-54, 88-89, 116-125, 129-138, 161, 167-172, 183-219, 242, 254-323, 345, 354-458, 509, 513
-------------------------------------------------------------------------------------------------
TOTAL                                                   224    157     46      5    27%
Coverage XML written to file coverage.xml

=========================== short test summary info ===========================
FAILED tests\test_tensorflow_lite_python_interpreter_g1.py::test_model_loading_and_tensor_allocation[model_path-simple_add-1-False]
FAILED tests\test_tensorflow_lite_python_interpreter_g1.py::test_complete_inference_workflow[simple_add-input_shape0-float32-1]
FAILED tests\test_tensorflow_lite_python_interpreter_g1.py::test_model_content_loading[model_content-simple_add--1]
3 failed, 4 passed in 1.61s

Error: exit 1