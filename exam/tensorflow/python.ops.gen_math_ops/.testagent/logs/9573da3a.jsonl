{"role": "user", "content": {"stage": "understand_function", "prompt": "你是一个 Python 代码分析助手，目标是为指定函数生成测试。请控制全文不超过 800 字，使用短句/列表呈现要点，避免重复。\n\n当前阶段：**理解目标函数**（Stage 1） → 产物 `function_doc.md`\n\n- 目标 FQN: tensorflow.python.ops.gen_math_ops\n- 工作目录/项目根: /Users/zzf1sh/Documents/Project/TestAgent-CLI-main/exam/tensorflow/python.ops.gen_math_ops\n- 预期测试文件路径: tests/test_tensorflow_python_ops_gen_math_ops.py\n\n## 需要做什么\n1) 使用 `inspect_python` 获取目标的签名、注解、docstring、源码片段、模块路径。\n   - 必须传递以下参数（JSON 格式）：\n     ```json\n     {\n       \"target\": \"tensorflow.python.ops.gen_math_ops\",\n       \"add_cwd_to_path\": true,\n       \"max_source_length\": 4000\n     }\n     ```\n   - 目标可能是 `pkg.module:func`、`pkg.module.Class.method`，也可能是一个模块\n   - 如果目标是模块：先列出模块导出的核心类/函数（可根据 `__all__` 或主要公共 API），再聚焦最核心的主类/函数做说明，并在\"风险与空白\"注明多实体情况\n2) 如有必要，用 `read_file` 查看源码文件/README 以补充约束（如张量形状、dtype、示例）。\n3) 生成 `function_doc.md`，格式如下（必须包含所有字段）：\n\n```markdown\n# tensorflow.python.ops.gen_math_ops - 函数说明\n\n## 1. 基本信息\n- **FQN**: tensorflow.python.ops.gen_math_ops\n- **模块文件**: `path/to/module.py`\n- **签名**: func(param1, param2=..., *args, **kwargs)\n- **对象类型**: function/method/class/callable\n\n## 2. 功能概述\n用 2-3 句话描述行为与返回值。\n\n## 3. 参数说明\n- name (类型/默认值): 约束、形状/范围、是否可选\n- ...\n\n## 4. 返回值\n- 类型/结构、关键字段、可能的 None/异常\n\n## 5. 文档要点\n- docstring 中的重要约束（张量形状、dtype、设备等）\n- 预期前置条件/后置条件\n\n## 6. 源码摘要\n- 关键路径/分支（最多 5 条），依赖的辅助函数或外部 API\n- 副作用（I/O、随机性、全局状态），只列事实，不展开叙述\n\n## 7. 示例与用法（如有）\n- 来源：docstring/源码/示例\n\n## 8. 风险与空白\n- 未提供的类型信息、模糊/缺失的约束\n- 需要在测试中特别覆盖的边界\n- 缺少信息时直接列在此处，不要在正文重复描述\n```\n\n只通过 `write_file` 输出 `function_doc.md`，不要直接在对话中粘贴完整文档。\n如果找不到 docstring/源码，要在文档的“风险与空白”中明确说明。\n\n## 用户反馈\nNo feedback\n"}, "stage": "understand_function"}
{"role": "assistant", "content": {"role": "assistant", "content": "我将按照您的要求分析目标函数。首先使用 `inspect_python` 获取目标模块的信息。", "reasoning_content": "", "tool_calls": [{"id": "call_00_HPDxg9mxmvKPWxuoz0F10IJ2", "type": "function", "function": {"name": "inspect_python", "arguments": "{\"target\": \"tensorflow.python.ops.gen_math_ops\", \"add_cwd_to_path\": true, \"max_source_length\": 4000}"}}]}, "stage": "understand_function"}
{"role": "tool", "content": {"role": "tool", "tool_call_id": "call_00_HPDxg9mxmvKPWxuoz0F10IJ2", "content": "{\n  \"target\": \"tensorflow.python.ops.gen_math_ops\",\n  \"module\": \"tensorflow.python.ops.gen_math_ops\",\n  \"module_file\": \"/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py\",\n  \"qualname\": null,\n  \"type\": \"module\",\n  \"docstring\": \"Python wrappers around TensorFlow ops.\\n\\nThis file is MACHINE GENERATED! Do not edit.\\nOriginal C++ source file: math_ops.cc\",\n  \"module_docstring\": \"\",\n  \"python_version\": \"3.10.19 (main, Oct 21 2025, 16:37:10) [Clang 20.1.8 ]\",\n  \"source\": \"\\\"\\\"\\\"Python wrappers around TensorFlow ops.\\n\\nThis file is MACHINE GENERATED! Do not edit.\\nOriginal C++ source file: math_ops.cc\\n\\\"\\\"\\\"\\n\\nimport collections\\n\\nfrom tensorflow.python import pywrap_tfe as pywrap_tfe\\nfrom tensorflow.python.eager import context as _context\\nfrom tensorflow.python.eager import core as _core\\nfrom tensorflow.python.eager import execute as _execute\\nfrom tensorflow.python.framework import dtypes as _dtypes\\n\\nfrom tensorflow.python.framework import op_def_registry as _op_def_registry\\nfrom tensorflow.python.framework import ops as _ops\\nfrom tensorflow.python.framework import op_def_library as _op_def_library\\nfrom tensorflow.python.util.deprecation import deprecated_endpoints\\nfrom tensorflow.python.util import dispatch as _dispatch\\nfrom tensorflow.python.util.tf_export import tf_export\\n\\nfrom typing import TypeVar\\n\\ndef _abs(x, name=None):\\n  r\\\"\\\"\\\"Computes the absolute value of a tensor.\\n\\n  Given a tensor `x`, this operation returns a tensor containing the absolute\\n  value of each element in `x`. For example, if x is an input element and y is\\n  an output element, this operation computes \\\\\\\\(y = |x|\\\\\\\\).\\n\\n  Args:\\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    A `Tensor`. Has the same type as `x`.\\n  \\\"\\\"\\\"\\n  _ctx = _context._context or _context.context()\\n  tld = _ctx._thread_local_data\\n  if tld.is_eager:\\n    try:\\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\\n        _ctx, \\\"Abs\\\", name, x)\\n      return _result\\n    except _core._NotOkStatusException as e:\\n      _ops.raise_from_not_ok_status(e, name)\\n    except _core._FallbackException:\\n      pass\\n    try:\\n      return _abs_eager_fallback(\\n          x, name=name, ctx=_ctx)\\n    except _core._SymbolicException:\\n      pass  # Add nodes to the TensorFlow graph.\\n  # Add nodes to the TensorFlow graph.\\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\\n        \\\"Abs\\\", x=x, name=name)\\n  _result = _outputs[:]\\n  if _execute.must_record_gradient():\\n    _attrs = (\\\"T\\\", _op._get_attr_type(\\\"T\\\"))\\n    _inputs_flat = _op.inputs\\n    _execute.record_gradient(\\n        \\\"Abs\\\", _inputs_flat, _attrs, _result)\\n  _result, = _result\\n  return _result\\n\\nAbs = tf_export(\\\"raw_ops.Abs\\\")(_ops.to_raw_op(_abs))\\n\\n\\ndef _abs_eager_fallback(x, name, ctx):\\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, ])\\n  _inputs_flat = [x]\\n  _attrs = (\\\"T\\\", _attr_T)\\n  _result = _execute.execute(b\\\"Abs\\\", 1, inputs=_inputs_flat, attrs=_attrs,\\n                             ctx=ctx, name=name)\\n  if _execute.must_record_gradient():\\n    _execute.record_gradient(\\n        \\\"Abs\\\", _inputs_flat, _attrs, _result)\\n  _result, = _result\\n  return _result\\n\\n\\ndef accumulate_nv2(inputs, shape, name=None):\\n  r\\\"\\\"\\\"Returns the element-wise sum of a list of tensors.\\n\\n  `tf.accumulate_n_v2` performs the same operation as `tf.add_n`, but does not\\n  wait for all of its inputs to be ready before beginning to sum. This can\\n  save memory if inputs are ready at different times, since minimum temporary\\n  storage is proportional to the output size rather than the inputs size.\\n\\n  Unlike the original `accumulate_n`, `accumulate_n_v2` is differentiable.\\n\\n  Returns a `Tensor` of same shape and type as the elements of `inputs`.\\n\\n  Args:\\n    inputs: A list of at least 1 `Tensor` objects with the same type in: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\\n      A list of `Tensor` objects, each with same shape and type.\\n    shape: A `tf.TensorShape` or list of `ints`.\\n      Shape of elements of `inputs`.\\n    name: A name for the operation (optional).\\n\\n  Returns:\\n    A `Tensor`. Has the same type as `inputs`.\\n  \\\"\\\"\\\"\\n  _ctx = _context._context or _context.context()\\n\\n# ... truncated ...\",\n  \"public_members\": [\n    {\n      \"name\": \"Abs\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"AccumulateNV2\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Acos\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Acosh\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Add\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"AddN\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"AddV2\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"All\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Angle\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Any\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"ApproximateEqual\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"ArgMax\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"ArgMin\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Asin\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Asinh\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Atan\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Atan2\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Atanh\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"BatchMatMul\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"BatchMatMulV2\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"BatchMatMulV3\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Betainc\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Bincount\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Bucketize\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Cast\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Ceil\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"ClipByValue\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Complex\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"ComplexAbs\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Conj\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Cos\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Cosh\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Cross\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Cumprod\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Cumsum\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"CumulativeLogsumexp\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"DenseBincount\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Digamma\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Div\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"DivNoNan\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Equal\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Erf\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Erfc\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Erfinv\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"EuclideanNorm\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Exp\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Expm1\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"Floor\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"FloorDiv\",\n      \"type\": \"function\"\n    },\n    {\n      \"name\": \"FloorMod\",\n      \"type\": \"function\"\n    }\n  ]\n}"}, "stage": "understand_function"}
{"role": "assistant", "content": {"role": "assistant", "content": "现在让我查看工作目录中的文件结构，了解项目环境：", "reasoning_content": "", "tool_calls": [{"id": "call_00_cyTAyZTMOA30Z5TnPo3zCUoG", "type": "function", "function": {"name": "list_files", "arguments": "{\"path\": \"/Users/zzf1sh/Documents/Project/TestAgent-CLI-main/exam/tensorflow/python.ops.gen_math_ops\"}"}}]}, "stage": "understand_function"}
{"role": "tool", "content": {"role": "tool", "tool_call_id": "call_00_cyTAyZTMOA30Z5TnPo3zCUoG", "content": ".testagent/\ntests/"}, "stage": "understand_function"}
{"role": "assistant", "content": {"role": "assistant", "content": "让我查看测试目录的内容：", "reasoning_content": "", "tool_calls": [{"id": "call_00_RamRbOJy37OQKc54NfsvZ6UP", "type": "function", "function": {"name": "list_files", "arguments": "{\"path\": \"/Users/zzf1sh/Documents/Project/TestAgent-CLI-main/exam/tensorflow/python.ops.gen_math_ops/tests\"}"}}]}, "stage": "understand_function"}
{"role": "tool", "content": {"role": "tool", "tool_call_id": "call_00_RamRbOJy37OQKc54NfsvZ6UP", "content": ""}, "stage": "understand_function"}
{"role": "assistant", "content": {"role": "assistant", "content": "现在让我查看模块的完整源码文件，以获取更多信息：", "reasoning_content": "", "tool_calls": [{"id": "call_00_DwpYS38H7aVfHlZ8k3wJo39Z", "type": "function", "function": {"name": "read_file", "arguments": "{\"path\": \"/opt/anaconda3/envs/testagent-experiment/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py\"}"}}]}, "stage": "understand_function"}
{"role": "tool", "content": {"role": "tool", "tool_call_id": "call_00_DwpYS38H7aVfHlZ8k3wJo39Z", "content": "\"\"\"Python wrappers around TensorFlow ops.\n\nThis file is MACHINE GENERATED! Do not edit.\nOriginal C++ source file: math_ops.cc\n\"\"\"\n\nimport collections\n\nfrom tensorflow.python import pywrap_tfe as pywrap_tfe\nfrom tensorflow.python.eager import context as _context\nfrom tensorflow.python.eager import core as _core\nfrom tensorflow.python.eager import execute as _execute\nfrom tensorflow.python.framework import dtypes as _dtypes\n\nfrom tensorflow.python.framework import op_def_registry as _op_def_registry\nfrom tensorflow.python.framework import ops as _ops\nfrom tensorflow.python.framework import op_def_library as _op_def_library\nfrom tensorflow.python.util.deprecation import deprecated_endpoints\nfrom tensorflow.python.util import dispatch as _dispatch\nfrom tensorflow.python.util.tf_export import tf_export\n\nfrom typing import TypeVar\n\ndef _abs(x, name=None):\n  r\"\"\"Computes the absolute value of a tensor.\n\n  Given a tensor `x`, this operation returns a tensor containing the absolute\n  value of each element in `x`. For example, if x is an input element and y is\n  an output element, this operation computes \\\\(y = |x|\\\\).\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Abs\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return _abs_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Abs\", x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Abs\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAbs = tf_export(\"raw_ops.Abs\")(_ops.to_raw_op(_abs))\n\n\ndef _abs_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Abs\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Abs\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef accumulate_nv2(inputs, shape, name=None):\n  r\"\"\"Returns the element-wise sum of a list of tensors.\n\n  `tf.accumulate_n_v2` performs the same operation as `tf.add_n`, but does not\n  wait for all of its inputs to be ready before beginning to sum. This can\n  save memory if inputs are ready at different times, since minimum temporary\n  storage is proportional to the output size rather than the inputs size.\n\n  Unlike the original `accumulate_n`, `accumulate_n_v2` is differentiable.\n\n  Returns a `Tensor` of same shape and type as the elements of `inputs`.\n\n  Args:\n    inputs: A list of at least 1 `Tensor` objects with the same type in: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n      A list of `Tensor` objects, each with same shape and type.\n    shape: A `tf.TensorShape` or list of `ints`.\n      Shape of elements of `inputs`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `inputs`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"AccumulateNV2\", name, inputs, \"shape\", shape)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return accumulate_nv2_eager_fallback(\n          inputs, shape=shape, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if not isinstance(inputs, (list, tuple)):\n    raise TypeError(\n        \"Expected list for 'inputs' argument to \"\n        \"'accumulate_nv2' Op, not %r.\" % inputs)\n  _attr_N = len(inputs)\n  shape = _execute.make_shape(shape, \"shape\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"AccumulateNV2\", inputs=inputs, shape=shape, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"N\", _op._get_attr_int(\"N\"), \"T\", _op._get_attr_type(\"T\"),\n              \"shape\", _op.get_attr(\"shape\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"AccumulateNV2\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAccumulateNV2 = tf_export(\"raw_ops.AccumulateNV2\")(_ops.to_raw_op(accumulate_nv2))\n\n\ndef accumulate_nv2_eager_fallback(inputs, shape, name, ctx):\n  if not isinstance(inputs, (list, tuple)):\n    raise TypeError(\n        \"Expected list for 'inputs' argument to \"\n        \"'accumulate_nv2' Op, not %r.\" % inputs)\n  _attr_N = len(inputs)\n  shape = _execute.make_shape(shape, \"shape\")\n  _attr_T, inputs = _execute.args_to_matching_eager(list(inputs), ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _inputs_flat = list(inputs)\n  _attrs = (\"N\", _attr_N, \"T\", _attr_T, \"shape\", shape)\n  _result = _execute.execute(b\"AccumulateNV2\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"AccumulateNV2\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef acos(x, name=None):\n  r\"\"\"Computes acos of x element-wise.\n\n  \n    Provided an input tensor, the `tf.math.acos` operation returns the inverse cosine of each element of the tensor. If `y = tf.math.cos(x)` then, `x = tf.math.acos(y)`.\n\n    Input range is `[-1, 1]` and the output has a range of `[0, pi]`.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Acos\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return acos_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Acos\", x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Acos\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAcos = tf_export(\"raw_ops.Acos\")(_ops.to_raw_op(acos))\n\n\ndef acos_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Acos\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Acos\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.acosh', 'acosh')\ndef acosh(x, name=None):\n  r\"\"\"Computes inverse hyperbolic cosine of x element-wise.\n\n  Given an input tensor, the function computes inverse hyperbolic cosine of every element.\n  Input range is `[1, inf]`. It returns `nan` if the input lies outside the range.\n\n  ```python\n  x = tf.constant([-2, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n  tf.math.acosh(x) ==> [nan nan 0. 0.62236255 5.9914584 9.903487 inf]\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Acosh\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_acosh(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return acosh_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            acosh, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_acosh(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Acosh\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          acosh, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Acosh\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAcosh = tf_export(\"raw_ops.Acosh\")(_ops.to_raw_op(acosh))\n_dispatcher_for_acosh = acosh._tf_type_based_dispatcher.Dispatch\n\n\ndef acosh_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Acosh\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Acosh\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef add(x, y, name=None):\n  r\"\"\"Returns x + y element-wise.\n\n  *NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Given two input tensors, the `tf.add` operation computes the sum for every element in the tensor.\n\n  Both input and output have a range `(-inf, inf)`.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Add\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return add_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Add\", x=x, y=y, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Add\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAdd = tf_export(\"raw_ops.Add\")(_ops.to_raw_op(add))\n\n\ndef add_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.uint8, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, _dtypes.string, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Add\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Add\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef add_n(inputs, name=None):\n  r\"\"\"Add all input tensors element wise.\n\n    Inputs must be of same size and shape.\n\n    ```python\n    x = [9, 7, 10]\n    tf.math.add_n(x) ==> 26\n    ```\n\n  Args:\n    inputs: A list of at least 1 `Tensor` objects with the same type in: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`, `variant`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `inputs`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"AddN\", name, inputs)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return add_n_eager_fallback(\n          inputs, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if not isinstance(inputs, (list, tuple)):\n    raise TypeError(\n        \"Expected list for 'inputs' argument to \"\n        \"'add_n' Op, not %r.\" % inputs)\n  _attr_N = len(inputs)\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"AddN\", inputs=inputs, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"N\", _op._get_attr_int(\"N\"), \"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"AddN\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAddN = tf_export(\"raw_ops.AddN\")(_ops.to_raw_op(add_n))\n\n\ndef add_n_eager_fallback(inputs, name, ctx):\n  if not isinstance(inputs, (list, tuple)):\n    raise TypeError(\n        \"Expected list for 'inputs' argument to \"\n        \"'add_n' Op, not %r.\" % inputs)\n  _attr_N = len(inputs)\n  _attr_T, inputs = _execute.args_to_matching_eager(list(inputs), ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, _dtypes.variant, ])\n  _inputs_flat = list(inputs)\n  _attrs = (\"N\", _attr_N, \"T\", _attr_T)\n  _result = _execute.execute(b\"AddN\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"AddN\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef add_v2(x, y, name=None):\n  r\"\"\"Returns x + y element-wise.\n\n  *NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"AddV2\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return add_v2_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"AddV2\", x=x, y=y, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"AddV2\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAddV2 = tf_export(\"raw_ops.AddV2\")(_ops.to_raw_op(add_v2))\n\n\ndef add_v2_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.uint8, _dtypes.uint16, _dtypes.uint32, _dtypes.uint64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"AddV2\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"AddV2\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef _all(input, axis, keep_dims=False, name=None):\n  r\"\"\"Computes the \"logical and\" of elements across dimensions of a tensor.\n\n  Reduces `input` along the dimensions given in `axis`. Unless\n  `keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n  `axis`. If `keep_dims` is true, the reduced dimensions are\n  retained with length 1.\n\n  Args:\n    input: A `Tensor` of type `bool`. The tensor to reduce.\n    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      The dimensions to reduce. Must be in the range\n      `[-rank(input), rank(input))`.\n    keep_dims: An optional `bool`. Defaults to `False`.\n      If true, retain reduced dimensions with length 1.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"All\", name, input, axis, \"keep_dims\", keep_dims)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return _all_eager_fallback(\n          input, axis, keep_dims=keep_dims, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"All\", input=input, reduction_indices=axis, keep_dims=keep_dims,\n               name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"keep_dims\", _op._get_attr_bool(\"keep_dims\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"All\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAll = tf_export(\"raw_ops.All\")(_ops.to_raw_op(_all))\n\n\ndef _all_eager_fallback(input, axis, keep_dims, name, ctx):\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _attr_Tidx, (axis,) = _execute.args_to_matching_eager([axis], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  input = _ops.convert_to_tensor(input, _dtypes.bool)\n  _inputs_flat = [input, axis]\n  _attrs = (\"keep_dims\", keep_dims, \"Tidx\", _attr_Tidx)\n  _result = _execute.execute(b\"All\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"All\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef angle(input, Tout=_dtypes.float32, name=None):\n  r\"\"\"Returns the argument of a complex number.\n\n  Given a tensor `input` of complex numbers, this operation returns a tensor of\n  type `float` that is the argument of each element in `input`. All elements in\n  `input` must be complex numbers of the form \\\\(a + bj\\\\), where *a*\n  is the real part and *b* is the imaginary part.\n\n  The argument returned by this operation is of the form \\\\(atan2(b, a)\\\\).\n\n  For example:\n\n  ```\n  # tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\n  tf.angle(input) ==> [2.0132, 1.056]\n  ```\n\n  @compatibility(numpy)\n  Equivalent to np.angle.\n  @end_compatibility\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `complex64`, `complex128`.\n    Tout: An optional `tf.DType` from: `tf.float32, tf.float64`. Defaults to `tf.float32`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `Tout`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Angle\", name, input, \"Tout\", Tout)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return angle_eager_fallback(\n          input, Tout=Tout, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if Tout is None:\n    Tout = _dtypes.float32\n  Tout = _execute.make_type(Tout, \"Tout\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Angle\", input=input, Tout=Tout, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tout\",\n              _op._get_attr_type(\"Tout\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Angle\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAngle = tf_export(\"raw_ops.Angle\")(_ops.to_raw_op(angle))\n\n\ndef angle_eager_fallback(input, Tout, name, ctx):\n  if Tout is None:\n    Tout = _dtypes.float32\n  Tout = _execute.make_type(Tout, \"Tout\")\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.complex64, _dtypes.complex128, ], _dtypes.complex64)\n  _inputs_flat = [input]\n  _attrs = (\"T\", _attr_T, \"Tout\", Tout)\n  _result = _execute.execute(b\"Angle\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Angle\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef _any(input, axis, keep_dims=False, name=None):\n  r\"\"\"Computes the \"logical or\" of elements across dimensions of a tensor.\n\n  Reduces `input` along the dimensions given in `axis`. Unless\n  `keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n  `axis`. If `keep_dims` is true, the reduced dimensions are\n  retained with length 1.\n\n  Args:\n    input: A `Tensor` of type `bool`. The tensor to reduce.\n    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      The dimensions to reduce. Must be in the range\n      `[-rank(input), rank(input))`.\n    keep_dims: An optional `bool`. Defaults to `False`.\n      If true, retain reduced dimensions with length 1.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Any\", name, input, axis, \"keep_dims\", keep_dims)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return _any_eager_fallback(\n          input, axis, keep_dims=keep_dims, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Any\", input=input, reduction_indices=axis, keep_dims=keep_dims,\n               name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"keep_dims\", _op._get_attr_bool(\"keep_dims\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Any\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAny = tf_export(\"raw_ops.Any\")(_ops.to_raw_op(_any))\n\n\ndef _any_eager_fallback(input, axis, keep_dims, name, ctx):\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _attr_Tidx, (axis,) = _execute.args_to_matching_eager([axis], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  input = _ops.convert_to_tensor(input, _dtypes.bool)\n  _inputs_flat = [input, axis]\n  _attrs = (\"keep_dims\", keep_dims, \"Tidx\", _attr_Tidx)\n  _result = _execute.execute(b\"Any\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Any\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef approximate_equal(x, y, tolerance=1e-05, name=None):\n  r\"\"\"Returns the truth value of abs(x-y) < tolerance element-wise.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n    y: A `Tensor`. Must have the same type as `x`.\n    tolerance: An optional `float`. Defaults to `1e-05`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"ApproximateEqual\", name, x, y, \"tolerance\", tolerance)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return approximate_equal_eager_fallback(\n          x, y, tolerance=tolerance, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if tolerance is None:\n    tolerance = 1e-05\n  tolerance = _execute.make_float(tolerance, \"tolerance\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"ApproximateEqual\", x=x, y=y, tolerance=tolerance, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"tolerance\",\n              _op.get_attr(\"tolerance\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"ApproximateEqual\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nApproximateEqual = tf_export(\"raw_ops.ApproximateEqual\")(_ops.to_raw_op(approximate_equal))\n\n\ndef approximate_equal_eager_fallback(x, y, tolerance, name, ctx):\n  if tolerance is None:\n    tolerance = 1e-05\n  tolerance = _execute.make_float(tolerance, \"tolerance\")\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T, \"tolerance\", tolerance)\n  _result = _execute.execute(b\"ApproximateEqual\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"ApproximateEqual\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef arg_max(input, dimension, output_type=_dtypes.int64, name=None):\n  r\"\"\"Returns the index with the largest value across dimensions of a tensor.\n\n  Note that in case of ties the identity of the return value is not guaranteed.\n\n  Usage:\n    ```python\n    import tensorflow as tf\n    a = [1, 10, 26.9, 2.8, 166.32, 62.3]\n    b = tf.math.argmax(input = a)\n    c = tf.keras.backend.eval(b)\n    # c = 4\n    # here a[4] = 166.32 which is the largest element of a across axis 0\n    ```\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`, `bool`.\n    dimension: A `Tensor`. Must be one of the following types: `int16`, `int32`, `int64`.\n      int16, int32 or int64, must be in the range `[-rank(input), rank(input))`.\n      Describes which dimension of the input Tensor to reduce across. For vectors,\n      use dimension = 0.\n    output_type: An optional `tf.DType` from: `tf.int16, tf.uint16, tf.int32, tf.int64`. Defaults to `tf.int64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `output_type`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"ArgMax\", name, input, dimension, \"output_type\", output_type)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return arg_max_eager_fallback(\n          input, dimension, output_type=output_type, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if output_type is None:\n    output_type = _dtypes.int64\n  output_type = _execute.make_type(output_type, \"output_type\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"ArgMax\", input=input, dimension=dimension, output_type=output_type,\n                  name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"), \"output_type\",\n              _op._get_attr_type(\"output_type\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"ArgMax\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nArgMax = tf_export(\"raw_ops.ArgMax\")(_ops.to_raw_op(arg_max))\n\n\ndef arg_max_eager_fallback(input, dimension, output_type, name, ctx):\n  if output_type is None:\n    output_type = _dtypes.int64\n  output_type = _execute.make_type(output_type, \"output_type\")\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, _dtypes.bool, ])\n  _attr_Tidx, (dimension,) = _execute.args_to_matching_eager([dimension], ctx, [_dtypes.int16, _dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [input, dimension]\n  _attrs = (\"T\", _attr_T, \"Tidx\", _attr_Tidx, \"output_type\", output_type)\n  _result = _execute.execute(b\"ArgMax\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"ArgMax\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef arg_min(input, dimension, output_type=_dtypes.int64, name=None):\n  r\"\"\"Returns the index with the smallest value across dimensions of a tensor.\n\n  Note that in case of ties the identity of the return value is not guaranteed.\n\n  Usage:\n    ```python\n    import tensorflow as tf\n    a = [1, 10, 26.9, 2.8, 166.32, 62.3]\n    b = tf.math.argmin(input = a)\n    c = tf.keras.backend.eval(b)\n    # c = 0\n    # here a[0] = 1 which is the smallest element of a across axis 0\n    ```\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`, `bool`.\n    dimension: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      int32 or int64, must be in the range `[-rank(input), rank(input))`.\n      Describes which dimension of the input Tensor to reduce across. For vectors,\n      use dimension = 0.\n    output_type: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `output_type`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"ArgMin\", name, input, dimension, \"output_type\", output_type)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return arg_min_eager_fallback(\n          input, dimension, output_type=output_type, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if output_type is None:\n    output_type = _dtypes.int64\n  output_type = _execute.make_type(output_type, \"output_type\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"ArgMin\", input=input, dimension=dimension, output_type=output_type,\n                  name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"), \"output_type\",\n              _op._get_attr_type(\"output_type\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"ArgMin\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nArgMin = tf_export(\"raw_ops.ArgMin\")(_ops.to_raw_op(arg_min))\n\n\ndef arg_min_eager_fallback(input, dimension, output_type, name, ctx):\n  if output_type is None:\n    output_type = _dtypes.int64\n  output_type = _execute.make_type(output_type, \"output_type\")\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, _dtypes.bool, ])\n  _attr_Tidx, (dimension,) = _execute.args_to_matching_eager([dimension], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [input, dimension]\n  _attrs = (\"T\", _attr_T, \"Tidx\", _attr_Tidx, \"output_type\", output_type)\n  _result = _execute.execute(b\"ArgMin\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"ArgMin\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.asin', 'asin')\ndef asin(x, name=None):\n  r\"\"\"Computes the trignometric inverse sine of x element-wise.\n\n  The `tf.math.asin` operation returns the inverse of `tf.math.sin`, such that\n  if `y = tf.math.sin(x)` then, `x = tf.math.asin(y)`.\n\n  **Note**: The output of `tf.math.asin` will lie within the invertible range\n  of sine, i.e [-pi/2, pi/2].\n\n  For example:\n\n  ```python\n  # Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]\n  x = tf.constant([1.047, 0.785])\n  y = tf.math.sin(x) # [0.8659266, 0.7068252]\n\n  tf.math.asin(y) # [1.047, 0.785] = x\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Asin\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_asin(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return asin_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            asin, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_asin(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Asin\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          asin, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Asin\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAsin = tf_export(\"raw_ops.Asin\")(_ops.to_raw_op(asin))\n_dispatcher_for_asin = asin._tf_type_based_dispatcher.Dispatch\n\n\ndef asin_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Asin\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Asin\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.asinh', 'asinh')\ndef asinh(x, name=None):\n  r\"\"\"Computes inverse hyperbolic sine of x element-wise.\n\n    Given an input tensor, this function computes inverse hyperbolic sine\n    for every element in the tensor. Both input and output has a range of\n    `[-inf, inf]`.\n\n    ```python\n    x = tf.constant([-float(\"inf\"), -2, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n    tf.math.asinh(x) ==> [-inf -1.4436355 -0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf]\n    ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Asinh\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_asinh(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return asinh_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            asinh, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_asinh(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Asinh\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          asinh, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Asinh\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAsinh = tf_export(\"raw_ops.Asinh\")(_ops.to_raw_op(asinh))\n_dispatcher_for_asinh = asinh._tf_type_based_dispatcher.Dispatch\n\n\ndef asinh_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Asinh\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Asinh\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.atan', 'atan')\ndef atan(x, name=None):\n  r\"\"\"Computes the trignometric inverse tangent of x element-wise.\n\n  The `tf.math.atan` operation returns the inverse of `tf.math.tan`, such that\n  if `y = tf.math.tan(x)` then, `x = tf.math.atan(y)`.\n\n  **Note**: The output of `tf.math.atan` will lie within the invertible range\n  of tan, i.e (-pi/2, pi/2).\n\n  For example:\n\n  ```python\n  # Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]\n  x = tf.constant([1.047, 0.785])\n  y = tf.math.tan(x) # [1.731261, 0.99920404]\n\n  tf.math.atan(y) # [1.047, 0.785] = x\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Atan\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_atan(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return atan_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            atan, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_atan(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Atan\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          atan, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Atan\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAtan = tf_export(\"raw_ops.Atan\")(_ops.to_raw_op(atan))\n_dispatcher_for_atan = atan._tf_type_based_dispatcher.Dispatch\n\n\ndef atan_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Atan\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Atan\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.atan2', 'atan2')\ndef atan2(y, x, name=None):\n  r\"\"\"Computes arctangent of `y/x` element-wise, respecting signs of the arguments.\n\n  This is the angle \\\\( \\theta \\in [-\\pi, \\pi] \\\\) such that\n  \\\\[ x = r \\cos(\\theta) \\\\]\n  and\n  \\\\[ y = r \\sin(\\theta) \\\\]\n  where \\\\(r = \\sqrt{x^2 + y^2} \\\\).\n\n  For example:\n\n  >>> x = [1., 1.]\n  >>> y = [1., -1.]\n  >>> print((tf.math.atan2(y,x) * (180 / np.pi)).numpy())\n  [ 45. -45.]\n\n  Args:\n    y: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    x: A `Tensor`. Must have the same type as `y`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `y`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Atan2\", name, y, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_atan2(\n          (y, x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return atan2_eager_fallback(\n          y, x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            atan2, (), dict(y=y, x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_atan2(\n        (y, x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Atan2\", y=y, x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          atan2, (), dict(y=y, x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Atan2\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAtan2 = tf_export(\"raw_ops.Atan2\")(_ops.to_raw_op(atan2))\n_dispatcher_for_atan2 = atan2._tf_type_based_dispatcher.Dispatch\n\n\ndef atan2_eager_fallback(y, x, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([y, x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  (y, x) = _inputs_T\n  _inputs_flat = [y, x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Atan2\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Atan2\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.atanh', 'atanh')\ndef atanh(x, name=None):\n  r\"\"\"Computes inverse hyperbolic tangent of x element-wise.\n\n    Given an input tensor, this function computes inverse hyperbolic tangent\n    for every element in the tensor. Input range is `[-1,1]` and output range is\n    `[-inf, inf]`. If input is `-1`, output will be `-inf` and if the\n    input is `1`, output will be `inf`. Values outside the range will have\n    `nan` as output.\n\n    ```python\n    x = tf.constant([-float(\"inf\"), -1, -0.5, 1, 0, 0.5, 10, float(\"inf\")])\n    tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]\n    ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Atanh\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_atanh(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return atanh_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            atanh, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_atanh(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Atanh\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          atanh, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Atanh\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nAtanh = tf_export(\"raw_ops.Atanh\")(_ops.to_raw_op(atanh))\n_dispatcher_for_atanh = atanh._tf_type_based_dispatcher.Dispatch\n\n\ndef atanh_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Atanh\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Atanh\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef batch_mat_mul(x, y, adj_x=False, adj_y=False, name=None):\n  r\"\"\"Multiplies slices of two tensors in batches.\n\n  Multiplies all slices of `Tensor` `x` and `y` (each slice can be\n  viewed as an element of a batch), and arranges the individual results\n  in a single output tensor of the same batch size. Each of the\n  individual slices can optionally be adjointed (to adjoint a matrix\n  means to transpose and conjugate it) before multiplication by setting\n  the `adj_x` or `adj_y` flag to `True`, which are by default `False`.\n\n  The input tensors `x` and `y` are 2-D or higher with shape `[..., r_x, c_x]`\n  and `[..., r_y, c_y]`.\n\n  The output tensor is 2-D or higher with shape `[..., r_o, c_o]`, where:\n\n      r_o = c_x if adj_x else r_x\n      c_o = r_y if adj_y else c_y\n\n  It is computed as:\n\n      output[..., :, :] = matrix(x[..., :, :]) * matrix(y[..., :, :])\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n      2-D or higher with shape `[..., r_x, c_x]`.\n    y: A `Tensor`. Must have the same type as `x`.\n      2-D or higher with shape `[..., r_y, c_y]`.\n    adj_x: An optional `bool`. Defaults to `False`.\n      If `True`, adjoint the slices of `x`. Defaults to `False`.\n    adj_y: An optional `bool`. Defaults to `False`.\n      If `True`, adjoint the slices of `y`. Defaults to `False`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"BatchMatMul\", name, x, y, \"adj_x\", adj_x, \"adj_y\", adj_y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return batch_mat_mul_eager_fallback(\n          x, y, adj_x=adj_x, adj_y=adj_y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if adj_x is None:\n    adj_x = False\n  adj_x = _execute.make_bool(adj_x, \"adj_x\")\n  if adj_y is None:\n    adj_y = False\n  adj_y = _execute.make_bool(adj_y, \"adj_y\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"adj_x\",\n              _op._get_attr_bool(\"adj_x\"), \"adj_y\",\n              _op._get_attr_bool(\"adj_y\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"BatchMatMul\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nBatchMatMul = tf_export(\"raw_ops.BatchMatMul\")(_ops.to_raw_op(batch_mat_mul))\n\n\ndef batch_mat_mul_eager_fallback(x, y, adj_x, adj_y, name, ctx):\n  if adj_x is None:\n    adj_x = False\n  adj_x = _execute.make_bool(adj_x, \"adj_x\")\n  if adj_y is None:\n    adj_y = False\n  adj_y = _execute.make_bool(adj_y, \"adj_y\")\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T, \"adj_x\", adj_x, \"adj_y\", adj_y)\n  _result = _execute.execute(b\"BatchMatMul\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"BatchMatMul\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef batch_mat_mul_v2(x, y, adj_x=False, adj_y=False, name=None):\n  r\"\"\"Multiplies slices of two tensors in batches.\n\n  Multiplies all slices of `Tensor` `x` and `y` (each slice can be\n  viewed as an element of a batch), and arranges the individual results\n  in a single output tensor of the same batch size. Each of the\n  individual slices can optionally be adjointed (to adjoint a matrix\n  means to transpose and conjugate it) before multiplication by setting\n  the `adj_x` or `adj_y` flag to `True`, which are by default `False`.\n\n  The input tensors `x` and `y` are 2-D or higher with shape `[..., r_x, c_x]`\n  and `[..., r_y, c_y]`.\n\n  The output tensor is 2-D or higher with shape `[..., r_o, c_o]`, where:\n\n      r_o = c_x if adj_x else r_x\n      c_o = r_y if adj_y else c_y\n\n  It is computed as:\n\n      output[..., :, :] = matrix(x[..., :, :]) * matrix(y[..., :, :])\n\n  *NOTE*: `BatchMatMulV2` supports broadcasting in the batch dimensions. More\n  about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n      2-D or higher with shape `[..., r_x, c_x]`.\n    y: A `Tensor`. Must have the same type as `x`.\n      2-D or higher with shape `[..., r_y, c_y]`.\n    adj_x: An optional `bool`. Defaults to `False`.\n      If `True`, adjoint the slices of `x`. Defaults to `False`.\n    adj_y: An optional `bool`. Defaults to `False`.\n      If `True`, adjoint the slices of `y`. Defaults to `False`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"BatchMatMulV2\", name, x, y, \"adj_x\", adj_x, \"adj_y\", adj_y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return batch_mat_mul_v2_eager_fallback(\n          x, y, adj_x=adj_x, adj_y=adj_y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if adj_x is None:\n    adj_x = False\n  adj_x = _execute.make_bool(adj_x, \"adj_x\")\n  if adj_y is None:\n    adj_y = False\n  adj_y = _execute.make_bool(adj_y, \"adj_y\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"BatchMatMulV2\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"adj_x\",\n              _op._get_attr_bool(\"adj_x\"), \"adj_y\",\n              _op._get_attr_bool(\"adj_y\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"BatchMatMulV2\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nBatchMatMulV2 = tf_export(\"raw_ops.BatchMatMulV2\")(_ops.to_raw_op(batch_mat_mul_v2))\n\n\ndef batch_mat_mul_v2_eager_fallback(x, y, adj_x, adj_y, name, ctx):\n  if adj_x is None:\n    adj_x = False\n  adj_x = _execute.make_bool(adj_x, \"adj_x\")\n  if adj_y is None:\n    adj_y = False\n  adj_y = _execute.make_bool(adj_y, \"adj_y\")\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T, \"adj_x\", adj_x, \"adj_y\", adj_y)\n  _result = _execute.execute(b\"BatchMatMulV2\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"BatchMatMulV2\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef batch_mat_mul_v3(x, y, Tout, adj_x=False, adj_y=False, name=None):\n  r\"\"\"Multiplies slices of two tensors in batches.\n\n  Multiplies all slices of `Tensor` `x` and `y` (each slice can be\n  viewed as an element of a batch), and arranges the individual results\n  in a single output tensor of the same batch size. Each of the\n  individual slices can optionally be adjointed (to adjoint a matrix\n  means to transpose and conjugate it) before multiplication by setting\n  the `adj_x` or `adj_y` flag to `True`, which are by default `False`.\n\n  The input tensors `x` and `y` are 2-D or higher with shape `[..., r_x, c_x]`\n  and `[..., r_y, c_y]`.\n\n  The output tensor is 2-D or higher with shape `[..., r_o, c_o]`, where:\n\n      r_o = c_x if adj_x else r_x\n      c_o = r_y if adj_y else c_y\n\n  It is computed as:\n\n      output[..., :, :] = matrix(x[..., :, :]) * matrix(y[..., :, :])\n\n  *NOTE*: `BatchMatMulV3` supports broadcasting in the batch dimensions. More\n  about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n      2-D or higher with shape `[..., r_x, c_x]`.\n    y: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n      2-D or higher with shape `[..., r_y, c_y]`.\n    Tout: A `tf.DType` from: `tf.bfloat16, tf.half, tf.float32, tf.float64, tf.int16, tf.int32, tf.int64, tf.complex64, tf.complex128`.\n      If not spcified, Tout is the same type to input type.\n    adj_x: An optional `bool`. Defaults to `False`.\n      If `True`, adjoint the slices of `x`. Defaults to `False`.\n    adj_y: An optional `bool`. Defaults to `False`.\n      If `True`, adjoint the slices of `y`. Defaults to `False`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `Tout`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"BatchMatMulV3\", name, x, y, \"Tout\", Tout, \"adj_x\", adj_x,\n        \"adj_y\", adj_y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return batch_mat_mul_v3_eager_fallback(\n          x, y, Tout=Tout, adj_x=adj_x, adj_y=adj_y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  Tout = _execute.make_type(Tout, \"Tout\")\n  if adj_x is None:\n    adj_x = False\n  adj_x = _execute.make_bool(adj_x, \"adj_x\")\n  if adj_y is None:\n    adj_y = False\n  adj_y = _execute.make_bool(adj_y, \"adj_y\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"BatchMatMulV3\", x=x, y=y, Tout=Tout, adj_x=adj_x, adj_y=adj_y,\n                         name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"Ta\", _op._get_attr_type(\"Ta\"), \"Tb\", _op._get_attr_type(\"Tb\"),\n              \"Tout\", _op._get_attr_type(\"Tout\"), \"adj_x\",\n              _op._get_attr_bool(\"adj_x\"), \"adj_y\",\n              _op._get_attr_bool(\"adj_y\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"BatchMatMulV3\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nBatchMatMulV3 = tf_export(\"raw_ops.BatchMatMulV3\")(_ops.to_raw_op(batch_mat_mul_v3))\n\n\ndef batch_mat_mul_v3_eager_fallback(x, y, Tout, adj_x, adj_y, name, ctx):\n  Tout = _execute.make_type(Tout, \"Tout\")\n  if adj_x is None:\n    adj_x = False\n  adj_x = _execute.make_bool(adj_x, \"adj_x\")\n  if adj_y is None:\n    adj_y = False\n  adj_y = _execute.make_bool(adj_y, \"adj_y\")\n  _attr_Ta, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.uint8, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  _attr_Tb, (y,) = _execute.args_to_matching_eager([y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.uint8, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x, y]\n  _attrs = (\"Ta\", _attr_Ta, \"Tb\", _attr_Tb, \"Tout\", Tout, \"adj_x\", adj_x,\n  \"adj_y\", adj_y)\n  _result = _execute.execute(b\"BatchMatMulV3\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"BatchMatMulV3\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.betainc', v1=['math.betainc', 'betainc'])\n@deprecated_endpoints('betainc')\ndef betainc(a, b, x, name=None):\n  r\"\"\"Compute the regularized incomplete beta integral \\\\(I_x(a, b)\\\\).\n\n  The regularized incomplete beta integral is defined as:\n\n\n  \\\\(I_x(a, b) = \\frac{B(x; a, b)}{B(a, b)}\\\\)\n\n  where\n\n\n  \\\\(B(x; a, b) = \\int_0^x t^{a-1} (1 - t)^{b-1} dt\\\\)\n\n\n  is the incomplete beta function and \\\\(B(a, b)\\\\) is the *complete*\n  beta function.\n\n  Args:\n    a: A `Tensor`. Must be one of the following types: `float32`, `float64`.\n    b: A `Tensor`. Must have the same type as `a`.\n    x: A `Tensor`. Must have the same type as `a`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `a`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Betainc\", name, a, b, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_betainc(\n          (a, b, x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return betainc_eager_fallback(\n          a, b, x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            betainc, (), dict(a=a, b=b, x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_betainc(\n        (a, b, x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Betainc\", a=a, b=b, x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          betainc, (), dict(a=a, b=b, x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Betainc\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nBetainc = tf_export(\"raw_ops.Betainc\")(_ops.to_raw_op(betainc))\n_dispatcher_for_betainc = betainc._tf_type_based_dispatcher.Dispatch\n\n\ndef betainc_eager_fallback(a, b, x, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([a, b, x], ctx, [_dtypes.float32, _dtypes.float64, ])\n  (a, b, x) = _inputs_T\n  _inputs_flat = [a, b, x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Betainc\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Betainc\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef bincount(arr, size, weights, name=None):\n  r\"\"\"Counts the number of occurrences of each value in an integer array.\n\n  Outputs a vector with length `size` and the same dtype as `weights`. If\n  `weights` are empty, then index `i` stores the number of times the value `i` is\n  counted in `arr`. If `weights` are non-empty, then index `i` stores the sum of\n  the value in `weights` at each index where the corresponding value in `arr` is\n  `i`.\n\n  Values in `arr` outside of the range [0, size) are ignored.\n\n  Args:\n    arr: A `Tensor` of type `int32`. int32 `Tensor`.\n    size: A `Tensor` of type `int32`. non-negative int32 scalar `Tensor`.\n    weights: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.\n      is an int32, int64, float32, or float64 `Tensor` with the same\n      shape as `arr`, or a length-0 `Tensor`, in which case it acts as all weights\n      equal to 1.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `weights`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Bincount\", name, arr, size, weights)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return bincount_eager_fallback(\n          arr, size, weights, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Bincount\", arr=arr, size=size, weights=weights, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Bincount\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nBincount = tf_export(\"raw_ops.Bincount\")(_ops.to_raw_op(bincount))\n\n\ndef bincount_eager_fallback(arr, size, weights, name, ctx):\n  _attr_T, (weights,) = _execute.args_to_matching_eager([weights], ctx, [_dtypes.int32, _dtypes.int64, _dtypes.float32, _dtypes.float64, ])\n  arr = _ops.convert_to_tensor(arr, _dtypes.int32)\n  size = _ops.convert_to_tensor(size, _dtypes.int32)\n  _inputs_flat = [arr, size, weights]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Bincount\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Bincount\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef bucketize(input, boundaries, name=None):\n  r\"\"\"Bucketizes 'input' based on 'boundaries'.\n\n  For example, if the inputs are\n      boundaries = [0, 10, 100]\n      input = [[-5, 10000]\n               [150,   10]\n               [5,    100]]\n\n  then the output will be\n      output = [[0, 3]\n                [3, 2]\n                [1, 3]]\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.\n      Any shape of Tensor contains with int or float type.\n    boundaries: A list of `floats`.\n      A sorted list of floats gives the boundary of the buckets.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `int32`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Bucketize\", name, input, \"boundaries\", boundaries)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return bucketize_eager_fallback(\n          input, boundaries=boundaries, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if not isinstance(boundaries, (list, tuple)):\n    raise TypeError(\n        \"Expected list for 'boundaries' argument to \"\n        \"'bucketize' Op, not %r.\" % boundaries)\n  boundaries = [_execute.make_float(_f, \"boundaries\") for _f in boundaries]\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Bucketize\", input=input, boundaries=boundaries, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"boundaries\",\n              _op.get_attr(\"boundaries\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Bucketize\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nBucketize = tf_export(\"raw_ops.Bucketize\")(_ops.to_raw_op(bucketize))\n\n\ndef bucketize_eager_fallback(input, boundaries, name, ctx):\n  if not isinstance(boundaries, (list, tuple)):\n    raise TypeError(\n        \"Expected list for 'boundaries' argument to \"\n        \"'bucketize' Op, not %r.\" % boundaries)\n  boundaries = [_execute.make_float(_f, \"boundaries\") for _f in boundaries]\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.int32, _dtypes.int64, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [input]\n  _attrs = (\"T\", _attr_T, \"boundaries\", boundaries)\n  _result = _execute.execute(b\"Bucketize\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Bucketize\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef cast(x, DstT, Truncate=False, name=None):\n  r\"\"\"Cast x of type SrcT to y of DstT.\n\n  Args:\n    x: A `Tensor`.\n    DstT: A `tf.DType`.\n    Truncate: An optional `bool`. Defaults to `False`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `DstT`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Cast\", name, x, \"DstT\", DstT, \"Truncate\", Truncate)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return cast_eager_fallback(\n          x, DstT=DstT, Truncate=Truncate, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  DstT = _execute.make_type(DstT, \"DstT\")\n  if Truncate is None:\n    Truncate = False\n  Truncate = _execute.make_bool(Truncate, \"Truncate\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Cast\", x=x, DstT=DstT, Truncate=Truncate, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"SrcT\", _op._get_attr_type(\"SrcT\"), \"DstT\",\n              _op._get_attr_type(\"DstT\"), \"Truncate\",\n              _op._get_attr_bool(\"Truncate\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Cast\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nCast = tf_export(\"raw_ops.Cast\")(_ops.to_raw_op(cast))\n\n\ndef cast_eager_fallback(x, DstT, Truncate, name, ctx):\n  DstT = _execute.make_type(DstT, \"DstT\")\n  if Truncate is None:\n    Truncate = False\n  Truncate = _execute.make_bool(Truncate, \"Truncate\")\n  _attr_SrcT, (x,) = _execute.args_to_matching_eager([x], ctx, [])\n  _inputs_flat = [x]\n  _attrs = (\"SrcT\", _attr_SrcT, \"DstT\", DstT, \"Truncate\", Truncate)\n  _result = _execute.execute(b\"Cast\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Cast\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef ceil(x, name=None):\n  r\"\"\"Returns element-wise smallest integer not less than x.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Ceil\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return ceil_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Ceil\", x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Ceil\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nCeil = tf_export(\"raw_ops.Ceil\")(_ops.to_raw_op(ceil))\n\n\ndef ceil_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Ceil\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Ceil\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef _clip_by_value(t, clip_value_min, clip_value_max, name=None):\n  r\"\"\"Clips tensor values to a specified min and max.\n\n  Given a tensor `t`, this operation returns a tensor of the same type and\n  shape as `t` with its values clipped to `clip_value_min` and `clip_value_max`.\n  Any values less than `clip_value_min` are set to `clip_value_min`. Any values\n  greater than `clip_value_max` are set to `clip_value_max`.\n\n  Args:\n    t: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n      A `Tensor`.\n    clip_value_min: A `Tensor`. Must have the same type as `t`.\n      A 0-D (scalar) `Tensor`, or a `Tensor` with the same shape\n      as `t`. The minimum value to clip by.\n    clip_value_max: A `Tensor`. Must have the same type as `t`.\n      A 0-D (scalar) `Tensor`, or a `Tensor` with the same shape\n      as `t`. The maximum value to clip by.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `t`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"ClipByValue\", name, t, clip_value_min, clip_value_max)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return _clip_by_value_eager_fallback(\n          t, clip_value_min, clip_value_max, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"ClipByValue\", t=t, clip_value_min=clip_value_min,\n                       clip_value_max=clip_value_max, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"ClipByValue\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nClipByValue = tf_export(\"raw_ops.ClipByValue\")(_ops.to_raw_op(_clip_by_value))\n\n\ndef _clip_by_value_eager_fallback(t, clip_value_min, clip_value_max, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([t, clip_value_min, clip_value_max], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  (t, clip_value_min, clip_value_max) = _inputs_T\n  _inputs_flat = [t, clip_value_min, clip_value_max]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"ClipByValue\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"ClipByValue\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef _complex(real, imag, Tout=_dtypes.complex64, name=None):\n  r\"\"\"Converts two real numbers to a complex number.\n\n  Given a tensor `real` representing the real part of a complex number, and a\n  tensor `imag` representing the imaginary part of a complex number, this\n  operation returns complex numbers elementwise of the form \\\\(a + bj\\\\), where\n  *a* represents the `real` part and *b* represents the `imag` part.\n\n  The input tensors `real` and `imag` must have the same shape.\n\n  For example:\n\n  ```\n  # tensor 'real' is [2.25, 3.25]\n  # tensor `imag` is [4.75, 5.75]\n  tf.complex(real, imag) ==> [[2.25 + 4.75j], [3.25 + 5.75j]]\n  ```\n\n  Args:\n    real: A `Tensor`. Must be one of the following types: `float32`, `float64`.\n    imag: A `Tensor`. Must have the same type as `real`.\n    Tout: An optional `tf.DType` from: `tf.complex64, tf.complex128`. Defaults to `tf.complex64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `Tout`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Complex\", name, real, imag, \"Tout\", Tout)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return _complex_eager_fallback(\n          real, imag, Tout=Tout, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if Tout is None:\n    Tout = _dtypes.complex64\n  Tout = _execute.make_type(Tout, \"Tout\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Complex\", real=real, imag=imag, Tout=Tout, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tout\",\n              _op._get_attr_type(\"Tout\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Complex\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nComplex = tf_export(\"raw_ops.Complex\")(_ops.to_raw_op(_complex))\n\n\ndef _complex_eager_fallback(real, imag, Tout, name, ctx):\n  if Tout is None:\n    Tout = _dtypes.complex64\n  Tout = _execute.make_type(Tout, \"Tout\")\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([real, imag], ctx, [_dtypes.float32, _dtypes.float64, ], _dtypes.float32)\n  (real, imag) = _inputs_T\n  _inputs_flat = [real, imag]\n  _attrs = (\"T\", _attr_T, \"Tout\", Tout)\n  _result = _execute.execute(b\"Complex\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Complex\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef complex_abs(x, Tout=_dtypes.float32, name=None):\n  r\"\"\"Computes the complex absolute value of a tensor.\n\n  Given a tensor `x` of complex numbers, this operation returns a tensor of type\n  `float` or `double` that is the absolute value of each element in `x`. All\n  elements in `x` must be complex numbers of the form \\\\(a + bj\\\\). The absolute\n  value is computed as \\\\( \\sqrt{a^2 + b^2}\\\\).\n\n  For example:\n\n  >>> x = tf.complex(3.0, 4.0)\n  >>> print((tf.raw_ops.ComplexAbs(x=x, Tout=tf.dtypes.float32, name=None)).numpy())\n  5.0\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `complex64`, `complex128`.\n    Tout: An optional `tf.DType` from: `tf.float32, tf.float64`. Defaults to `tf.float32`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `Tout`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"ComplexAbs\", name, x, \"Tout\", Tout)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return complex_abs_eager_fallback(\n          x, Tout=Tout, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if Tout is None:\n    Tout = _dtypes.float32\n  Tout = _execute.make_type(Tout, \"Tout\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"ComplexAbs\", x=x, Tout=Tout, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tout\",\n              _op._get_attr_type(\"Tout\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"ComplexAbs\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nComplexAbs = tf_export(\"raw_ops.ComplexAbs\")(_ops.to_raw_op(complex_abs))\n\n\ndef complex_abs_eager_fallback(x, Tout, name, ctx):\n  if Tout is None:\n    Tout = _dtypes.float32\n  Tout = _execute.make_type(Tout, \"Tout\")\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.complex64, _dtypes.complex128, ], _dtypes.complex64)\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T, \"Tout\", Tout)\n  _result = _execute.execute(b\"ComplexAbs\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"ComplexAbs\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef conj(input, name=None):\n  r\"\"\"Returns the complex conjugate of a complex number.\n\n  Given a tensor `input` of complex numbers, this operation returns a tensor of\n  complex numbers that are the complex conjugate of each element in `input`. The\n  complex numbers in `input` must be of the form \\\\(a + bj\\\\), where *a* is the\n  real part and *b* is the imaginary part.\n\n  The complex conjugate returned by this operation is of the form \\\\(a - bj\\\\).\n\n  For example:\n\n  ```\n  # tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\n  tf.conj(input) ==> [-2.25 - 4.75j, 3.25 - 5.75j]\n  ```\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `complex64`, `complex128`, `variant`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `input`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Conj\", name, input)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return conj_eager_fallback(\n          input, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Conj\", input=input, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Conj\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nConj = tf_export(\"raw_ops.Conj\")(_ops.to_raw_op(conj))\n\n\ndef conj_eager_fallback(input, name, ctx):\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.complex64, _dtypes.complex128, _dtypes.variant, ], _dtypes.complex64)\n  _inputs_flat = [input]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Conj\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Conj\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.cos', 'cos')\ndef cos(x, name=None):\n  r\"\"\"Computes cos of x element-wise.\n\n    Given an input tensor, this function computes cosine of every\n    element in the tensor. Input range is `(-inf, inf)` and\n    output range is `[-1,1]`. If input lies outside the boundary, `nan`\n    is returned.\n\n    ```python\n    x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n    tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]\n    ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Cos\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_cos(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return cos_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            cos, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_cos(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Cos\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          cos, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Cos\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nCos = tf_export(\"raw_ops.Cos\")(_ops.to_raw_op(cos))\n_dispatcher_for_cos = cos._tf_type_based_dispatcher.Dispatch\n\n\ndef cos_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Cos\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Cos\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.cosh', 'cosh')\ndef cosh(x, name=None):\n  r\"\"\"Computes hyperbolic cosine of x element-wise.\n\n    Given an input tensor, this function computes hyperbolic cosine of every\n    element in the tensor. Input range is `[-inf, inf]` and output range\n    is `[1, inf]`.\n\n    ```python\n    x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 2, 10, float(\"inf\")])\n    tf.math.cosh(x) ==> [inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf]\n    ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Cosh\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_cosh(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return cosh_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            cosh, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_cosh(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Cosh\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          cosh, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Cosh\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nCosh = tf_export(\"raw_ops.Cosh\")(_ops.to_raw_op(cosh))\n_dispatcher_for_cosh = cosh._tf_type_based_dispatcher.Dispatch\n\n\ndef cosh_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Cosh\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Cosh\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('linalg.cross', v1=['linalg.cross', 'cross'])\n@deprecated_endpoints('cross')\ndef cross(a, b, name=None):\n  r\"\"\"Compute the pairwise cross product.\n\n  `a` and `b` must be the same shape; they can either be simple 3-element vectors,\n  or any shape where the innermost dimension is 3. In the latter case, each pair\n  of corresponding 3-element vectors is cross-multiplied independently.\n\n  Args:\n    a: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n      A tensor containing 3-element vectors.\n    b: A `Tensor`. Must have the same type as `a`.\n      Another tensor, of same type and shape as `a`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `a`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Cross\", name, a, b)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_cross(\n          (a, b, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return cross_eager_fallback(\n          a, b, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            cross, (), dict(a=a, b=b, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_cross(\n        (a, b, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Cross\", a=a, b=b, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          cross, (), dict(a=a, b=b, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Cross\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nCross = tf_export(\"raw_ops.Cross\")(_ops.to_raw_op(cross))\n_dispatcher_for_cross = cross._tf_type_based_dispatcher.Dispatch\n\n\ndef cross_eager_fallback(a, b, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([a, b], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.int64, _dtypes.bfloat16, _dtypes.uint16, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  (a, b) = _inputs_T\n  _inputs_flat = [a, b]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Cross\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Cross\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef cumprod(x, axis, exclusive=False, reverse=False, name=None):\n  r\"\"\"Compute the cumulative product of the tensor `x` along `axis`.\n\n  By default, this op performs an inclusive cumprod, which means that the first\n  element of the input is identical to the first element of the output:\n\n  ```python\n  tf.cumprod([a, b, c])  # => [a, a * b, a * b * c]\n  ```\n\n  By setting the `exclusive` kwarg to `True`, an exclusive cumprod is\n  performed instead:\n\n  ```python\n  tf.cumprod([a, b, c], exclusive=True)  # => [1, a, a * b]\n  ```\n\n  By setting the `reverse` kwarg to `True`, the cumprod is performed in the\n  opposite direction:\n\n  ```python\n  tf.cumprod([a, b, c], reverse=True)  # => [a * b * c, b * c, c]\n  ```\n\n  This is more efficient than using separate `tf.reverse` ops.\n\n  The `reverse` and `exclusive` kwargs can also be combined:\n\n  ```python\n  tf.cumprod([a, b, c], exclusive=True, reverse=True)  # => [b * c, c, 1]\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n      A `Tensor`. Must be one of the following types: `float32`, `float64`,\n      `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,\n      `complex128`, `qint8`, `quint8`, `qint32`, `half`.\n    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A `Tensor` of type `int32` (default: 0). Must be in the range\n      `[-rank(x), rank(x))`.\n    exclusive: An optional `bool`. Defaults to `False`.\n      If `True`, perform exclusive cumprod.\n    reverse: An optional `bool`. Defaults to `False`.\n      A `bool` (default: False).\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Cumprod\", name, x, axis, \"exclusive\", exclusive, \"reverse\",\n        reverse)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return cumprod_eager_fallback(\n          x, axis, exclusive=exclusive, reverse=reverse, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if exclusive is None:\n    exclusive = False\n  exclusive = _execute.make_bool(exclusive, \"exclusive\")\n  if reverse is None:\n    reverse = False\n  reverse = _execute.make_bool(reverse, \"reverse\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Cumprod\", x=x, axis=axis, exclusive=exclusive, reverse=reverse,\n                   name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"exclusive\", _op._get_attr_bool(\"exclusive\"), \"reverse\",\n              _op._get_attr_bool(\"reverse\"), \"T\", _op._get_attr_type(\"T\"),\n              \"Tidx\", _op._get_attr_type(\"Tidx\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Cumprod\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nCumprod = tf_export(\"raw_ops.Cumprod\")(_ops.to_raw_op(cumprod))\n\n\ndef cumprod_eager_fallback(x, axis, exclusive, reverse, name, ctx):\n  if exclusive is None:\n    exclusive = False\n  exclusive = _execute.make_bool(exclusive, \"exclusive\")\n  if reverse is None:\n    reverse = False\n  reverse = _execute.make_bool(reverse, \"reverse\")\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tidx, (axis,) = _execute.args_to_matching_eager([axis], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [x, axis]\n  _attrs = (\"exclusive\", exclusive, \"reverse\", reverse, \"T\", _attr_T, \"Tidx\",\n  _attr_Tidx)\n  _result = _execute.execute(b\"Cumprod\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Cumprod\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef cumsum(x, axis, exclusive=False, reverse=False, name=None):\n  r\"\"\"Compute the cumulative sum of the tensor `x` along `axis`.\n\n  By default, this op performs an inclusive cumsum, which means that the first\n  element of the input is identical to the first element of the output:\n\n  ```python\n  tf.cumsum([a, b, c])  # => [a, a + b, a + b + c]\n  ```\n\n  By setting the `exclusive` kwarg to `True`, an exclusive cumsum is\n  performed instead:\n\n  ```python\n  tf.cumsum([a, b, c], exclusive=True)  # => [0, a, a + b]\n  ```\n\n  By setting the `reverse` kwarg to `True`, the cumsum is performed in the\n  opposite direction:\n\n  ```python\n  tf.cumsum([a, b, c], reverse=True)  # => [a + b + c, b + c, c]\n  ```\n\n  This is more efficient than using separate `tf.reverse` ops.\n\n  The `reverse` and `exclusive` kwargs can also be combined:\n\n  ```python\n  tf.cumsum([a, b, c], exclusive=True, reverse=True)  # => [b + c, c, 0]\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n      A `Tensor`. Must be one of the following types: `float32`, `float64`,\n      `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,\n      `complex128`, `qint8`, `quint8`, `qint32`, `half`.\n    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A `Tensor` of type `int32` (default: 0). Must be in the range\n      `[-rank(x), rank(x))`.\n    exclusive: An optional `bool`. Defaults to `False`.\n      If `True`, perform exclusive cumsum.\n    reverse: An optional `bool`. Defaults to `False`.\n      A `bool` (default: False).\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Cumsum\", name, x, axis, \"exclusive\", exclusive, \"reverse\",\n        reverse)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return cumsum_eager_fallback(\n          x, axis, exclusive=exclusive, reverse=reverse, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if exclusive is None:\n    exclusive = False\n  exclusive = _execute.make_bool(exclusive, \"exclusive\")\n  if reverse is None:\n    reverse = False\n  reverse = _execute.make_bool(reverse, \"reverse\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Cumsum\", x=x, axis=axis, exclusive=exclusive, reverse=reverse,\n                  name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"exclusive\", _op._get_attr_bool(\"exclusive\"), \"reverse\",\n              _op._get_attr_bool(\"reverse\"), \"T\", _op._get_attr_type(\"T\"),\n              \"Tidx\", _op._get_attr_type(\"Tidx\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Cumsum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nCumsum = tf_export(\"raw_ops.Cumsum\")(_ops.to_raw_op(cumsum))\n\n\ndef cumsum_eager_fallback(x, axis, exclusive, reverse, name, ctx):\n  if exclusive is None:\n    exclusive = False\n  exclusive = _execute.make_bool(exclusive, \"exclusive\")\n  if reverse is None:\n    reverse = False\n  reverse = _execute.make_bool(reverse, \"reverse\")\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tidx, (axis,) = _execute.args_to_matching_eager([axis], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [x, axis]\n  _attrs = (\"exclusive\", exclusive, \"reverse\", reverse, \"T\", _attr_T, \"Tidx\",\n  _attr_Tidx)\n  _result = _execute.execute(b\"Cumsum\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Cumsum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef cumulative_logsumexp(x, axis, exclusive=False, reverse=False, name=None):\n  r\"\"\"Compute the cumulative product of the tensor `x` along `axis`.\n\n  By default, this op performs an inclusive cumulative log-sum-exp,\n  which means that the first\n  element of the input is identical to the first element of the output:\n  ```python\n  tf.math.cumulative_logsumexp([a, b, c])  # => [a, log(exp(a) + exp(b)), log(exp(a) + exp(b) + exp(c))]\n  ```\n\n  By setting the `exclusive` kwarg to `True`, an exclusive cumulative log-sum-exp is\n  performed instead:\n  ```python\n  tf.cumulative_logsumexp([a, b, c], exclusive=True)  # => [-inf, a, log(exp(a) * exp(b))]\n  ```\n  Note that the neutral element of the log-sum-exp operation is `-inf`,\n  however, for performance reasons, the minimal value representable by the\n  floating point type is used instead.\n\n  By setting the `reverse` kwarg to `True`, the cumulative log-sum-exp is performed in the\n  opposite direction.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.\n      A `Tensor`. Must be one of the following types: `float16`, `float32`, `float64`.\n    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A `Tensor` of type `int32` (default: 0). Must be in the range\n      `[-rank(x), rank(x))`.\n    exclusive: An optional `bool`. Defaults to `False`.\n      If `True`, perform exclusive cumulative log-sum-exp.\n    reverse: An optional `bool`. Defaults to `False`.\n      A `bool` (default: False).\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"CumulativeLogsumexp\", name, x, axis, \"exclusive\", exclusive,\n        \"reverse\", reverse)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return cumulative_logsumexp_eager_fallback(\n          x, axis, exclusive=exclusive, reverse=reverse, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if exclusive is None:\n    exclusive = False\n  exclusive = _execute.make_bool(exclusive, \"exclusive\")\n  if reverse is None:\n    reverse = False\n  reverse = _execute.make_bool(reverse, \"reverse\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"CumulativeLogsumexp\", x=x, axis=axis, exclusive=exclusive,\n                               reverse=reverse, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"exclusive\", _op._get_attr_bool(\"exclusive\"), \"reverse\",\n              _op._get_attr_bool(\"reverse\"), \"T\", _op._get_attr_type(\"T\"),\n              \"Tidx\", _op._get_attr_type(\"Tidx\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"CumulativeLogsumexp\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nCumulativeLogsumexp = tf_export(\"raw_ops.CumulativeLogsumexp\")(_ops.to_raw_op(cumulative_logsumexp))\n\n\ndef cumulative_logsumexp_eager_fallback(x, axis, exclusive, reverse, name, ctx):\n  if exclusive is None:\n    exclusive = False\n  exclusive = _execute.make_bool(exclusive, \"exclusive\")\n  if reverse is None:\n    reverse = False\n  reverse = _execute.make_bool(reverse, \"reverse\")\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _attr_Tidx, (axis,) = _execute.args_to_matching_eager([axis], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [x, axis]\n  _attrs = (\"exclusive\", exclusive, \"reverse\", reverse, \"T\", _attr_T, \"Tidx\",\n  _attr_Tidx)\n  _result = _execute.execute(b\"CumulativeLogsumexp\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"CumulativeLogsumexp\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef dense_bincount(input, size, weights, binary_output=False, name=None):\n  r\"\"\"Counts the number of occurrences of each value in an integer array.\n\n  Outputs a vector with length `size` and the same dtype as `weights`. If\n  `weights` are empty, then index `i` stores the number of times the value `i` is\n  counted in `arr`. If `weights` are non-empty, then index `i` stores the sum of\n  the value in `weights` at each index where the corresponding value in `arr` is\n  `i`.\n\n  Values in `arr` outside of the range [0, size) are ignored.\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      1D or 2D int `Tensor`.\n    size: A `Tensor`. Must have the same type as `input`.\n      non-negative int scalar `Tensor`.\n    weights: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.\n      is an int32, int64, float32, or float64 `Tensor` with the same\n      shape as `arr`, or a length-0 `Tensor`, in which case it acts as all weights\n      equal to 1.\n    binary_output: An optional `bool`. Defaults to `False`.\n      bool; Whether the kernel should count the appearance or number of occurrences.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `weights`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"DenseBincount\", name, input, size, weights, \"binary_output\",\n        binary_output)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return dense_bincount_eager_fallback(\n          input, size, weights, binary_output=binary_output, name=name,\n          ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if binary_output is None:\n    binary_output = False\n  binary_output = _execute.make_bool(binary_output, \"binary_output\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"DenseBincount\", input=input, size=size, weights=weights,\n                         binary_output=binary_output, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"Tidx\", _op._get_attr_type(\"Tidx\"), \"T\",\n              _op._get_attr_type(\"T\"), \"binary_output\",\n              _op._get_attr_bool(\"binary_output\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"DenseBincount\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nDenseBincount = tf_export(\"raw_ops.DenseBincount\")(_ops.to_raw_op(dense_bincount))\n\n\ndef dense_bincount_eager_fallback(input, size, weights, binary_output, name, ctx):\n  if binary_output is None:\n    binary_output = False\n  binary_output = _execute.make_bool(binary_output, \"binary_output\")\n  _attr_Tidx, _inputs_Tidx = _execute.args_to_matching_eager([input, size], ctx, [_dtypes.int32, _dtypes.int64, ])\n  (input, size) = _inputs_Tidx\n  _attr_T, (weights,) = _execute.args_to_matching_eager([weights], ctx, [_dtypes.int32, _dtypes.int64, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [input, size, weights]\n  _attrs = (\"Tidx\", _attr_Tidx, \"T\", _attr_T, \"binary_output\", binary_output)\n  _result = _execute.execute(b\"DenseBincount\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"DenseBincount\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.digamma', v1=['math.digamma', 'digamma'])\n@deprecated_endpoints('digamma')\ndef digamma(x, name=None):\n  r\"\"\"Computes Psi, the derivative of Lgamma (the log of the absolute value of\n\n  `Gamma(x)`), element-wise.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Digamma\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_digamma(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return digamma_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            digamma, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_digamma(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Digamma\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          digamma, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Digamma\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nDigamma = tf_export(\"raw_ops.Digamma\")(_ops.to_raw_op(digamma))\n_dispatcher_for_digamma = digamma._tf_type_based_dispatcher.Dispatch\n\n\ndef digamma_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Digamma\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Digamma\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef div(x, y, name=None):\n  r\"\"\"Returns x / y element-wise.\n\n  *NOTE*: `Div` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `uint32`, `uint64`, `int64`, `complex64`, `complex128`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Div\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return div_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Div\", x=x, y=y, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Div\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nDiv = tf_export(\"raw_ops.Div\")(_ops.to_raw_op(div))\n\n\ndef div_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.uint8, _dtypes.int8, _dtypes.uint16, _dtypes.int16, _dtypes.int32, _dtypes.uint32, _dtypes.uint64, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Div\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Div\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef div_no_nan(x, y, name=None):\n  r\"\"\"Returns 0 if the denominator is zero.\n\n  \n  *NOTE*: `DivNoNan` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `half`, `float32`, `bfloat16`, `float64`, `complex64`, `complex128`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"DivNoNan\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return div_no_nan_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"DivNoNan\", x=x, y=y, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"DivNoNan\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nDivNoNan = tf_export(\"raw_ops.DivNoNan\")(_ops.to_raw_op(div_no_nan))\n\n\ndef div_no_nan_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.half, _dtypes.float32, _dtypes.bfloat16, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"DivNoNan\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"DivNoNan\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef equal(x, y, incompatible_shape_error=True, name=None):\n  r\"\"\"Returns the truth value of (x == y) element-wise.\n\n  *NOTE*: `Equal` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  ```python\n  x = tf.constant([2, 4])\n  y = tf.constant(2)\n  tf.math.equal(x, y) ==> array([True, False])\n\n  x = tf.constant([2, 4])\n  y = tf.constant([2, 4])\n  tf.math.equal(x, y) ==> array([True,  True])\n  ```\n\n  Args:\n    x: A `Tensor`.\n    y: A `Tensor`. Must have the same type as `x`.\n    incompatible_shape_error: An optional `bool`. Defaults to `True`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Equal\", name, x, y, \"incompatible_shape_error\",\n        incompatible_shape_error)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return equal_eager_fallback(\n          x, y, incompatible_shape_error=incompatible_shape_error, name=name,\n          ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if incompatible_shape_error is None:\n    incompatible_shape_error = True\n  incompatible_shape_error = _execute.make_bool(incompatible_shape_error, \"incompatible_shape_error\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Equal\", x=x, y=y, incompatible_shape_error=incompatible_shape_error,\n                 name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"incompatible_shape_error\",\n              _op._get_attr_bool(\"incompatible_shape_error\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Equal\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nEqual = tf_export(\"raw_ops.Equal\")(_ops.to_raw_op(equal))\n\n\ndef equal_eager_fallback(x, y, incompatible_shape_error, name, ctx):\n  if incompatible_shape_error is None:\n    incompatible_shape_error = True\n  incompatible_shape_error = _execute.make_bool(incompatible_shape_error, \"incompatible_shape_error\")\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T, \"incompatible_shape_error\",\n  incompatible_shape_error)\n  _result = _execute.execute(b\"Equal\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Equal\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.erf', v1=['math.erf', 'erf'])\n@deprecated_endpoints('erf')\ndef erf(x, name=None):\n  r\"\"\"Computes the [Gauss error function](https://en.wikipedia.org/wiki/Error_function) of `x` element-wise. In statistics, for non-negative values of $x$, the error function has the following interpretation: for a random variable $Y$ that is normally distributed with mean 0 and variance $1/\\sqrt{2}$, $erf(x)$ is the probability that $Y$ falls in the range $[−x, x]$.\n\n  For example:\n\n  >>> tf.math.erf([[1.0, 2.0, 3.0], [0.0, -1.0, -2.0]])\n  <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n  array([[ 0.8427007,  0.9953223,  0.999978 ],\n         [ 0.       , -0.8427007, -0.9953223]], dtype=float32)>\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Erf\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_erf(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return erf_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            erf, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_erf(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Erf\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          erf, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Erf\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nErf = tf_export(\"raw_ops.Erf\")(_ops.to_raw_op(erf))\n_dispatcher_for_erf = erf._tf_type_based_dispatcher.Dispatch\n\n\ndef erf_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Erf\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Erf\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.erfc', v1=['math.erfc', 'erfc'])\n@deprecated_endpoints('erfc')\ndef erfc(x, name=None):\n  r\"\"\"Computes the complementary error function of `x` element-wise.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Erfc\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_erfc(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return erfc_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            erfc, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_erfc(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Erfc\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          erfc, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Erfc\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nErfc = tf_export(\"raw_ops.Erfc\")(_ops.to_raw_op(erfc))\n_dispatcher_for_erfc = erfc._tf_type_based_dispatcher.Dispatch\n\n\ndef erfc_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Erfc\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Erfc\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef erfinv(x, name=None):\n  r\"\"\"TODO: add doc.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Erfinv\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return erfinv_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Erfinv\", x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Erfinv\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nErfinv = tf_export(\"raw_ops.Erfinv\")(_ops.to_raw_op(erfinv))\n\n\ndef erfinv_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Erfinv\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Erfinv\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef euclidean_norm(input, axis, keep_dims=False, name=None):\n  r\"\"\"Computes the euclidean norm of elements across dimensions of a tensor.\n\n  Reduces `input` along the dimensions given in `axis`. Unless\n  `keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n  `axis`. If `keep_dims` is true, the reduced dimensions are\n  retained with length 1.\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n      The tensor to reduce.\n    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      The dimensions to reduce. Must be in the range\n      `[-rank(input), rank(input))`.\n    keep_dims: An optional `bool`. Defaults to `False`.\n      If true, retain reduced dimensions with length 1.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `input`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"EuclideanNorm\", name, input, axis, \"keep_dims\", keep_dims)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return euclidean_norm_eager_fallback(\n          input, axis, keep_dims=keep_dims, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"EuclideanNorm\", input=input, reduction_indices=axis,\n                         keep_dims=keep_dims, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"keep_dims\", _op._get_attr_bool(\"keep_dims\"), \"T\",\n              _op._get_attr_type(\"T\"), \"Tidx\", _op._get_attr_type(\"Tidx\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"EuclideanNorm\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nEuclideanNorm = tf_export(\"raw_ops.EuclideanNorm\")(_ops.to_raw_op(euclidean_norm))\n\n\ndef euclidean_norm_eager_fallback(input, axis, keep_dims, name, ctx):\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tidx, (axis,) = _execute.args_to_matching_eager([axis], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [input, axis]\n  _attrs = (\"keep_dims\", keep_dims, \"T\", _attr_T, \"Tidx\", _attr_Tidx)\n  _result = _execute.execute(b\"EuclideanNorm\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"EuclideanNorm\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef exp(x, name=None):\n  r\"\"\"Computes exponential of x element-wise.  \\\\(y = e^x\\\\).\n\n    This function computes the exponential of every element in the input tensor.\n    i.e. `exp(x)` or `e^(x)`, where `x` is the input tensor.\n    `e` denotes Euler's number and is approximately equal to 2.718281.\n    Output is positive for any real input.\n\n    ```python\n    x = tf.constant(2.0)\n    tf.math.exp(x) ==> 7.389056\n\n    x = tf.constant([2.0, 8.0])\n    tf.math.exp(x) ==> array([7.389056, 2980.958], dtype=float32)\n    ```\n\n    For complex numbers, the exponential value is calculated as follows:\n\n    ```\n    e^(x+iy) = e^x * e^iy = e^x * (cos y + i sin y)\n    ```\n\n    Let's consider complex number 1+1j as an example.\n    e^1 * (cos 1 + i sin 1) = 2.7182818284590 * (0.54030230586+0.8414709848j)\n\n    ```python\n    x = tf.constant(1 + 1j)\n    tf.math.exp(x) ==> 1.4686939399158851+2.2873552871788423j\n    ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Exp\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return exp_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Exp\", x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Exp\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nExp = tf_export(\"raw_ops.Exp\")(_ops.to_raw_op(exp))\n\n\ndef exp_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Exp\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Exp\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.expm1', v1=['math.expm1', 'expm1'])\n@deprecated_endpoints('expm1')\ndef expm1(x, name=None):\n  r\"\"\"Computes `exp(x) - 1` element-wise.\n\n    i.e. `exp(x) - 1` or `e^(x) - 1`, where `x` is the input tensor.\n    `e` denotes Euler's number and is approximately equal to 2.718281.\n\n    ```python\n    x = tf.constant(2.0)\n    tf.math.expm1(x) ==> 6.389056\n\n    x = tf.constant([2.0, 8.0])\n    tf.math.expm1(x) ==> array([6.389056, 2979.958], dtype=float32)\n\n    x = tf.constant(1 + 1j)\n    tf.math.expm1(x) ==> (0.46869393991588515+2.2873552871788423j)\n    ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Expm1\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_expm1(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return expm1_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            expm1, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_expm1(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Expm1\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          expm1, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Expm1\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nExpm1 = tf_export(\"raw_ops.Expm1\")(_ops.to_raw_op(expm1))\n_dispatcher_for_expm1 = expm1._tf_type_based_dispatcher.Dispatch\n\n\ndef expm1_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Expm1\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Expm1\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef floor(x, name=None):\n  r\"\"\"Returns element-wise largest integer not greater than x.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Floor\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return floor_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Floor\", x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Floor\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nFloor = tf_export(\"raw_ops.Floor\")(_ops.to_raw_op(floor))\n\n\ndef floor_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Floor\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Floor\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export(v1=['floor_div'])\n@deprecated_endpoints('floor_div')\ndef floor_div(x, y, name=None):\n  r\"\"\"Returns x // y element-wise.\n\n  *NOTE*: `floor_div` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `uint32`, `uint64`, `int64`, `complex64`, `complex128`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"FloorDiv\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_floor_div(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return floor_div_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            floor_div, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_floor_div(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"FloorDiv\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          floor_div, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"FloorDiv\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nFloorDiv = tf_export(\"raw_ops.FloorDiv\")(_ops.to_raw_op(floor_div))\n_dispatcher_for_floor_div = floor_div._tf_type_based_dispatcher.Dispatch\n\n\ndef floor_div_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.uint8, _dtypes.int8, _dtypes.uint16, _dtypes.int16, _dtypes.int32, _dtypes.uint32, _dtypes.uint64, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"FloorDiv\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"FloorDiv\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.floormod', 'math.mod', v1=['math.floormod', 'floormod', 'math.mod', 'mod'])\n@deprecated_endpoints('floormod', 'mod')\ndef floor_mod(x, y, name=None):\n  r\"\"\"Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n\n  true, this follows Python semantics in that the result here is consistent\n  with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n\n  *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `int8`, `int16`, `int32`, `int64`, `uint8`, `uint16`, `uint32`, `uint64`, `bfloat16`, `half`, `float32`, `float64`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"FloorMod\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_floor_mod(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return floor_mod_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            floor_mod, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_floor_mod(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"FloorMod\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          floor_mod, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"FloorMod\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nFloorMod = tf_export(\"raw_ops.FloorMod\")(_ops.to_raw_op(floor_mod))\n_dispatcher_for_floor_mod = floor_mod._tf_type_based_dispatcher.Dispatch\n\n\ndef floor_mod_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.uint8, _dtypes.uint16, _dtypes.uint32, _dtypes.uint64, _dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"FloorMod\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"FloorMod\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.greater', 'greater')\ndef greater(x, y, name=None):\n  r\"\"\"Returns the truth value of (x > y) element-wise.\n\n  *NOTE*: `math.greater` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Example:\n\n  ```python\n  x = tf.constant([5, 4, 6])\n  y = tf.constant([5, 2, 5])\n  tf.math.greater(x, y) ==> [False, True, True]\n\n  x = tf.constant([5, 4, 6])\n  y = tf.constant([5])\n  tf.math.greater(x, y) ==> [False, False, True]\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Greater\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_greater(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return greater_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            greater, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_greater(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Greater\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          greater, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Greater\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nGreater = tf_export(\"raw_ops.Greater\")(_ops.to_raw_op(greater))\n_dispatcher_for_greater = greater._tf_type_based_dispatcher.Dispatch\n\n\ndef greater_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.int64, _dtypes.bfloat16, _dtypes.uint16, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Greater\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Greater\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.greater_equal', 'greater_equal')\ndef greater_equal(x, y, name=None):\n  r\"\"\"Returns the truth value of (x >= y) element-wise.\n\n  *NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Example:\n\n  ```python\n  x = tf.constant([5, 4, 6, 7])\n  y = tf.constant([5, 2, 5, 10])\n  tf.math.greater_equal(x, y) ==> [True, True, True, False]\n\n  x = tf.constant([5, 4, 6, 7])\n  y = tf.constant([5])\n  tf.math.greater_equal(x, y) ==> [True, False, True, True]\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"GreaterEqual\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_greater_equal(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return greater_equal_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            greater_equal, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_greater_equal(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"GreaterEqual\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          greater_equal, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"GreaterEqual\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nGreaterEqual = tf_export(\"raw_ops.GreaterEqual\")(_ops.to_raw_op(greater_equal))\n_dispatcher_for_greater_equal = greater_equal._tf_type_based_dispatcher.Dispatch\n\n\ndef greater_equal_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.int64, _dtypes.bfloat16, _dtypes.uint16, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"GreaterEqual\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"GreaterEqual\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef _histogram_fixed_width(values, value_range, nbins, dtype=_dtypes.int32, name=None):\n  r\"\"\"Return histogram of values.\n\n  Given the tensor `values`, this operation returns a rank 1 histogram counting\n  the number of entries in `values` that fall into every bin.  The bins are\n  equal width and determined by the arguments `value_range` and `nbins`.\n\n  ```python\n  # Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)\n  nbins = 5\n  value_range = [0.0, 5.0]\n  new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]\n\n  with tf.get_default_session() as sess:\n    hist = tf.histogram_fixed_width(new_values, value_range, nbins=5)\n    variables.global_variables_initializer().run()\n    sess.run(hist) => [2, 1, 1, 0, 2]\n  ```\n\n  Args:\n    values: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.\n      Numeric `Tensor`.\n    value_range: A `Tensor`. Must have the same type as `values`.\n      Shape [2] `Tensor` of same `dtype` as `values`.\n      values <= value_range[0] will be mapped to hist[0],\n      values >= value_range[1] will be mapped to hist[-1].\n    nbins: A `Tensor` of type `int32`.\n      Scalar `int32 Tensor`.  Number of histogram bins.\n    dtype: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int32`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `dtype`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"HistogramFixedWidth\", name, values, value_range, nbins,\n        \"dtype\", dtype)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return _histogram_fixed_width_eager_fallback(\n          values, value_range, nbins, dtype=dtype, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if dtype is None:\n    dtype = _dtypes.int32\n  dtype = _execute.make_type(dtype, \"dtype\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"HistogramFixedWidth\", values=values, value_range=value_range,\n                               nbins=nbins, dtype=dtype, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"dtype\",\n              _op._get_attr_type(\"dtype\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"HistogramFixedWidth\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nHistogramFixedWidth = tf_export(\"raw_ops.HistogramFixedWidth\")(_ops.to_raw_op(_histogram_fixed_width))\n\n\ndef _histogram_fixed_width_eager_fallback(values, value_range, nbins, dtype, name, ctx):\n  if dtype is None:\n    dtype = _dtypes.int32\n  dtype = _execute.make_type(dtype, \"dtype\")\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([values, value_range], ctx, [_dtypes.int32, _dtypes.int64, _dtypes.float32, _dtypes.float64, ])\n  (values, value_range) = _inputs_T\n  nbins = _ops.convert_to_tensor(nbins, _dtypes.int32)\n  _inputs_flat = [values, value_range, nbins]\n  _attrs = (\"T\", _attr_T, \"dtype\", dtype)\n  _result = _execute.execute(b\"HistogramFixedWidth\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"HistogramFixedWidth\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.igamma', v1=['math.igamma', 'igamma'])\n@deprecated_endpoints('igamma')\ndef igamma(a, x, name=None):\n  r\"\"\"Compute the lower regularized incomplete Gamma function `P(a, x)`.\n\n  The lower regularized incomplete Gamma function is defined as:\n\n\n  \\\\(P(a, x) = gamma(a, x) / Gamma(a) = 1 - Q(a, x)\\\\)\n\n  where\n\n  \\\\(gamma(a, x) = \\\\int_{0}^{x} t^{a-1} exp(-t) dt\\\\)\n\n  is the lower incomplete Gamma function.\n\n  Note, above `Q(a, x)` (`Igammac`) is the upper regularized complete\n  Gamma function.\n\n  Args:\n    a: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    x: A `Tensor`. Must have the same type as `a`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `a`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Igamma\", name, a, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_igamma(\n          (a, x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return igamma_eager_fallback(\n          a, x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            igamma, (), dict(a=a, x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_igamma(\n        (a, x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Igamma\", a=a, x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          igamma, (), dict(a=a, x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Igamma\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nIgamma = tf_export(\"raw_ops.Igamma\")(_ops.to_raw_op(igamma))\n_dispatcher_for_igamma = igamma._tf_type_based_dispatcher.Dispatch\n\n\ndef igamma_eager_fallback(a, x, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([a, x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  (a, x) = _inputs_T\n  _inputs_flat = [a, x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Igamma\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Igamma\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef igamma_grad_a(a, x, name=None):\n  r\"\"\"Computes the gradient of `igamma(a, x)` wrt `a`.\n\n  Args:\n    a: A `Tensor`. Must be one of the following types: `float32`, `float64`.\n    x: A `Tensor`. Must have the same type as `a`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `a`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"IgammaGradA\", name, a, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return igamma_grad_a_eager_fallback(\n          a, x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"IgammaGradA\", a=a, x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"IgammaGradA\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nIgammaGradA = tf_export(\"raw_ops.IgammaGradA\")(_ops.to_raw_op(igamma_grad_a))\n\n\ndef igamma_grad_a_eager_fallback(a, x, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([a, x], ctx, [_dtypes.float32, _dtypes.float64, ])\n  (a, x) = _inputs_T\n  _inputs_flat = [a, x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"IgammaGradA\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"IgammaGradA\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.igammac', v1=['math.igammac', 'igammac'])\n@deprecated_endpoints('igammac')\ndef igammac(a, x, name=None):\n  r\"\"\"Compute the upper regularized incomplete Gamma function `Q(a, x)`.\n\n  The upper regularized incomplete Gamma function is defined as:\n\n  \\\\(Q(a, x) = Gamma(a, x) / Gamma(a) = 1 - P(a, x)\\\\)\n\n  where\n\n  \\\\(Gamma(a, x) = \\int_{x}^{\\infty} t^{a-1} exp(-t) dt\\\\)\n\n  is the upper incomplete Gamma function.\n\n  Note, above `P(a, x)` (`Igamma`) is the lower regularized complete\n  Gamma function.\n\n  Args:\n    a: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    x: A `Tensor`. Must have the same type as `a`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `a`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Igammac\", name, a, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_igammac(\n          (a, x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return igammac_eager_fallback(\n          a, x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            igammac, (), dict(a=a, x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_igammac(\n        (a, x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Igammac\", a=a, x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          igammac, (), dict(a=a, x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Igammac\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nIgammac = tf_export(\"raw_ops.Igammac\")(_ops.to_raw_op(igammac))\n_dispatcher_for_igammac = igammac._tf_type_based_dispatcher.Dispatch\n\n\ndef igammac_eager_fallback(a, x, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([a, x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  (a, x) = _inputs_T\n  _inputs_flat = [a, x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Igammac\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Igammac\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef imag(input, Tout=_dtypes.float32, name=None):\n  r\"\"\"Returns the imaginary part of a complex number.\n\n  Given a tensor `input` of complex numbers, this operation returns a tensor of\n  type `float` that is the imaginary part of each element in `input`. All\n  elements in `input` must be complex numbers of the form \\\\(a + bj\\\\), where *a*\n  is the real part and *b* is the imaginary part returned by this operation.\n\n  For example:\n\n  ```\n  # tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\n  tf.imag(input) ==> [4.75, 5.75]\n  ```\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `complex64`, `complex128`.\n    Tout: An optional `tf.DType` from: `tf.float32, tf.float64`. Defaults to `tf.float32`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `Tout`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Imag\", name, input, \"Tout\", Tout)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return imag_eager_fallback(\n          input, Tout=Tout, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if Tout is None:\n    Tout = _dtypes.float32\n  Tout = _execute.make_type(Tout, \"Tout\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Imag\", input=input, Tout=Tout, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tout\",\n              _op._get_attr_type(\"Tout\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Imag\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nImag = tf_export(\"raw_ops.Imag\")(_ops.to_raw_op(imag))\n\n\ndef imag_eager_fallback(input, Tout, name, ctx):\n  if Tout is None:\n    Tout = _dtypes.float32\n  Tout = _execute.make_type(Tout, \"Tout\")\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.complex64, _dtypes.complex128, ], _dtypes.complex64)\n  _inputs_flat = [input]\n  _attrs = (\"T\", _attr_T, \"Tout\", Tout)\n  _result = _execute.execute(b\"Imag\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Imag\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef inv(x, name=None):\n  r\"\"\"Computes the reciprocal of x element-wise.\n\n  I.e., \\\\(y = 1 / x\\\\).\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Inv\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return inv_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Inv\", x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Inv\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nInv = tf_export(\"raw_ops.Inv\")(_ops.to_raw_op(inv))\n\n\ndef inv_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Inv\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Inv\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef inv_grad(y, dy, name=None):\n  r\"\"\"Computes the gradient for the inverse of `x` wrt its input.\n\n  Specifically, `grad = -dy * y*y`, where `y = 1/x`, and `dy`\n  is the corresponding input gradient.\n\n  Args:\n    y: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    dy: A `Tensor`. Must have the same type as `y`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `y`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"InvGrad\", name, y, dy)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return inv_grad_eager_fallback(\n          y, dy, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"InvGrad\", y=y, dy=dy, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"InvGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nInvGrad = tf_export(\"raw_ops.InvGrad\")(_ops.to_raw_op(inv_grad))\n\n\ndef inv_grad_eager_fallback(y, dy, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([y, dy], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  (y, dy) = _inputs_T\n  _inputs_flat = [y, dy]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"InvGrad\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"InvGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.is_finite', v1=['math.is_finite', 'debugging.is_finite', 'is_finite'])\n@deprecated_endpoints('debugging.is_finite', 'is_finite')\ndef is_finite(x, name=None):\n  r\"\"\"Returns which elements of x are finite.\n\n  @compatibility(numpy)\n  Equivalent to np.isfinite\n  @end_compatibility\n\n  Example:\n\n  ```python\n  x = tf.constant([5.0, 4.8, 6.8, np.inf, np.nan])\n  tf.math.is_finite(x) ==> [True, True, True, False, False]\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"IsFinite\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_is_finite(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return is_finite_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            is_finite, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_is_finite(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"IsFinite\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          is_finite, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"IsFinite\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nIsFinite = tf_export(\"raw_ops.IsFinite\")(_ops.to_raw_op(is_finite))\n_dispatcher_for_is_finite = is_finite._tf_type_based_dispatcher.Dispatch\n\n\ndef is_finite_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"IsFinite\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"IsFinite\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.is_inf', v1=['math.is_inf', 'debugging.is_inf', 'is_inf'])\n@deprecated_endpoints('debugging.is_inf', 'is_inf')\ndef is_inf(x, name=None):\n  r\"\"\"Returns which elements of x are Inf.\n\n  @compatibility(numpy)\n  Equivalent to np.isinf\n  @end_compatibility\n\n  Example:\n\n  ```python\n  x = tf.constant([5.0, np.inf, 6.8, np.inf])\n  tf.math.is_inf(x) ==> [False, True, False, True]\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"IsInf\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_is_inf(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return is_inf_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            is_inf, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_is_inf(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"IsInf\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          is_inf, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"IsInf\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nIsInf = tf_export(\"raw_ops.IsInf\")(_ops.to_raw_op(is_inf))\n_dispatcher_for_is_inf = is_inf._tf_type_based_dispatcher.Dispatch\n\n\ndef is_inf_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"IsInf\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"IsInf\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.is_nan', v1=['math.is_nan', 'debugging.is_nan', 'is_nan'])\n@deprecated_endpoints('debugging.is_nan', 'is_nan')\ndef is_nan(x, name=None):\n  r\"\"\"Returns which elements of x are NaN.\n\n  @compatibility(numpy)\n  Equivalent to np.isnan\n  @end_compatibility\n\n  Example:\n\n  ```python\n  x = tf.constant([5.0, np.nan, 6.8, np.nan, np.inf])\n  tf.math.is_nan(x) ==> [False, True, False, True, False]\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"IsNan\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_is_nan(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return is_nan_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            is_nan, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_is_nan(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"IsNan\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          is_nan, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"IsNan\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nIsNan = tf_export(\"raw_ops.IsNan\")(_ops.to_raw_op(is_nan))\n_dispatcher_for_is_nan = is_nan._tf_type_based_dispatcher.Dispatch\n\n\ndef is_nan_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"IsNan\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"IsNan\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.less', 'less')\ndef less(x, y, name=None):\n  r\"\"\"Returns the truth value of (x < y) element-wise.\n\n  *NOTE*: `math.less` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Example:\n\n  ```python\n  x = tf.constant([5, 4, 6])\n  y = tf.constant([5])\n  tf.math.less(x, y) ==> [False, True, False]\n\n  x = tf.constant([5, 4, 6])\n  y = tf.constant([5, 6, 7])\n  tf.math.less(x, y) ==> [False, True, True]\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Less\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_less(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return less_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            less, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_less(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Less\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          less, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Less\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nLess = tf_export(\"raw_ops.Less\")(_ops.to_raw_op(less))\n_dispatcher_for_less = less._tf_type_based_dispatcher.Dispatch\n\n\ndef less_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.int64, _dtypes.bfloat16, _dtypes.uint16, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Less\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Less\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.less_equal', 'less_equal')\ndef less_equal(x, y, name=None):\n  r\"\"\"Returns the truth value of (x <= y) element-wise.\n\n  *NOTE*: `math.less_equal` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Example:\n\n  ```python\n  x = tf.constant([5, 4, 6])\n  y = tf.constant([5])\n  tf.math.less_equal(x, y) ==> [True, True, False]\n\n  x = tf.constant([5, 4, 6])\n  y = tf.constant([5, 6, 6])\n  tf.math.less_equal(x, y) ==> [True, True, True]\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"LessEqual\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_less_equal(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return less_equal_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            less_equal, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_less_equal(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"LessEqual\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          less_equal, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"LessEqual\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nLessEqual = tf_export(\"raw_ops.LessEqual\")(_ops.to_raw_op(less_equal))\n_dispatcher_for_less_equal = less_equal._tf_type_based_dispatcher.Dispatch\n\n\ndef less_equal_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.int64, _dtypes.bfloat16, _dtypes.uint16, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"LessEqual\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"LessEqual\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.lgamma', v1=['math.lgamma', 'lgamma'])\n@deprecated_endpoints('lgamma')\ndef lgamma(x, name=None):\n  r\"\"\"Computes the log of the absolute value of `Gamma(x)` element-wise.\n\n    For positive numbers, this function computes log((input - 1)!) for every element in the tensor.\n    `lgamma(5) = log((5-1)!) = log(4!) = log(24) = 3.1780539`\n\n  Example:\n\n  ```python\n  x = tf.constant([0, 0.5, 1, 4.5, -4, -5.6])\n  tf.math.lgamma(x) ==> [inf, 0.5723649, 0., 2.4537368, inf, -4.6477685]\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Lgamma\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_lgamma(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return lgamma_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            lgamma, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_lgamma(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Lgamma\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          lgamma, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Lgamma\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nLgamma = tf_export(\"raw_ops.Lgamma\")(_ops.to_raw_op(lgamma))\n_dispatcher_for_lgamma = lgamma._tf_type_based_dispatcher.Dispatch\n\n\ndef lgamma_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Lgamma\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Lgamma\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef lin_space(start, stop, num, name=None):\n  r\"\"\"Generates values in an interval.\n\n  A sequence of `num` evenly-spaced values are generated beginning at `start`.\n  If `num > 1`, the values in the sequence increase by `stop - start / num - 1`,\n  so that the last one is exactly `stop`.\n\n  For example:\n\n  ```\n  tf.linspace(10.0, 12.0, 3, name=\"linspace\") => [ 10.0  11.0  12.0]\n  ```\n\n  Args:\n    start: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n      0-D tensor. First entry in the range.\n    stop: A `Tensor`. Must have the same type as `start`.\n      0-D tensor. Last entry in the range.\n    num: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      0-D tensor. Number of values to generate.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `start`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"LinSpace\", name, start, stop, num)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return lin_space_eager_fallback(\n          start, stop, num, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"LinSpace\", start=start, stop=stop, num=num, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"LinSpace\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nLinSpace = tf_export(\"raw_ops.LinSpace\")(_ops.to_raw_op(lin_space))\n\n\ndef lin_space_eager_fallback(start, stop, num, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([start, stop], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  (start, stop) = _inputs_T\n  _attr_Tidx, (num,) = _execute.args_to_matching_eager([num], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [start, stop, num]\n  _attrs = (\"T\", _attr_T, \"Tidx\", _attr_Tidx)\n  _result = _execute.execute(b\"LinSpace\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"LinSpace\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.log', v1=['math.log', 'log'])\n@deprecated_endpoints('log')\ndef log(x, name=None):\n  r\"\"\"Computes natural logarithm of x element-wise.\n\n  I.e., \\\\(y = \\log_e x\\\\).\n\n  Example:\n  >>> x = tf.constant([0, 0.5, 1, 5])\n  >>> tf.math.log(x)\n  <tf.Tensor: shape=(4,), dtype=float32, numpy=array([      -inf, -0.6931472,  0.       ,  1.609438 ], dtype=float32)>\n\n  See: https://en.wikipedia.org/wiki/Logarithm\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Log\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_log(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return log_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            log, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_log(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Log\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          log, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Log\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nLog = tf_export(\"raw_ops.Log\")(_ops.to_raw_op(log))\n_dispatcher_for_log = log._tf_type_based_dispatcher.Dispatch\n\n\ndef log_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Log\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Log\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.log1p', v1=['math.log1p', 'log1p'])\n@deprecated_endpoints('log1p')\ndef log1p(x, name=None):\n  r\"\"\"Computes natural logarithm of (1 + x) element-wise.\n\n  I.e., \\\\(y = \\log_e (1 + x)\\\\).\n\n  Example:\n  >>> x = tf.constant([0, 0.5, 1, 5])\n  >>> tf.math.log1p(x)\n  <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.       , 0.4054651, 0.6931472, 1.7917595], dtype=float32)>\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Log1p\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_log1p(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return log1p_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            log1p, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_log1p(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Log1p\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          log1p, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Log1p\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nLog1p = tf_export(\"raw_ops.Log1p\")(_ops.to_raw_op(log1p))\n_dispatcher_for_log1p = log1p._tf_type_based_dispatcher.Dispatch\n\n\ndef log1p_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Log1p\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Log1p\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.logical_and', 'logical_and')\ndef logical_and(x, y, name=None):\n  r\"\"\"Returns the truth value of x AND y element-wise.\n\n  Logical AND function.\n\n  Requires that `x` and `y` have the same shape or have\n  [broadcast-compatible](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n  shapes. For example, `x` and `y` can be:\n\n    - Two single elements of type `bool`.\n    - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n      be calculated by applying logical AND with the single element to each\n      element in the larger Tensor.\n    - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n      the result will be the element-wise logical AND of the two input tensors.\n\n  You can also use the `&` operator instead.\n\n  Usage:\n\n    >>> a = tf.constant([True])\n    >>> b = tf.constant([False])\n    >>> tf.math.logical_and(a, b)\n    <tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>\n    >>> a & b\n    <tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>\n\n    >>> c = tf.constant([True])\n    >>> x = tf.constant([False, True, True, False])\n    >>> tf.math.logical_and(c, x)\n    <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n    >>> c & x\n    <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n\n    >>> y = tf.constant([False, False, True, True])\n    >>> z = tf.constant([False, True, False, True])\n    >>> tf.math.logical_and(y, z)\n    <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False, True])>\n    >>> y & z\n    <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False, True])>\n\n    This op also supports broadcasting\n\n    >>> tf.logical_and([[True, False]], [[True], [False]])\n    <tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n      array([[ True, False],\n             [False, False]])>\n\n  The reduction version of this elementwise operation is `tf.math.reduce_all`.\n\n  Args:\n      x: A `tf.Tensor` of type bool.\n      y: A `tf.Tensor` of type bool.\n      name: A name for the operation (optional).\n\n  Returns:\n    A `tf.Tensor` of type bool with the shape that `x` and `y` broadcast to.\n\n  Args:\n    x: A `Tensor` of type `bool`.\n    y: A `Tensor` of type `bool`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"LogicalAnd\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_logical_and(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return logical_and_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            logical_and, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_logical_and(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"LogicalAnd\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          logical_and, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = ()\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"LogicalAnd\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nLogicalAnd = tf_export(\"raw_ops.LogicalAnd\")(_ops.to_raw_op(logical_and))\n_dispatcher_for_logical_and = logical_and._tf_type_based_dispatcher.Dispatch\n\n\ndef logical_and_eager_fallback(x, y, name, ctx):\n  x = _ops.convert_to_tensor(x, _dtypes.bool)\n  y = _ops.convert_to_tensor(y, _dtypes.bool)\n  _inputs_flat = [x, y]\n  _attrs = None\n  _result = _execute.execute(b\"LogicalAnd\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"LogicalAnd\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.logical_not', 'logical_not')\ndef logical_not(x, name=None):\n  r\"\"\"Returns the truth value of `NOT x` element-wise.\n\n  Example:\n\n  >>> tf.math.logical_not(tf.constant([True, False]))\n  <tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n\n  Args:\n    x: A `Tensor` of type `bool`. A `Tensor` of type `bool`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"LogicalNot\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_logical_not(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return logical_not_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            logical_not, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_logical_not(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"LogicalNot\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          logical_not, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = ()\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"LogicalNot\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nLogicalNot = tf_export(\"raw_ops.LogicalNot\")(_ops.to_raw_op(logical_not))\n_dispatcher_for_logical_not = logical_not._tf_type_based_dispatcher.Dispatch\n\n\ndef logical_not_eager_fallback(x, name, ctx):\n  x = _ops.convert_to_tensor(x, _dtypes.bool)\n  _inputs_flat = [x]\n  _attrs = None\n  _result = _execute.execute(b\"LogicalNot\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"LogicalNot\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.logical_or', 'logical_or')\ndef logical_or(x, y, name=None):\n  r\"\"\"Returns the truth value of x OR y element-wise.\n\n  Logical OR function.\n\n  Requires that `x` and `y` have the same shape or have\n  [broadcast-compatible](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n  shapes. For example, `x` and `y` can be:\n\n  - Two single elements of type `bool`.\n  - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n    be calculated by applying logical OR with the single element to each\n    element in the larger Tensor.\n  - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n    the result will be the element-wise logical OR of the two input tensors.\n\n  You can also use the `|` operator instead.\n\n  Usage:\n\n    >>> a = tf.constant([True])\n    >>> b = tf.constant([False])\n    >>> tf.math.logical_or(a, b)\n    <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n    >>> a | b\n    <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n\n    >>> c = tf.constant([False])\n    >>> x = tf.constant([False, True, True, False])\n    >>> tf.math.logical_or(c, x)\n    <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>\n    >>> c | x\n    <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True,  True, False])>\n\n    >>> y = tf.constant([False, False, True, True])\n    >>> z = tf.constant([False, True, False, True])\n    >>> tf.math.logical_or(y, z)\n    <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>\n    >>> y | z\n    <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, True, True, True])>\n\n    This op also supports broadcasting\n\n    >>> tf.logical_or([[True, False]], [[True], [False]])\n    <tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n    array([[ True,  True],\n         [ True, False]])>\n\n  The reduction version of this elementwise operation is `tf.math.reduce_any`.\n\n  Args:\n      x: A `tf.Tensor` of type bool.\n      y: A `tf.Tensor` of type bool.\n      name: A name for the operation (optional).\n\n  Returns:\n    A `tf.Tensor` of type bool with the shape that `x` and `y` broadcast to.\n\n  Args:\n    x: A `Tensor` of type `bool`.\n    y: A `Tensor` of type `bool`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"LogicalOr\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_logical_or(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return logical_or_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            logical_or, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_logical_or(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"LogicalOr\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          logical_or, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = ()\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"LogicalOr\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nLogicalOr = tf_export(\"raw_ops.LogicalOr\")(_ops.to_raw_op(logical_or))\n_dispatcher_for_logical_or = logical_or._tf_type_based_dispatcher.Dispatch\n\n\ndef logical_or_eager_fallback(x, y, name, ctx):\n  x = _ops.convert_to_tensor(x, _dtypes.bool)\n  y = _ops.convert_to_tensor(y, _dtypes.bool)\n  _inputs_flat = [x, y]\n  _attrs = None\n  _result = _execute.execute(b\"LogicalOr\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"LogicalOr\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef mat_mul(a, b, transpose_a=False, transpose_b=False, name=None):\n  r\"\"\"Multiply the matrix \"a\" by the matrix \"b\".\n\n  The inputs must be two-dimensional matrices and the inner dimension of\n  \"a\" (after being transposed if transpose_a is true) must match the\n  outer dimension of \"b\" (after being transposed if transposed_b is\n  true).\n\n  *Note*: The default kernel implementation for MatMul on GPUs uses\n  cublas.\n\n  Args:\n    a: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n    b: A `Tensor`. Must have the same type as `a`.\n    transpose_a: An optional `bool`. Defaults to `False`.\n      If true, \"a\" is transposed before multiplication.\n    transpose_b: An optional `bool`. Defaults to `False`.\n      If true, \"b\" is transposed before multiplication.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `a`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"MatMul\", name, a, b, \"transpose_a\", transpose_a, \"transpose_b\",\n        transpose_b)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return mat_mul_eager_fallback(\n          a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name,\n          ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if transpose_a is None:\n    transpose_a = False\n  transpose_a = _execute.make_bool(transpose_a, \"transpose_a\")\n  if transpose_b is None:\n    transpose_b = False\n  transpose_b = _execute.make_bool(transpose_b, \"transpose_b\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"MatMul\", a=a, b=b, transpose_a=transpose_a, transpose_b=transpose_b,\n                  name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"transpose_a\", _op._get_attr_bool(\"transpose_a\"), \"transpose_b\",\n              _op._get_attr_bool(\"transpose_b\"), \"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"MatMul\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nMatMul = tf_export(\"raw_ops.MatMul\")(_ops.to_raw_op(mat_mul))\n\n\ndef mat_mul_eager_fallback(a, b, transpose_a, transpose_b, name, ctx):\n  if transpose_a is None:\n    transpose_a = False\n  transpose_a = _execute.make_bool(transpose_a, \"transpose_a\")\n  if transpose_b is None:\n    transpose_b = False\n  transpose_b = _execute.make_bool(transpose_b, \"transpose_b\")\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([a, b], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  (a, b) = _inputs_T\n  _inputs_flat = [a, b]\n  _attrs = (\"transpose_a\", transpose_a, \"transpose_b\", transpose_b, \"T\",\n  _attr_T)\n  _result = _execute.execute(b\"MatMul\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"MatMul\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef _max(input, axis, keep_dims=False, name=None):\n  r\"\"\"Computes the maximum of elements across dimensions of a tensor.\n\n  Reduces `input` along the dimensions given in `axis`. Unless\n  `keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n  `axis`. If `keep_dims` is true, the reduced dimensions are\n  retained with length 1.\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`, `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n      The tensor to reduce.\n    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      The dimensions to reduce. Must be in the range\n      `[-rank(input), rank(input))`.\n    keep_dims: An optional `bool`. Defaults to `False`.\n      If true, retain reduced dimensions with length 1.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `input`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Max\", name, input, axis, \"keep_dims\", keep_dims)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return _max_eager_fallback(\n          input, axis, keep_dims=keep_dims, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Max\", input=input, reduction_indices=axis, keep_dims=keep_dims,\n               name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"keep_dims\", _op._get_attr_bool(\"keep_dims\"), \"T\",\n              _op._get_attr_type(\"T\"), \"Tidx\", _op._get_attr_type(\"Tidx\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Max\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nMax = tf_export(\"raw_ops.Max\")(_ops.to_raw_op(_max))\n\n\ndef _max_eager_fallback(input, axis, keep_dims, name, ctx):\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.int64, _dtypes.bfloat16, _dtypes.uint16, _dtypes.half, _dtypes.uint32, _dtypes.uint64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.qint16, _dtypes.quint16, ])\n  _attr_Tidx, (axis,) = _execute.args_to_matching_eager([axis], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [input, axis]\n  _attrs = (\"keep_dims\", keep_dims, \"T\", _attr_T, \"Tidx\", _attr_Tidx)\n  _result = _execute.execute(b\"Max\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Max\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.maximum', 'maximum')\ndef maximum(x, y, name=None):\n  r\"\"\"Returns the max of x and y (i.e. x > y ? x : y) element-wise.\n\n  Example:\n\n  >>> x = tf.constant([0., 0., 0., 0.])\n  >>> y = tf.constant([-2., 0., 2., 5.])\n  >>> tf.math.maximum(x, y)\n  <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>\n\n  Note that `maximum` supports [broadcast semantics](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) for `x` and `y`.\n\n  >>> x = tf.constant([-5., 0., 0., 0.])\n  >>> y = tf.constant([-3.])\n  >>> tf.math.maximum(x, y)\n  <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-3., 0., 0., 0.], dtype=float32)>\n\n  The reduction version of this elementwise operation is `tf.math.reduce_max`\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `uint32`, `int64`, `uint64`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Maximum\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_maximum(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return maximum_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            maximum, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_maximum(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Maximum\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          maximum, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Maximum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nMaximum = tf_export(\"raw_ops.Maximum\")(_ops.to_raw_op(maximum))\n_dispatcher_for_maximum = maximum._tf_type_based_dispatcher.Dispatch\n\n\ndef maximum_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.uint8, _dtypes.int16, _dtypes.uint16, _dtypes.int32, _dtypes.uint32, _dtypes.int64, _dtypes.uint64, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Maximum\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Maximum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef mean(input, axis, keep_dims=False, name=None):\n  r\"\"\"Computes the mean of elements across dimensions of a tensor.\n\n  Reduces `input` along the dimensions given in `axis`. Unless\n  `keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n  `axis`. If `keep_dims` is true, the reduced dimensions are\n  retained with length 1.\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n      The tensor to reduce.\n    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      The dimensions to reduce. Must be in the range\n      `[-rank(input), rank(input))`.\n    keep_dims: An optional `bool`. Defaults to `False`.\n      If true, retain reduced dimensions with length 1.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `input`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Mean\", name, input, axis, \"keep_dims\", keep_dims)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return mean_eager_fallback(\n          input, axis, keep_dims=keep_dims, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Mean\", input=input, reduction_indices=axis, keep_dims=keep_dims,\n                name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"keep_dims\", _op._get_attr_bool(\"keep_dims\"), \"T\",\n              _op._get_attr_type(\"T\"), \"Tidx\", _op._get_attr_type(\"Tidx\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Mean\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nMean = tf_export(\"raw_ops.Mean\")(_ops.to_raw_op(mean))\n\n\ndef mean_eager_fallback(input, axis, keep_dims, name, ctx):\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tidx, (axis,) = _execute.args_to_matching_eager([axis], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [input, axis]\n  _attrs = (\"keep_dims\", keep_dims, \"T\", _attr_T, \"Tidx\", _attr_Tidx)\n  _result = _execute.execute(b\"Mean\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Mean\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef _min(input, axis, keep_dims=False, name=None):\n  r\"\"\"Computes the minimum of elements across dimensions of a tensor.\n\n  Reduces `input` along the dimensions given in `axis`. Unless\n  `keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n  `axis`. If `keep_dims` is true, the reduced dimensions are\n  retained with length 1.\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`, `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n      The tensor to reduce.\n    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      The dimensions to reduce. Must be in the range\n      `[-rank(input), rank(input))`.\n    keep_dims: An optional `bool`. Defaults to `False`.\n      If true, retain reduced dimensions with length 1.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `input`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Min\", name, input, axis, \"keep_dims\", keep_dims)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return _min_eager_fallback(\n          input, axis, keep_dims=keep_dims, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Min\", input=input, reduction_indices=axis, keep_dims=keep_dims,\n               name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"keep_dims\", _op._get_attr_bool(\"keep_dims\"), \"T\",\n              _op._get_attr_type(\"T\"), \"Tidx\", _op._get_attr_type(\"Tidx\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Min\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nMin = tf_export(\"raw_ops.Min\")(_ops.to_raw_op(_min))\n\n\ndef _min_eager_fallback(input, axis, keep_dims, name, ctx):\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.int64, _dtypes.bfloat16, _dtypes.uint16, _dtypes.half, _dtypes.uint32, _dtypes.uint64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.qint16, _dtypes.quint16, ])\n  _attr_Tidx, (axis,) = _execute.args_to_matching_eager([axis], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [input, axis]\n  _attrs = (\"keep_dims\", keep_dims, \"T\", _attr_T, \"Tidx\", _attr_Tidx)\n  _result = _execute.execute(b\"Min\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Min\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.minimum', 'minimum')\ndef minimum(x, y, name=None):\n  r\"\"\"Returns the min of x and y (i.e. x < y ? x : y) element-wise.\n\n  Both inputs are number-type tensors (except complex).  `minimum` expects that\n  both tensors have the same `dtype`.\n\n  Examples:\n\n  >>> x = tf.constant([0., 0., 0., 0.])\n  >>> y = tf.constant([-5., -2., 0., 3.])\n  >>> tf.math.minimum(x, y)\n  <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>\n\n  Note that `minimum` supports [broadcast semantics](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) for `x` and `y`.\n\n  >>> x = tf.constant([-5., 0., 0., 0.])\n  >>> y = tf.constant([-3.])\n  >>> tf.math.minimum(x, y)\n  <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -3., -3., -3.], dtype=float32)>\n\n  The reduction version of this elementwise operation is `tf.math.reduce_min`\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `uint32`, `int64`, `uint64`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Minimum\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_minimum(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return minimum_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            minimum, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_minimum(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Minimum\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          minimum, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Minimum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nMinimum = tf_export(\"raw_ops.Minimum\")(_ops.to_raw_op(minimum))\n_dispatcher_for_minimum = minimum._tf_type_based_dispatcher.Dispatch\n\n\ndef minimum_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.uint8, _dtypes.int16, _dtypes.uint16, _dtypes.int32, _dtypes.uint32, _dtypes.int64, _dtypes.uint64, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Minimum\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Minimum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef mod(x, y, name=None):\n  r\"\"\"Returns element-wise remainder of division. This emulates C semantics in that\n\n  the result here is consistent with a truncating divide. E.g.\n  `tf.truncatediv(x, y) * y + truncate_mod(x, y) = x`.\n\n  *NOTE*: `Mod` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `half`, `half`, `bfloat16`, `float32`, `float64`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Mod\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return mod_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Mod\", x=x, y=y, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Mod\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nMod = tf_export(\"raw_ops.Mod\")(_ops.to_raw_op(mod))\n\n\ndef mod_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.int32, _dtypes.int64, _dtypes.half, _dtypes.half, _dtypes.bfloat16, _dtypes.float32, _dtypes.float64, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Mod\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Mod\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef mul(x, y, name=None):\n  r\"\"\"Returns x * y element-wise.\n\n  *NOTE*: `Multiply` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `uint32`, `uint64`, `int64`, `complex64`, `complex128`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Mul\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return mul_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Mul\", x=x, y=y, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Mul\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nMul = tf_export(\"raw_ops.Mul\")(_ops.to_raw_op(mul))\n\n\ndef mul_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.uint8, _dtypes.int8, _dtypes.uint16, _dtypes.int16, _dtypes.int32, _dtypes.uint32, _dtypes.uint64, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Mul\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Mul\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef mul_no_nan(x, y, name=None):\n  r\"\"\"Returns x * y element-wise. Returns zero if y is zero, even if x if infinite or NaN.\n\n  *NOTE*: `MulNoNan` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"MulNoNan\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return mul_no_nan_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"MulNoNan\", x=x, y=y, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"MulNoNan\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nMulNoNan = tf_export(\"raw_ops.MulNoNan\")(_ops.to_raw_op(mul_no_nan))\n\n\ndef mul_no_nan_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"MulNoNan\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"MulNoNan\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef ndtri(x, name=None):\n  r\"\"\"TODO: add doc.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Ndtri\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return ndtri_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Ndtri\", x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Ndtri\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nNdtri = tf_export(\"raw_ops.Ndtri\")(_ops.to_raw_op(ndtri))\n\n\ndef ndtri_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Ndtri\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Ndtri\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.negative', 'negative')\ndef neg(x, name=None):\n  r\"\"\"Computes numerical negative value element-wise.\n\n  I.e., \\\\(y = -x\\\\).\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Neg\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_neg(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return neg_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            neg, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_neg(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Neg\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          neg, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Neg\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nNeg = tf_export(\"raw_ops.Neg\")(_ops.to_raw_op(neg))\n_dispatcher_for_neg = neg._tf_type_based_dispatcher.Dispatch\n\n\ndef neg_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Neg\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Neg\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.nextafter')\ndef next_after(x1, x2, name=None):\n  r\"\"\"Returns the next representable value of `x1` in the direction of `x2`, element-wise.\n\n  This operation returns the same result as the C++ std::nextafter function.\n\n  It can also return a subnormal number.\n\n  @compatibility(cpp)\n  Equivalent to C++ std::nextafter function.\n  @end_compatibility\n\n  Args:\n    x1: A `Tensor`. Must be one of the following types: `float64`, `float32`.\n    x2: A `Tensor`. Must have the same type as `x1`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x1`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"NextAfter\", name, x1, x2)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_next_after(\n          (x1, x2, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return next_after_eager_fallback(\n          x1, x2, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            next_after, (), dict(x1=x1, x2=x2, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_next_after(\n        (x1, x2, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"NextAfter\", x1=x1, x2=x2, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          next_after, (), dict(x1=x1, x2=x2, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"NextAfter\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nNextAfter = tf_export(\"raw_ops.NextAfter\")(_ops.to_raw_op(next_after))\n_dispatcher_for_next_after = next_after._tf_type_based_dispatcher.Dispatch\n\n\ndef next_after_eager_fallback(x1, x2, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x1, x2], ctx, [_dtypes.float64, _dtypes.float32, ], _dtypes.float32)\n  (x1, x2) = _inputs_T\n  _inputs_flat = [x1, x2]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"NextAfter\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"NextAfter\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef not_equal(x, y, incompatible_shape_error=True, name=None):\n  r\"\"\"Returns the truth value of (x != y) element-wise.\n\n  *NOTE*: `NotEqual` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`.\n    y: A `Tensor`. Must have the same type as `x`.\n    incompatible_shape_error: An optional `bool`. Defaults to `True`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `bool`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"NotEqual\", name, x, y, \"incompatible_shape_error\",\n        incompatible_shape_error)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return not_equal_eager_fallback(\n          x, y, incompatible_shape_error=incompatible_shape_error, name=name,\n          ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if incompatible_shape_error is None:\n    incompatible_shape_error = True\n  incompatible_shape_error = _execute.make_bool(incompatible_shape_error, \"incompatible_shape_error\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"NotEqual\", x=x, y=y,\n                    incompatible_shape_error=incompatible_shape_error,\n                    name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"incompatible_shape_error\",\n              _op._get_attr_bool(\"incompatible_shape_error\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"NotEqual\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nNotEqual = tf_export(\"raw_ops.NotEqual\")(_ops.to_raw_op(not_equal))\n\n\ndef not_equal_eager_fallback(x, y, incompatible_shape_error, name, ctx):\n  if incompatible_shape_error is None:\n    incompatible_shape_error = True\n  incompatible_shape_error = _execute.make_bool(incompatible_shape_error, \"incompatible_shape_error\")\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T, \"incompatible_shape_error\",\n  incompatible_shape_error)\n  _result = _execute.execute(b\"NotEqual\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"NotEqual\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.polygamma', v1=['math.polygamma', 'polygamma'])\n@deprecated_endpoints('polygamma')\ndef polygamma(a, x, name=None):\n  r\"\"\"Compute the polygamma function \\\\(\\psi^{(n)}(x)\\\\).\n\n  The polygamma function is defined as:\n\n\n  \\\\(\\psi^{(a)}(x) = \\frac{d^a}{dx^a} \\psi(x)\\\\)\n\n  where \\\\(\\psi(x)\\\\) is the digamma function.\n  The polygamma function is defined only for non-negative integer orders \\\\a\\\\.\n\n  Args:\n    a: A `Tensor`. Must be one of the following types: `float32`, `float64`.\n    x: A `Tensor`. Must have the same type as `a`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `a`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Polygamma\", name, a, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_polygamma(\n          (a, x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return polygamma_eager_fallback(\n          a, x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            polygamma, (), dict(a=a, x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_polygamma(\n        (a, x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Polygamma\", a=a, x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          polygamma, (), dict(a=a, x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Polygamma\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nPolygamma = tf_export(\"raw_ops.Polygamma\")(_ops.to_raw_op(polygamma))\n_dispatcher_for_polygamma = polygamma._tf_type_based_dispatcher.Dispatch\n\n\ndef polygamma_eager_fallback(a, x, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([a, x], ctx, [_dtypes.float32, _dtypes.float64, ])\n  (a, x) = _inputs_T\n  _inputs_flat = [a, x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Polygamma\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Polygamma\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef _pow(x, y, name=None):\n  r\"\"\"Computes the power of one value to another.\n\n  Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n  corresponding elements in `x` and `y`. For example:\n\n  ```\n  # tensor 'x' is [[2, 2]], [3, 3]]\n  # tensor 'y' is [[8, 16], [2, 3]]\n  tf.pow(x, y) ==> [[256, 65536], [9, 27]]\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `float32`, `half`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Pow\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return _pow_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Pow\", x=x, y=y, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Pow\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nPow = tf_export(\"raw_ops.Pow\")(_ops.to_raw_op(_pow))\n\n\ndef _pow_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.float32, _dtypes.half, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Pow\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Pow\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef prod(input, axis, keep_dims=False, name=None):\n  r\"\"\"Computes the product of elements across dimensions of a tensor.\n\n  Reduces `input` along the dimensions given in `axis`. Unless\n  `keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n  `axis`. If `keep_dims` is true, the reduced dimensions are\n  retained with length 1.\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n      The tensor to reduce.\n    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      The dimensions to reduce. Must be in the range\n      `[-rank(input), rank(input))`.\n    keep_dims: An optional `bool`. Defaults to `False`.\n      If true, retain reduced dimensions with length 1.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `input`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Prod\", name, input, axis, \"keep_dims\", keep_dims)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return prod_eager_fallback(\n          input, axis, keep_dims=keep_dims, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Prod\", input=input, reduction_indices=axis, keep_dims=keep_dims,\n                name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"keep_dims\", _op._get_attr_bool(\"keep_dims\"), \"T\",\n              _op._get_attr_type(\"T\"), \"Tidx\", _op._get_attr_type(\"Tidx\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Prod\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nProd = tf_export(\"raw_ops.Prod\")(_ops.to_raw_op(prod))\n\n\ndef prod_eager_fallback(input, axis, keep_dims, name, ctx):\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tidx, (axis,) = _execute.args_to_matching_eager([axis], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [input, axis]\n  _attrs = (\"keep_dims\", keep_dims, \"T\", _attr_T, \"Tidx\", _attr_Tidx)\n  _result = _execute.execute(b\"Prod\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Prod\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n_QuantizeDownAndShrinkRangeOutput = collections.namedtuple(\n    \"QuantizeDownAndShrinkRange\",\n    [\"output\", \"output_min\", \"output_max\"])\n\n\ndef quantize_down_and_shrink_range(input, input_min, input_max, out_type, name=None):\n  r\"\"\"Convert the quantized 'input' tensor into a lower-precision 'output', using the\n\n  actual distribution of the values to maximize the usage of the lower bit depth\n  and adjusting the output min and max ranges accordingly.\n\n  [input_min, input_max] are scalar floats that specify the range for the float\n  interpretation of the 'input' data. For example, if input_min is -1.0f and\n  input_max is 1.0f, and we are dealing with quint16 quantized data, then a 0\n  value in the 16-bit data should be interpreted as -1.0f, and a 65535 means 1.0f.\n\n  This operator tries to squeeze as much precision as possible into an output with\n  a lower bit depth by calculating the actual min and max values found in the\n  data. For example, maybe that quint16 input has no values lower than 16,384 and\n  none higher than 49,152. That means only half the range is actually needed, all\n  the float interpretations are between -0.5f and 0.5f, so if we want to compress\n  the data into a quint8 output, we can use that range rather than the theoretical\n  -1.0f to 1.0f that is suggested by the input min and max.\n\n  In practice, this is most useful for taking output from operations like\n  QuantizedMatMul that can produce higher bit-depth outputs than their inputs and\n  may have large potential output ranges, but in practice have a distribution of\n  input values that only uses a small fraction of the possible range. By feeding\n  that output into this operator, we can reduce it from 32 bits down to 8 with\n  minimal loss of accuracy.\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n    input_min: A `Tensor` of type `float32`.\n      The float value that the minimum quantized input value represents.\n    input_max: A `Tensor` of type `float32`.\n      The float value that the maximum quantized input value represents.\n    out_type: A `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`.\n      The type of the output. Should be a lower bit depth than Tinput.\n    name: A name for the operation (optional).\n\n  Returns:\n    A tuple of `Tensor` objects (output, output_min, output_max).\n\n    output: A `Tensor` of type `out_type`.\n    output_min: A `Tensor` of type `float32`.\n    output_max: A `Tensor` of type `float32`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"QuantizeDownAndShrinkRange\", name, input, input_min, input_max,\n        \"out_type\", out_type)\n      _result = _QuantizeDownAndShrinkRangeOutput._make(_result)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return quantize_down_and_shrink_range_eager_fallback(\n          input, input_min, input_max, out_type=out_type, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  out_type = _execute.make_type(out_type, \"out_type\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"QuantizeDownAndShrinkRange\", input=input, input_min=input_min,\n                                      input_max=input_max, out_type=out_type,\n                                      name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"Tinput\", _op._get_attr_type(\"Tinput\"), \"out_type\",\n              _op._get_attr_type(\"out_type\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"QuantizeDownAndShrinkRange\", _inputs_flat, _attrs, _result)\n  _result = _QuantizeDownAndShrinkRangeOutput._make(_result)\n  return _result\n\nQuantizeDownAndShrinkRange = tf_export(\"raw_ops.QuantizeDownAndShrinkRange\")(_ops.to_raw_op(quantize_down_and_shrink_range))\n\n\ndef quantize_down_and_shrink_range_eager_fallback(input, input_min, input_max, out_type, name, ctx):\n  out_type = _execute.make_type(out_type, \"out_type\")\n  _attr_Tinput, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.qint16, _dtypes.quint16, ])\n  input_min = _ops.convert_to_tensor(input_min, _dtypes.float32)\n  input_max = _ops.convert_to_tensor(input_max, _dtypes.float32)\n  _inputs_flat = [input, input_min, input_max]\n  _attrs = (\"Tinput\", _attr_Tinput, \"out_type\", out_type)\n  _result = _execute.execute(b\"QuantizeDownAndShrinkRange\", 3,\n                             inputs=_inputs_flat, attrs=_attrs, ctx=ctx,\n                             name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"QuantizeDownAndShrinkRange\", _inputs_flat, _attrs, _result)\n  _result = _QuantizeDownAndShrinkRangeOutput._make(_result)\n  return _result\n\n_QuantizedAddOutput = collections.namedtuple(\n    \"QuantizedAdd\",\n    [\"z\", \"min_z\", \"max_z\"])\n\n\ndef quantized_add(x, y, min_x, max_x, min_y, max_y, Toutput=_dtypes.qint32, name=None):\n  r\"\"\"Returns x + y element-wise, working on quantized buffers.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n    y: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n    min_x: A `Tensor` of type `float32`.\n      The float value that the lowest quantized `x` value represents.\n    max_x: A `Tensor` of type `float32`.\n      The float value that the highest quantized `x` value represents.\n    min_y: A `Tensor` of type `float32`.\n      The float value that the lowest quantized `y` value represents.\n    max_y: A `Tensor` of type `float32`.\n      The float value that the highest quantized `y` value represents.\n    Toutput: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.qint32`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A tuple of `Tensor` objects (z, min_z, max_z).\n\n    z: A `Tensor` of type `Toutput`.\n    min_z: A `Tensor` of type `float32`.\n    max_z: A `Tensor` of type `float32`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"QuantizedAdd\", name, x, y, min_x, max_x, min_y, max_y,\n        \"Toutput\", Toutput)\n      _result = _QuantizedAddOutput._make(_result)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return quantized_add_eager_fallback(\n          x, y, min_x, max_x, min_y, max_y, Toutput=Toutput, name=name,\n          ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if Toutput is None:\n    Toutput = _dtypes.qint32\n  Toutput = _execute.make_type(Toutput, \"Toutput\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"QuantizedAdd\", x=x, y=y, min_x=min_x, max_x=max_x, min_y=min_y,\n                        max_y=max_y, Toutput=Toutput, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T1\", _op._get_attr_type(\"T1\"), \"T2\", _op._get_attr_type(\"T2\"),\n              \"Toutput\", _op._get_attr_type(\"Toutput\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"QuantizedAdd\", _inputs_flat, _attrs, _result)\n  _result = _QuantizedAddOutput._make(_result)\n  return _result\n\nQuantizedAdd = tf_export(\"raw_ops.QuantizedAdd\")(_ops.to_raw_op(quantized_add))\n\n\ndef quantized_add_eager_fallback(x, y, min_x, max_x, min_y, max_y, Toutput, name, ctx):\n  if Toutput is None:\n    Toutput = _dtypes.qint32\n  Toutput = _execute.make_type(Toutput, \"Toutput\")\n  _attr_T1, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.qint16, _dtypes.quint16, ])\n  _attr_T2, (y,) = _execute.args_to_matching_eager([y], ctx, [_dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.qint16, _dtypes.quint16, ])\n  min_x = _ops.convert_to_tensor(min_x, _dtypes.float32)\n  max_x = _ops.convert_to_tensor(max_x, _dtypes.float32)\n  min_y = _ops.convert_to_tensor(min_y, _dtypes.float32)\n  max_y = _ops.convert_to_tensor(max_y, _dtypes.float32)\n  _inputs_flat = [x, y, min_x, max_x, min_y, max_y]\n  _attrs = (\"T1\", _attr_T1, \"T2\", _attr_T2, \"Toutput\", Toutput)\n  _result = _execute.execute(b\"QuantizedAdd\", 3, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"QuantizedAdd\", _inputs_flat, _attrs, _result)\n  _result = _QuantizedAddOutput._make(_result)\n  return _result\n\n_QuantizedMatMulOutput = collections.namedtuple(\n    \"QuantizedMatMul\",\n    [\"out\", \"min_out\", \"max_out\"])\n\n\ndef quantized_mat_mul(a, b, min_a, max_a, min_b, max_b, Toutput=_dtypes.qint32, transpose_a=False, transpose_b=False, Tactivation=_dtypes.quint8, name=None):\n  r\"\"\"Perform a quantized matrix multiplication of  `a` by the matrix `b`.\n\n  The inputs must be two-dimensional matrices and the inner dimension of\n  `a` (after being transposed if `transpose_a` is non-zero) must match the\n  outer dimension of `b` (after being transposed if `transposed_b` is\n  non-zero).\n\n  Args:\n    a: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n      Must be a two-dimensional tensor.\n    b: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n      Must be a two-dimensional tensor.\n    min_a: A `Tensor` of type `float32`.\n      The float value that the lowest quantized `a` value represents.\n    max_a: A `Tensor` of type `float32`.\n      The float value that the highest quantized `a` value represents.\n    min_b: A `Tensor` of type `float32`.\n      The float value that the lowest quantized `b` value represents.\n    max_b: A `Tensor` of type `float32`.\n      The float value that the highest quantized `b` value represents.\n    Toutput: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.qint32`.\n    transpose_a: An optional `bool`. Defaults to `False`.\n      If true, `a` is transposed before multiplication.\n    transpose_b: An optional `bool`. Defaults to `False`.\n      If true, `b` is transposed before multiplication.\n    Tactivation: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.quint8`.\n      The type of output produced by activation function\n      following this operation.\n    name: A name for the operation (optional).\n\n  Returns:\n    A tuple of `Tensor` objects (out, min_out, max_out).\n\n    out: A `Tensor` of type `Toutput`.\n    min_out: A `Tensor` of type `float32`.\n    max_out: A `Tensor` of type `float32`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"QuantizedMatMul\", name, a, b, min_a, max_a, min_b, max_b,\n        \"Toutput\", Toutput, \"transpose_a\", transpose_a, \"transpose_b\",\n        transpose_b, \"Tactivation\", Tactivation)\n      _result = _QuantizedMatMulOutput._make(_result)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return quantized_mat_mul_eager_fallback(\n          a, b, min_a, max_a, min_b, max_b, Toutput=Toutput,\n          transpose_a=transpose_a, transpose_b=transpose_b,\n          Tactivation=Tactivation, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if Toutput is None:\n    Toutput = _dtypes.qint32\n  Toutput = _execute.make_type(Toutput, \"Toutput\")\n  if transpose_a is None:\n    transpose_a = False\n  transpose_a = _execute.make_bool(transpose_a, \"transpose_a\")\n  if transpose_b is None:\n    transpose_b = False\n  transpose_b = _execute.make_bool(transpose_b, \"transpose_b\")\n  if Tactivation is None:\n    Tactivation = _dtypes.quint8\n  Tactivation = _execute.make_type(Tactivation, \"Tactivation\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"QuantizedMatMul\", a=a, b=b, min_a=min_a, max_a=max_a, min_b=min_b,\n                           max_b=max_b, Toutput=Toutput,\n                           transpose_a=transpose_a, transpose_b=transpose_b,\n                           Tactivation=Tactivation, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T1\", _op._get_attr_type(\"T1\"), \"T2\", _op._get_attr_type(\"T2\"),\n              \"Toutput\", _op._get_attr_type(\"Toutput\"), \"transpose_a\",\n              _op._get_attr_bool(\"transpose_a\"), \"transpose_b\",\n              _op._get_attr_bool(\"transpose_b\"), \"Tactivation\",\n              _op._get_attr_type(\"Tactivation\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"QuantizedMatMul\", _inputs_flat, _attrs, _result)\n  _result = _QuantizedMatMulOutput._make(_result)\n  return _result\n\nQuantizedMatMul = tf_export(\"raw_ops.QuantizedMatMul\")(_ops.to_raw_op(quantized_mat_mul))\n\n\ndef quantized_mat_mul_eager_fallback(a, b, min_a, max_a, min_b, max_b, Toutput, transpose_a, transpose_b, Tactivation, name, ctx):\n  if Toutput is None:\n    Toutput = _dtypes.qint32\n  Toutput = _execute.make_type(Toutput, \"Toutput\")\n  if transpose_a is None:\n    transpose_a = False\n  transpose_a = _execute.make_bool(transpose_a, \"transpose_a\")\n  if transpose_b is None:\n    transpose_b = False\n  transpose_b = _execute.make_bool(transpose_b, \"transpose_b\")\n  if Tactivation is None:\n    Tactivation = _dtypes.quint8\n  Tactivation = _execute.make_type(Tactivation, \"Tactivation\")\n  _attr_T1, (a,) = _execute.args_to_matching_eager([a], ctx, [_dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.qint16, _dtypes.quint16, ])\n  _attr_T2, (b,) = _execute.args_to_matching_eager([b], ctx, [_dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.qint16, _dtypes.quint16, ])\n  min_a = _ops.convert_to_tensor(min_a, _dtypes.float32)\n  max_a = _ops.convert_to_tensor(max_a, _dtypes.float32)\n  min_b = _ops.convert_to_tensor(min_b, _dtypes.float32)\n  max_b = _ops.convert_to_tensor(max_b, _dtypes.float32)\n  _inputs_flat = [a, b, min_a, max_a, min_b, max_b]\n  _attrs = (\"T1\", _attr_T1, \"T2\", _attr_T2, \"Toutput\", Toutput, \"transpose_a\",\n  transpose_a, \"transpose_b\", transpose_b, \"Tactivation\", Tactivation)\n  _result = _execute.execute(b\"QuantizedMatMul\", 3, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"QuantizedMatMul\", _inputs_flat, _attrs, _result)\n  _result = _QuantizedMatMulOutput._make(_result)\n  return _result\n\n_QuantizedMulOutput = collections.namedtuple(\n    \"QuantizedMul\",\n    [\"z\", \"min_z\", \"max_z\"])\n\n\ndef quantized_mul(x, y, min_x, max_x, min_y, max_y, Toutput=_dtypes.qint32, name=None):\n  r\"\"\"Returns x * y element-wise, working on quantized buffers.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n    y: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n    min_x: A `Tensor` of type `float32`.\n      The float value that the lowest quantized `x` value represents.\n    max_x: A `Tensor` of type `float32`.\n      The float value that the highest quantized `x` value represents.\n    min_y: A `Tensor` of type `float32`.\n      The float value that the lowest quantized `y` value represents.\n    max_y: A `Tensor` of type `float32`.\n      The float value that the highest quantized `y` value represents.\n    Toutput: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.qint32`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A tuple of `Tensor` objects (z, min_z, max_z).\n\n    z: A `Tensor` of type `Toutput`.\n    min_z: A `Tensor` of type `float32`.\n    max_z: A `Tensor` of type `float32`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"QuantizedMul\", name, x, y, min_x, max_x, min_y, max_y,\n        \"Toutput\", Toutput)\n      _result = _QuantizedMulOutput._make(_result)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return quantized_mul_eager_fallback(\n          x, y, min_x, max_x, min_y, max_y, Toutput=Toutput, name=name,\n          ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if Toutput is None:\n    Toutput = _dtypes.qint32\n  Toutput = _execute.make_type(Toutput, \"Toutput\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"QuantizedMul\", x=x, y=y, min_x=min_x, max_x=max_x, min_y=min_y,\n                        max_y=max_y, Toutput=Toutput, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T1\", _op._get_attr_type(\"T1\"), \"T2\", _op._get_attr_type(\"T2\"),\n              \"Toutput\", _op._get_attr_type(\"Toutput\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"QuantizedMul\", _inputs_flat, _attrs, _result)\n  _result = _QuantizedMulOutput._make(_result)\n  return _result\n\nQuantizedMul = tf_export(\"raw_ops.QuantizedMul\")(_ops.to_raw_op(quantized_mul))\n\n\ndef quantized_mul_eager_fallback(x, y, min_x, max_x, min_y, max_y, Toutput, name, ctx):\n  if Toutput is None:\n    Toutput = _dtypes.qint32\n  Toutput = _execute.make_type(Toutput, \"Toutput\")\n  _attr_T1, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.qint16, _dtypes.quint16, ])\n  _attr_T2, (y,) = _execute.args_to_matching_eager([y], ctx, [_dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.qint16, _dtypes.quint16, ])\n  min_x = _ops.convert_to_tensor(min_x, _dtypes.float32)\n  max_x = _ops.convert_to_tensor(max_x, _dtypes.float32)\n  min_y = _ops.convert_to_tensor(min_y, _dtypes.float32)\n  max_y = _ops.convert_to_tensor(max_y, _dtypes.float32)\n  _inputs_flat = [x, y, min_x, max_x, min_y, max_y]\n  _attrs = (\"T1\", _attr_T1, \"T2\", _attr_T2, \"Toutput\", Toutput)\n  _result = _execute.execute(b\"QuantizedMul\", 3, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"QuantizedMul\", _inputs_flat, _attrs, _result)\n  _result = _QuantizedMulOutput._make(_result)\n  return _result\n\n\ndef ragged_bincount(splits, values, size, weights, binary_output=False, name=None):\n  r\"\"\"Counts the number of occurrences of each value in an integer array.\n\n  Outputs a vector with length `size` and the same dtype as `weights`. If\n  `weights` are empty, then index `i` stores the number of times the value `i` is\n  counted in `arr`. If `weights` are non-empty, then index `i` stores the sum of\n  the value in `weights` at each index where the corresponding value in `arr` is\n  `i`.\n\n  Values in `arr` outside of the range [0, size) are ignored.\n\n  Args:\n    splits: A `Tensor` of type `int64`. 1D int64 `Tensor`.\n    values: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      2D int `Tensor`.\n    size: A `Tensor`. Must have the same type as `values`.\n      non-negative int scalar `Tensor`.\n    weights: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.\n      is an int32, int64, float32, or float64 `Tensor` with the same\n      shape as `input`, or a length-0 `Tensor`, in which case it acts as all weights\n      equal to 1.\n    binary_output: An optional `bool`. Defaults to `False`.\n      bool; Whether the kernel should count the appearance or number of occurrences.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `weights`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"RaggedBincount\", name, splits, values, size, weights,\n        \"binary_output\", binary_output)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return ragged_bincount_eager_fallback(\n          splits, values, size, weights, binary_output=binary_output,\n          name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if binary_output is None:\n    binary_output = False\n  binary_output = _execute.make_bool(binary_output, \"binary_output\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"RaggedBincount\", splits=splits, values=values, size=size,\n                          weights=weights, binary_output=binary_output,\n                          name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"Tidx\", _op._get_attr_type(\"Tidx\"), \"T\",\n              _op._get_attr_type(\"T\"), \"binary_output\",\n              _op._get_attr_bool(\"binary_output\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"RaggedBincount\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nRaggedBincount = tf_export(\"raw_ops.RaggedBincount\")(_ops.to_raw_op(ragged_bincount))\n\n\ndef ragged_bincount_eager_fallback(splits, values, size, weights, binary_output, name, ctx):\n  if binary_output is None:\n    binary_output = False\n  binary_output = _execute.make_bool(binary_output, \"binary_output\")\n  _attr_Tidx, _inputs_Tidx = _execute.args_to_matching_eager([values, size], ctx, [_dtypes.int32, _dtypes.int64, ])\n  (values, size) = _inputs_Tidx\n  _attr_T, (weights,) = _execute.args_to_matching_eager([weights], ctx, [_dtypes.int32, _dtypes.int64, _dtypes.float32, _dtypes.float64, ])\n  splits = _ops.convert_to_tensor(splits, _dtypes.int64)\n  _inputs_flat = [splits, values, size, weights]\n  _attrs = (\"Tidx\", _attr_Tidx, \"T\", _attr_T, \"binary_output\", binary_output)\n  _result = _execute.execute(b\"RaggedBincount\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"RaggedBincount\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef _range(start, limit, delta, name=None):\n  r\"\"\"Creates a sequence of numbers.\n\n  This operation creates a sequence of numbers that begins at `start` and\n  extends by increments of `delta` up to but not including `limit`.\n\n  For example:\n\n  ```\n  # 'start' is 3\n  # 'limit' is 18\n  # 'delta' is 3\n  tf.range(start, limit, delta) ==> [3, 6, 9, 12, 15]\n  ```\n\n  Args:\n    start: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `uint16`, `uint32`.\n      0-D (scalar). First entry in the sequence.\n    limit: A `Tensor`. Must have the same type as `start`.\n      0-D (scalar). Upper limit of sequence, exclusive.\n    delta: A `Tensor`. Must have the same type as `start`.\n      0-D (scalar). Optional. Default is 1. Number that increments `start`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `start`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Range\", name, start, limit, delta)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return _range_eager_fallback(\n          start, limit, delta, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Range\", start=start, limit=limit, delta=delta, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"Tidx\", _op._get_attr_type(\"Tidx\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Range\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nRange = tf_export(\"raw_ops.Range\")(_ops.to_raw_op(_range))\n\n\ndef _range_eager_fallback(start, limit, delta, name, ctx):\n  _attr_Tidx, _inputs_Tidx = _execute.args_to_matching_eager([start, limit, delta], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.uint16, _dtypes.uint32, ], _dtypes.int32)\n  (start, limit, delta) = _inputs_Tidx\n  _inputs_flat = [start, limit, delta]\n  _attrs = (\"Tidx\", _attr_Tidx)\n  _result = _execute.execute(b\"Range\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Range\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef real(input, Tout=_dtypes.float32, name=None):\n  r\"\"\"Returns the real part of a complex number.\n\n  Given a tensor `input` of complex numbers, this operation returns a tensor of\n  type `float` that is the real part of each element in `input`. All elements in\n  `input` must be complex numbers of the form \\\\(a + bj\\\\), where *a* is the real\n   part returned by this operation and *b* is the imaginary part.\n\n  For example:\n\n  ```\n  # tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]\n  tf.real(input) ==> [-2.25, 3.25]\n  ```\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `complex64`, `complex128`.\n    Tout: An optional `tf.DType` from: `tf.float32, tf.float64`. Defaults to `tf.float32`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `Tout`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Real\", name, input, \"Tout\", Tout)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return real_eager_fallback(\n          input, Tout=Tout, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if Tout is None:\n    Tout = _dtypes.float32\n  Tout = _execute.make_type(Tout, \"Tout\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Real\", input=input, Tout=Tout, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tout\",\n              _op._get_attr_type(\"Tout\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Real\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nReal = tf_export(\"raw_ops.Real\")(_ops.to_raw_op(real))\n\n\ndef real_eager_fallback(input, Tout, name, ctx):\n  if Tout is None:\n    Tout = _dtypes.float32\n  Tout = _execute.make_type(Tout, \"Tout\")\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.complex64, _dtypes.complex128, ], _dtypes.complex64)\n  _inputs_flat = [input]\n  _attrs = (\"T\", _attr_T, \"Tout\", Tout)\n  _result = _execute.execute(b\"Real\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Real\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('realdiv')\ndef real_div(x, y, name=None):\n  r\"\"\"Returns x / y element-wise for real types.\n\n  If `x` and `y` are reals, this will return the floating-point division.\n\n  *NOTE*: `Div` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `uint32`, `uint64`, `int64`, `complex64`, `complex128`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"RealDiv\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_real_div(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return real_div_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            real_div, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_real_div(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"RealDiv\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          real_div, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"RealDiv\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nRealDiv = tf_export(\"raw_ops.RealDiv\")(_ops.to_raw_op(real_div))\n_dispatcher_for_real_div = real_div._tf_type_based_dispatcher.Dispatch\n\n\ndef real_div_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.uint8, _dtypes.int8, _dtypes.uint16, _dtypes.int16, _dtypes.int32, _dtypes.uint32, _dtypes.uint64, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"RealDiv\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"RealDiv\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.reciprocal', v1=['math.reciprocal', 'reciprocal'])\n@deprecated_endpoints('reciprocal')\ndef reciprocal(x, name=None):\n  r\"\"\"Computes the reciprocal of x element-wise.\n\n  I.e., \\\\(y = 1 / x\\\\).\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Reciprocal\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_reciprocal(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return reciprocal_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            reciprocal, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_reciprocal(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Reciprocal\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          reciprocal, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Reciprocal\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nReciprocal = tf_export(\"raw_ops.Reciprocal\")(_ops.to_raw_op(reciprocal))\n_dispatcher_for_reciprocal = reciprocal._tf_type_based_dispatcher.Dispatch\n\n\ndef reciprocal_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Reciprocal\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Reciprocal\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef reciprocal_grad(y, dy, name=None):\n  r\"\"\"Computes the gradient for the inverse of `x` wrt its input.\n\n  Specifically, `grad = -dy * y*y`, where `y = 1/x`, and `dy`\n  is the corresponding input gradient.\n\n  Args:\n    y: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    dy: A `Tensor`. Must have the same type as `y`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `y`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"ReciprocalGrad\", name, y, dy)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return reciprocal_grad_eager_fallback(\n          y, dy, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"ReciprocalGrad\", y=y, dy=dy, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"ReciprocalGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nReciprocalGrad = tf_export(\"raw_ops.ReciprocalGrad\")(_ops.to_raw_op(reciprocal_grad))\n\n\ndef reciprocal_grad_eager_fallback(y, dy, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([y, dy], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  (y, dy) = _inputs_T\n  _inputs_flat = [y, dy]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"ReciprocalGrad\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"ReciprocalGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n_RequantizationRangeOutput = collections.namedtuple(\n    \"RequantizationRange\",\n    [\"output_min\", \"output_max\"])\n\n\ndef requantization_range(input, input_min, input_max, name=None):\n  r\"\"\"Computes a range that covers the actual values present in a quantized tensor.\n\n  Given a quantized tensor described by `(input, input_min, input_max)`, outputs a\n  range that covers the actual values present in that tensor. This op is typically\n  used to produce the `requested_output_min` and `requested_output_max` for\n  `Requantize`.\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n    input_min: A `Tensor` of type `float32`.\n      The float value that the minimum quantized input value represents.\n    input_max: A `Tensor` of type `float32`.\n      The float value that the maximum quantized input value represents.\n    name: A name for the operation (optional).\n\n  Returns:\n    A tuple of `Tensor` objects (output_min, output_max).\n\n    output_min: A `Tensor` of type `float32`.\n    output_max: A `Tensor` of type `float32`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"RequantizationRange\", name, input, input_min, input_max)\n      _result = _RequantizationRangeOutput._make(_result)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return requantization_range_eager_fallback(\n          input, input_min, input_max, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"RequantizationRange\", input=input, input_min=input_min,\n                               input_max=input_max, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"Tinput\", _op._get_attr_type(\"Tinput\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"RequantizationRange\", _inputs_flat, _attrs, _result)\n  _result = _RequantizationRangeOutput._make(_result)\n  return _result\n\nRequantizationRange = tf_export(\"raw_ops.RequantizationRange\")(_ops.to_raw_op(requantization_range))\n\n\ndef requantization_range_eager_fallback(input, input_min, input_max, name, ctx):\n  _attr_Tinput, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.qint16, _dtypes.quint16, ])\n  input_min = _ops.convert_to_tensor(input_min, _dtypes.float32)\n  input_max = _ops.convert_to_tensor(input_max, _dtypes.float32)\n  _inputs_flat = [input, input_min, input_max]\n  _attrs = (\"Tinput\", _attr_Tinput)\n  _result = _execute.execute(b\"RequantizationRange\", 2, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"RequantizationRange\", _inputs_flat, _attrs, _result)\n  _result = _RequantizationRangeOutput._make(_result)\n  return _result\n\n_RequantizationRangePerChannelOutput = collections.namedtuple(\n    \"RequantizationRangePerChannel\",\n    [\"output_min\", \"output_max\"])\n\n\ndef requantization_range_per_channel(input, input_min, input_max, clip_value_max, name=None):\n  r\"\"\"Computes requantization range per channel.\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n      The original input tensor.\n    input_min: A `Tensor` of type `float32`.\n      The minimum value of the input tensor\n    input_max: A `Tensor` of type `float32`.\n      The maximum value of the input tensor.\n    clip_value_max: A `float`.\n      The maximum value of the output that needs to be clipped.\n      Example: set this to 6 for Relu6.\n    name: A name for the operation (optional).\n\n  Returns:\n    A tuple of `Tensor` objects (output_min, output_max).\n\n    output_min: A `Tensor` of type `float32`.\n    output_max: A `Tensor` of type `float32`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"RequantizationRangePerChannel\", name, input, input_min,\n        input_max, \"clip_value_max\", clip_value_max)\n      _result = _RequantizationRangePerChannelOutput._make(_result)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return requantization_range_per_channel_eager_fallback(\n          input, input_min, input_max, clip_value_max=clip_value_max,\n          name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  clip_value_max = _execute.make_float(clip_value_max, \"clip_value_max\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"RequantizationRangePerChannel\", input=input, input_min=input_min,\n                                         input_max=input_max,\n                                         clip_value_max=clip_value_max,\n                                         name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"clip_value_max\",\n              _op.get_attr(\"clip_value_max\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"RequantizationRangePerChannel\", _inputs_flat, _attrs, _result)\n  _result = _RequantizationRangePerChannelOutput._make(_result)\n  return _result\n\nRequantizationRangePerChannel = tf_export(\"raw_ops.RequantizationRangePerChannel\")(_ops.to_raw_op(requantization_range_per_channel))\n\n\ndef requantization_range_per_channel_eager_fallback(input, input_min, input_max, clip_value_max, name, ctx):\n  clip_value_max = _execute.make_float(clip_value_max, \"clip_value_max\")\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.qint16, _dtypes.quint16, ], _dtypes.qint32)\n  input_min = _ops.convert_to_tensor(input_min, _dtypes.float32)\n  input_max = _ops.convert_to_tensor(input_max, _dtypes.float32)\n  _inputs_flat = [input, input_min, input_max]\n  _attrs = (\"T\", _attr_T, \"clip_value_max\", clip_value_max)\n  _result = _execute.execute(b\"RequantizationRangePerChannel\", 2,\n                             inputs=_inputs_flat, attrs=_attrs, ctx=ctx,\n                             name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"RequantizationRangePerChannel\", _inputs_flat, _attrs, _result)\n  _result = _RequantizationRangePerChannelOutput._make(_result)\n  return _result\n\n_RequantizeOutput = collections.namedtuple(\n    \"Requantize\",\n    [\"output\", \"output_min\", \"output_max\"])\n\n\ndef requantize(input, input_min, input_max, requested_output_min, requested_output_max, out_type, name=None):\n  r\"\"\"Converts the quantized `input` tensor into a lower-precision `output`.\n\n  Converts the quantized `input` tensor into a lower-precision `output`, using the\n  output range specified with `requested_output_min` and `requested_output_max`.\n\n  `[input_min, input_max]` are scalar floats that specify the range for the float\n  interpretation of the `input` data. For example, if `input_min` is -1.0f and\n  `input_max` is 1.0f, and we are dealing with `quint16` quantized data, then a 0\n  value in the 16-bit data should be interpreted as -1.0f, and a 65535 means 1.0f.\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n    input_min: A `Tensor` of type `float32`.\n      The float value that the minimum quantized input value represents.\n    input_max: A `Tensor` of type `float32`.\n      The float value that the maximum quantized input value represents.\n    requested_output_min: A `Tensor` of type `float32`.\n      The float value that the minimum quantized output value represents.\n    requested_output_max: A `Tensor` of type `float32`.\n      The float value that the maximum quantized output value represents.\n    out_type: A `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`.\n      The type of the output. Should be a lower bit depth than Tinput.\n    name: A name for the operation (optional).\n\n  Returns:\n    A tuple of `Tensor` objects (output, output_min, output_max).\n\n    output: A `Tensor` of type `out_type`.\n    output_min: A `Tensor` of type `float32`.\n    output_max: A `Tensor` of type `float32`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Requantize\", name, input, input_min, input_max,\n        requested_output_min, requested_output_max, \"out_type\", out_type)\n      _result = _RequantizeOutput._make(_result)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return requantize_eager_fallback(\n          input, input_min, input_max, requested_output_min,\n          requested_output_max, out_type=out_type, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  out_type = _execute.make_type(out_type, \"out_type\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Requantize\", input=input, input_min=input_min, input_max=input_max,\n                      requested_output_min=requested_output_min,\n                      requested_output_max=requested_output_max,\n                      out_type=out_type, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"Tinput\", _op._get_attr_type(\"Tinput\"), \"out_type\",\n              _op._get_attr_type(\"out_type\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Requantize\", _inputs_flat, _attrs, _result)\n  _result = _RequantizeOutput._make(_result)\n  return _result\n\nRequantize = tf_export(\"raw_ops.Requantize\")(_ops.to_raw_op(requantize))\n\n\ndef requantize_eager_fallback(input, input_min, input_max, requested_output_min, requested_output_max, out_type, name, ctx):\n  out_type = _execute.make_type(out_type, \"out_type\")\n  _attr_Tinput, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.qint16, _dtypes.quint16, ])\n  input_min = _ops.convert_to_tensor(input_min, _dtypes.float32)\n  input_max = _ops.convert_to_tensor(input_max, _dtypes.float32)\n  requested_output_min = _ops.convert_to_tensor(requested_output_min, _dtypes.float32)\n  requested_output_max = _ops.convert_to_tensor(requested_output_max, _dtypes.float32)\n  _inputs_flat = [input, input_min, input_max, requested_output_min, requested_output_max]\n  _attrs = (\"Tinput\", _attr_Tinput, \"out_type\", out_type)\n  _result = _execute.execute(b\"Requantize\", 3, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Requantize\", _inputs_flat, _attrs, _result)\n  _result = _RequantizeOutput._make(_result)\n  return _result\n\n_RequantizePerChannelOutput = collections.namedtuple(\n    \"RequantizePerChannel\",\n    [\"output\", \"output_min\", \"output_max\"])\n\n\ndef requantize_per_channel(input, input_min, input_max, requested_output_min, requested_output_max, out_type=_dtypes.quint8, name=None):\n  r\"\"\"Requantizes input with min and max values known per channel.\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n      The original input tensor.\n    input_min: A `Tensor` of type `float32`.\n      The minimum value of the input tensor\n    input_max: A `Tensor` of type `float32`.\n      The maximum value of the input tensor.\n    requested_output_min: A `Tensor` of type `float32`.\n      The minimum value of the output tensor requested.\n    requested_output_max: A `Tensor` of type `float32`.\n      The maximum value of the output tensor requested.\n    out_type: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.quint8`.\n      The quantized type of output tensor that needs to be converted.\n    name: A name for the operation (optional).\n\n  Returns:\n    A tuple of `Tensor` objects (output, output_min, output_max).\n\n    output: A `Tensor` of type `out_type`.\n    output_min: A `Tensor` of type `float32`.\n    output_max: A `Tensor` of type `float32`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"RequantizePerChannel\", name, input, input_min, input_max,\n        requested_output_min, requested_output_max, \"out_type\", out_type)\n      _result = _RequantizePerChannelOutput._make(_result)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return requantize_per_channel_eager_fallback(\n          input, input_min, input_max, requested_output_min,\n          requested_output_max, out_type=out_type, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if out_type is None:\n    out_type = _dtypes.quint8\n  out_type = _execute.make_type(out_type, \"out_type\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"RequantizePerChannel\", input=input, input_min=input_min,\n                                input_max=input_max,\n                                requested_output_min=requested_output_min,\n                                requested_output_max=requested_output_max,\n                                out_type=out_type, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"out_type\",\n              _op._get_attr_type(\"out_type\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"RequantizePerChannel\", _inputs_flat, _attrs, _result)\n  _result = _RequantizePerChannelOutput._make(_result)\n  return _result\n\nRequantizePerChannel = tf_export(\"raw_ops.RequantizePerChannel\")(_ops.to_raw_op(requantize_per_channel))\n\n\ndef requantize_per_channel_eager_fallback(input, input_min, input_max, requested_output_min, requested_output_max, out_type, name, ctx):\n  if out_type is None:\n    out_type = _dtypes.quint8\n  out_type = _execute.make_type(out_type, \"out_type\")\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.qint16, _dtypes.quint16, ], _dtypes.qint32)\n  input_min = _ops.convert_to_tensor(input_min, _dtypes.float32)\n  input_max = _ops.convert_to_tensor(input_max, _dtypes.float32)\n  requested_output_min = _ops.convert_to_tensor(requested_output_min, _dtypes.float32)\n  requested_output_max = _ops.convert_to_tensor(requested_output_max, _dtypes.float32)\n  _inputs_flat = [input, input_min, input_max, requested_output_min, requested_output_max]\n  _attrs = (\"T\", _attr_T, \"out_type\", out_type)\n  _result = _execute.execute(b\"RequantizePerChannel\", 3, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"RequantizePerChannel\", _inputs_flat, _attrs, _result)\n  _result = _RequantizePerChannelOutput._make(_result)\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.rint', v1=['math.rint', 'rint'])\n@deprecated_endpoints('rint')\ndef rint(x, name=None):\n  r\"\"\"Returns element-wise integer closest to x.\n\n  If the result is midway between two representable values,\n  the even representable is chosen.\n  For example:\n\n  ```\n  rint(-1.5) ==> -2.0\n  rint(0.5000001) ==> 1.0\n  rint([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0]) ==> [-2., -2., -0., 0., 2., 2., 2.]\n  ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Rint\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_rint(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return rint_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            rint, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_rint(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Rint\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          rint, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Rint\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nRint = tf_export(\"raw_ops.Rint\")(_ops.to_raw_op(rint))\n_dispatcher_for_rint = rint._tf_type_based_dispatcher.Dispatch\n\n\ndef rint_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Rint\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Rint\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef round(x, name=None):\n  r\"\"\"Rounds the values of a tensor to the nearest integer, element-wise.\n\n  Rounds half to even.  Also known as bankers rounding. If you want to round\n  according to the current system rounding mode use std::cint.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Round\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return round_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Round\", x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Round\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nRound = tf_export(\"raw_ops.Round\")(_ops.to_raw_op(round))\n\n\ndef round_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Round\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Round\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef rsqrt(x, name=None):\n  r\"\"\"Computes reciprocal of square root of x element-wise.\n\n  I.e., \\\\(y = 1 / \\sqrt{x}\\\\).\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Rsqrt\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return rsqrt_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Rsqrt\", x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Rsqrt\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nRsqrt = tf_export(\"raw_ops.Rsqrt\")(_ops.to_raw_op(rsqrt))\n\n\ndef rsqrt_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Rsqrt\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Rsqrt\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef rsqrt_grad(y, dy, name=None):\n  r\"\"\"Computes the gradient for the rsqrt of `x` wrt its input.\n\n  Specifically, `grad = dy * -0.5 * y^3`, where `y = rsqrt(x)`, and `dy`\n  is the corresponding input gradient.\n\n  Args:\n    y: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    dy: A `Tensor`. Must have the same type as `y`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `y`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"RsqrtGrad\", name, y, dy)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return rsqrt_grad_eager_fallback(\n          y, dy, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"RsqrtGrad\", y=y, dy=dy, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"RsqrtGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nRsqrtGrad = tf_export(\"raw_ops.RsqrtGrad\")(_ops.to_raw_op(rsqrt_grad))\n\n\ndef rsqrt_grad_eager_fallback(y, dy, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([y, dy], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  (y, dy) = _inputs_T\n  _inputs_flat = [y, dy]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"RsqrtGrad\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"RsqrtGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.segment_max', v1=['math.segment_max', 'segment_max'])\n@deprecated_endpoints('segment_max')\ndef segment_max(data, segment_ids, name=None):\n  r\"\"\"Computes the maximum along segments of a tensor.\n\n  Read\n  [the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\n  for an explanation of segments.\n\n  Computes a tensor such that\n  \\\\(output_i = \\max_j(data_j)\\\\) where `max` is over `j` such\n  that `segment_ids[j] == i`.\n\n  If the max is empty for a given segment ID `i`, `output[i] = 0`.\n\n  Caution: On CPU, values in `segment_ids` are always validated to be sorted,\n  and an error is thrown for indices that are not increasing. On GPU, this\n  does not throw an error for unsorted indices. On GPU, out-of-order indices\n  result in safe but unspecified behavior, which may include treating\n  out-of-order indices as the same as a smaller following index.\n\n  <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n  <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentMax.png\" alt>\n  </div>\n\n  For example:\n\n  >>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n  >>> tf.math.segment_max(c, tf.constant([0, 0, 1])).numpy()\n  array([[4, 3, 3, 4],\n         [5, 6, 7, 8]], dtype=int32)\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor whose size is equal to the size of `data`'s\n      first dimension.  Values should be sorted and can be repeated.\n\n      Caution: The values are always validated to be sorted on CPU, never validated\n      on GPU.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SegmentMax\", name, data, segment_ids)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_segment_max(\n          (data, segment_ids, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return segment_max_eager_fallback(\n          data, segment_ids, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            segment_max, (), dict(data=data, segment_ids=segment_ids,\n                                  name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_segment_max(\n        (data, segment_ids, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SegmentMax\", data=data, segment_ids=segment_ids, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          segment_max, (), dict(data=data, segment_ids=segment_ids, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tindices\",\n              _op._get_attr_type(\"Tindices\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SegmentMax\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSegmentMax = tf_export(\"raw_ops.SegmentMax\")(_ops.to_raw_op(segment_max))\n_dispatcher_for_segment_max = segment_max._tf_type_based_dispatcher.Dispatch\n\n\ndef segment_max_eager_fallback(data, segment_ids, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.int64, _dtypes.bfloat16, _dtypes.uint16, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tindices, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ])\n  _inputs_flat = [data, segment_ids]\n  _attrs = (\"T\", _attr_T, \"Tindices\", _attr_Tindices)\n  _result = _execute.execute(b\"SegmentMax\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SegmentMax\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.segment_mean', v1=['math.segment_mean', 'segment_mean'])\n@deprecated_endpoints('segment_mean')\ndef segment_mean(data, segment_ids, name=None):\n  r\"\"\"Computes the mean along segments of a tensor.\n\n  Read\n  [the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\n  for an explanation of segments.\n\n  Computes a tensor such that\n  \\\\(output_i = \\frac{\\sum_j data_j}{N}\\\\) where `mean` is\n  over `j` such that `segment_ids[j] == i` and `N` is the total number of\n  values summed.\n\n  If the mean is empty for a given segment ID `i`, `output[i] = 0`.\n\n  Caution: On CPU, values in `segment_ids` are always validated to be sorted,\n  and an error is thrown for indices that are not increasing. On GPU, this\n  does not throw an error for unsorted indices. On GPU, out-of-order indices\n  result in safe but unspecified behavior, which may include treating\n  out-of-order indices as a smaller following index when computing the numerator\n  of the mean.\n\n  <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n  <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentMean.png\" alt>\n  </div>\n\n  For example:\n\n  >>> c = tf.constant([[1.0,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n  >>> tf.math.segment_mean(c, tf.constant([0, 0, 1])).numpy()\n  array([[2.5, 2.5, 2.5, 2.5],\n         [5., 6., 7., 8.]], dtype=float32)\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor whose size is equal to the size of `data`'s\n      first dimension.  Values should be sorted and can be repeated.\n\n      Caution: The values are always validated to be sorted on CPU, never validated\n      on GPU.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SegmentMean\", name, data, segment_ids)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_segment_mean(\n          (data, segment_ids, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return segment_mean_eager_fallback(\n          data, segment_ids, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            segment_mean, (), dict(data=data, segment_ids=segment_ids,\n                                   name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_segment_mean(\n        (data, segment_ids, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SegmentMean\", data=data, segment_ids=segment_ids, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          segment_mean, (), dict(data=data, segment_ids=segment_ids,\n                                 name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tindices\",\n              _op._get_attr_type(\"Tindices\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SegmentMean\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSegmentMean = tf_export(\"raw_ops.SegmentMean\")(_ops.to_raw_op(segment_mean))\n_dispatcher_for_segment_mean = segment_mean._tf_type_based_dispatcher.Dispatch\n\n\ndef segment_mean_eager_fallback(data, segment_ids, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tindices, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ])\n  _inputs_flat = [data, segment_ids]\n  _attrs = (\"T\", _attr_T, \"Tindices\", _attr_Tindices)\n  _result = _execute.execute(b\"SegmentMean\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SegmentMean\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.segment_min', v1=['math.segment_min', 'segment_min'])\n@deprecated_endpoints('segment_min')\ndef segment_min(data, segment_ids, name=None):\n  r\"\"\"Computes the minimum along segments of a tensor.\n\n  Read\n  [the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\n  for an explanation of segments.\n\n  Computes a tensor such that\n  \\\\(output_i = \\min_j(data_j)\\\\) where `min` is over `j` such\n  that `segment_ids[j] == i`.\n\n  If the min is empty for a given segment ID `i`, `output[i] = 0`.\n\n  Caution: On CPU, values in `segment_ids` are always validated to be sorted,\n  and an error is thrown for indices that are not increasing. On GPU, this\n  does not throw an error for unsorted indices. On GPU, out-of-order indices\n  result in safe but unspecified behavior, which may include treating\n  out-of-order indices as the same as a smaller following index.\n\n  <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n  <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentMin.png\" alt>\n  </div>\n\n  For example:\n\n  >>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n  >>> tf.math.segment_min(c, tf.constant([0, 0, 1])).numpy()\n  array([[1, 2, 2, 1],\n         [5, 6, 7, 8]], dtype=int32)\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor whose size is equal to the size of `data`'s\n      first dimension.  Values should be sorted and can be repeated.\n\n      Caution: The values are always validated to be sorted on CPU, never validated\n      on GPU.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SegmentMin\", name, data, segment_ids)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_segment_min(\n          (data, segment_ids, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return segment_min_eager_fallback(\n          data, segment_ids, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            segment_min, (), dict(data=data, segment_ids=segment_ids,\n                                  name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_segment_min(\n        (data, segment_ids, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SegmentMin\", data=data, segment_ids=segment_ids, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          segment_min, (), dict(data=data, segment_ids=segment_ids, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tindices\",\n              _op._get_attr_type(\"Tindices\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SegmentMin\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSegmentMin = tf_export(\"raw_ops.SegmentMin\")(_ops.to_raw_op(segment_min))\n_dispatcher_for_segment_min = segment_min._tf_type_based_dispatcher.Dispatch\n\n\ndef segment_min_eager_fallback(data, segment_ids, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.int64, _dtypes.bfloat16, _dtypes.uint16, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tindices, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ])\n  _inputs_flat = [data, segment_ids]\n  _attrs = (\"T\", _attr_T, \"Tindices\", _attr_Tindices)\n  _result = _execute.execute(b\"SegmentMin\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SegmentMin\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.segment_prod', v1=['math.segment_prod', 'segment_prod'])\n@deprecated_endpoints('segment_prod')\ndef segment_prod(data, segment_ids, name=None):\n  r\"\"\"Computes the product along segments of a tensor.\n\n  Read\n  [the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\n  for an explanation of segments.\n\n  Computes a tensor such that\n  \\\\(output_i = \\prod_j data_j\\\\) where the product is over `j` such\n  that `segment_ids[j] == i`.\n\n  If the product is empty for a given segment ID `i`, `output[i] = 1`.\n\n  Caution: On CPU, values in `segment_ids` are always validated to be sorted,\n  and an error is thrown for indices that are not increasing. On GPU, this\n  does not throw an error for unsorted indices. On GPU, out-of-order indices\n  result in safe but unspecified behavior, which may include treating\n  out-of-order indices as the same as a smaller following index.\n\n  <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n  <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentProd.png\" alt>\n  </div>\n\n  For example:\n\n  >>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n  >>> tf.math.segment_prod(c, tf.constant([0, 0, 1])).numpy()\n  array([[4, 6, 6, 4],\n         [5, 6, 7, 8]], dtype=int32)\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor whose size is equal to the size of `data`'s\n      first dimension.  Values should be sorted and can be repeated.\n\n      Caution: The values are always validated to be sorted on CPU, never validated\n      on GPU.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SegmentProd\", name, data, segment_ids)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_segment_prod(\n          (data, segment_ids, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return segment_prod_eager_fallback(\n          data, segment_ids, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            segment_prod, (), dict(data=data, segment_ids=segment_ids,\n                                   name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_segment_prod(\n        (data, segment_ids, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SegmentProd\", data=data, segment_ids=segment_ids, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          segment_prod, (), dict(data=data, segment_ids=segment_ids,\n                                 name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tindices\",\n              _op._get_attr_type(\"Tindices\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SegmentProd\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSegmentProd = tf_export(\"raw_ops.SegmentProd\")(_ops.to_raw_op(segment_prod))\n_dispatcher_for_segment_prod = segment_prod._tf_type_based_dispatcher.Dispatch\n\n\ndef segment_prod_eager_fallback(data, segment_ids, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tindices, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ])\n  _inputs_flat = [data, segment_ids]\n  _attrs = (\"T\", _attr_T, \"Tindices\", _attr_Tindices)\n  _result = _execute.execute(b\"SegmentProd\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SegmentProd\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.segment_sum', v1=['math.segment_sum', 'segment_sum'])\n@deprecated_endpoints('segment_sum')\ndef segment_sum(data, segment_ids, name=None):\n  r\"\"\"Computes the sum along segments of a tensor.\n\n  Read\n  [the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\n  for an explanation of segments.\n\n  Computes a tensor such that\n  \\\\(output_i = \\sum_j data_j\\\\) where sum is over `j` such\n  that `segment_ids[j] == i`.\n\n  If the sum is empty for a given segment ID `i`, `output[i] = 0`.\n\n  Caution: On CPU, values in `segment_ids` are always validated to be sorted,\n  and an error is thrown for indices that are not increasing. On GPU, this\n  does not throw an error for unsorted indices. On GPU, out-of-order indices\n  result in safe but unspecified behavior, which may include treating\n  out-of-order indices as the same as a smaller following index.\n\n  <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n  <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/SegmentSum.png\" alt>\n  </div>\n\n  For example:\n\n  >>> c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])\n  >>> tf.math.segment_sum(c, tf.constant([0, 0, 1])).numpy()\n  array([[5, 5, 5, 5],\n         [5, 6, 7, 8]], dtype=int32)\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor whose size is equal to the size of `data`'s\n      first dimension.  Values should be sorted and can be repeated.\n\n      Caution: The values are always validated to be sorted on CPU, never validated\n      on GPU.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SegmentSum\", name, data, segment_ids)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_segment_sum(\n          (data, segment_ids, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return segment_sum_eager_fallback(\n          data, segment_ids, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            segment_sum, (), dict(data=data, segment_ids=segment_ids,\n                                  name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_segment_sum(\n        (data, segment_ids, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SegmentSum\", data=data, segment_ids=segment_ids, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          segment_sum, (), dict(data=data, segment_ids=segment_ids, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tindices\",\n              _op._get_attr_type(\"Tindices\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SegmentSum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSegmentSum = tf_export(\"raw_ops.SegmentSum\")(_ops.to_raw_op(segment_sum))\n_dispatcher_for_segment_sum = segment_sum._tf_type_based_dispatcher.Dispatch\n\n\ndef segment_sum_eager_fallback(data, segment_ids, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tindices, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ])\n  _inputs_flat = [data, segment_ids]\n  _attrs = (\"T\", _attr_T, \"Tindices\", _attr_Tindices)\n  _result = _execute.execute(b\"SegmentSum\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SegmentSum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef select(condition, x, y, name=None):\n  r\"\"\"Selects elements from `x` or `y`, depending on `condition`.\n\n  The `x`, and `y` tensors must all have the same shape, and the\n  output will also have that shape.\n\n  The `condition` tensor must be a scalar if `x` and `y` are scalars.\n  If `x` and `y` are vectors or higher rank, then `condition` must be either a\n  scalar, a vector with size matching the first dimension of `x`, or must have\n  the same shape as `x`.\n\n  The `condition` tensor acts as a mask that chooses, based on the value at each\n  element, whether the corresponding element / row in the output should be\n  taken from `x` (if true) or `y` (if false).\n\n  If `condition` is a vector and `x` and `y` are higher rank matrices, then\n  it chooses which row (outer dimension) to copy from `x` and `y`.\n  If `condition` has the same shape as `x` and `y`, then it chooses which\n  element to copy from `x` and `y`.\n\n  For example:\n\n  ```python\n  # 'condition' tensor is [[True,  False]\n  #                        [False, True]]\n  # 't' is [[1, 2],\n  #         [3, 4]]\n  # 'e' is [[5, 6],\n  #         [7, 8]]\n  select(condition, t, e)  # => [[1, 6], [7, 4]]\n\n\n  # 'condition' tensor is [True, False]\n  # 't' is [[1, 2],\n  #         [3, 4]]\n  # 'e' is [[5, 6],\n  #         [7, 8]]\n  select(condition, t, e) ==> [[1, 2],\n                               [7, 8]]\n\n  ```\n\n  Args:\n    condition: A `Tensor` of type `bool`.\n    x:  A `Tensor` which may have the same shape as `condition`.\n      If `condition` is rank 1, `x` may have higher rank,\n      but its first dimension must match the size of `condition`.\n    y:  A `Tensor` with the same type and shape as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `t`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Select\", name, condition, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return select_eager_fallback(\n          condition, x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Select\", condition=condition, t=x, e=y, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Select\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSelect = tf_export(\"raw_ops.Select\")(_ops.to_raw_op(select))\n\n\ndef select_eager_fallback(condition, x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [])\n  (x, y) = _inputs_T\n  condition = _ops.convert_to_tensor(condition, _dtypes.bool)\n  _inputs_flat = [condition, x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Select\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Select\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef select_v2(condition, t, e, name=None):\n  r\"\"\"TODO: add doc.\n\n  Args:\n    condition: A `Tensor` of type `bool`.\n    t: A `Tensor`.\n    e: A `Tensor`. Must have the same type as `t`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `t`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SelectV2\", name, condition, t, e)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return select_v2_eager_fallback(\n          condition, t, e, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SelectV2\", condition=condition, t=t, e=e, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SelectV2\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSelectV2 = tf_export(\"raw_ops.SelectV2\")(_ops.to_raw_op(select_v2))\n\n\ndef select_v2_eager_fallback(condition, t, e, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([t, e], ctx, [])\n  (t, e) = _inputs_T\n  condition = _ops.convert_to_tensor(condition, _dtypes.bool)\n  _inputs_flat = [condition, t, e]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"SelectV2\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SelectV2\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sigmoid(x, name=None):\n  r\"\"\"Computes sigmoid of `x` element-wise.\n\n  Specifically, `y = 1 / (1 + exp(-x))`.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Sigmoid\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sigmoid_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Sigmoid\", x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Sigmoid\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSigmoid = tf_export(\"raw_ops.Sigmoid\")(_ops.to_raw_op(sigmoid))\n\n\ndef sigmoid_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Sigmoid\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Sigmoid\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sigmoid_grad(y, dy, name=None):\n  r\"\"\"Computes the gradient of the sigmoid of `x` wrt its input.\n\n  Specifically, `grad = dy * y * (1 - y)`, where `y = sigmoid(x)`, and\n  `dy` is the corresponding input gradient.\n\n  Args:\n    y: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    dy: A `Tensor`. Must have the same type as `y`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `y`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SigmoidGrad\", name, y, dy)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sigmoid_grad_eager_fallback(\n          y, dy, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SigmoidGrad\", y=y, dy=dy, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SigmoidGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSigmoidGrad = tf_export(\"raw_ops.SigmoidGrad\")(_ops.to_raw_op(sigmoid_grad))\n\n\ndef sigmoid_grad_eager_fallback(y, dy, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([y, dy], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  (y, dy) = _inputs_T\n  _inputs_flat = [y, dy]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"SigmoidGrad\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SigmoidGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sign(x, name=None):\n  r\"\"\"Returns an element-wise indication of the sign of a number.\n\n  `y = sign(x) = -1` if `x < 0`; 0 if `x == 0`; 1 if `x > 0`.\n\n  For complex numbers, `y = sign(x) = x / |x|` if `x != 0`, otherwise `y = 0`.\n\n  Example usage:\n  >>> tf.math.sign([0., 2., -3.])\n  <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.,  1., -1.], dtype=float32)>\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Sign\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sign_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Sign\", x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Sign\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSign = tf_export(\"raw_ops.Sign\")(_ops.to_raw_op(sign))\n\n\ndef sign_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Sign\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Sign\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.sin', 'sin')\ndef sin(x, name=None):\n  r\"\"\"Computes sine of x element-wise.\n\n    Given an input tensor, this function computes sine of every\n    element in the tensor. Input range is `(-inf, inf)` and\n    output range is `[-1,1]`.\n\n    ```python\n    x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10, float(\"inf\")])\n    tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]\n    ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Sin\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_sin(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return sin_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            sin, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_sin(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Sin\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          sin, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Sin\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSin = tf_export(\"raw_ops.Sin\")(_ops.to_raw_op(sin))\n_dispatcher_for_sin = sin._tf_type_based_dispatcher.Dispatch\n\n\ndef sin_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Sin\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Sin\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.sinh', 'sinh')\ndef sinh(x, name=None):\n  r\"\"\"Computes hyperbolic sine of x element-wise.\n\n    Given an input tensor, this function computes hyperbolic sine of every\n    element in the tensor. Input range is `[-inf,inf]` and output range\n    is `[-inf,inf]`.\n\n    ```python\n    x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 2, 10, float(\"inf\")])\n    tf.math.sinh(x) ==> [-inf -4.0515420e+03 -5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf]\n    ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Sinh\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_sinh(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return sinh_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            sinh, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_sinh(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Sinh\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          sinh, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Sinh\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSinh = tf_export(\"raw_ops.Sinh\")(_ops.to_raw_op(sinh))\n_dispatcher_for_sinh = sinh._tf_type_based_dispatcher.Dispatch\n\n\ndef sinh_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Sinh\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Sinh\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sobol_sample(dim, num_results, skip, dtype=_dtypes.float32, name=None):\n  r\"\"\"Generates points from the Sobol sequence.\n\n  Creates a Sobol sequence with `num_results` samples. Each sample has dimension\n  `dim`. Skips the first `skip` samples.\n\n  Args:\n    dim: A `Tensor` of type `int32`.\n      Positive scalar `Tensor` representing each sample's dimension.\n    num_results: A `Tensor` of type `int32`.\n      Positive scalar `Tensor` of dtype int32. The number of Sobol points to return\n      in the output.\n    skip: A `Tensor` of type `int32`.\n      Positive scalar `Tensor` of dtype int32. The number of initial points of the\n      Sobol sequence to skip.\n    dtype: An optional `tf.DType` from: `tf.float32, tf.float64`. Defaults to `tf.float32`.\n      The type of the sample. One of: `float32` or `float64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `dtype`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SobolSample\", name, dim, num_results, skip, \"dtype\", dtype)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sobol_sample_eager_fallback(\n          dim, num_results, skip, dtype=dtype, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if dtype is None:\n    dtype = _dtypes.float32\n  dtype = _execute.make_type(dtype, \"dtype\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SobolSample\", dim=dim, num_results=num_results, skip=skip,\n                       dtype=dtype, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"dtype\", _op._get_attr_type(\"dtype\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SobolSample\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSobolSample = tf_export(\"raw_ops.SobolSample\")(_ops.to_raw_op(sobol_sample))\n\n\ndef sobol_sample_eager_fallback(dim, num_results, skip, dtype, name, ctx):\n  if dtype is None:\n    dtype = _dtypes.float32\n  dtype = _execute.make_type(dtype, \"dtype\")\n  dim = _ops.convert_to_tensor(dim, _dtypes.int32)\n  num_results = _ops.convert_to_tensor(num_results, _dtypes.int32)\n  skip = _ops.convert_to_tensor(skip, _dtypes.int32)\n  _inputs_flat = [dim, num_results, skip]\n  _attrs = (\"dtype\", dtype)\n  _result = _execute.execute(b\"SobolSample\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SobolSample\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sparse_bincount(indices, values, dense_shape, size, weights, binary_output=False, name=None):\n  r\"\"\"Counts the number of occurrences of each value in an integer array.\n\n  Outputs a vector with length `size` and the same dtype as `weights`. If\n  `weights` are empty, then index `i` stores the number of times the value `i` is\n  counted in `arr`. If `weights` are non-empty, then index `i` stores the sum of\n  the value in `weights` at each index where the corresponding value in `arr` is\n  `i`.\n\n  Values in `arr` outside of the range [0, size) are ignored.\n\n  Args:\n    indices: A `Tensor` of type `int64`. 2D int64 `Tensor`.\n    values: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      1D int `Tensor`.\n    dense_shape: A `Tensor` of type `int64`. 1D int64 `Tensor`.\n    size: A `Tensor`. Must have the same type as `values`.\n      non-negative int scalar `Tensor`.\n    weights: A `Tensor`. Must be one of the following types: `int32`, `int64`, `float32`, `float64`.\n      is an int32, int64, float32, or float64 `Tensor` with the same\n      shape as `input`, or a length-0 `Tensor`, in which case it acts as all weights\n      equal to 1.\n    binary_output: An optional `bool`. Defaults to `False`.\n      bool; Whether the kernel should count the appearance or number of occurrences.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `weights`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SparseBincount\", name, indices, values, dense_shape, size,\n        weights, \"binary_output\", binary_output)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sparse_bincount_eager_fallback(\n          indices, values, dense_shape, size, weights,\n          binary_output=binary_output, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if binary_output is None:\n    binary_output = False\n  binary_output = _execute.make_bool(binary_output, \"binary_output\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SparseBincount\", indices=indices, values=values,\n                          dense_shape=dense_shape, size=size, weights=weights,\n                          binary_output=binary_output, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"Tidx\", _op._get_attr_type(\"Tidx\"), \"T\",\n              _op._get_attr_type(\"T\"), \"binary_output\",\n              _op._get_attr_bool(\"binary_output\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SparseBincount\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSparseBincount = tf_export(\"raw_ops.SparseBincount\")(_ops.to_raw_op(sparse_bincount))\n\n\ndef sparse_bincount_eager_fallback(indices, values, dense_shape, size, weights, binary_output, name, ctx):\n  if binary_output is None:\n    binary_output = False\n  binary_output = _execute.make_bool(binary_output, \"binary_output\")\n  _attr_Tidx, _inputs_Tidx = _execute.args_to_matching_eager([values, size], ctx, [_dtypes.int32, _dtypes.int64, ])\n  (values, size) = _inputs_Tidx\n  _attr_T, (weights,) = _execute.args_to_matching_eager([weights], ctx, [_dtypes.int32, _dtypes.int64, _dtypes.float32, _dtypes.float64, ])\n  indices = _ops.convert_to_tensor(indices, _dtypes.int64)\n  dense_shape = _ops.convert_to_tensor(dense_shape, _dtypes.int64)\n  _inputs_flat = [indices, values, dense_shape, size, weights]\n  _attrs = (\"Tidx\", _attr_Tidx, \"T\", _attr_T, \"binary_output\", binary_output)\n  _result = _execute.execute(b\"SparseBincount\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SparseBincount\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sparse_mat_mul(a, b, transpose_a=False, transpose_b=False, a_is_sparse=False, b_is_sparse=False, name=None):\n  r\"\"\"Multiply matrix \"a\" by matrix \"b\".\n\n  The inputs must be two-dimensional matrices and the inner dimension of \"a\" must\n  match the outer dimension of \"b\". Both \"a\" and \"b\" must be `Tensor`s not\n  `SparseTensor`s.  This op is optimized for the case where at least one of \"a\" or\n  \"b\" is sparse, in the sense that they have a large proportion of zero values.\n  The breakeven for using this versus a dense matrix multiply on one platform was\n  30% zero values in the sparse matrix.\n\n  The gradient computation of this operation will only take advantage of sparsity\n  in the input gradient when that gradient comes from a Relu.\n\n  Args:\n    a: A `Tensor`. Must be one of the following types: `float32`, `bfloat16`.\n    b: A `Tensor`. Must be one of the following types: `float32`, `bfloat16`.\n    transpose_a: An optional `bool`. Defaults to `False`.\n    transpose_b: An optional `bool`. Defaults to `False`.\n    a_is_sparse: An optional `bool`. Defaults to `False`.\n    b_is_sparse: An optional `bool`. Defaults to `False`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `float32`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SparseMatMul\", name, a, b, \"transpose_a\", transpose_a,\n        \"transpose_b\", transpose_b, \"a_is_sparse\", a_is_sparse, \"b_is_sparse\",\n        b_is_sparse)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sparse_mat_mul_eager_fallback(\n          a, b, transpose_a=transpose_a, transpose_b=transpose_b,\n          a_is_sparse=a_is_sparse, b_is_sparse=b_is_sparse, name=name,\n          ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if transpose_a is None:\n    transpose_a = False\n  transpose_a = _execute.make_bool(transpose_a, \"transpose_a\")\n  if transpose_b is None:\n    transpose_b = False\n  transpose_b = _execute.make_bool(transpose_b, \"transpose_b\")\n  if a_is_sparse is None:\n    a_is_sparse = False\n  a_is_sparse = _execute.make_bool(a_is_sparse, \"a_is_sparse\")\n  if b_is_sparse is None:\n    b_is_sparse = False\n  b_is_sparse = _execute.make_bool(b_is_sparse, \"b_is_sparse\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SparseMatMul\", a=a, b=b, transpose_a=transpose_a,\n                        transpose_b=transpose_b, a_is_sparse=a_is_sparse,\n                        b_is_sparse=b_is_sparse, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"transpose_a\", _op._get_attr_bool(\"transpose_a\"), \"transpose_b\",\n              _op._get_attr_bool(\"transpose_b\"), \"a_is_sparse\",\n              _op._get_attr_bool(\"a_is_sparse\"), \"b_is_sparse\",\n              _op._get_attr_bool(\"b_is_sparse\"), \"Ta\",\n              _op._get_attr_type(\"Ta\"), \"Tb\", _op._get_attr_type(\"Tb\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SparseMatMul\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSparseMatMul = tf_export(\"raw_ops.SparseMatMul\")(_ops.to_raw_op(sparse_mat_mul))\n\n\ndef sparse_mat_mul_eager_fallback(a, b, transpose_a, transpose_b, a_is_sparse, b_is_sparse, name, ctx):\n  if transpose_a is None:\n    transpose_a = False\n  transpose_a = _execute.make_bool(transpose_a, \"transpose_a\")\n  if transpose_b is None:\n    transpose_b = False\n  transpose_b = _execute.make_bool(transpose_b, \"transpose_b\")\n  if a_is_sparse is None:\n    a_is_sparse = False\n  a_is_sparse = _execute.make_bool(a_is_sparse, \"a_is_sparse\")\n  if b_is_sparse is None:\n    b_is_sparse = False\n  b_is_sparse = _execute.make_bool(b_is_sparse, \"b_is_sparse\")\n  _attr_Ta, (a,) = _execute.args_to_matching_eager([a], ctx, [_dtypes.float32, _dtypes.bfloat16, ], _dtypes.float32)\n  _attr_Tb, (b,) = _execute.args_to_matching_eager([b], ctx, [_dtypes.float32, _dtypes.bfloat16, ], _dtypes.float32)\n  _inputs_flat = [a, b]\n  _attrs = (\"transpose_a\", transpose_a, \"transpose_b\", transpose_b,\n  \"a_is_sparse\", a_is_sparse, \"b_is_sparse\", b_is_sparse, \"Ta\", _attr_Ta,\n  \"Tb\", _attr_Tb)\n  _result = _execute.execute(b\"SparseMatMul\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SparseMatMul\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sparse_segment_mean(data, indices, segment_ids, name=None):\n  r\"\"\"Computes the mean along sparse segments of a tensor.\n\n  See `tf.sparse.segment_sum` for usage examples.\n\n  Like `SegmentMean`, but `segment_ids` can have rank less than `data`'s first\n  dimension, selecting a subset of dimension 0, specified by `indices`.\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor. Has same rank as `segment_ids`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor. Values should be sorted and can be repeated.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SparseSegmentMean\", name, data, indices, segment_ids)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sparse_segment_mean_eager_fallback(\n          data, indices, segment_ids, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SparseSegmentMean\", data=data, indices=indices,\n                             segment_ids=segment_ids, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"), \"Tsegmentids\",\n              _op._get_attr_type(\"Tsegmentids\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SparseSegmentMean\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSparseSegmentMean = tf_export(\"raw_ops.SparseSegmentMean\")(_ops.to_raw_op(sparse_segment_mean))\n\n\ndef sparse_segment_mean_eager_fallback(data, indices, segment_ids, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _attr_Tidx, (indices,) = _execute.args_to_matching_eager([indices], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _attr_Tsegmentids, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [data, indices, segment_ids]\n  _attrs = (\"T\", _attr_T, \"Tidx\", _attr_Tidx, \"Tsegmentids\",\n  _attr_Tsegmentids)\n  _result = _execute.execute(b\"SparseSegmentMean\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SparseSegmentMean\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sparse_segment_mean_grad(grad, indices, segment_ids, output_dim0, name=None):\n  r\"\"\"Computes gradients for SparseSegmentMean.\n\n  Returns tensor \"output\" with same shape as grad, except for dimension 0 whose\n  value is output_dim0.\n\n  Args:\n    grad: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n      gradient propagated to the SparseSegmentMean op.\n    indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      indices passed to the corresponding SparseSegmentMean op.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      segment_ids passed to the corresponding SparseSegmentMean op.\n    output_dim0: A `Tensor` of type `int32`.\n      dimension 0 of \"data\" passed to SparseSegmentMean op.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `grad`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SparseSegmentMeanGrad\", name, grad, indices, segment_ids,\n        output_dim0)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sparse_segment_mean_grad_eager_fallback(\n          grad, indices, segment_ids, output_dim0, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SparseSegmentMeanGrad\", grad=grad, indices=indices,\n                                 segment_ids=segment_ids,\n                                 output_dim0=output_dim0, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"), \"Tsegmentids\",\n              _op._get_attr_type(\"Tsegmentids\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SparseSegmentMeanGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSparseSegmentMeanGrad = tf_export(\"raw_ops.SparseSegmentMeanGrad\")(_ops.to_raw_op(sparse_segment_mean_grad))\n\n\ndef sparse_segment_mean_grad_eager_fallback(grad, indices, segment_ids, output_dim0, name, ctx):\n  _attr_T, (grad,) = _execute.args_to_matching_eager([grad], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _attr_Tidx, (indices,) = _execute.args_to_matching_eager([indices], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _attr_Tsegmentids, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  output_dim0 = _ops.convert_to_tensor(output_dim0, _dtypes.int32)\n  _inputs_flat = [grad, indices, segment_ids, output_dim0]\n  _attrs = (\"T\", _attr_T, \"Tidx\", _attr_Tidx, \"Tsegmentids\",\n  _attr_Tsegmentids)\n  _result = _execute.execute(b\"SparseSegmentMeanGrad\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SparseSegmentMeanGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sparse_segment_mean_with_num_segments(data, indices, segment_ids, num_segments, name=None):\n  r\"\"\"Computes the mean along sparse segments of a tensor.\n\n  Like `SparseSegmentMean`, but allows missing ids in `segment_ids`. If an id is\n  missing, the `output` tensor at that position will be zeroed.\n\n  Read\n  [the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\n  for an explanation of segments.\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor. Has same rank as `segment_ids`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor. Values should be sorted and can be repeated.\n    num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      Should equal the number of distinct segment IDs.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SparseSegmentMeanWithNumSegments\", name, data, indices,\n        segment_ids, num_segments)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sparse_segment_mean_with_num_segments_eager_fallback(\n          data, indices, segment_ids, num_segments, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SparseSegmentMeanWithNumSegments\", data=data, indices=indices,\n                                            segment_ids=segment_ids,\n                                            num_segments=num_segments,\n                                            name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"), \"Tnumsegments\",\n              _op._get_attr_type(\"Tnumsegments\"), \"Tsegmentids\",\n              _op._get_attr_type(\"Tsegmentids\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SparseSegmentMeanWithNumSegments\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSparseSegmentMeanWithNumSegments = tf_export(\"raw_ops.SparseSegmentMeanWithNumSegments\")(_ops.to_raw_op(sparse_segment_mean_with_num_segments))\n\n\ndef sparse_segment_mean_with_num_segments_eager_fallback(data, indices, segment_ids, num_segments, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _attr_Tidx, (indices,) = _execute.args_to_matching_eager([indices], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _attr_Tnumsegments, (num_segments,) = _execute.args_to_matching_eager([num_segments], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _attr_Tsegmentids, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [data, indices, segment_ids, num_segments]\n  _attrs = (\"T\", _attr_T, \"Tidx\", _attr_Tidx, \"Tnumsegments\",\n  _attr_Tnumsegments, \"Tsegmentids\", _attr_Tsegmentids)\n  _result = _execute.execute(b\"SparseSegmentMeanWithNumSegments\", 1,\n                             inputs=_inputs_flat, attrs=_attrs, ctx=ctx,\n                             name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SparseSegmentMeanWithNumSegments\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sparse_segment_sqrt_n(data, indices, segment_ids, name=None):\n  r\"\"\"Computes the sum along sparse segments of a tensor divided by the sqrt of N.\n\n  N is the size of the segment being reduced.\n\n  See `tf.sparse.segment_sum` for usage examples.\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor. Has same rank as `segment_ids`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor. Values should be sorted and can be repeated.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SparseSegmentSqrtN\", name, data, indices, segment_ids)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sparse_segment_sqrt_n_eager_fallback(\n          data, indices, segment_ids, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SparseSegmentSqrtN\", data=data, indices=indices,\n                              segment_ids=segment_ids, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"), \"Tsegmentids\",\n              _op._get_attr_type(\"Tsegmentids\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SparseSegmentSqrtN\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSparseSegmentSqrtN = tf_export(\"raw_ops.SparseSegmentSqrtN\")(_ops.to_raw_op(sparse_segment_sqrt_n))\n\n\ndef sparse_segment_sqrt_n_eager_fallback(data, indices, segment_ids, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _attr_Tidx, (indices,) = _execute.args_to_matching_eager([indices], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _attr_Tsegmentids, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [data, indices, segment_ids]\n  _attrs = (\"T\", _attr_T, \"Tidx\", _attr_Tidx, \"Tsegmentids\",\n  _attr_Tsegmentids)\n  _result = _execute.execute(b\"SparseSegmentSqrtN\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SparseSegmentSqrtN\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sparse_segment_sqrt_n_grad(grad, indices, segment_ids, output_dim0, name=None):\n  r\"\"\"Computes gradients for SparseSegmentSqrtN.\n\n  Returns tensor \"output\" with same shape as grad, except for dimension 0 whose\n  value is output_dim0.\n\n  Args:\n    grad: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n      gradient propagated to the SparseSegmentSqrtN op.\n    indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      indices passed to the corresponding SparseSegmentSqrtN op.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      segment_ids passed to the corresponding SparseSegmentSqrtN op.\n    output_dim0: A `Tensor` of type `int32`.\n      dimension 0 of \"data\" passed to SparseSegmentSqrtN op.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `grad`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SparseSegmentSqrtNGrad\", name, grad, indices, segment_ids,\n        output_dim0)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sparse_segment_sqrt_n_grad_eager_fallback(\n          grad, indices, segment_ids, output_dim0, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SparseSegmentSqrtNGrad\", grad=grad, indices=indices,\n                                  segment_ids=segment_ids,\n                                  output_dim0=output_dim0, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"), \"Tsegmentids\",\n              _op._get_attr_type(\"Tsegmentids\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SparseSegmentSqrtNGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSparseSegmentSqrtNGrad = tf_export(\"raw_ops.SparseSegmentSqrtNGrad\")(_ops.to_raw_op(sparse_segment_sqrt_n_grad))\n\n\ndef sparse_segment_sqrt_n_grad_eager_fallback(grad, indices, segment_ids, output_dim0, name, ctx):\n  _attr_T, (grad,) = _execute.args_to_matching_eager([grad], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _attr_Tidx, (indices,) = _execute.args_to_matching_eager([indices], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _attr_Tsegmentids, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  output_dim0 = _ops.convert_to_tensor(output_dim0, _dtypes.int32)\n  _inputs_flat = [grad, indices, segment_ids, output_dim0]\n  _attrs = (\"T\", _attr_T, \"Tidx\", _attr_Tidx, \"Tsegmentids\",\n  _attr_Tsegmentids)\n  _result = _execute.execute(b\"SparseSegmentSqrtNGrad\", 1,\n                             inputs=_inputs_flat, attrs=_attrs, ctx=ctx,\n                             name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SparseSegmentSqrtNGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sparse_segment_sqrt_n_with_num_segments(data, indices, segment_ids, num_segments, name=None):\n  r\"\"\"Computes the sum along sparse segments of a tensor divided by the sqrt of N.\n\n  N is the size of the segment being reduced.\n\n  Like `SparseSegmentSqrtN`, but allows missing ids in `segment_ids`. If an id is\n  missing, the `output` tensor at that position will be zeroed.\n\n  Read\n  [the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\n  for an explanation of segments.\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n    indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor. Has same rank as `segment_ids`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor. Values should be sorted and can be repeated.\n    num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      Should equal the number of distinct segment IDs.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SparseSegmentSqrtNWithNumSegments\", name, data, indices,\n        segment_ids, num_segments)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sparse_segment_sqrt_n_with_num_segments_eager_fallback(\n          data, indices, segment_ids, num_segments, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SparseSegmentSqrtNWithNumSegments\", data=data, indices=indices,\n                                             segment_ids=segment_ids,\n                                             num_segments=num_segments,\n                                             name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"), \"Tnumsegments\",\n              _op._get_attr_type(\"Tnumsegments\"), \"Tsegmentids\",\n              _op._get_attr_type(\"Tsegmentids\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SparseSegmentSqrtNWithNumSegments\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSparseSegmentSqrtNWithNumSegments = tf_export(\"raw_ops.SparseSegmentSqrtNWithNumSegments\")(_ops.to_raw_op(sparse_segment_sqrt_n_with_num_segments))\n\n\ndef sparse_segment_sqrt_n_with_num_segments_eager_fallback(data, indices, segment_ids, num_segments, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _attr_Tidx, (indices,) = _execute.args_to_matching_eager([indices], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _attr_Tnumsegments, (num_segments,) = _execute.args_to_matching_eager([num_segments], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _attr_Tsegmentids, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [data, indices, segment_ids, num_segments]\n  _attrs = (\"T\", _attr_T, \"Tidx\", _attr_Tidx, \"Tnumsegments\",\n  _attr_Tnumsegments, \"Tsegmentids\", _attr_Tsegmentids)\n  _result = _execute.execute(b\"SparseSegmentSqrtNWithNumSegments\", 1,\n                             inputs=_inputs_flat, attrs=_attrs, ctx=ctx,\n                             name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SparseSegmentSqrtNWithNumSegments\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sparse_segment_sum(data, indices, segment_ids, name=None):\n  r\"\"\"Computes the sum along sparse segments of a tensor.\n\n  Read\n  [the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\n  for an explanation of segments.\n\n  Like `SegmentSum`, but `segment_ids` can have rank less than `data`'s first\n  dimension, selecting a subset of dimension 0, specified by `indices`.\n\n  For example:\n\n  ```python\n  c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])\n\n  # Select two rows, one segment.\n  tf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))\n  # => [[0 0 0 0]]\n\n  # Select two rows, two segment.\n  tf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))\n  # => [[ 1  2  3  4]\n  #     [-1 -2 -3 -4]]\n\n  # Select all rows, two segments.\n  tf.sparse_segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))\n  # => [[0 0 0 0]\n  #     [5 6 7 8]]\n\n  # Which is equivalent to:\n  tf.segment_sum(c, tf.constant([0, 0, 1]))\n  ```\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n    indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor. Has same rank as `segment_ids`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor. Values should be sorted and can be repeated.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SparseSegmentSum\", name, data, indices, segment_ids)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sparse_segment_sum_eager_fallback(\n          data, indices, segment_ids, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SparseSegmentSum\", data=data, indices=indices,\n                            segment_ids=segment_ids, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"), \"Tsegmentids\",\n              _op._get_attr_type(\"Tsegmentids\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SparseSegmentSum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSparseSegmentSum = tf_export(\"raw_ops.SparseSegmentSum\")(_ops.to_raw_op(sparse_segment_sum))\n\n\ndef sparse_segment_sum_eager_fallback(data, indices, segment_ids, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.int64, _dtypes.bfloat16, _dtypes.uint16, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tidx, (indices,) = _execute.args_to_matching_eager([indices], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _attr_Tsegmentids, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [data, indices, segment_ids]\n  _attrs = (\"T\", _attr_T, \"Tidx\", _attr_Tidx, \"Tsegmentids\",\n  _attr_Tsegmentids)\n  _result = _execute.execute(b\"SparseSegmentSum\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SparseSegmentSum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sparse_segment_sum_grad(grad, indices, segment_ids, output_dim0, name=None):\n  r\"\"\"Computes gradients for SparseSegmentSum.\n\n  Returns tensor \"output\" with same shape as grad, except for dimension 0 whose\n  value is output_dim0.\n\n  Args:\n    grad: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n      gradient propagated to the SparseSegmentSum op.\n    indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      indices passed to the corresponding SparseSegmentSum op.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      segment_ids passed to the corresponding SparseSegmentSum op.\n    output_dim0: A `Tensor` of type `int32`.\n      dimension 0 of \"data\" passed to SparseSegmentSum op.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `grad`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SparseSegmentSumGrad\", name, grad, indices, segment_ids,\n        output_dim0)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sparse_segment_sum_grad_eager_fallback(\n          grad, indices, segment_ids, output_dim0, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SparseSegmentSumGrad\", grad=grad, indices=indices,\n                                segment_ids=segment_ids,\n                                output_dim0=output_dim0, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"), \"Tsegmentids\",\n              _op._get_attr_type(\"Tsegmentids\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SparseSegmentSumGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSparseSegmentSumGrad = tf_export(\"raw_ops.SparseSegmentSumGrad\")(_ops.to_raw_op(sparse_segment_sum_grad))\n\n\ndef sparse_segment_sum_grad_eager_fallback(grad, indices, segment_ids, output_dim0, name, ctx):\n  _attr_T, (grad,) = _execute.args_to_matching_eager([grad], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  _attr_Tidx, (indices,) = _execute.args_to_matching_eager([indices], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _attr_Tsegmentids, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  output_dim0 = _ops.convert_to_tensor(output_dim0, _dtypes.int32)\n  _inputs_flat = [grad, indices, segment_ids, output_dim0]\n  _attrs = (\"T\", _attr_T, \"Tidx\", _attr_Tidx, \"Tsegmentids\",\n  _attr_Tsegmentids)\n  _result = _execute.execute(b\"SparseSegmentSumGrad\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SparseSegmentSumGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sparse_segment_sum_with_num_segments(data, indices, segment_ids, num_segments, name=None):\n  r\"\"\"Computes the sum along sparse segments of a tensor.\n\n  Like `SparseSegmentSum`, but allows missing ids in `segment_ids`. If an id is\n  missing, the `output` tensor at that position will be zeroed.\n\n  Read\n  [the section on segmentation](https://tensorflow.org/api_docs/python/tf/sparse#Segmentation)\n  for an explanation of segments.\n\n  For example:\n\n  ```python\n  c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])\n\n  tf.sparse_segment_sum_with_num_segments(\n      c, tf.constant([0, 1]), tf.constant([0, 0]), num_segments=3)\n  # => [[0 0 0 0]\n  #     [0 0 0 0]\n  #     [0 0 0 0]]\n\n  tf.sparse_segment_sum_with_num_segments(c,\n                                          tf.constant([0, 1]),\n                                          tf.constant([0, 2],\n                                          num_segments=4))\n  # => [[ 1  2  3  4]\n  #     [ 0  0  0  0]\n  #     [-1 -2 -3 -4]\n  #     [ 0  0  0  0]]\n  ```\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n    indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor. Has same rank as `segment_ids`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A 1-D tensor. Values should be sorted and can be repeated.\n    num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      Should equal the number of distinct segment IDs.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SparseSegmentSumWithNumSegments\", name, data, indices,\n        segment_ids, num_segments)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sparse_segment_sum_with_num_segments_eager_fallback(\n          data, indices, segment_ids, num_segments, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SparseSegmentSumWithNumSegments\", data=data, indices=indices,\n                                           segment_ids=segment_ids,\n                                           num_segments=num_segments,\n                                           name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tidx\",\n              _op._get_attr_type(\"Tidx\"), \"Tnumsegments\",\n              _op._get_attr_type(\"Tnumsegments\"), \"Tsegmentids\",\n              _op._get_attr_type(\"Tsegmentids\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SparseSegmentSumWithNumSegments\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSparseSegmentSumWithNumSegments = tf_export(\"raw_ops.SparseSegmentSumWithNumSegments\")(_ops.to_raw_op(sparse_segment_sum_with_num_segments))\n\n\ndef sparse_segment_sum_with_num_segments_eager_fallback(data, indices, segment_ids, num_segments, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.int64, _dtypes.bfloat16, _dtypes.uint16, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tidx, (indices,) = _execute.args_to_matching_eager([indices], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _attr_Tnumsegments, (num_segments,) = _execute.args_to_matching_eager([num_segments], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _attr_Tsegmentids, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [data, indices, segment_ids, num_segments]\n  _attrs = (\"T\", _attr_T, \"Tidx\", _attr_Tidx, \"Tnumsegments\",\n  _attr_Tnumsegments, \"Tsegmentids\", _attr_Tsegmentids)\n  _result = _execute.execute(b\"SparseSegmentSumWithNumSegments\", 1,\n                             inputs=_inputs_flat, attrs=_attrs, ctx=ctx,\n                             name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SparseSegmentSumWithNumSegments\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sqrt(x, name=None):\n  r\"\"\"Computes square root of x element-wise.\n\n  I.e., \\\\(y = \\sqrt{x} = x^{1/2}\\\\).\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Sqrt\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sqrt_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Sqrt\", x=x, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Sqrt\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSqrt = tf_export(\"raw_ops.Sqrt\")(_ops.to_raw_op(sqrt))\n\n\ndef sqrt_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Sqrt\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Sqrt\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sqrt_grad(y, dy, name=None):\n  r\"\"\"Computes the gradient for the sqrt of `x` wrt its input.\n\n  Specifically, `grad = dy * 0.5 / y`, where `y = sqrt(x)`, and `dy`\n  is the corresponding input gradient.\n\n  Args:\n    y: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    dy: A `Tensor`. Must have the same type as `y`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `y`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SqrtGrad\", name, y, dy)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sqrt_grad_eager_fallback(\n          y, dy, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SqrtGrad\", y=y, dy=dy, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SqrtGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSqrtGrad = tf_export(\"raw_ops.SqrtGrad\")(_ops.to_raw_op(sqrt_grad))\n\n\ndef sqrt_grad_eager_fallback(y, dy, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([y, dy], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  (y, dy) = _inputs_T\n  _inputs_flat = [y, dy]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"SqrtGrad\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SqrtGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.square', 'square')\ndef square(x, name=None):\n  r\"\"\"Computes square of x element-wise.\n\n  I.e., \\\\(y = x * x = x^2\\\\).\n\n  >>> tf.math.square([-2., 0., 3.])\n  <tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `uint8`, `uint16`, `uint32`, `uint64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Square\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_square(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return square_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            square, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_square(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Square\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          square, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Square\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSquare = tf_export(\"raw_ops.Square\")(_ops.to_raw_op(square))\n_dispatcher_for_square = square._tf_type_based_dispatcher.Dispatch\n\n\ndef square_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.uint8, _dtypes.uint16, _dtypes.uint32, _dtypes.uint64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Square\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Square\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.squared_difference', v1=['math.squared_difference', 'squared_difference'])\n@deprecated_endpoints('squared_difference')\ndef squared_difference(x, y, name=None):\n  r\"\"\"Returns conj(x - y)(x - y) element-wise.\n\n  *NOTE*: `math.squared_difference` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"SquaredDifference\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_squared_difference(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return squared_difference_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            squared_difference, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_squared_difference(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"SquaredDifference\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          squared_difference, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"SquaredDifference\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSquaredDifference = tf_export(\"raw_ops.SquaredDifference\")(_ops.to_raw_op(squared_difference))\n_dispatcher_for_squared_difference = squared_difference._tf_type_based_dispatcher.Dispatch\n\n\ndef squared_difference_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"SquaredDifference\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"SquaredDifference\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef sub(x, y, name=None):\n  r\"\"\"Returns x - y element-wise.\n\n  *NOTE*: `tf.subtract` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Both input and output have a range `(-inf, inf)`.\n\n  Example usages below.\n\n  Subtract operation between an array and a scalar:\n\n  >>> x = [1, 2, 3, 4, 5]\n  >>> y = 1\n  >>> tf.subtract(x, y)\n  <tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>\n  >>> tf.subtract(y, x)\n  <tf.Tensor: shape=(5,), dtype=int32,\n  numpy=array([ 0, -1, -2, -3, -4], dtype=int32)>\n\n  Note that binary `-` operator can be used instead:\n\n  >>> x = tf.convert_to_tensor([1, 2, 3, 4, 5])\n  >>> y = tf.convert_to_tensor(1)\n  >>> x - y\n  <tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>\n\n  Subtract operation between an array and a tensor of same shape:\n\n  >>> x = [1, 2, 3, 4, 5]\n  >>> y = tf.constant([5, 4, 3, 2, 1])\n  >>> tf.subtract(y, x)\n  <tf.Tensor: shape=(5,), dtype=int32,\n  numpy=array([ 4,  2,  0, -2, -4], dtype=int32)>\n\n  **Warning**: If one of the inputs (`x` or `y`) is a tensor and the other is a\n  non-tensor, the non-tensor input will adopt (or get casted to) the data type\n  of the tensor input. This can potentially cause unwanted overflow or underflow\n  conversion.\n\n  For example,\n\n  >>> x = tf.constant([1, 2], dtype=tf.int8)\n  >>> y = [2**8 + 1, 2**8 + 2]\n  >>> tf.subtract(x, y)\n  <tf.Tensor: shape=(2,), dtype=int8, numpy=array([0, 0], dtype=int8)>\n\n  When subtracting two input values of different shapes, `tf.subtract` follows the\n  [general broadcasting rules](https://numpy.org/doc/stable/user/basics.broadcasting.html#general-broadcasting-rules)\n  . The two input array shapes are compared element-wise. Starting with the\n  trailing dimensions, the two dimensions either have to be equal or one of them\n  needs to be `1`.\n\n  For example,\n\n  >>> x = np.ones(6).reshape(2, 3, 1)\n  >>> y = np.ones(6).reshape(2, 1, 3)\n  >>> tf.subtract(x, y)\n  <tf.Tensor: shape=(2, 3, 3), dtype=float64, numpy=\n  array([[[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]],\n         [[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]]])>\n\n  Example with inputs of different dimensions:\n\n  >>> x = np.ones(6).reshape(2, 3, 1)\n  >>> y = np.ones(6).reshape(1, 6)\n  >>> tf.subtract(x, y)\n  <tf.Tensor: shape=(2, 3, 6), dtype=float64, numpy=\n  array([[[0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0.]],\n         [[0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0., 0.]]])>\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `uint32`, `uint64`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Sub\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return sub_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Sub\", x=x, y=y, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Sub\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSub = tf_export(\"raw_ops.Sub\")(_ops.to_raw_op(sub))\n\n\ndef sub_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.uint8, _dtypes.int8, _dtypes.uint16, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, _dtypes.uint32, _dtypes.uint64, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Sub\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Sub\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef _sum(input, axis, keep_dims=False, name=None):\n  r\"\"\"Computes the sum of elements across dimensions of a tensor.\n\n  Reduces `input` along the dimensions given in `axis`. Unless\n  `keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in\n  `axis`. If `keep_dims` is true, the reduced dimensions are\n  retained with length 1.\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n      The tensor to reduce.\n    axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      The dimensions to reduce. Must be in the range\n      `[-rank(input), rank(input))`.\n    keep_dims: An optional `bool`. Defaults to `False`.\n      If true, retain reduced dimensions with length 1.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `input`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Sum\", name, input, axis, \"keep_dims\", keep_dims)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return _sum_eager_fallback(\n          input, axis, keep_dims=keep_dims, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Sum\", input=input, reduction_indices=axis, keep_dims=keep_dims,\n               name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"keep_dims\", _op._get_attr_bool(\"keep_dims\"), \"T\",\n              _op._get_attr_type(\"T\"), \"Tidx\", _op._get_attr_type(\"Tidx\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Sum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nSum = tf_export(\"raw_ops.Sum\")(_ops.to_raw_op(_sum))\n\n\ndef _sum_eager_fallback(input, axis, keep_dims, name, ctx):\n  if keep_dims is None:\n    keep_dims = False\n  keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n  _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tidx, (axis,) = _execute.args_to_matching_eager([axis], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [input, axis]\n  _attrs = (\"keep_dims\", keep_dims, \"T\", _attr_T, \"Tidx\", _attr_Tidx)\n  _result = _execute.execute(b\"Sum\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Sum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.tan', 'tan')\ndef tan(x, name=None):\n  r\"\"\"Computes tan of x element-wise.\n\n    Given an input tensor, this function computes tangent of every\n    element in the tensor. Input range is `(-inf, inf)` and\n    output range is `(-inf, inf)`. If input lies outside the boundary, `nan`\n    is returned.\n\n    ```python\n    x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n    tf.math.tan(x) ==> [nan 0.45231566 -0.5463025 1.5574077 2.572152 -1.7925274 0.32097113 nan]\n    ```\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Tan\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_tan(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return tan_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            tan, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_tan(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Tan\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          tan, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Tan\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nTan = tf_export(\"raw_ops.Tan\")(_ops.to_raw_op(tan))\n_dispatcher_for_tan = tan._tf_type_based_dispatcher.Dispatch\n\n\ndef tan_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.int8, _dtypes.int16, _dtypes.int32, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Tan\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Tan\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.tanh', 'nn.tanh', 'tanh')\ndef tanh(x, name=None):\n  r\"\"\"Computes hyperbolic tangent of `x` element-wise.\n\n    Given an input tensor, this function computes hyperbolic tangent of every\n    element in the tensor. Input range is `[-inf, inf]` and\n    output range is `[-1,1]`.\n\n    >>> x = tf.constant([-float(\"inf\"), -5, -0.5, 1, 1.2, 2, 3, float(\"inf\")])\n    >>> tf.math.tanh(x)\n    <tf.Tensor: shape=(8,), dtype=float32, numpy=\n    array([-1.0, -0.99990916, -0.46211717,  0.7615942 ,  0.8336547 ,\n            0.9640276 ,  0.9950547 ,  1.0], dtype=float32)>\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Tanh\", name, x)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_tanh(\n          (x, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return tanh_eager_fallback(\n          x, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            tanh, (), dict(x=x, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_tanh(\n        (x, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Tanh\", x=x, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          tanh, (), dict(x=x, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Tanh\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nTanh = tf_export(\"raw_ops.Tanh\")(_ops.to_raw_op(tanh))\n_dispatcher_for_tanh = tanh._tf_type_based_dispatcher.Dispatch\n\n\ndef tanh_eager_fallback(x, name, ctx):\n  _attr_T, (x,) = _execute.args_to_matching_eager([x], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  _inputs_flat = [x]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Tanh\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Tanh\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef tanh_grad(y, dy, name=None):\n  r\"\"\"Computes the gradient for the tanh of `x` wrt its input.\n\n  Specifically, `grad = dy * (1 - y*y)`, where `y = tanh(x)`, and `dy`\n  is the corresponding input gradient.\n\n  Args:\n    y: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n    dy: A `Tensor`. Must have the same type as `y`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `y`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"TanhGrad\", name, y, dy)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return tanh_grad_eager_fallback(\n          y, dy, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"TanhGrad\", y=y, dy=dy, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"TanhGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nTanhGrad = tf_export(\"raw_ops.TanhGrad\")(_ops.to_raw_op(tanh_grad))\n\n\ndef tanh_grad_eager_fallback(y, dy, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([y, dy], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  (y, dy) = _inputs_T\n  _inputs_flat = [y, dy]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"TanhGrad\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"TanhGrad\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('truncatediv')\ndef truncate_div(x, y, name=None):\n  r\"\"\"Returns x / y element-wise for integer types.\n\n  Truncation designates that negative numbers will round fractional quantities\n  toward zero. I.e. -7 / 5 = -1. This matches C semantics but it is different\n  than Python semantics. See `FloorDiv` for a division function that matches\n  Python Semantics.\n\n  *NOTE*: `truncatediv` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `uint32`, `uint64`, `int64`, `complex64`, `complex128`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"TruncateDiv\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_truncate_div(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return truncate_div_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            truncate_div, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_truncate_div(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"TruncateDiv\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          truncate_div, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"TruncateDiv\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nTruncateDiv = tf_export(\"raw_ops.TruncateDiv\")(_ops.to_raw_op(truncate_div))\n_dispatcher_for_truncate_div = truncate_div._tf_type_based_dispatcher.Dispatch\n\n\ndef truncate_div_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.uint8, _dtypes.int8, _dtypes.uint16, _dtypes.int16, _dtypes.int32, _dtypes.uint32, _dtypes.uint64, _dtypes.int64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"TruncateDiv\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"TruncateDiv\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('truncatemod')\ndef truncate_mod(x, y, name=None):\n  r\"\"\"Returns element-wise remainder of division. This emulates C semantics in that\n\n  the result here is consistent with a truncating divide. E.g. `truncate(x / y) *\n  y + truncate_mod(x, y) = x`.\n\n  *NOTE*: `truncatemod` supports broadcasting. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"TruncateMod\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_truncate_mod(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return truncate_mod_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            truncate_mod, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_truncate_mod(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"TruncateMod\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          truncate_mod, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"TruncateMod\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nTruncateMod = tf_export(\"raw_ops.TruncateMod\")(_ops.to_raw_op(truncate_mod))\n_dispatcher_for_truncate_mod = truncate_mod._tf_type_based_dispatcher.Dispatch\n\n\ndef truncate_mod_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.int32, _dtypes.int64, _dtypes.bfloat16, _dtypes.half, _dtypes.float32, _dtypes.float64, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"TruncateMod\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"TruncateMod\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.unsorted_segment_max', v1=['math.unsorted_segment_max', 'unsorted_segment_max'])\n@deprecated_endpoints('unsorted_segment_max')\ndef unsorted_segment_max(data, segment_ids, num_segments, name=None):\n  r\"\"\"Computes the maximum along segments of a tensor.\n\n  Read\n  [the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\n  for an explanation of segments.\n\n  This operator is similar to `tf.math.unsorted_segment_sum`,\n  Instead of computing the sum over segments, it computes the maximum such that:\n\n  \\\\(output_i = \\max_{j...} data[j...]\\\\) where max is over tuples `j...` such\n  that `segment_ids[j...] == i`.\n\n  If the maximum is empty for a given segment ID `i`, it outputs the smallest\n  possible value for the specific numeric type,\n  `output[i] = numeric_limits<T>::lowest()`.\n\n  If the given segment ID `i` is negative, then the corresponding value is\n  dropped, and will not be included in the result.\n\n  Caution: On CPU, values in `segment_ids` are always validated to be less than\n  `num_segments`, and an error is thrown for out-of-bound indices. On GPU, this\n  does not throw an error for out-of-bound indices. On Gpu, out-of-bound indices\n  result in safe but unspecified behavior, which may include ignoring\n  out-of-bound indices or outputting a tensor with a 0 stored in the first\n  dimension of its shape if `num_segments` is 0.\n\n  <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n  <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/UnsortedSegmentMax.png\" alt>\n  </div>\n\n  For example:\n\n  >>> c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])\n  >>> tf.math.unsorted_segment_max(c, tf.constant([0, 1, 0]), num_segments=2).numpy()\n  array([[4, 3, 3, 4],\n         [5,  6, 7, 8]], dtype=int32)\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A tensor whose shape is a prefix of `data.shape`.\n      The values must be less than `num_segments`.\n\n      Caution: The values are always validated to be in range on CPU, never validated\n      on GPU.\n    num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"UnsortedSegmentMax\", name, data, segment_ids, num_segments)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_unsorted_segment_max(\n          (data, segment_ids, num_segments, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return unsorted_segment_max_eager_fallback(\n          data, segment_ids, num_segments, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            unsorted_segment_max, (), dict(data=data, segment_ids=segment_ids,\n                                           num_segments=num_segments,\n                                           name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_unsorted_segment_max(\n        (data, segment_ids, num_segments, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"UnsortedSegmentMax\", data=data, segment_ids=segment_ids,\n                              num_segments=num_segments, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          unsorted_segment_max, (), dict(data=data, segment_ids=segment_ids,\n                                         num_segments=num_segments, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tindices\",\n              _op._get_attr_type(\"Tindices\"), \"Tnumsegments\",\n              _op._get_attr_type(\"Tnumsegments\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"UnsortedSegmentMax\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nUnsortedSegmentMax = tf_export(\"raw_ops.UnsortedSegmentMax\")(_ops.to_raw_op(unsorted_segment_max))\n_dispatcher_for_unsorted_segment_max = unsorted_segment_max._tf_type_based_dispatcher.Dispatch\n\n\ndef unsorted_segment_max_eager_fallback(data, segment_ids, num_segments, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.int64, _dtypes.bfloat16, _dtypes.uint16, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tindices, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ])\n  _attr_Tnumsegments, (num_segments,) = _execute.args_to_matching_eager([num_segments], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [data, segment_ids, num_segments]\n  _attrs = (\"T\", _attr_T, \"Tindices\", _attr_Tindices, \"Tnumsegments\",\n  _attr_Tnumsegments)\n  _result = _execute.execute(b\"UnsortedSegmentMax\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"UnsortedSegmentMax\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.unsorted_segment_min', v1=['math.unsorted_segment_min', 'unsorted_segment_min'])\n@deprecated_endpoints('unsorted_segment_min')\ndef unsorted_segment_min(data, segment_ids, num_segments, name=None):\n  r\"\"\"Computes the minimum along segments of a tensor.\n\n  Read\n  [the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\n  for an explanation of segments.\n\n  This operator is similar to `tf.math.unsorted_segment_sum`,\n  Instead of computing the sum over segments, it computes the minimum such that:\n\n  \\\\(output_i = \\min_{j...} data_[j...]\\\\) where min is over tuples `j...` such\n  that `segment_ids[j...] == i`.\n\n  If the minimum is empty for a given segment ID `i`, it outputs the largest\n  possible value for the specific numeric type,\n  `output[i] = numeric_limits<T>::max()`.\n\n  For example:\n\n  >>> c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])\n  >>> tf.math.unsorted_segment_min(c, tf.constant([0, 1, 0]), num_segments=2).numpy()\n  array([[1, 2, 2, 1],\n         [5, 6, 7, 8]], dtype=int32)\n\n  If the given segment ID `i` is negative, then the corresponding value is\n  dropped, and will not be included in the result.\n\n  Caution: On CPU, values in `segment_ids` are always validated to be less than\n  `num_segments`, and an error is thrown for out-of-bound indices. On GPU, this\n  does not throw an error for out-of-bound indices. On Gpu, out-of-bound indices\n  result in safe but unspecified behavior, which may include ignoring\n  out-of-bound indices or outputting a tensor with a 0 stored in the first\n  dimension of its shape if `num_segments` is 0.\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A tensor whose shape is a prefix of `data.shape`.\n      The values must be less than `num_segments`.\n\n      Caution: The values are always validated to be in range on CPU, never validated\n      on GPU.\n    num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"UnsortedSegmentMin\", name, data, segment_ids, num_segments)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_unsorted_segment_min(\n          (data, segment_ids, num_segments, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return unsorted_segment_min_eager_fallback(\n          data, segment_ids, num_segments, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            unsorted_segment_min, (), dict(data=data, segment_ids=segment_ids,\n                                           num_segments=num_segments,\n                                           name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_unsorted_segment_min(\n        (data, segment_ids, num_segments, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"UnsortedSegmentMin\", data=data, segment_ids=segment_ids,\n                              num_segments=num_segments, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          unsorted_segment_min, (), dict(data=data, segment_ids=segment_ids,\n                                         num_segments=num_segments, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tindices\",\n              _op._get_attr_type(\"Tindices\"), \"Tnumsegments\",\n              _op._get_attr_type(\"Tnumsegments\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"UnsortedSegmentMin\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nUnsortedSegmentMin = tf_export(\"raw_ops.UnsortedSegmentMin\")(_ops.to_raw_op(unsorted_segment_min))\n_dispatcher_for_unsorted_segment_min = unsorted_segment_min._tf_type_based_dispatcher.Dispatch\n\n\ndef unsorted_segment_min_eager_fallback(data, segment_ids, num_segments, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.int64, _dtypes.bfloat16, _dtypes.uint16, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tindices, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ])\n  _attr_Tnumsegments, (num_segments,) = _execute.args_to_matching_eager([num_segments], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [data, segment_ids, num_segments]\n  _attrs = (\"T\", _attr_T, \"Tindices\", _attr_Tindices, \"Tnumsegments\",\n  _attr_Tnumsegments)\n  _result = _execute.execute(b\"UnsortedSegmentMin\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"UnsortedSegmentMin\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.unsorted_segment_prod', v1=['math.unsorted_segment_prod', 'unsorted_segment_prod'])\n@deprecated_endpoints('unsorted_segment_prod')\ndef unsorted_segment_prod(data, segment_ids, num_segments, name=None):\n  r\"\"\"Computes the product along segments of a tensor.\n\n  Read\n  [the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\n  for an explanation of segments.\n\n  This operator is similar to `tf.math.unsorted_segment_sum`,\n  Instead of computing the sum over segments, it computes the product of all\n  entries belonging to a segment such that:\n\n  \\\\(output_i = \\prod_{j...} data[j...]\\\\) where the product is over tuples\n  `j...` such that `segment_ids[j...] == i`.\n\n  For example:\n\n  >>> c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])\n  >>> tf.math.unsorted_segment_prod(c, tf.constant([0, 1, 0]), num_segments=2).numpy()\n  array([[4, 6, 6, 4],\n         [5, 6, 7, 8]], dtype=int32)\n\n  If there is no entry for a given segment ID `i`, it outputs 1.\n\n  If the given segment ID `i` is negative, then the corresponding value is\n  dropped, and will not be included in the result.\n  Caution: On CPU, values in `segment_ids` are always validated to be less than\n  `num_segments`, and an error is thrown for out-of-bound indices. On GPU, this\n  does not throw an error for out-of-bound indices. On Gpu, out-of-bound indices\n  result in safe but unspecified behavior, which may include ignoring\n  out-of-bound indices or outputting a tensor with a 0 stored in the first\n  dimension of its shape if `num_segments` is 0.\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A tensor whose shape is a prefix of `data.shape`.\n      The values must be less than `num_segments`.\n\n      Caution: The values are always validated to be in range on CPU, never validated\n      on GPU.\n    num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"UnsortedSegmentProd\", name, data, segment_ids, num_segments)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_unsorted_segment_prod(\n          (data, segment_ids, num_segments, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return unsorted_segment_prod_eager_fallback(\n          data, segment_ids, num_segments, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            unsorted_segment_prod, (), dict(data=data,\n                                            segment_ids=segment_ids,\n                                            num_segments=num_segments,\n                                            name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_unsorted_segment_prod(\n        (data, segment_ids, num_segments, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"UnsortedSegmentProd\", data=data, segment_ids=segment_ids,\n                               num_segments=num_segments, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          unsorted_segment_prod, (), dict(data=data, segment_ids=segment_ids,\n                                          num_segments=num_segments,\n                                          name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tindices\",\n              _op._get_attr_type(\"Tindices\"), \"Tnumsegments\",\n              _op._get_attr_type(\"Tnumsegments\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"UnsortedSegmentProd\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nUnsortedSegmentProd = tf_export(\"raw_ops.UnsortedSegmentProd\")(_ops.to_raw_op(unsorted_segment_prod))\n_dispatcher_for_unsorted_segment_prod = unsorted_segment_prod._tf_type_based_dispatcher.Dispatch\n\n\ndef unsorted_segment_prod_eager_fallback(data, segment_ids, num_segments, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tindices, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ])\n  _attr_Tnumsegments, (num_segments,) = _execute.args_to_matching_eager([num_segments], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [data, segment_ids, num_segments]\n  _attrs = (\"T\", _attr_T, \"Tindices\", _attr_Tindices, \"Tnumsegments\",\n  _attr_Tnumsegments)\n  _result = _execute.execute(b\"UnsortedSegmentProd\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"UnsortedSegmentProd\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.unsorted_segment_sum', v1=['math.unsorted_segment_sum', 'unsorted_segment_sum'])\n@deprecated_endpoints('unsorted_segment_sum')\ndef unsorted_segment_sum(data, segment_ids, num_segments, name=None):\n  r\"\"\"Computes the sum along segments of a tensor.\n\n  Read\n  [the section on segmentation](https://tensorflow.org/api_docs/python/tf/math#Segmentation)\n  for an explanation of segments.\n\n  Computes a tensor such that\n  \\\\(output[i] = \\sum_{j...} data[j...]\\\\) where the sum is over tuples `j...` such\n  that `segment_ids[j...] == i`.  Unlike `SegmentSum`, `segment_ids`\n  need not be sorted and need not cover all values in the full\n  range of valid values.\n\n  If the sum is empty for a given segment ID `i`, `output[i] = 0`.\n  If the given segment ID `i` is negative, the value is dropped and will not be\n  added to the sum of the segment.\n\n  `num_segments` should equal the number of distinct segment IDs.\n\n  Caution: On CPU, values in `segment_ids` are always validated to be less than\n  `num_segments`, and an error is thrown for out-of-bound indices. On GPU, this\n  does not throw an error for out-of-bound indices. On Gpu, out-of-bound indices\n  result in safe but unspecified behavior, which may include ignoring\n  out-of-bound indices or outputting a tensor with a 0 stored in the first\n  dimension of its shape if `num_segments` is 0.\n\n  <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n  <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/UnsortedSegmentSum.png\" alt>\n  </div>\n\n  >>> c = [[1,2,3,4], [5,6,7,8], [4,3,2,1]]\n  >>> tf.math.unsorted_segment_sum(c, [0, 1, 0], num_segments=2).numpy()\n  array([[5, 5, 5, 5],\n         [5, 6, 7, 8]], dtype=int32)\n\n  Args:\n    data: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n    segment_ids: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n      A tensor whose shape is a prefix of `data.shape`.\n      The values must be less than `num_segments`.\n\n      Caution: The values are always validated to be in range on CPU, never validated\n      on GPU.\n    num_segments: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `data`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"UnsortedSegmentSum\", name, data, segment_ids, num_segments)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_unsorted_segment_sum(\n          (data, segment_ids, num_segments, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return unsorted_segment_sum_eager_fallback(\n          data, segment_ids, num_segments, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            unsorted_segment_sum, (), dict(data=data, segment_ids=segment_ids,\n                                           num_segments=num_segments,\n                                           name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_unsorted_segment_sum(\n        (data, segment_ids, num_segments, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"UnsortedSegmentSum\", data=data, segment_ids=segment_ids,\n                              num_segments=num_segments, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          unsorted_segment_sum, (), dict(data=data, segment_ids=segment_ids,\n                                         num_segments=num_segments, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"), \"Tindices\",\n              _op._get_attr_type(\"Tindices\"), \"Tnumsegments\",\n              _op._get_attr_type(\"Tnumsegments\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"UnsortedSegmentSum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nUnsortedSegmentSum = tf_export(\"raw_ops.UnsortedSegmentSum\")(_ops.to_raw_op(unsorted_segment_sum))\n_dispatcher_for_unsorted_segment_sum = unsorted_segment_sum._tf_type_based_dispatcher.Dispatch\n\n\ndef unsorted_segment_sum_eager_fallback(data, segment_ids, num_segments, name, ctx):\n  _attr_T, (data,) = _execute.args_to_matching_eager([data], ctx, [_dtypes.float32, _dtypes.float64, _dtypes.int32, _dtypes.uint8, _dtypes.int16, _dtypes.int8, _dtypes.complex64, _dtypes.int64, _dtypes.qint8, _dtypes.quint8, _dtypes.qint32, _dtypes.bfloat16, _dtypes.uint16, _dtypes.complex128, _dtypes.half, _dtypes.uint32, _dtypes.uint64, ])\n  _attr_Tindices, (segment_ids,) = _execute.args_to_matching_eager([segment_ids], ctx, [_dtypes.int32, _dtypes.int64, ])\n  _attr_Tnumsegments, (num_segments,) = _execute.args_to_matching_eager([num_segments], ctx, [_dtypes.int32, _dtypes.int64, ], _dtypes.int32)\n  _inputs_flat = [data, segment_ids, num_segments]\n  _attrs = (\"T\", _attr_T, \"Tindices\", _attr_Tindices, \"Tnumsegments\",\n  _attr_Tnumsegments)\n  _result = _execute.execute(b\"UnsortedSegmentSum\", 1, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"UnsortedSegmentSum\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.xdivy')\ndef xdivy(x, y, name=None):\n  r\"\"\"Returns 0 if x == 0, and x / y otherwise, elementwise.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `complex64`, `complex128`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Xdivy\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_xdivy(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return xdivy_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            xdivy, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_xdivy(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Xdivy\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          xdivy, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Xdivy\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nXdivy = tf_export(\"raw_ops.Xdivy\")(_ops.to_raw_op(xdivy))\n_dispatcher_for_xdivy = xdivy._tf_type_based_dispatcher.Dispatch\n\n\ndef xdivy_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Xdivy\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Xdivy\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\ndef xlog1py(x, y, name=None):\n  r\"\"\"Returns 0 if x == 0, and x * log1p(y) otherwise, elementwise.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `complex64`, `complex128`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Xlog1py\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      return xlog1py_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n  # Add nodes to the TensorFlow graph.\n  _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Xlog1py\", x=x, y=y, name=name)\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Xlog1py\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nXlog1py = tf_export(\"raw_ops.Xlog1py\")(_ops.to_raw_op(xlog1py))\n\n\ndef xlog1py_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Xlog1py\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Xlog1py\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.xlogy')\ndef xlogy(x, y, name=None):\n  r\"\"\"Returns 0 if x == 0, and x * log(y) otherwise, elementwise.\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `complex64`, `complex128`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Xlogy\", name, x, y)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_xlogy(\n          (x, y, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return xlogy_eager_fallback(\n          x, y, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            xlogy, (), dict(x=x, y=y, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_xlogy(\n        (x, y, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Xlogy\", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          xlogy, (), dict(x=x, y=y, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Xlogy\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nXlogy = tf_export(\"raw_ops.Xlogy\")(_ops.to_raw_op(xlogy))\n_dispatcher_for_xlogy = xlogy._tf_type_based_dispatcher.Dispatch\n\n\ndef xlogy_eager_fallback(x, y, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, [_dtypes.half, _dtypes.float32, _dtypes.float64, _dtypes.complex64, _dtypes.complex128, ])\n  (x, y) = _inputs_T\n  _inputs_flat = [x, y]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Xlogy\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Xlogy\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n\n@_dispatch.add_fallback_dispatch_list\n@_dispatch.add_type_based_api_dispatcher\n@tf_export('math.zeta', v1=['math.zeta', 'zeta'])\n@deprecated_endpoints('zeta')\ndef zeta(x, q, name=None):\n  r\"\"\"Compute the Hurwitz zeta function \\\\(\\zeta(x, q)\\\\).\n\n  The Hurwitz zeta function is defined as:\n\n\n  \\\\(\\zeta(x, q) = \\sum_{n=0}^{\\infty} (q + n)^{-x}\\\\)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `float32`, `float64`.\n    q: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  \"\"\"\n  _ctx = _context._context or _context.context()\n  tld = _ctx._thread_local_data\n  if tld.is_eager:\n    try:\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\n        _ctx, \"Zeta\", name, x, q)\n      return _result\n    except _core._NotOkStatusException as e:\n      _ops.raise_from_not_ok_status(e, name)\n    except _core._FallbackException:\n      pass\n    try:\n      _result = _dispatcher_for_zeta(\n          (x, q, name,), None)\n      if _result is not NotImplemented:\n        return _result\n      return zeta_eager_fallback(\n          x, q, name=name, ctx=_ctx)\n    except _core._SymbolicException:\n      pass  # Add nodes to the TensorFlow graph.\n    except (TypeError, ValueError):\n      _result = _dispatch.dispatch(\n            zeta, (), dict(x=x, q=q, name=name)\n          )\n      if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n        return _result\n      raise\n  else:\n    _result = _dispatcher_for_zeta(\n        (x, q, name,), None)\n    if _result is not NotImplemented:\n      return _result\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n        \"Zeta\", x=x, q=q, name=name)\n  except (TypeError, ValueError):\n    _result = _dispatch.dispatch(\n          zeta, (), dict(x=x, q=q, name=name)\n        )\n    if _result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return _result\n    raise\n  _result = _outputs[:]\n  if _execute.must_record_gradient():\n    _attrs = (\"T\", _op._get_attr_type(\"T\"))\n    _inputs_flat = _op.inputs\n    _execute.record_gradient(\n        \"Zeta\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\nZeta = tf_export(\"raw_ops.Zeta\")(_ops.to_raw_op(zeta))\n_dispatcher_for_zeta = zeta._tf_type_based_dispatcher.Dispatch\n\n\ndef zeta_eager_fallback(x, q, name, ctx):\n  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, q], ctx, [_dtypes.float32, _dtypes.float64, ])\n  (x, q) = _inputs_T\n  _inputs_flat = [x, q]\n  _attrs = (\"T\", _attr_T)\n  _result = _execute.execute(b\"Zeta\", 1, inputs=_inputs_flat, attrs=_attrs,\n                             ctx=ctx, name=name)\n  if _execute.must_record_gradient():\n    _execute.record_gradient(\n        \"Zeta\", _inputs_flat, _attrs, _result)\n  _result, = _result\n  return _result\n\n"}, "stage": "understand_function"}
