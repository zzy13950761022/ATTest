
# ==== BLOCK:HEADER START ====
"""
Test cases for tensorflow.python.ops.embedding_ops module.
Generated by ATTest for embedding lookup operations.
"""

import math
import numpy as np
import pytest
import tensorflow as tf
from unittest import mock

# Import target functions
from tensorflow.python.ops.embedding_ops import (
    embedding_lookup_v2,
    embedding_lookup_sparse_v2,
    safe_embedding_lookup_sparse_v2,
)

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Helper functions
def create_embedding_params(shape, dtype=tf.float32):
    """Create embedding parameters with deterministic values."""
    vocab_size, embedding_dim = shape
    # Create deterministic embedding values
    values = np.arange(vocab_size * embedding_dim, dtype=np.float32).reshape(shape)
    return tf.constant(values, dtype=dtype)

def create_sparse_tensor(indices, values, dense_shape, dtype=tf.int64):
    """Create a SparseTensor from indices and values."""
    return tf.SparseTensor(
        indices=indices,
        values=values,
        dense_shape=dense_shape
    )

def assert_tensor_equal(actual, expected, rtol=1e-5, atol=1e-8):
    """Assert that two tensors are equal within tolerance."""
    np.testing.assert_allclose(
        actual.numpy(),
        expected.numpy(),
        rtol=rtol,
        atol=atol
    )

def assert_no_nan_inf(tensor):
    """Assert that tensor contains no NaN or Inf values."""
    # Handle IndexedSlices (common for embedding gradients)
    if isinstance(tensor, tf.IndexedSlices):
        # Check values in IndexedSlices
        tensor_np = tensor.values.numpy()
    else:
        # Regular tensor
        tensor_np = tensor.numpy()
    
    assert not np.any(np.isnan(tensor_np)), "Tensor contains NaN values"
    assert not np.any(np.isinf(tensor_np)), "Tensor contains Inf values"

def compute_l2_norm(tensor, axis=-1):
    """Compute L2 norm of tensor along specified axis."""
    return tf.norm(tensor, ord=2, axis=axis)
# ==== BLOCK:HEADER END ====

class TestEmbeddingOps:
    """Test class for embedding_ops module."""
    
    # ==== BLOCK:CASE_01 START ====
    @pytest.mark.parametrize("params_shape,ids_shape,dtype,max_norm,partition_strategy", [
        # TC-01: 基本功能验证
        ([5, 3], [3], tf.float32, None, "mod"),
        # 参数扩展：更大规模、float64精度、div分区策略
        ([100, 16], [10], tf.float64, None, "div"),
    ])
    def test_embedding_lookup_v2_basic(self, params_shape, ids_shape, dtype, max_norm, partition_strategy):
        """TC-01: embedding_lookup_v2 基本功能验证"""
        # 创建确定性嵌入参数
        vocab_size, embedding_dim = params_shape
        params = create_embedding_params(params_shape, dtype)
        
        # 创建确定性ID
        ids = tf.constant(np.random.randint(0, vocab_size, size=ids_shape), dtype=tf.int32)
        
        # 调用目标函数
        result = embedding_lookup_v2(
            params=params,
            ids=ids,
            max_norm=max_norm,
            name="test_lookup"
        )
        
        # 弱断言验证
        # 1. shape_match: 输出形状匹配预期
        expected_shape = tuple(ids_shape) + (embedding_dim,)
        assert result.shape == expected_shape, f"Expected shape {expected_shape}, got {result.shape}"
        
        # 2. dtype_match: 数据类型匹配
        assert result.dtype == dtype, f"Expected dtype {dtype}, got {result.dtype}"
        
        # 3. values_correct: 值正确性验证（手动计算预期值）
        # 使用tf.gather手动计算预期值
        expected = tf.gather(params, ids)
        assert_tensor_equal(result, expected, rtol=1e-5, atol=1e-8)
        
        # 4. no_nan_inf: 无NaN或Inf值
        assert_no_nan_inf(result)
        
        # 5. 验证max_norm参数（如果提供）
        if max_norm is not None:
            # 计算每个嵌入向量的L2范数
            norms = compute_l2_norm(result, axis=-1)
            # 验证所有范数都小于等于max_norm
            max_actual_norm = tf.reduce_max(norms).numpy()
            assert max_actual_norm <= max_norm + 1e-6, \
                f"Max norm {max_actual_norm} exceeds max_norm {max_norm}"
        
        # 6. 验证梯度（可选，在强断言中启用）
        # 这里只做基本验证，确保可以计算梯度
        with tf.GradientTape() as tape:
            tape.watch(params)
            output = embedding_lookup_v2(params, ids, max_norm=max_norm)
            loss = tf.reduce_sum(output)
        
        # 确保梯度可以计算
        gradients = tape.gradient(loss, params)
        assert gradients is not None, "Gradients should not be None"
        # 注意：在弱断言中不验证梯度的NaN/Inf，因为梯度可能是IndexedSlices类型
        # 强断言中会进行更严格的验证
    # ==== BLOCK:CASE_01 END ====
    
    # ==== BLOCK:CASE_02 START ====
    @pytest.mark.parametrize("params_shape,sparse_indices,sparse_values,combiner,dtype,with_weights,weights_values", [
        # TC-02: 组合器验证 - mean组合器
        ([10, 4], [[0, 0], [0, 1], [1, 0]], [1, 3, 0], "mean", tf.float32, False, None),
        # 参数扩展：sqrtn组合器、带权重、更多稀疏条目
        ([20, 8], [[0, 0], [0, 1], [1, 0], [1, 1]], [2, 5, 8, 12], "sqrtn", tf.float32, True, [0.5, 1.0, 2.0, 0.25]),
    ])
    def test_embedding_lookup_sparse_v2_combiner(self, params_shape, sparse_indices, sparse_values, combiner, dtype, with_weights, weights_values):
        """TC-02: embedding_lookup_sparse_v2 组合器验证"""
        # 创建确定性嵌入参数
        vocab_size, embedding_dim = params_shape
        params = create_embedding_params(params_shape, dtype)
        
        # 创建稀疏张量
        sparse_indices_tf = tf.constant(sparse_indices, dtype=tf.int64)
        sparse_values_tf = tf.constant(sparse_values, dtype=tf.int64)
        
        # 确定密集形状（基于最大索引）
        max_row = max(idx[0] for idx in sparse_indices) + 1
        max_col = max(idx[1] for idx in sparse_indices) + 1
        dense_shape = [max_row, max_col]
        
        sp_ids = tf.SparseTensor(
            indices=sparse_indices_tf,
            values=sparse_values_tf,
            dense_shape=dense_shape
        )
        
        # 创建权重（如果需要）
        sp_weights = None
        if with_weights and weights_values:
            sp_weights = tf.SparseTensor(
                indices=sparse_indices_tf,
                values=tf.constant(weights_values, dtype=dtype),
                dense_shape=dense_shape
            )
        
        # 注意：在弱断言测试中，我们不mock内部函数，而是直接测试功能
        # 因为mock路径问题，我们改为直接测试函数行为
        
        # 调用目标函数
        result = embedding_lookup_sparse_v2(
            params=params,
            sp_ids=sp_ids,
            sp_weights=sp_weights,
            combiner=combiner,
            max_norm=None,
            name="test_sparse_lookup"
        )
        
        # 弱断言验证
        # 1. shape_match: 输出形状匹配预期
        expected_shape = (max_row, embedding_dim)
        assert result.shape == expected_shape, f"Expected shape {expected_shape}, got {result.shape}"
        
        # 2. dtype_match: 数据类型匹配
        assert result.dtype == dtype, f"Expected dtype {dtype}, got {result.dtype}"
        
        # 3. combiner_logic: 组合器逻辑验证
        # 验证组合器参数被正确处理 - 通过检查输出形状和类型来间接验证
        # 注意：在弱断言中，我们不验证具体的组合器实现，只验证函数能正常运行
        
        # 4. no_nan_inf: 无NaN或Inf值
        assert_no_nan_inf(result)
        
        # 5. 验证权重处理
        if with_weights:
            # 确保权重张量被正确传递
            # 在实际测试中，我们会验证权重被正确应用
            pass
        
        # 6. 验证稀疏索引边界
        # 确保所有稀疏值都在有效范围内
        max_id = tf.reduce_max(sparse_values_tf).numpy()
        assert max_id < vocab_size, f"Sparse id {max_id} exceeds vocab size {vocab_size}"
        
        # 7. 验证梯度（基本验证）
        with tf.GradientTape() as tape:
            tape.watch(params)
            output = embedding_lookup_sparse_v2(params, sp_ids, sp_weights, combiner=combiner)
            loss = tf.reduce_sum(output)
        
        # 确保梯度可以计算
        gradients = tape.gradient(loss, params)
        assert gradients is not None, "Gradients should not be None"
    # ==== BLOCK:CASE_02 END ====
    
    # ==== BLOCK:CASE_03 START ====
    @pytest.mark.parametrize("params_shape,ids_shape,dtype,max_norm,partition_strategy", [
        # TC-03: L2范数裁剪边界条件
        ([3, 2], [2], tf.float32, 1.0, "mod"),
        # 参数扩展：float16精度、更严格的裁剪阈值
        ([4, 3], [3], tf.float16, 0.5, "mod"),
    ])
    def test_embedding_lookup_v2_norm_clipping(self, params_shape, ids_shape, dtype, max_norm, partition_strategy):
        """TC-03: L2范数裁剪边界条件"""
        # 创建确定性嵌入参数
        vocab_size, embedding_dim = params_shape
        
        # 创建可能超过范数限制的嵌入参数
        # 使用较大的值以确保需要裁剪
        params_values = np.random.randn(*params_shape).astype(np.float32) * 10.0
        params = tf.constant(params_values, dtype=dtype)
        
        # 创建确定性ID
        ids = tf.constant(np.random.randint(0, vocab_size, size=ids_shape), dtype=tf.int32)
        
        # 注意：在弱断言测试中，我们不mock内部函数，而是直接测试功能
        # 因为mock路径问题，我们改为直接测试函数行为
        
        # 调用目标函数
        result = embedding_lookup_v2(
            params=params,
            ids=ids,
            max_norm=max_norm,
            name="test_norm_clipping"
        )
        
        # 弱断言验证
        # 1. shape_match: 输出形状匹配预期
        expected_shape = tuple(ids_shape) + (embedding_dim,)
        assert result.shape == expected_shape, f"Expected shape {expected_shape}, got {result.shape}"
        
        # 2. dtype_match: 数据类型匹配
        assert result.dtype == dtype, f"Expected dtype {dtype}, got {result.dtype}"
        
        # 3. norm_clipped: 范数裁剪验证
        # 计算每个嵌入向量的L2范数
        norms = compute_l2_norm(result, axis=-1)
        
        # 验证所有范数都小于等于max_norm（加上容差）
        # 根据数据类型调整容差
        if dtype == tf.float16:
            tolerance = 1e-3  # float16精度较低
        else:
            tolerance = 1e-6  # float32/float64精度较高
        
        max_actual_norm = tf.reduce_max(norms).numpy()
        assert max_actual_norm <= max_norm + tolerance, \
            f"Max norm {max_actual_norm} exceeds max_norm {max_norm} (tolerance: {tolerance})"
        
        # 4. no_nan_inf: 无NaN或Inf值
        assert_no_nan_inf(result)
        
        # 5. max_norm_respected: max_norm参数被尊重
        # 验证裁剪逻辑正确性
        # 获取未裁剪的嵌入（通过tf.gather）
        raw_embeddings = tf.gather(params, ids)
        raw_norms = compute_l2_norm(raw_embeddings, axis=-1)
        
        # 对于每个需要裁剪的嵌入，验证它被正确缩放
        for i in range(len(ids)):
            raw_norm = raw_norms[i].numpy()
            clipped_norm = norms[i].numpy()
            
            if raw_norm > max_norm:
                # 如果原始范数超过限制，应该被裁剪
                expected_norm = max_norm
                # 根据数据类型调整容差
                if dtype == tf.float16:
                    tolerance = 1e-3  # float16精度较低
                else:
                    tolerance = 1e-6  # float32/float64精度较高
                assert abs(clipped_norm - expected_norm) < tolerance, \
                    f"Embedding {i}: clipped norm {clipped_norm}, expected {expected_norm}, diff {abs(clipped_norm - expected_norm)} (tolerance: {tolerance})"
            else:
                # 如果原始范数未超过限制，应该保持不变
                # 根据数据类型调整容差
                if dtype == tf.float16:
                    tolerance = 1e-3  # float16精度较低
                else:
                    tolerance = 1e-6  # float32/float64精度较高
                assert abs(clipped_norm - raw_norm) < tolerance, \
                    f"Embedding {i}: norm changed from {raw_norm} to {clipped_norm} but should be unchanged (tolerance: {tolerance})"
        
        # 6. 验证梯度（基本验证）
        with tf.GradientTape() as tape:
            tape.watch(params)
            output = embedding_lookup_v2(params, ids, max_norm=max_norm)
            loss = tf.reduce_sum(output)
        
        # 确保梯度可以计算
        gradients = tape.gradient(loss, params)
        assert gradients is not None, "Gradients should not be None"
        # 注意：在弱断言中不验证梯度的NaN/Inf，因为梯度可能是IndexedSlices类型
        
        # 7. 验证边缘情况：max_norm为0
        if max_norm == 0:
            # 所有嵌入应该变为零向量
            expected_zero = tf.zeros_like(result)
            # 根据数据类型调整容差
            if dtype == tf.float16:
                zero_tolerance = 1e-3
            else:
                zero_tolerance = 1e-8
            assert_tensor_equal(result, expected_zero, rtol=zero_tolerance, atol=zero_tolerance)
    # ==== BLOCK:CASE_03 END ====
    
    # ==== BLOCK:CASE_04 START ====
    # TC-04: 分区策略差异验证 (DEFERRED - placeholder)
    # ==== BLOCK:CASE_04 END ====
    
    # ==== BLOCK:CASE_05 START ====
    # TC-05: safe_embedding_lookup_sparse_v2 默认ID处理 (DEFERRED - placeholder)
    # ==== BLOCK:CASE_05 END ====

# ==== BLOCK:FOOTER START ====
# Footer block - additional helper functions and cleanup
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
# ==== BLOCK:FOOTER END ====