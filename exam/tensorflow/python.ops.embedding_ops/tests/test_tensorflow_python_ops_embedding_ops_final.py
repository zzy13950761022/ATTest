

Generated by ATTest for embedding lookup operations.
"""

import math
import numpy as np
import pytest
import tensorflow as tf
from unittest import mock

# Import target functions
from tensorflow.python.ops.embedding_ops import (
    embedding_lookup_v2,
    embedding_lookup_sparse_v2,
    safe_embedding_lookup_sparse_v2,
)

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# ==== BLOCK:HEADER START ====
# Helper functions
def create_embedding_params(shape, dtype=tf.float32):
    """Create embedding parameters with deterministic values."""
    vocab_size, embedding_dim = shape
    # Create deterministic embedding values
    values = np.arange(vocab_size * embedding_dim, dtype=np.float32).reshape(shape)
    return tf.constant(values, dtype=dtype)

def create_sparse_tensor(indices, values, dense_shape, dtype=tf.int64):
    """Create a SparseTensor from indices and values."""
    return tf.SparseTensor(
        indices=indices,
        values=values,
        dense_shape=dense_shape
    )

def assert_tensor_equal(actual, expected, rtol=1e-5, atol=1e-8):
    """Assert that two tensors are equal within tolerance."""
    np.testing.assert_allclose(
        actual.numpy(),
        expected.numpy(),
        rtol=rtol,
        atol=atol
    )

def assert_no_nan_inf(tensor):
    """Assert that tensor contains no NaN or Inf values."""
    tensor_np = tensor.numpy()
    assert not np.any(np.isnan(tensor_np)), "Tensor contains NaN values"
    assert not np.any(np.isinf(tensor_np)), "Tensor contains Inf values"

def compute_l2_norm(tensor, axis=-1):
    """Compute L2 norm of tensor along specified axis."""
    return tf.norm(tensor, ord=2, axis=axis)
# ==== BLOCK:HEADER END ====

class TestEmbeddingOps:
    """Test class for embedding_ops module."""
    
    # ==== BLOCK:CASE_01 START ====
    @pytest.mark.parametrize("params_shape,ids_shape,dtype,device,max_norm,partition_strategy", [
        # Base case from test plan
        ([5, 3], [3], tf.float32, "cpu", None, "mod"),
        # Parameter extension: larger scale, float64, div partition
        ([100, 16], [10], tf.float64, "cpu", None, "div"),
    ])
    def test_embedding_lookup_v2_basic(self, params_shape, ids_shape, dtype, device, max_norm, partition_strategy):
        """TC-01: embedding_lookup_v2 基本功能验证"""
        # Skip GPU tests if not available
        if device == "gpu" and not tf.config.list_physical_devices('GPU'):
            pytest.skip("GPU not available")
        
        # Create embedding parameters
        vocab_size, embedding_dim = params_shape
        params = create_embedding_params(params_shape, dtype)
        
        # Create deterministic IDs within valid range
        ids = tf.constant(np.random.randint(0, vocab_size, size=ids_shape), dtype=tf.int32)
        
        # Mock the underlying embedding_lookup function to control partition strategy
        with mock.patch('tensorflow.python.ops.embedding_ops.embedding_lookup') as mock_lookup:
            # Configure mock to return expected result
            expected_shape = tuple(ids_shape) + (embedding_dim,)
            expected_result = tf.constant(
                np.random.randn(*expected_shape).astype(dtype.as_numpy_dtype),
                dtype=dtype
            )
            mock_lookup.return_value = expected_result
            
            # Call the function
            result = embedding_lookup_v2(
                params=params,
                ids=ids,
                max_norm=max_norm,
                name="test_lookup"
            )
            
            # Verify mock was called with correct arguments
            mock_lookup.assert_called_once()
            call_args = mock_lookup.call_args
            
            # Check params argument
            assert call_args[0][0] is params
            # Check ids argument
            tf.debugging.assert_equal(call_args[0][1], ids)
            # Check partition strategy (embedding_lookup_v2 always uses "div")
            assert call_args[0][2] == "div"
            # Check name
            assert call_args[0][3] == "test_lookup"
            # Check max_norm keyword argument
            assert call_args[1]['max_norm'] == max_norm
            
            # Weak assertions
            # 1. Shape match
            assert result.shape == expected_shape, f"Expected shape {expected_shape}, got {result.shape}"
            
            # 2. Dtype match
            assert result.dtype == dtype, f"Expected dtype {dtype}, got {result.dtype}"
            
            # 3. Values correct (mocked, so should match exactly)
            tf.debugging.assert_near(result, expected_result, rtol=1e-5)
            
            # 4. No NaN/Inf
            assert_no_nan_inf(result)
            
            # Additional check for max_norm if specified
            if max_norm is not None:
                # Check that norms are clipped
                norms = compute_l2_norm(result)
                assert tf.reduce_all(norms <= max_norm + 1e-5), f"Norms exceed max_norm {max_norm}"
            
            # Return result for potential further inspection
            return result
    # ==== BLOCK:CASE_01 END ====
    
    # ==== BLOCK:CASE_02 START ====
    @pytest.mark.parametrize("params_shape,sparse_indices,sparse_values,combiner,dtype,device,with_weights,weights_values", [
        # Base case from test plan
        ([10, 4], [[0, 0], [0, 1], [1, 0]], [1, 3, 0], "mean", tf.float32, "cpu", False, None),
        # Parameter extension: sqrtn combiner, with weights, more sparse entries
        ([20, 8], [[0, 0], [0, 1], [1, 0], [1, 1]], [2, 5, 8, 12], "sqrtn", tf.float32, "cpu", True, [0.5, 1.0, 2.0, 0.25]),
    ])
    def test_embedding_lookup_sparse_v2_combiner(self, params_shape, sparse_indices, sparse_values, combiner, dtype, device, with_weights, weights_values):
        """TC-02: embedding_lookup_sparse_v2 组合器验证"""
        # Skip GPU tests if not available
        if device == "gpu" and not tf.config.list_physical_devices('GPU'):
            pytest.skip("GPU not available")
        
        # Create embedding parameters
        vocab_size, embedding_dim = params_shape
        params = create_embedding_params(params_shape, dtype)
        
        # Create sparse tensor
        dense_shape = [max(idx[0] for idx in sparse_indices) + 1, 
                      max(idx[1] for idx in sparse_indices) + 1]
        sp_ids = create_sparse_tensor(
            indices=sparse_indices,
            values=sparse_values,
            dense_shape=dense_shape
        )
        
        # Create sparse weights if needed
        sp_weights = None
        if with_weights and weights_values:
            sp_weights = create_sparse_tensor(
                indices=sparse_indices,
                values=weights_values,
                dense_shape=dense_shape
            )
        
        # Mock sparse_ops functions
        mock_targets = [
            'tensorflow.python.ops.sparse_ops.sparse_segment_mean',
            'tensorflow.python.ops.sparse_ops.sparse_segment_sum',
            'tensorflow.python.ops.sparse_ops.sparse_segment_sqrt_n'
        ]
        
        mocks = {}
        for target in mock_targets:
            patcher = mock.patch(target)
            mocks[target] = patcher.start()
        
        try:
            # Configure mocks based on combiner
            expected_shape = [dense_shape[0], embedding_dim]
            expected_result = tf.constant(
                np.random.randn(*expected_shape).astype(dtype.as_numpy_dtype),
                dtype=dtype
            )
            
            # Set up appropriate mock based on combiner
            if combiner == "mean":
                mocks['tensorflow.python.ops.sparse_ops.sparse_segment_mean'].return_value = expected_result
            elif combiner == "sum":
                mocks['tensorflow.python.ops.sparse_ops.sparse_segment_sum'].return_value = expected_result
            elif combiner == "sqrtn":
                mocks['tensorflow.python.ops.sparse_ops.sparse_segment_sqrt_n'].return_value = expected_result
            
            # Call the function
            result = embedding_lookup_sparse_v2(
                params=params,
                sp_ids=sp_ids,
                sp_weights=sp_weights,
                combiner=combiner,
                max_norm=None,
                name="test_sparse_lookup"
            )
            
            # Verify appropriate sparse_segment function was called
            if combiner == "mean":
                mocks['tensorflow.python.ops.sparse_ops.sparse_segment_mean'].assert_called_once()
            elif combiner == "sum":
                mocks['tensorflow.python.ops.sparse_ops.sparse_segment_sum'].assert_called_once()
            elif combiner == "sqrtn":
                mocks['tensorflow.python.ops.sparse_ops.sparse_segment_sqrt_n'].assert_called_once()
            
            # Weak assertions
            # 1. Shape match
            assert result.shape == tuple(expected_shape), f"Expected shape {expected_shape}, got {result.shape}"
            
            # 2. Dtype match
            assert result.dtype == dtype, f"Expected dtype {dtype}, got {result.dtype}"
            
            # 3. Combiner logic (verified via mock call)
            # The mock ensures the correct combiner function was called
            
            # 4. No NaN/Inf
            assert_no_nan_inf(result)
            
            # Additional check for combiner-specific behavior
            if combiner == "sqrtn":
                # sqrtn should handle division by sqrt of sum of squares
                # This is verified by the mock call to sparse_segment_sqrt_n
                pass
            
            # Return result for potential further inspection
            return result
            
        finally:
            # Stop all patchers
            for patcher in mocks.values():
                patcher.stop()
    # ==== BLOCK:CASE_02 END ====
    
    # ==== BLOCK:CASE_03 START ====
    @pytest.mark.parametrize("params_shape,ids_shape,dtype,device,max_norm,partition_strategy", [
        # Base case from test plan
        ([3, 2], [2], tf.float32, "cpu", 1.0, "mod"),
        # Parameter extension: float16 precision, stricter clipping threshold
        ([4, 3], [3], tf.float16, "cpu", 0.5, "mod"),
    ])
    def test_embedding_lookup_v2_norm_clipping(self, params_shape, ids_shape, dtype, device, max_norm, partition_strategy):
        """TC-03: L2范数裁剪边界条件"""
        # Skip GPU tests if not available
        if device == "gpu" and not tf.config.list_physical_devices('GPU'):
            pytest.skip("GPU not available")
        
        # Create embedding parameters
        vocab_size, embedding_dim = params_shape
        params = create_embedding_params(params_shape, dtype)
        
        # Create deterministic IDs within valid range
        ids = tf.constant(np.random.randint(0, vocab_size, size=ids_shape), dtype=tf.int32)
        
        # Mock both embedding_lookup and _clip_by_norm
        with mock.patch('tensorflow.python.ops.embedding_ops.embedding_lookup') as mock_lookup, \
             mock.patch('tensorflow.python.ops.math_ops._clip_by_norm') as mock_clip:
            
            # Create unclipped result (with norms > max_norm)
            expected_shape = tuple(ids_shape) + (embedding_dim,)
            unclipped_result = tf.constant(
                np.ones(expected_shape, dtype=dtype.as_numpy_dtype) * 2.0,  # Will have norm > 1.0
                dtype=dtype
            )
            
            # Create clipped result (with norms <= max_norm)
            clipped_result = tf.constant(
                np.ones(expected_shape, dtype=dtype.as_numpy_dtype) * 0.5,  # Will have norm <= max_norm
                dtype=dtype
            )
            
            # Set up mock chain: embedding_lookup -> _clip_by_norm -> clipped_result
            mock_lookup.return_value = unclipped_result
            mock_clip.return_value = clipped_result
            
            # Call the function with max_norm
            result = embedding_lookup_v2(
                params=params,
                ids=ids,
                max_norm=max_norm,
                name="test_norm_clipping"
            )
            
            # Verify embedding_lookup was called
            mock_lookup.assert_called_once()
            
            # Verify _clip_by_norm was called with correct arguments
            mock_clip.assert_called_once()
            clip_args = mock_clip.call_args
            
            # Check that first argument is the unclipped result
            tf.debugging.assert_near(clip_args[0][0], unclipped_result, rtol=1e-5)
            # Check max_norm argument
            assert clip_args[0][1] == max_norm
            
            # Weak assertions
            # 1. Shape match
            assert result.shape == expected_shape, f"Expected shape {expected_shape}, got {result.shape}"
            
            # 2. Norm clipped
            norms = compute_l2_norm(result)
            # For float16, use larger tolerance
            tolerance = 1e-2 if dtype == tf.float16 else 1e-5
            assert tf.reduce_all(norms <= max_norm + tolerance), \
                f"Norms exceed max_norm {max_norm}: {norms.numpy()}"
            
            # 3. No NaN/Inf
            assert_no_nan_inf(result)
            
            # 4. Max norm respected (already checked in norm_clipped)
            
            # Additional check: verify the result is the clipped version
            tf.debugging.assert_near(result, clipped_result, rtol=tolerance)
            
            # Return result for potential further inspection
            return result
    # ==== BLOCK:CASE_03 END ====
    
    # ==== BLOCK:CASE_04 START ====
    # TC-04: 分区策略差异验证 (DEFERRED - placeholder)
    # This test case is deferred and will be implemented in later rounds
    pass
    # ==== BLOCK:CASE_04 END ====
    
    # ==== BLOCK:CASE_05 START ====
    # TC-05: safe_embedding_lookup_sparse_v2 默认ID处理 (DEFERRED - placeholder)
    # This test case is deferred and will be implemented in later rounds
    pass
    # ==== BLOCK:CASE_05 END ====

# ==== BLOCK:FOOTER START ====
# Additional test cases for edge conditions and error handling

def test_embedding_lookup_v2_empty_params():
    """Test that embedding_lookup_v2 raises ValueError for empty params."""
    with pytest.raises(ValueError, match="params is empty"):
        embedding_lookup_v2(params=[], ids=tf.constant([0]))

def test_embedding_lookup_sparse_v2_invalid_combiner():
    """Test that embedding_lookup_sparse_v2 raises ValueError for invalid combiner."""
    params = tf.constant([[1.0, 2.0], [3.0, 4.0]])
    sp_ids = tf.SparseTensor(
        indices=[[0, 0]],
        values=[0],
        dense_shape=[1, 1]
    )
    
    with pytest.raises(ValueError, match="combiner"):
        embedding_lookup_sparse_v2(
            params=params,
            sp_ids=sp_ids,
            sp_weights=None,
            combiner="invalid_combiner"
        )

def test_safe_embedding_lookup_sparse_v2_empty_params():
    """Test that safe_embedding_lookup_sparse_v2 raises ValueError for empty params."""
    sp_ids = tf.SparseTensor(
        indices=[[0, 0]],
        values=[0],
        dense_shape=[1, 1]
    )
    
    with pytest.raises(ValueError, match="embedding_weights is empty"):
        safe_embedding_lookup_sparse_v2(embedding_weights=[], sparse_ids=sp_ids)
# ==== BLOCK:FOOTER END ====