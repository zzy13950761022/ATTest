"""
Test cases for tensorflow.python.ops.special_math_ops module.
Generated by ATTest for smoke testing.
"""

import numpy as np
import tensorflow as tf
import pytest
from tensorflow.python.ops import special_math_ops

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# ==== BLOCK:HEADER START ====
"""
Test cases for tensorflow.python.ops.special_math_ops module.
Generated by ATTest for smoke testing.
"""

import numpy as np
import tensorflow as tf
import pytest
import math
from tensorflow.python.ops import special_math_ops

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Tolerance settings based on requirements.md
FLOAT32_RTOL = 1e-5
FLOAT32_ATOL = 1e-6
FLOAT64_RTOL = 1e-10
FLOAT64_ATOL = 1e-12

# Helper functions
def create_random_tensor(shape, dtype, distribution="normal"):
    """Create random tensor with specified distribution."""
    if distribution == "normal":
        return tf.random.normal(shape=shape, dtype=dtype, seed=42)
    elif distribution == "uniform":
        return tf.random.uniform(shape=shape, dtype=dtype, seed=42, minval=0.1, maxval=1.0)
    elif distribution == "positive":
        return tf.abs(tf.random.normal(shape=shape, dtype=dtype, seed=42)) + 0.1
    else:
        raise ValueError(f"Unknown distribution: {distribution}")

def assert_tensor_properties(tensor, expected_shape=None, expected_dtype=None):
    """Assert basic tensor properties."""
    assert tensor is not None
    if expected_shape is not None:
        assert tensor.shape.as_list() == list(expected_shape)
    if expected_dtype is not None:
        assert tensor.dtype == expected_dtype
    # Check for finite values
    assert tf.reduce_all(tf.math.is_finite(tensor)).numpy()
# ==== BLOCK:HEADER END ====

# ==== BLOCK:CASE_01 START ====
import pytest

@pytest.mark.parametrize("func_name,dtype,shape,distribution,note", [
    ("bessel_i0", tf.float32, [3, 4], "normal", "贝塞尔函数i0基本功能"),
    ("bessel_i0e", tf.float32, [3, 4], "normal", "修改的贝塞尔函数i0e数值稳定性"),
    ("bessel_i1", tf.float64, [2, 3], "normal", "贝塞尔函数i1不同数据类型"),
    ("bessel_i1e", tf.float32, [2, 2], "normal", "修改的贝塞尔函数i1e数值稳定性"),
    ("bessel_j0", tf.float32, [3, 3], "normal", "第一类贝塞尔函数J0"),
    ("bessel_j1", tf.float64, [2, 2], "normal", "第一类贝塞尔函数J1"),
    ("bessel_k0", tf.float32, [2, 3], "positive", "第二类修正贝塞尔函数K0"),
    ("bessel_k0e", tf.float32, [2, 3], "positive", "修改的贝塞尔函数K0e数值稳定性"),
    ("bessel_k1", tf.float64, [2, 2], "positive", "第二类修正贝塞尔函数K1"),
    ("bessel_k1e", tf.float64, [2, 2], "positive", "修改的贝塞尔函数K1e数值稳定性"),
    ("bessel_y0", tf.float32, [3, 2], "positive", "第二类贝塞尔函数Y0"),
    ("bessel_y1", tf.float64, [2, 3], "positive", "第二类贝塞尔函数Y1"),
])
def test_bessel_functions_basic(func_name, dtype, shape, distribution, note):
    """Test bessel functions with various configurations."""
    # Get the function from special_math_ops
    try:
        func = getattr(special_math_ops, func_name)
    except AttributeError:
        pytest.skip(f"{func_name} not available in this TensorFlow version")
    
    # Create input tensor
    x = create_random_tensor(shape, dtype, distribution=distribution)
    
    # For K and Y functions, ensure positive inputs
    if func_name.startswith("bessel_k") or func_name.startswith("bessel_y"):
        # Ensure positive inputs for K and Y functions
        x = tf.abs(x) + 0.1
    
    # Call the function
    result = func(x)
    
    # Weak assertions (shape, dtype, finite, basic_property)
    assert_tensor_properties(result, expected_shape=shape, expected_dtype=dtype)
    
    # Basic property tests based on function type
    tolerance = FLOAT64_ATOL if dtype == tf.float64 else FLOAT32_ATOL
    
    # Test with specific values to verify properties
    test_values = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=dtype)
    
    # Adjust test values for functions that require positive inputs
    if func_name.startswith("bessel_k") or func_name.startswith("bessel_y"):
        test_values = tf.abs(test_values) + 0.1
    
    test_result = func(test_values)
    
    # Check function-specific properties
    if func_name in ["bessel_i0", "bessel_i0e", "bessel_i1", "bessel_i1e"]:
        # Modified Bessel functions of the first kind
        if func_name in ["bessel_i0", "bessel_i0e"]:
            # Even functions: i0(-x) = i0(x), i0e(-x) = i0e(x)
            assert tf.abs(test_result[0] - test_result[4]) < tolerance
            assert tf.abs(test_result[1] - test_result[3]) < tolerance
            # i0(0) = 1, i0e(0) = 1
            assert tf.abs(test_result[2] - 1.0) < tolerance
        else:  # i1, i1e
            # Odd functions: i1(-x) = -i1(x), i1e(-x) = -i1e(x)
            assert tf.abs(test_result[0] + test_result[4]) < tolerance
            assert tf.abs(test_result[1] + test_result[3]) < tolerance
            # i1(0) = 0, i1e(0) = 0
            assert tf.abs(test_result[2]) < tolerance
    
    elif func_name in ["bessel_j0", "bessel_j1"]:
        # Bessel functions of the first kind
        if func_name == "bessel_j0":
            # Even function: J0(-x) = J0(x)
            assert tf.abs(test_result[0] - test_result[4]) < tolerance
            assert tf.abs(test_result[1] - test_result[3]) < tolerance
            # J0(0) = 1
            assert tf.abs(test_result[2] - 1.0) < tolerance
        else:  # j1
            # Odd function: J1(-x) = -J1(x)
            assert tf.abs(test_result[0] + test_result[4]) < tolerance
            assert tf.abs(test_result[1] + test_result[3]) < tolerance
            # J1(0) = 0
            assert tf.abs(test_result[2]) < tolerance
    
    elif func_name in ["bessel_k0", "bessel_k0e", "bessel_k1", "bessel_k1e"]:
        # Modified Bessel functions of the second kind
        # K functions are defined for positive real arguments
        # They are not symmetric functions
        # Check that results are positive for positive inputs
        assert tf.reduce_all(test_result > 0).numpy()
        
        # Test numerical stability for modified functions (K0e, K1e)
        if func_name in ["bessel_k0e", "bessel_k1e"]:
            # K0e(x) = exp(x) * K0(x), K1e(x) = exp(x) * K1(x)
            # These should be more numerically stable for large x
            large_input = tf.constant([5.0, 10.0, 20.0], dtype=dtype)
            if func_name == "bessel_k0e":
                base_func = special_math_ops.bessel_k0
            else:  # bessel_k1e
                base_func = special_math_ops.bessel_k1
            
            base_result = base_func(large_input)
            modified_result = func(large_input)
            
            # Check that modified version is larger than base for positive x
            # since exp(x) > 1 for x > 0
            assert tf.reduce_all(modified_result > base_result).numpy()
    
    elif func_name in ["bessel_y0", "bessel_y1"]:
        # Bessel functions of the second kind
        # Y functions have singularities at 0
        # For positive inputs, they should be real
        assert tf.reduce_all(tf.math.is_finite(test_result[3:])).numpy()  # Positive inputs
        
        # Check odd/even properties (Y0 is even, Y1 is odd for real arguments)
        if func_name == "bessel_y0":
            # Y0(-x) = Y0(x) for real x (with appropriate branch cut handling)
            # For positive test values, we can't test negative directly
            pass
        else:  # y1
            # Y1(-x) = -Y1(x) for real x (with appropriate branch cut handling)
            pass
    
    # Test numerical stability for modified functions (i0e, i1e)
    if func_name in ["bessel_i0e", "bessel_i1e"]:
        # i0e(x) = exp(-|x|) * i0(x), i1e(x) = exp(-|x|) * i1(x)
        # These should be more numerically stable for large x
        large_input = tf.constant([10.0, 20.0, 30.0], dtype=dtype)
        if func_name == "bessel_i0e":
            base_func = special_math_ops.bessel_i0
        else:  # bessel_i1e
            base_func = special_math_ops.bessel_i1
        
        base_result = base_func(large_input)
        modified_result = func(large_input)
        
        # Check that modified version is smaller than base for large positive x
        # since exp(-x) < 1 for x > 0
        assert tf.reduce_all(modified_result < base_result).numpy()
# ==== BLOCK:CASE_01 END ====

# ==== BLOCK:CASE_02 START ====
import pytest

@pytest.mark.parametrize("equation,shape_a,shape_b,dtype,optimize,note", [
    ("ij,jk->ik", [2, 3], [3, 4], tf.float32, "greedy", "矩阵乘法核心操作"),
    ("i,i->", [5], [5], tf.float32, "optimal", "点积操作与优化策略"),
    ("ij->ji", [3, 4], None, tf.float64, "auto", "转置操作"),
])
def test_einsum_operations(equation, shape_a, shape_b, dtype, optimize, note):
    """Test einsum function with various operations."""
    # Create input tensors
    a = create_random_tensor(shape_a, dtype, distribution="normal")
    
    if shape_b is not None:
        b = create_random_tensor(shape_b, dtype, distribution="normal")
        inputs = (a, b)
    else:
        # For transpose operation, only one input
        inputs = (a,)
    
    # Call einsum with optimization
    result = special_math_ops.einsum(equation, *inputs, optimize=optimize)
    
    # Determine expected shape from equation
    if equation == "ij,jk->ik":
        expected_shape = [shape_a[0], shape_b[1]]
    elif equation == "i,i->":
        expected_shape = []  # Scalar
    elif equation == "ij->ji":
        expected_shape = [shape_a[1], shape_a[0]]
    else:
        # For other equations, we'll compute expected shape dynamically
        # This is a simplified approach - in production you'd want a more robust solution
        expected_shape = None
    
    # Weak assertions (shape, dtype, finite, basic_property)
    if expected_shape is not None:
        assert_tensor_properties(result, expected_shape=expected_shape, expected_dtype=dtype)
    else:
        assert_tensor_properties(result, expected_dtype=dtype)
    
    # Basic property: einsum result should match manual computation
    if equation == "ij,jk->ik":
        manual_result = tf.matmul(a, b)
        diff = tf.abs(result - manual_result)
        max_diff = tf.reduce_max(diff).numpy()
        tolerance = FLOAT64_ATOL if dtype == tf.float64 else FLOAT32_ATOL
        assert max_diff < tolerance, f"einsum and matmul differ by {max_diff}"
    
    elif equation == "i,i->":
        manual_result = tf.reduce_sum(a * b)
        diff = tf.abs(result - manual_result)
        max_diff = diff.numpy()
        tolerance = FLOAT64_ATOL if dtype == tf.float64 else FLOAT32_ATOL
        assert max_diff < tolerance, f"einsum and dot product differ by {max_diff}"
    
    elif equation == "ij->ji":
        manual_result = tf.transpose(a)
        diff = tf.abs(result - manual_result)
        max_diff = tf.reduce_max(diff).numpy()
        tolerance = FLOAT64_ATOL if dtype == tf.float64 else FLOAT32_ATOL
        assert max_diff < tolerance, f"einsum and transpose differ by {max_diff}"
    
    # Test alternative equation format (without explicit output) for binary operations
    if shape_b is not None and "->" in equation:
        # Try implicit output format
        implicit_eq = equation.split("->")[0]
        try:
            result_implicit = special_math_ops.einsum(implicit_eq, *inputs, optimize=optimize)
            diff_implicit = tf.abs(result - result_implicit)
            max_diff_implicit = tf.reduce_max(diff_implicit).numpy()
            tolerance = FLOAT64_ATOL if dtype == tf.float64 else FLOAT32_ATOL
            assert max_diff_implicit < tolerance, f"Explicit and implicit equations differ by {max_diff_implicit}"
        except Exception as e:
            # Some equations might not work with implicit format
            print(f"Implicit equation {implicit_eq} failed: {e}")
    
    # Test with different optimization strategies (for matrix multiplication)
    if equation == "ij,jk->ik":
        for opt_strategy in ["greedy", "optimal", "auto"]:
            try:
                result_opt = special_math_ops.einsum(equation, a, b, optimize=opt_strategy)
                # All optimization strategies should produce same result
                diff_opt = tf.abs(result - result_opt)
                max_diff_opt = tf.reduce_max(diff_opt).numpy()
                tolerance = FLOAT64_ATOL if dtype == tf.float64 else FLOAT32_ATOL
                assert max_diff_opt < tolerance, f"Optimization strategy {opt_strategy} differs by {max_diff_opt}"
            except Exception as e:
                # Some optimization strategies might not be available
                print(f"Optimization strategy {opt_strategy} failed: {e}")
# ==== BLOCK:CASE_02 END ====

# ==== BLOCK:CASE_03 START ====
import pytest

@pytest.mark.parametrize("boundary_type,dtype,note", [
    ("zero", tf.float32, "零边界处理"),
    ("large", tf.float32, "大值边界处理"),
])
def test_dawsn_boundary_values(boundary_type, dtype, note):
    """Test dawsn function with various boundary values."""
    
    if boundary_type == "zero":
        # Zero boundary
        zero_input = tf.constant([0.0], dtype=dtype)
        
        # Small values near zero
        small_values = tf.constant([-0.001, -0.0001, 0.0001, 0.001], dtype=dtype)
        
        # Test zero input
        zero_result = special_math_ops.dawsn(zero_input)
        
        # Weak assertions for zero input
        assert_tensor_properties(zero_result, expected_shape=[1], expected_dtype=dtype)
        
        # Basic property: dawsn(0) = 0
        tolerance = FLOAT64_ATOL if dtype == tf.float64 else FLOAT32_ATOL
        assert tf.abs(zero_result[0]) < tolerance, f"dawsn(0) = {zero_result[0]}, expected 0"
        
        # Test small values
        small_result = special_math_ops.dawsn(small_values)
        assert_tensor_properties(small_result, expected_shape=[4], expected_dtype=dtype)
        
        # Basic property: dawsn is an odd function (dawsn(-x) = -dawsn(x))
        # Check symmetry for small values
        assert tf.abs(small_result[0] + small_result[3]) < tolerance, f"Odd symmetry failed for ±0.001"
        assert tf.abs(small_result[1] + small_result[2]) < tolerance, f"Odd symmetry failed for ±0.0001"
        
    elif boundary_type == "large":
        # Large boundary values
        large_values = tf.constant([-10.0, -5.0, 5.0, 10.0], dtype=dtype)
        large_result = special_math_ops.dawsn(large_values)
        
        # Check that results are finite
        assert tf.reduce_all(tf.math.is_finite(large_result)).numpy()
        
        # Check odd symmetry for large values
        tolerance = FLOAT64_ATOL if dtype == tf.float64 else FLOAT32_ATOL
        assert tf.abs(large_result[0] + large_result[3]) < tolerance, f"Odd symmetry failed for ±10.0"
        assert tf.abs(large_result[1] + large_result[2]) < tolerance, f"Odd symmetry failed for ±5.0"
        
        # Test with very large values
        very_large = tf.constant([100.0, -100.0], dtype=dtype)
        very_large_result = special_math_ops.dawsn(very_large)
        
        # Should still be finite
        assert tf.reduce_all(tf.math.is_finite(very_large_result)).numpy()
        
        # Check odd symmetry
        assert tf.abs(very_large_result[0] + very_large_result[1]) < tolerance
        
        # Test that dawsn approaches 0 as x -> infinity
        # For large x, dawsn(x) ≈ 1/(2x)
        asymptotic_input = tf.constant([100.0], dtype=dtype)
        asymptotic_result = special_math_ops.dawsn(asymptotic_input)
        expected_asymptotic = 1.0 / (2.0 * 100.0)  # 1/(2x)
        # Allow larger tolerance for asymptotic approximation
        assert tf.abs(asymptotic_result[0] - expected_asymptotic) < 1e-3
    
    # Test with known values from documentation (common for both boundary types)
    known_input = tf.constant([-1.0, -0.5, 0.5, 1.0], dtype=dtype)
    known_result = special_math_ops.dawsn(known_input)
    
    # Check odd symmetry for known values
    tolerance = FLOAT64_ATOL if dtype == tf.float64 else FLOAT32_ATOL
    assert tf.abs(known_result[0] + known_result[3]) < tolerance, f"Odd symmetry failed for ±1.0"
    assert tf.abs(known_result[1] + known_result[2]) < tolerance, f"Odd symmetry failed for ±0.5"
# ==== BLOCK:CASE_03 END ====

# ==== BLOCK:CASE_04 START ====
import pytest

@pytest.mark.parametrize("func_name,dtype,shape,distribution,note", [
    ("fresnel_cos", tf.float64, [2, 2], "uniform", "Fresnel余弦函数高精度计算"),
    ("fresnel_sin", tf.float32, [2, 2], "uniform", "Fresnel正弦函数"),
])
def test_fresnel_functions(func_name, dtype, shape, distribution, note):
    """Test fresnel functions with various configurations."""
    # Get the function from special_math_ops
    # Note: fresnel_sin might not be available in all versions
    try:
        func = getattr(special_math_ops, func_name)
    except AttributeError:
        if func_name == "fresnel_sin":
            pytest.skip(f"{func_name} not available in this TensorFlow version")
        else:
            raise
    
    # Create input tensor
    x = create_random_tensor(shape, dtype, distribution=distribution)
    
    # Call the function
    result = func(x)
    
    # Weak assertions (shape, dtype, finite, basic_property)
    assert_tensor_properties(result, expected_shape=shape, expected_dtype=dtype)
    
    # Basic property: fresnel functions are odd functions (C(-x) = -C(x), S(-x) = -S(x))
    # Test with specific values
    test_values = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=dtype)
    test_result = func(test_values)
    
    # Check odd symmetry
    tolerance = FLOAT64_ATOL if dtype == tf.float64 else FLOAT32_ATOL
    assert tf.abs(test_result[0] + test_result[4]) < tolerance, f"Odd symmetry failed for ±2.0"
    assert tf.abs(test_result[1] + test_result[3]) < tolerance, f"Odd symmetry failed for ±1.0"
    
    # Check fresnel(0) = 0
    assert tf.abs(test_result[2]) < tolerance, f"{func_name}(0) = {test_result[2]}, expected 0"
    
    # Test with known values from documentation
    known_input = tf.constant([-1.0, -0.1, 0.1, 1.0], dtype=dtype)
    known_result = func(known_input)
    
    # Check odd symmetry for known values
    assert tf.abs(known_result[0] + known_result[3]) < tolerance, f"Odd symmetry failed for ±1.0"
    assert tf.abs(known_result[1] + known_result[2]) < tolerance, f"Odd symmetry failed for ±0.1"
    
    # Test dtype compatibility: also test with float32 if input is float64
    if dtype == tf.float64:
        x_f32 = tf.cast(x, tf.float32)
        result_f32 = func(x_f32)
        
        # Convert float64 result to float32 for comparison
        result_f64_as_f32 = tf.cast(result, tf.float32)
        
        # Check that float32 and float64 results are close (within float32 tolerance)
        diff = tf.abs(result_f32 - result_f64_as_f32)
        max_diff = tf.reduce_max(diff).numpy()
        assert max_diff < FLOAT32_ATOL, f"float32 and float64 results differ by {max_diff}"
    
    # Test range: fresnel functions should be bounded
    reasonable_input = tf.constant([-5.0, -3.0, -1.0, 0.0, 1.0, 3.0, 5.0], dtype=dtype)
    reasonable_result = func(reasonable_input)
    
    # Check bounds (fresnel integrals are bounded)
    assert tf.reduce_all(tf.abs(reasonable_result) < 1.0).numpy()
    
    # Test consistency between fresnel_cos and fresnel_sin if both are available
    if func_name == "fresnel_cos":
        try:
            from tensorflow.python.ops.special_math_ops import fresnel_sin
            sin_func = fresnel_sin
        except ImportError:
            sin_func = None
        
        if sin_func is not None:
            # Test that both functions produce results with same shape and dtype
            cos_result = result
            sin_result = sin_func(x)
            
            assert cos_result.shape == sin_result.shape
            assert cos_result.dtype == sin_result.dtype
            
            # Both should be finite
            assert tf.reduce_all(tf.math.is_finite(cos_result)).numpy()
            assert tf.reduce_all(tf.math.is_finite(sin_result)).numpy()
# ==== BLOCK:CASE_04 END ====

# ==== BLOCK:CASE_05 START ====
import pytest
import math

@pytest.mark.parametrize("dtype,shape,distribution,note", [
    (tf.float32, [3, 4, 2], "positive", "lbeta基本功能"),
    (tf.float64, [2, 2, 2], "positive", "lbeta高精度计算"),
])
def test_lbeta_functions(dtype, shape, distribution, note):
    """Test lbeta function with various configurations."""
    # Create input tensor with positive values
    x = create_random_tensor(shape, dtype, distribution=distribution)
    
    # Call the function
    result = special_math_ops.lbeta(x)
    
    # Expected shape: reduce along last dimension
    expected_shape = shape[:-1]
    
    # Weak assertions (shape, dtype, finite, basic_property)
    assert_tensor_properties(result, expected_shape=expected_shape, expected_dtype=dtype)
    
    # Basic property: lbeta should be real and finite for positive inputs
    # (already checked by assert_tensor_properties)
    
    # Test with simple 2D case to verify computation
    # For 2-element vector [a, b], lbeta([a,b]) = lgamma(a) + lgamma(b) - lgamma(a+b)
    simple_input = tf.constant([[1.5, 2.5], [3.0, 4.0]], dtype=dtype)
    simple_result = special_math_ops.lbeta(simple_input)
    
    # Manual computation
    def manual_lbeta(ab):
        a, b = ab[0], ab[1]
        return math.lgamma(a) + math.lgamma(b) - math.lgamma(a + b)
    
    # Compare with manual computation
    tolerance = FLOAT64_ATOL if dtype == tf.float64 else FLOAT32_ATOL
    for i in range(2):
        expected = manual_lbeta(simple_input[i].numpy())
        actual = simple_result[i].numpy()
        assert abs(actual - expected) < tolerance, \
            f"lbeta computation mismatch at index {i}: {actual} vs {expected}"
    
    # Test property: lbeta is symmetric for 2D case
    # lbeta([a,b]) = lbeta([b,a])
    symmetric_input1 = tf.constant([[2.0, 3.0]], dtype=dtype)
    symmetric_input2 = tf.constant([[3.0, 2.0]], dtype=dtype)
    result1 = special_math_ops.lbeta(symmetric_input1)
    result2 = special_math_ops.lbeta(symmetric_input2)
    assert tf.abs(result1 - result2) < tolerance, "lbeta should be symmetric"
    
    # Test with empty last dimension (edge case)
    # According to documentation: if last dimension is empty, returns -inf
    empty_input = tf.constant([], shape=[2, 0], dtype=dtype)
    empty_result = special_math_ops.lbeta(empty_input)
    assert empty_result.shape.as_list() == [2]
    # Check that result is -inf
    assert tf.reduce_all(tf.math.is_inf(empty_result)).numpy()
    assert tf.reduce_all(empty_result < 0).numpy()  # Should be -inf
    
    # Test with single element in last dimension
    single_input = tf.constant([[1.5], [2.5], [3.5]], dtype=dtype)
    single_result = special_math_ops.lbeta(single_input)
    # For single element, lbeta(x) = lgamma(x) - lgamma(x) = 0
    assert tf.reduce_all(tf.abs(single_result) < tolerance).numpy()
    
    # Test with very small positive values
    small_input = tf.constant([[1e-10, 1e-10]], dtype=dtype)
    small_result = special_math_ops.lbeta(small_input)
    assert tf.math.is_finite(small_result[0]).numpy()
    
    # Test with large values
    large_input = tf.constant([[1e6, 2e6]], dtype=dtype)
    large_result = special_math_ops.lbeta(large_input)
    assert tf.math.is_finite(large_result[0]).numpy()
    
    # Test with negative values (should produce NaN or error)
    negative_input = tf.constant([[-1.0, 2.0]], dtype=dtype)
    negative_result = special_math_ops.lbeta(negative_input)
    # Result might be NaN for negative inputs
    # This is implementation-dependent
    # We just check it doesn't crash
# ==== BLOCK:CASE_05 END ====

# ==== BLOCK:FOOTER START ====
# Additional test cases for error handling

def test_einsum_invalid_equation():
    """Test einsum with invalid equation string."""
    # Test with empty equation
    a = tf.constant([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32)
    b = tf.constant([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32)
    
    with pytest.raises(ValueError):
        special_math_ops.einsum("", a, b)
    
    # Test with mismatched dimensions
    # Equation "ij,jk->ik" expects: a is i×j, b is j×k
    # Create truly mismatched dimensions: a is 2×3, b is 4×5 (j=3 vs j=4)
    a_mismatch = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=tf.float32)  # 2×3
    b_mismatch = tf.constant([[1.0, 2.0, 3.0, 4.0, 5.0],
                              [6.0, 7.0, 8.0, 9.0, 10.0],
                              [11.0, 12.0, 13.0, 14.0, 15.0],
                              [16.0, 17.0, 18.0, 19.0, 20.0]], dtype=tf.float32)  # 4×5
    
    # TensorFlow throws InvalidArgumentError for dimension mismatches, not ValueError
    with pytest.raises(tf.errors.InvalidArgumentError):
        special_math_ops.einsum("ij,jk->ik", a_mismatch, b_mismatch)
    
    # Test with invalid equation format (missing -> but has output indices)
    with pytest.raises(ValueError):
        special_math_ops.einsum("ij,jk,ik", a, b)  # Should be "ij,jk->ik"
    
    # Test with wrong number of inputs
    with pytest.raises(ValueError):
        special_math_ops.einsum("ij,jk->ik", a)  # Missing second input

def test_bessel_i0_invalid_dtype():
    """Test bessel_i0 with invalid dtype."""
    # Test with integer dtype (should fail)
    int_input = tf.constant([1, 2, 3], dtype=tf.int32)
    with pytest.raises(Exception):  # Could be TypeError or other exception
        special_math_ops.bessel_i0(int_input)

def test_dawsn_extreme_values():
    """Test dawsn with extreme values."""
    dtype = tf.float32
    
    # Test with very large values
    large_input = tf.constant([1e6, -1e6], dtype=dtype)
    large_result = special_math_ops.dawsn(large_input)
    
    # Should still be finite
    assert tf.reduce_all(tf.math.is_finite(large_result)).numpy()
    
    # Check odd symmetry
    assert tf.abs(large_result[0] + large_result[1]) < FLOAT32_ATOL

def test_fresnel_cos_consistency():
    """Test consistency between fresnel_cos and fresnel_sin."""
    # Note: This test requires both functions to be available
    try:
        from tensorflow.python.ops.special_math_ops import fresnel_sin
        
        dtype = tf.float32
        test_input = tf.constant([0.5, 1.0, 2.0], dtype=dtype)
        
        cos_result = special_math_ops.fresnel_cos(test_input)
        sin_result = fresnel_sin(test_input)
        
        # Both should have same shape and dtype
        assert cos_result.shape == sin_result.shape
        assert cos_result.dtype == sin_result.dtype
        
        # Both should be finite
        assert tf.reduce_all(tf.math.is_finite(cos_result)).numpy()
        assert tf.reduce_all(tf.math.is_finite(sin_result)).numpy()
        
    except ImportError:
        # fresnel_sin might not be available in all versions
        pass

def test_lbeta_edge_cases():
    """Test lbeta with various edge cases."""
    dtype = tf.float32
    
    # Test with very small positive values
    small_input = tf.constant([[1e-10, 1e-10]], dtype=dtype)
    small_result = special_math_ops.lbeta(small_input)
    assert tf.math.is_finite(small_result[0]).numpy()
    
    # Test with large values
    large_input = tf.constant([[1e6, 2e6]], dtype=dtype)
    large_result = special_math_ops.lbeta(large_input)
    assert tf.math.is_finite(large_result[0]).numpy()
    
    # Test with negative values (should produce NaN or error)
    negative_input = tf.constant([[-1.0, 2.0]], dtype=dtype)
    negative_result = special_math_ops.lbeta(negative_input)
    # Result might be NaN for negative inputs
    # This is implementation-dependent

def test_expint_function():
    """Test exponential integral function."""
    try:
        func = special_math_ops.expint
    except AttributeError:
        pytest.skip("expint not available in this TensorFlow version")
    
    dtype = tf.float32
    # Create input with positive values (expint is defined for positive real arguments)
    x = tf.constant([0.1, 0.5, 1.0, 2.0, 5.0], dtype=dtype)
    
    result = func(x)
    
    # Check basic properties
    assert result.shape == x.shape
    assert result.dtype == dtype
    assert tf.reduce_all(tf.math.is_finite(result)).numpy()
    
    # expint should be positive for positive inputs
    assert tf.reduce_all(result > 0).numpy()
    
    # Test with known values
    # expint(1) ≈ 1.895117816
    assert tf.abs(result[2] - 1.895117816) < 1e-5
    
    # Test with float64 for higher precision
    dtype64 = tf.float64
    x64 = tf.constant([0.1, 0.5, 1.0, 2.0, 5.0], dtype=dtype64)
    result64 = func(x64)
    
    assert result64.shape == x64.shape
    assert result64.dtype == dtype64
    assert tf.reduce_all(tf.math.is_finite(result64)).numpy()

def test_spence_function():
    """Test Spence's function (dilogarithm)."""
    try:
        func = special_math_ops.spence
    except AttributeError:
        pytest.skip("spence not available in this TensorFlow version")
    
    dtype = tf.float32
    # Create input with values where Spence's function is defined
    x = tf.constant([0.1, 0.5, 1.0, 2.0, 5.0], dtype=dtype)
    
    result = func(x)
    
    # Check basic properties
    assert result.shape == x.shape
    assert result.dtype == dtype
    assert tf.reduce_all(tf.math.is_finite(result)).numpy()
    
    # Test with known values
    # spence(1) = π²/6 ≈ 1.6449340668482264
    assert tf.abs(result[2] - 1.6449340668482264) < 1e-5
    
    # Test with float64 for higher precision
    dtype64 = tf.float64
    x64 = tf.constant([0.1, 0.5, 1.0, 2.0, 5.0], dtype=dtype64)
    result64 = func(x64)
    
    assert result64.shape == x64.shape
    assert result64.dtype == dtype64
    assert tf.reduce_all(tf.math.is_finite(result64)).numpy()
    
    # Test special value: spence(1) = π²/6
    one_input = tf.constant([1.0], dtype=dtype64)
    one_result = func(one_input)
    expected = math.pi**2 / 6.0
    assert tf.abs(one_result[0] - expected) < FLOAT64_ATOL

def test_special_functions_consistency():
    """Test consistency between related special functions."""
    dtype = tf.float32
    
    # Test relationship between i0 and i0e
    x = tf.constant([0.1, 0.5, 1.0, 2.0, 5.0], dtype=dtype)
    
    i0_result = special_math_ops.bessel_i0(x)
    i0e_result = special_math_ops.bessel_i0e(x)
    
    # i0e(x) = exp(-|x|) * i0(x)
    expected_i0e = tf.exp(-tf.abs(x)) * i0_result
    diff = tf.abs(i0e_result - expected_i0e)
    max_diff = tf.reduce_max(diff).numpy()
    assert max_diff < FLOAT32_ATOL, f"i0e relationship failed: max diff = {max_diff}"
    
    # Test relationship between i1 and i1e
    i1_result = special_math_ops.bessel_i1(x)
    try:
        i1e_result = special_math_ops.bessel_i1e(x)
        # i1e(x) = exp(-|x|) * i1(x)
        expected_i1e = tf.exp(-tf.abs(x)) * i1_result
        diff_i1e = tf.abs(i1e_result - expected_i1e)
        max_diff_i1e = tf.reduce_max(diff_i1e).numpy()
        assert max_diff_i1e < FLOAT32_ATOL, f"i1e relationship failed: max diff = {max_diff_i1e}"
    except AttributeError:
        pass  # i1e might not be available
# ==== BLOCK:FOOTER END ====