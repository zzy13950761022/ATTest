"""
Unit tests for tensorflow.python.ops.nn_impl module.
Generated by ATTest-CLI.
"""
import math
import numpy as np
import pytest
import tensorflow as tf
from unittest import mock

# Import the target module
from tensorflow.python.ops import nn_impl

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# ==== BLOCK:HEADER START ====
# Test class and common fixtures
class TestNNImpl:
    """Test class for tensorflow.python.ops.nn_impl module."""
    
    @pytest.fixture(autouse=True)
    def setup_teardown(self):
        """Setup and teardown for each test."""
        # Reset any mocked objects
        self._mocks = {}
        yield
        # Cleanup mocks
        for mock_obj in self._mocks.values():
            if hasattr(mock_obj, 'stop'):
                mock_obj.stop()
        self._mocks.clear()
    
    def mock_dependency(self, module_path, **kwargs):
        """Mock a dependency module."""
        mock_obj = mock.patch(module_path, **kwargs)
        self._mocks[module_path] = mock_obj
        return mock_obj.start()
    
    def assert_tensor_close(self, tensor_a, tensor_b, rtol=1e-6, atol=1e-6):
        """Assert that two tensors are close within tolerance."""
        if isinstance(tensor_a, tf.Tensor):
            tensor_a = tensor_a.numpy()
        if isinstance(tensor_b, tf.Tensor):
            tensor_b = tensor_b.numpy()
        np.testing.assert_allclose(tensor_a, tensor_b, rtol=rtol, atol=atol)
    
    def assert_tensor_finite(self, tensor):
        """Assert that tensor contains only finite values."""
        if isinstance(tensor, tf.Tensor):
            tensor = tensor.numpy()
        assert np.all(np.isfinite(tensor)), f"Tensor contains non-finite values: {tensor}"
    
    def assert_tensor_shape(self, tensor, expected_shape):
        """Assert tensor has expected shape."""
        if isinstance(tensor, tf.Tensor):
            actual_shape = tensor.shape.as_list()
        else:
            actual_shape = list(tensor.shape)
        assert actual_shape == list(expected_shape), \
            f"Shape mismatch: expected {expected_shape}, got {actual_shape}"
    
    def assert_tensor_dtype(self, tensor, expected_dtype):
        """Assert tensor has expected dtype."""
        if isinstance(tensor, tf.Tensor):
            actual_dtype = tensor.dtype.name
        else:
            actual_dtype = tensor.dtype.name
        assert actual_dtype == expected_dtype, \
            f"Dtype mismatch: expected {expected_dtype}, got {actual_dtype}"
    
    def create_random_tensor(self, shape, dtype="float32", mean=0.0, std=1.0):
        """Create a random tensor with specified shape and dtype."""
        np_array = np.random.randn(*shape).astype(dtype) * std + mean
        return tf.constant(np_array, dtype=dtype)
    
    def compute_l2_norm(self, tensor, axis=None, epsilon=1e-12):
        """Compute L2 norm of a tensor along specified axis."""
        if isinstance(tensor, tf.Tensor):
            tensor = tensor.numpy()
        square_sum = np.sum(tensor**2, axis=axis, keepdims=True)
        return np.sqrt(np.maximum(square_sum, epsilon))
# ==== BLOCK:HEADER END ====

# ==== BLOCK:CASE_01 START ====
    @pytest.mark.parametrize(
        "x_shape,dtype,axis,epsilon",
        [
            # Base case from test plan
            ([2, 3], "float32", -1, 1e-12),
            # Parameter extensions
            ([1, 5], "float64", 0, 1e-14),
            ([4, 4, 4], "float32", [1, 2], 1e-12),
        ]
    )
    def test_l2_normalize_basic(self, x_shape, dtype, axis, epsilon):
        """Test l2_normalize basic functionality with weak assertions."""
        # Generate random input tensor
        x_np = np.random.randn(*x_shape).astype(dtype)
        x_tf = tf.constant(x_np, dtype=dtype)
        
        # Call the function
        result = nn_impl.l2_normalize(x_tf, axis=axis, epsilon=epsilon)
        
        # Weak assertions (shape, dtype, finite)
        self.assert_tensor_shape(result, x_shape)
        self.assert_tensor_dtype(result, dtype)
        self.assert_tensor_finite(result)
        
        # Verify L2 norm is close to 1 along specified axis
        if isinstance(axis, list):
            # For multiple axes, compute norm over all specified axes
            norm_axes = tuple(axis)
        else:
            norm_axes = axis
        
        # Compute L2 norm manually
        if norm_axes is not None:
            # Compute norm along specified axis/axes
            square_sum = np.sum(x_np**2, axis=norm_axes, keepdims=True)
            norm = np.sqrt(np.maximum(square_sum, epsilon))
            expected = x_np / norm
            
            # Reshape expected to match result shape
            if isinstance(norm_axes, int):
                # For single axis, numpy keeps dimension
                pass
            elif isinstance(norm_axes, list) and len(norm_axes) > 1:
                # For multiple axes, we need to handle broadcasting
                # numpy reduces all specified dimensions
                pass
            
            # Compare with tolerance
            self.assert_tensor_close(result, expected, rtol=1e-6, atol=1e-6)
        else:
            # When axis=None, normalize over all dimensions
            norm = np.sqrt(np.maximum(np.sum(x_np**2), epsilon))
            expected = x_np / norm
            self.assert_tensor_close(result, expected, rtol=1e-6, atol=1e-6)
        
        # Additional weak assertion: norm should be close to 1
        if norm_axes is not None:
            # Compute norm of result along the same axis
            result_np = result.numpy()
            result_norm = np.sqrt(np.sum(result_np**2, axis=norm_axes))
            
            # Check that norm is close to 1 (within tolerance)
            if isinstance(result_norm, np.ndarray):
                assert np.allclose(result_norm, 1.0, rtol=1e-6, atol=1e-6), \
                    f"L2 norm not close to 1: {result_norm}"
            else:
                assert abs(result_norm - 1.0) < 1e-6, \
                    f"L2 norm not close to 1: {result_norm}"
# ==== BLOCK:CASE_01 END ====

# ==== BLOCK:CASE_02 START ====
    @pytest.mark.parametrize(
        "features_shape,dtype,beta",
        [
            # Base case from test plan
            ([3, 4], "float32", 1.0),
            # Parameter extension
            ([5], "float64", 0.5),
        ]
    )
    def test_swish_activation(self, features_shape, dtype, beta):
        """Test swish activation function with weak assertions."""
        # Generate random input tensor
        features_np = np.random.randn(*features_shape).astype(dtype)
        features_tf = tf.constant(features_np, dtype=dtype)
        
        # Call the function
        result = nn_impl.swish(features_tf, beta=beta)
        
        # Weak assertions (shape, dtype, finite)
        self.assert_tensor_shape(result, features_shape)
        self.assert_tensor_dtype(result, dtype)
        self.assert_tensor_finite(result)
        
        # Verify swish property: result = x * sigmoid(beta * x)
        # Compute expected value manually
        beta_tensor = tf.constant(beta, dtype=dtype)
        expected = features_tf * tf.math.sigmoid(beta_tensor * features_tf)
        
        # Compare with tolerance
        self.assert_tensor_close(result, expected, rtol=1e-6, atol=1e-6)
        
        # Additional weak assertion: swish should preserve sign
        result_np = result.numpy()
        features_np = features_np
        
        # For positive inputs, output should be positive
        pos_mask = features_np > 0
        if np.any(pos_mask):
            assert np.all(result_np[pos_mask] > 0), \
                "Swish should produce positive output for positive inputs"
        
        # For negative inputs, output should be negative
        neg_mask = features_np < 0
        if np.any(neg_mask):
            assert np.all(result_np[neg_mask] < 0), \
                "Swish should produce negative output for negative inputs"
        
        # For zero inputs, output should be zero
        zero_mask = features_np == 0
        if np.any(zero_mask):
            assert np.allclose(result_np[zero_mask], 0, atol=1e-6), \
                "Swish should produce zero output for zero inputs"
        
        # Test edge cases
        # Test with beta = 0 (should be x * 0.5)
        if beta == 0:
            expected_beta_zero = features_tf * 0.5
            self.assert_tensor_close(result, expected_beta_zero, rtol=1e-6, atol=1e-6)
        
        # Test with very large beta (should approximate ReLU)
        # Note: This is a property check, not an assertion
        if beta > 10:
            # For large beta, sigmoid(beta*x) approaches step function
            # swish(x) ≈ x for x > 0, ≈ 0 for x < 0
            pass
# ==== BLOCK:CASE_02 END ====

# ==== BLOCK:CASE_03 START ====
    @pytest.mark.parametrize(
        "x_shape,dtype,training",
        [
            # Base case from test plan
            ([2, 3, 4], "float32", True),
            # Parameter extension
            ([10, 20], "float32", False),
        ]
    )
    def test_batch_normalization(self, x_shape, dtype, training):
        """Test batch_normalization with weak assertions."""
        # Generate random input tensor
        x_np = np.random.randn(*x_shape).astype(dtype)
        x_tf = tf.constant(x_np, dtype=dtype)
        
        # Create mean and variance tensors
        # For batch normalization, mean and variance should have compatible shapes
        # If training=True, we use batch statistics
        # If training=False, we use moving statistics
        
        # Determine the last dimension (depth/channel dimension)
        if len(x_shape) >= 2:
            # Common case: last dimension is feature/channel dimension
            depth_dim = x_shape[-1]
            mean_shape = [depth_dim]
            variance_shape = [depth_dim]
        else:
            # For 1D tensors, use scalar mean/variance
            mean_shape = []
            variance_shape = []
        
        # Create mean and variance
        mean_np = np.random.randn(*mean_shape).astype(dtype) * 0.1  # Small values
        variance_np = np.abs(np.random.randn(*variance_shape).astype(dtype)) * 0.1 + 0.01  # Positive
        
        mean_tf = tf.constant(mean_np, dtype=dtype)
        variance_tf = tf.constant(variance_np, dtype=dtype)
        
        # Create optional scale and offset
        scale_np = np.random.randn(*mean_shape).astype(dtype) * 0.5 + 1.0
        offset_np = np.random.randn(*mean_shape).astype(dtype) * 0.2
        
        scale_tf = tf.constant(scale_np, dtype=dtype)
        offset_tf = tf.constant(offset_np, dtype=dtype)
        
        # Set variance epsilon
        variance_epsilon = 1e-3
        
        # Mock the dependencies if needed
        # According to test plan, CASE_03 requires mock for math_ops, array_ops, check_ops
        # But for basic functionality test, we can test without mock first
        
        # Call the function with all parameters
        result = nn_impl.batch_normalization(
            x=x_tf,
            mean=mean_tf,
            variance=variance_tf,
            offset=offset_tf,
            scale=scale_tf,
            variance_epsilon=variance_epsilon
        )
        
        # Weak assertions (shape, dtype, finite)
        self.assert_tensor_shape(result, x_shape)
        self.assert_tensor_dtype(result, dtype)
        self.assert_tensor_finite(result)
        
        # Compute expected value manually: (x - mean) / sqrt(variance + epsilon) * scale + offset
        # Handle broadcasting: mean/variance/scale/offset might need to be broadcasted to x shape
        
        # Reshape parameters for broadcasting
        if len(mean_shape) == 1 and len(x_shape) > 1:
            # Reshape to [1, 1, ..., depth] for broadcasting
            reshape_dims = [1] * (len(x_shape) - 1) + [depth_dim]
            mean_np_reshaped = mean_np.reshape(reshape_dims)
            variance_np_reshaped = variance_np.reshape(reshape_dims)
            scale_np_reshaped = scale_np.reshape(reshape_dims)
            offset_np_reshaped = offset_np.reshape(reshape_dims)
        else:
            mean_np_reshaped = mean_np
            variance_np_reshaped = variance_np
            scale_np_reshaped = scale_np
            offset_np_reshaped = offset_np
        
        # Manual computation
        inv = 1.0 / np.sqrt(variance_np_reshaped + variance_epsilon)
        inv_scaled = inv * scale_np_reshaped
        expected = x_np * inv_scaled + (offset_np_reshaped - mean_np_reshaped * inv_scaled)
        
        # Compare with tolerance
        self.assert_tensor_close(result, expected, rtol=1e-6, atol=1e-6)
        
        # Additional weak assertion: normalized range check
        # When scale=1 and offset=0, normalized values should have mean ~0 and variance ~1
        # But with arbitrary scale/offset, we can't make this assertion
        
        # Test edge cases
        # Test with scale=None
        result_no_scale = nn_impl.batch_normalization(
            x=x_tf,
            mean=mean_tf,
            variance=variance_tf,
            offset=offset_tf,
            scale=None,
            variance_epsilon=variance_epsilon
        )
        self.assert_tensor_finite(result_no_scale)
        self.assert_tensor_shape(result_no_scale, x_shape)
        
        # Test with offset=None
        result_no_offset = nn_impl.batch_normalization(
            x=x_tf,
            mean=mean_tf,
            variance=variance_tf,
            offset=None,
            scale=scale_tf,
            variance_epsilon=variance_epsilon
        )
        self.assert_tensor_finite(result_no_offset)
        self.assert_tensor_shape(result_no_offset, x_shape)
        
        # Test with both scale and offset as None
        result_basic = nn_impl.batch_normalization(
            x=x_tf,
            mean=mean_tf,
            variance=variance_tf,
            offset=None,
            scale=None,
            variance_epsilon=variance_epsilon
        )
        self.assert_tensor_finite(result_basic)
        self.assert_tensor_shape(result_basic, x_shape)
        
        # Verify that with scale=1 and offset=0, we get pure normalization
        scale_one = tf.constant(1.0, dtype=dtype)
        offset_zero = tf.constant(0.0, dtype=dtype)
        
        result_pure = nn_impl.batch_normalization(
            x=x_tf,
            mean=mean_tf,
            variance=variance_tf,
            offset=offset_zero,
            scale=scale_one,
            variance_epsilon=variance_epsilon
        )
        
        # Compute expected pure normalization
        expected_pure = (x_np - mean_np_reshaped) / np.sqrt(variance_np_reshaped + variance_epsilon)
        self.assert_tensor_close(result_pure, expected_pure, rtol=1e-6, atol=1e-6)
# ==== BLOCK:CASE_03 END ====

# ==== BLOCK:CASE_04 START ====
    @pytest.mark.parametrize(
        "x_shape,dtype,axes,keepdims",
        [
            # Base case from test plan
            ([2, 3, 4, 5], "float32", [1, 2], False),
            # Parameter extension
            ([3], "float64", 0, True),
        ]
    )
    def test_moments_statistics(self, x_shape, dtype, axes, keepdims):
        """Test moments function for computing mean and variance with weak assertions."""
        # Generate random input tensor
        x_np = np.random.randn(*x_shape).astype(dtype)
        x_tf = tf.constant(x_np, dtype=dtype)
        
        # Call the function
        mean_tf, variance_tf = nn_impl.moments(
            x=x_tf,
            axes=axes,
            keepdims=keepdims
        )
        
        # Weak assertions for mean (shape, dtype, finite)
        # Compute expected shape for mean
        if isinstance(axes, int):
            axes_list = [axes]
        else:
            axes_list = list(axes)
        
        # Calculate expected shape
        expected_shape = list(x_shape)
        if not keepdims:
            # Remove dimensions that are reduced
            for axis in sorted(axes_list, reverse=True):
                # Handle negative axis indices
                if axis < 0:
                    axis = len(x_shape) + axis
                expected_shape.pop(axis)
        else:
            # Keep dimensions but set them to 1
            for axis in axes_list:
                if axis < 0:
                    axis = len(x_shape) + axis
                expected_shape[axis] = 1
        
        self.assert_tensor_shape(mean_tf, expected_shape)
        self.assert_tensor_dtype(mean_tf, dtype)
        self.assert_tensor_finite(mean_tf)
        
        # Weak assertions for variance (shape, dtype, finite)
        self.assert_tensor_shape(variance_tf, expected_shape)
        self.assert_tensor_dtype(variance_tf, dtype)
        self.assert_tensor_finite(variance_tf)
        
        # Compute expected mean and variance using numpy
        # Note: numpy uses unbiased variance by default (ddof=1), 
        # but TensorFlow uses sample variance (ddof=0)
        
        # Fix: numpy.mean expects tuple for multiple axes, not list
        if isinstance(axes, list):
            axes_tuple = tuple(axes)
        else:
            axes_tuple = axes
        
        expected_mean = np.mean(x_np, axis=axes_tuple, keepdims=keepdims)
        
        # For variance, we need to match TensorFlow's implementation
        # TensorFlow computes: mean((x - mean(x))^2)
        if keepdims:
            # When keepdims=True, we need to handle broadcasting
            mean_for_var = np.mean(x_np, axis=axes_tuple, keepdims=True)
            squared_diff = (x_np - mean_for_var) ** 2
            expected_variance = np.mean(squared_diff, axis=axes_tuple, keepdims=True)
        else:
            # When keepdims=False, we can use numpy's var with ddof=0
            expected_variance = np.var(x_np, axis=axes_tuple, ddof=0)
        
        # Compare with tolerance
        # Use appropriate tolerance based on dtype
        rtol = 1e-6 if dtype == "float32" else 1e-12
        atol = 1e-6 if dtype == "float32" else 1e-12
        
        self.assert_tensor_close(mean_tf, expected_mean, rtol=rtol, atol=atol)
        self.assert_tensor_close(variance_tf, expected_variance, rtol=rtol, atol=atol)
        
        # Additional weak assertion: variance should be non-negative
        variance_np = variance_tf.numpy()
        assert np.all(variance_np >= -1e-10), f"Variance contains negative values: {variance_np}"
        
        # Basic stats check: variance should be >= 0
        # Allow small negative values due to numerical precision
        assert np.all(variance_np >= -abs(atol)), \
            f"Variance has significantly negative values: {variance_np}"
        
        # Test edge cases
        # Test with all zeros input
        x_zeros = tf.zeros(x_shape, dtype=dtype)
        mean_zeros, variance_zeros = nn_impl.moments(
            x=x_zeros,
            axes=axes,
            keepdims=keepdims
        )
        
        # Mean should be zero
        self.assert_tensor_close(mean_zeros, tf.zeros_like(mean_zeros), rtol=rtol, atol=atol)
        
        # Variance should be zero
        self.assert_tensor_close(variance_zeros, tf.zeros_like(variance_zeros), rtol=rtol, atol=atol)
        
        # Test with constant input
        constant_value = 5.0
        x_constant = tf.constant(np.full(x_shape, constant_value), dtype=dtype)
        mean_constant, variance_constant = nn_impl.moments(
            x=x_constant,
            axes=axes,
            keepdims=keepdims
        )
        
        # Mean should be the constant value
        expected_mean_constant = np.full(expected_shape, constant_value, dtype=dtype)
        self.assert_tensor_close(mean_constant, expected_mean_constant, rtol=rtol, atol=atol)
        
        # Variance should be zero
        expected_variance_constant = np.zeros(expected_shape, dtype=dtype)
        self.assert_tensor_close(variance_constant, expected_variance_constant, rtol=rtol, atol=atol)
        
        # Test with single element tensor (edge case)
        if len(x_shape) == 1 and x_shape[0] == 1:
            x_single = tf.constant([7.0], dtype=dtype)
            mean_single, variance_single = nn_impl.moments(
                x=x_single,
                axes=0,
                keepdims=keepdims
            )
            
            # Mean should be the single value
            self.assert_tensor_close(mean_single, tf.constant([7.0], dtype=dtype), rtol=rtol, atol=atol)
            
            # Variance should be zero (single sample)
            self.assert_tensor_close(variance_single, tf.constant([0.0], dtype=dtype), rtol=rtol, atol=atol)
        
        # Test with negative axis indices
        if isinstance(axes, int):
            negative_axis = axes - len(x_shape) if axes >= 0 else axes
            mean_neg, variance_neg = nn_impl.moments(
                x=x_tf,
                axes=negative_axis,
                keepdims=keepdims
            )
            
            # Results should be identical to positive axis index
            self.assert_tensor_close(mean_neg, mean_tf, rtol=rtol, atol=atol)
            self.assert_tensor_close(variance_neg, variance_tf, rtol=rtol, atol=atol)
        
        # Test with shift parameter (should be ignored according to documentation)
        # The shift parameter is documented as not used in current implementation
        mean_shift, variance_shift = nn_impl.moments(
            x=x_tf,
            axes=axes,
            shift=10.0,  # Arbitrary shift value
            keepdims=keepdims
        )
        
        # Results should be identical regardless of shift
        self.assert_tensor_close(mean_shift, mean_tf, rtol=rtol, atol=atol)
        self.assert_tensor_close(variance_shift, variance_tf, rtol=rtol, atol=atol)
# ==== BLOCK:CASE_04 END ====

# ==== BLOCK:CASE_05 START ====
    @pytest.mark.parametrize(
        "targets_shape,log_input_shape,dtype,compute_full_loss",
        [
            # Base case from test plan
            ([2, 3], [2, 3], "float32", False),
            # Additional test cases for coverage
            ([2, 3], [2, 3], "float64", True),
            ([5], [5], "float32", False),
            ([1, 1], [1, 1], "float32", True),
        ]
    )
    def test_log_poisson_loss_basic(self, targets_shape, log_input_shape, dtype, compute_full_loss):
        """Test log_poisson_loss function with weak assertions."""
        # Generate random input tensors
        # log_input should be float values (log of Poisson rate parameter)
        log_input_np = np.random.randn(*log_input_shape).astype(dtype)
        log_input_tf = tf.constant(log_input_np, dtype=dtype)
        
        # targets should be non-negative (Poisson counts)
        # Use absolute value to ensure non-negative
        targets_np = np.abs(np.random.randn(*targets_shape).astype(dtype))
        targets_tf = tf.constant(targets_np, dtype=dtype)
        
        # Call the function
        result = nn_impl.log_poisson_loss(
            targets=targets_tf,
            log_input=log_input_tf,
            compute_full_loss=compute_full_loss
        )
        
        # Weak assertions (shape, dtype, finite)
        self.assert_tensor_shape(result, log_input_shape)
        self.assert_tensor_dtype(result, dtype)
        self.assert_tensor_finite(result)
        
        # Compute expected value manually based on formula:
        # loss = exp(log_input) - targets * log_input
        # If compute_full_loss=True, add Stirling's approximation term
        
        # Basic loss: exp(c) - z * c where c = log_input, z = targets
        expected_basic = np.exp(log_input_np) - targets_np * log_input_np
        
        if compute_full_loss:
            # Add Stirling's approximation term:
            # z * log(z) - z + 0.5 * log(2 * pi * z)
            # Only for z > 1, otherwise use 0
            
            # Create constants with correct dtype
            point_five = np.array(0.5, dtype=dtype)
            two_pi = np.array(2 * np.pi, dtype=dtype)
            
            # Compute Stirling's approximation
            # Handle z = 0 or z = 1 (use 0 for these cases)
            stirling = np.zeros_like(targets_np, dtype=dtype)
            
            # For z > 1, compute the full term
            mask = targets_np > 1.0
            if np.any(mask):
                z = targets_np[mask]
                stirling[mask] = z * np.log(z) - z + point_five * np.log(two_pi * z)
            
            # For 0 <= z <= 1, use 0 (already initialized)
            expected = expected_basic + stirling
        else:
            expected = expected_basic
        
        # Compare with tolerance
        # Use appropriate tolerance based on dtype
        rtol = 1e-6 if dtype == "float32" else 1e-12
        atol = 1e-6 if dtype == "float32" else 1e-12
        
        self.assert_tensor_close(result, expected, rtol=rtol, atol=atol)
        
        # Additional weak assertion: loss should be non-negative for valid inputs
        # Poisson loss is always non-negative for valid parameters
        result_np = result.numpy()
        
        # Note: For some inputs, loss can be negative due to numerical issues
        # or when log_input is very negative. We'll check but allow small negatives.
        assert np.all(result_np >= -abs(atol)), \
            f"Loss contains significantly negative values: {result_np}"
        
        # Test edge cases
        
        # Test with zero targets
        targets_zero = tf.zeros(targets_shape, dtype=dtype)
        result_zero = nn_impl.log_poisson_loss(
            targets=targets_zero,
            log_input=log_input_tf,
            compute_full_loss=compute_full_loss
        )
        
        # When targets = 0, loss = exp(log_input) - 0 * log_input = exp(log_input)
        expected_zero = np.exp(log_input_np)
        if compute_full_loss:
            # Stirling term is 0 for targets <= 1
            pass  # expected_zero already correct
        
        self.assert_tensor_close(result_zero, expected_zero, rtol=rtol, atol=atol)
        
        # Test with log_input = 0 (rate parameter = 1)
        log_input_zero = tf.zeros(log_input_shape, dtype=dtype)
        result_log_zero = nn_impl.log_poisson_loss(
            targets=targets_tf,
            log_input=log_input_zero,
            compute_full_loss=compute_full_loss
        )
        
        # When log_input = 0, loss = exp(0) - targets * 0 = 1
        expected_log_zero = np.ones_like(targets_np, dtype=dtype)
        if compute_full_loss:
            # Add Stirling term if needed
            if compute_full_loss:
                point_five = np.array(0.5, dtype=dtype)
                two_pi = np.array(2 * np.pi, dtype=dtype)
                stirling = np.zeros_like(targets_np, dtype=dtype)
                mask = targets_np > 1.0
                if np.any(mask):
                    z = targets_np[mask]
                    stirling[mask] = z * np.log(z) - z + point_five * np.log(two_pi * z)
                expected_log_zero += stirling
        
        self.assert_tensor_close(result_log_zero, expected_log_zero, rtol=rtol, atol=atol)
        
        # Test with both zero
        result_both_zero = nn_impl.log_poisson_loss(
            targets=targets_zero,
            log_input=log_input_zero,
            compute_full_loss=compute_full_loss
        )
        
        expected_both_zero = np.ones(log_input_shape, dtype=dtype)
        self.assert_tensor_close(result_both_zero, expected_both_zero, rtol=rtol, atol=atol)
        
        # Test shape mismatch should raise ValueError
        # Create mismatched shapes
        if targets_shape != log_input_shape:
            # This should already be caught by the test parameterization
            # But we can test with explicitly mismatched shapes
            mismatched_shape = list(targets_shape)
            mismatched_shape[0] += 1  # Change first dimension
            
            log_input_mismatch = tf.constant(
                np.random.randn(*mismatched_shape).astype(dtype),
                dtype=dtype
            )
            
            with pytest.raises(ValueError, match="must have the same shape"):
                nn_impl.log_poisson_loss(
                    targets=targets_tf,
                    log_input=log_input_mismatch,
                    compute_full_loss=compute_full_loss
                )
        
        # Test with very large values (check for overflow)
        # Use moderate values to avoid overflow
        log_input_large = tf.constant(np.full(log_input_shape, 10.0), dtype=dtype)
        targets_large = tf.constant(np.full(targets_shape, 100.0), dtype=dtype)
        
        result_large = nn_impl.log_poisson_loss(
            targets=targets_large,
            log_input=log_input_large,
            compute_full_loss=compute_full_loss
        )
        
        self.assert_tensor_finite(result_large)
        
        # Test with very small values (check for underflow)
        log_input_small = tf.constant(np.full(log_input_shape, -100.0), dtype=dtype)
        targets_small = tf.constant(np.full(targets_shape, 0.1), dtype=dtype)
        
        result_small = nn_impl.log_poisson_loss(
            targets=targets_small,
            log_input=log_input_small,
            compute_full_loss=compute_full_loss
        )
        
        self.assert_tensor_finite(result_small)
        
        # Verify that compute_full_loss=True gives larger or equal loss
        # Fix: Stirling approximation term can be negative for small z values
        # We need to adjust the assertion to handle this case
        if not compute_full_loss:
            # Run same test with compute_full_loss=True
            result_full = nn_impl.log_poisson_loss(
                targets=targets_tf,
                log_input=log_input_tf,
                compute_full_loss=True
            )
            
            # Full loss should be >= basic loss for z > 1
            # But for 0 <= z <= 1, Stirling term is 0, so they should be equal
            result_full_np = result_full.numpy()
            
            # Check that full loss is close to basic loss (within tolerance)
            # This is more robust than checking full_loss >= basic_loss
            # because Stirling approximation can be slightly negative for small z
            diff = result_full_np - result_np
            
            # For targets <= 1, Stirling term is 0, so diff should be 0
            # For targets > 1, Stirling term should be >= 0, but can be slightly negative
            # due to numerical precision
            
            # Calculate tolerance based on dtype and magnitude of values
            # Use relative tolerance for larger values, absolute for small values
            tolerance = 1e-4 if dtype == "float32" else 1e-10
            
            # Check that differences are within tolerance
            # Allow small negative differences due to numerical precision
            max_abs_diff = np.max(np.abs(diff))
            assert max_abs_diff <= tolerance, \
                f"Difference between full and basic loss too large: {max_abs_diff}"
        
        # Test property: loss is convex in log_input
        # For Poisson distribution, the loss is convex
        # We can test this by checking second derivative is positive
        # But for weak assertions, we'll skip this
        
        # Test gradient property (numerical gradient check would be strong assertion)
        # Skip for weak assertions
# ==== BLOCK:CASE_05 END ====

# ==== BLOCK:FOOTER START ====
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
# ==== BLOCK:FOOTER END ====