"""
Test cases for tensorflow.python.data.experimental.ops.interleave_ops - G2 group
Testing sample_from_datasets_v2 and choose_from_datasets_v2 function families
"""
import warnings
import numpy as np
import pytest
import tensorflow as tf
from tensorflow.python.data.experimental.ops import interleave_ops


# ==== BLOCK:HEADER START ====
# Test fixtures and helper functions for G2 group
@pytest.fixture
def tf_record_simulated_dataset():
    """Simulate TFRecord dataset for testing."""
    def _create_dataset(size=10):
        data = tf.data.Dataset.range(size)
        return data.map(lambda x: tf.io.serialize_tensor(tf.cast(x, tf.float32)))
    return _create_dataset


@pytest.fixture
def simple_range_dataset():
    """Create simple range dataset for testing."""
    def _create_dataset(size=10):
        return tf.data.Dataset.range(size)
    return _create_dataset


def count_dataset_elements(dataset):
    """Count elements in a dataset."""
    count = 0
    for _ in dataset:
        count += 1
    return count


def capture_deprecation_warnings():
    """Context manager to capture deprecation warnings."""
    return warnings.catch_warnings(record=True)
# ==== BLOCK:HEADER END ====


# ==== BLOCK:CASE_03 START ====
# TC-03: sample_from_datasets_v2 基本采样
@pytest.mark.parametrize(
    "datasets_count,weights,seed,dataset_size,stop_on_empty",
    [
        (2, [0.5, 0.5], 42, 20, False),
    ]
)
def test_sample_from_datasets_v2_basic(
    datasets_count, weights, seed, dataset_size, stop_on_empty
):
    """Test basic sampling functionality of sample_from_datasets_v2."""
    # Create datasets with offset values to distinguish them
    datasets = []
    for i in range(datasets_count):
        offset = i * 100  # Different offset for each dataset
        dataset = tf.data.Dataset.range(offset, offset + dataset_size)
        datasets.append(dataset)
    
    # Apply sample_from_datasets_v2 - deprecation warnings are logged, not captured by warnings module
    # We'll verify the function works correctly instead of checking for warnings
    result_dataset = interleave_ops.sample_from_datasets_v2(
        datasets=datasets,
        weights=weights,
        seed=seed,
        stop_on_empty_dataset=stop_on_empty
    )
    
    # Verify dataset structure
    assert isinstance(result_dataset, tf.data.Dataset)
    
    # Count elements (weak assertion)
    element_count = count_dataset_elements(result_dataset)
    total_elements = datasets_count * dataset_size
    assert element_count == total_elements, f"Expected {total_elements} elements, got {element_count}"
    
    # Verify sampling distribution (weak assertion)
    # Collect samples to verify they come from all datasets
    samples = []
    for elem in result_dataset:
        samples.append(elem.numpy())
    
    # Check that we have elements from all datasets
    # Each dataset has values in range [i*100, i*100 + dataset_size)
    from_datasets = [False] * datasets_count
    for sample in samples:
        for i in range(datasets_count):
            if i * 100 <= sample < i * 100 + dataset_size:
                from_datasets[i] = True
                break
    
    # All datasets should be represented in the samples
    assert all(from_datasets), f"Not all datasets are represented in samples: {from_datasets}"
    
    # Verify seed reproducibility (weak assertion)
    if seed is not None:
        # Create another dataset with same seed
        dataset2 = interleave_ops.sample_from_datasets_v2(
            datasets=datasets,
            weights=weights,
            seed=seed,
            stop_on_empty_dataset=stop_on_empty
        )
        
        # Compare first few elements
        samples1 = [next(iter(result_dataset)).numpy() for _ in range(min(5, element_count))]
        samples2 = [next(iter(dataset2)).numpy() for _ in range(min(5, element_count))]
        
        # With same seed, first elements should match
        assert samples1 == samples2, f"With same seed, samples should match: {samples1} != {samples2}"
# ==== BLOCK:CASE_03 END ====


# ==== BLOCK:CASE_04 START ====
# TC-04: choose_from_datasets_v2 基本选择
@pytest.mark.parametrize(
    "datasets_count,choice_pattern,dataset_size,stop_on_empty",
    [
        (3, "sequential", 15, False),
    ]
)
def test_choose_from_datasets_v2_basic(
    datasets_count, choice_pattern, dataset_size, stop_on_empty
):
    """Test basic selection functionality of choose_from_datasets_v2."""
    # Create datasets with offset values to distinguish them
    datasets = []
    for i in range(datasets_count):
        offset = i * 100  # Different offset for each dataset
        dataset = tf.data.Dataset.range(offset, offset + dataset_size)
        datasets.append(dataset)
    
    # Create choice dataset based on pattern
    if choice_pattern == "sequential":
        # Sequential pattern: 0, 1, 2, 0, 1, 2, ...
        choice_values = list(range(datasets_count)) * (dataset_size // datasets_count + 1)
        choice_dataset = tf.data.Dataset.from_tensor_slices(choice_values[:dataset_size])
    elif choice_pattern == "random":
        # Random pattern (fixed seed for reproducibility)
        tf.random.set_seed(42)
        choice_dataset = tf.data.Dataset.random(
            seed=42
        ).map(lambda x: tf.cast(x * datasets_count, tf.int64))
    else:
        # Single choice pattern
        choice_dataset = tf.data.Dataset.from_tensors(0).repeat(dataset_size)
    
    # Apply choose_from_datasets_v2 with deprecation warning capture
    with capture_deprecation_warnings() as w:
        result_dataset = interleave_ops.choose_from_datasets_v2(
            datasets=datasets,
            choice_dataset=choice_dataset,
            stop_on_empty_dataset=stop_on_empty
        )
        
        # Verify deprecation warning is raised
        assert len(w) > 0, "Deprecation warning should be raised"
        assert any("deprecated" in str(warning.message).lower() for warning in w)
    
    # Verify dataset structure
    assert isinstance(result_dataset, tf.data.Dataset)
    
    # Count elements (weak assertion)
    element_count = count_dataset_elements(result_dataset)
    # Should have same number of elements as choice dataset
    choice_count = count_dataset_elements(choice_dataset)
    assert element_count == choice_count, f"Expected {choice_count} elements, got {element_count}"
    
    # Verify selection logic (weak assertion)
    # Collect choices and results
    choices = list(choice_dataset.as_numpy_iterator())
    results = list(result_dataset.as_numpy_iterator())
    
    # Verify each result comes from the chosen dataset
    for i, (choice, result) in enumerate(zip(choices, results)):
        expected_min = choice * 100
        expected_max = expected_min + dataset_size
        assert expected_min <= result < expected_max, (
            f"Element {i}: choice={choice}, result={result} "
            f"not in expected range [{expected_min}, {expected_max})"
        )
    
    # Verify element count matches expectations
    assert len(results) == dataset_size, f"Expected {dataset_size} elements, got {len(results)}"
    
    # For sequential pattern, verify pattern is followed
    if choice_pattern == "sequential":
        for i, (choice, result) in enumerate(zip(choices, results)):
            expected_value = (choice * 100) + (i // datasets_count)
            # Allow for some flexibility in exact values due to dataset consumption
            # But verify it's from the correct dataset
            assert (choice * 100) <= result < (choice * 100 + dataset_size), (
                f"Element {i}: result {result} not from dataset {choice}"
            )
# ==== BLOCK:CASE_04 END ====


# ==== BLOCK:CASE_07 START ====
# TC-07: DEFERRED - sample_from_datasets_v2 扩展参数
# Placeholder for deferred test case
# ==== BLOCK:CASE_07 END ====


# ==== BLOCK:CASE_08 START ====
# TC-08: DEFERRED - choose_from_datasets_v2 扩展参数
# Placeholder for deferred test case
# ==== BLOCK:CASE_08 END ====


# ==== BLOCK:FOOTER START ====
# Additional helper functions and cleanup for G2 group
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
# ==== BLOCK:FOOTER END ====