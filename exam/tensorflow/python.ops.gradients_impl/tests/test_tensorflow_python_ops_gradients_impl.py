"""
Test cases for tensorflow.python.ops.gradients_impl module.
Generated by TestAgent.
"""
import math
import numpy as np
import pytest
import tensorflow as tf
from unittest.mock import Mock, patch, MagicMock
from tensorflow.python.ops.gradients_impl import gradients
from tensorflow.python.ops.gradients_util import _GradientsHelper
from tensorflow.python.framework.ops import get_default_graph

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# ==== BLOCK:HEADER START ====
# Test fixtures and helper functions
@pytest.fixture
def tf_session():
    """Create and yield a TensorFlow session."""
    with tf.compat.v1.Session() as sess:
        yield sess
        # Don't reset default graph in TensorFlow 2.x

def assert_gradient_shape_matches(grad, x, sess):
    """Assert gradient shape matches input tensor shape."""
    if grad is not None:
        grad_shape = sess.run(grad).shape
        x_shape = sess.run(x).shape
        assert grad_shape == x_shape, f"Gradient shape {grad_shape} != input shape {x_shape}"

def assert_gradient_dtype_matches(grad, x, sess):
    """Assert gradient dtype matches input tensor dtype."""
    if grad is not None:
        grad_dtype = sess.run(grad).dtype
        x_dtype = sess.run(x).dtype
        assert grad_dtype == x_dtype, f"Gradient dtype {grad_dtype} != input dtype {x_dtype}"

def assert_gradient_values_finite(grad, sess):
    """Assert gradient values are finite (not NaN or Inf)."""
    if grad is not None:
        grad_values = sess.run(grad)
        assert np.all(np.isfinite(grad_values)), f"Gradient contains NaN or Inf values: {grad_values}"

def create_tensor(shape, dtype, value_range=(0.0, 1.0)):
    """Create a tensor with random values in specified range."""
    if dtype == tf.float32:
        values = np.random.uniform(value_range[0], value_range[1], shape).astype(np.float32)
    elif dtype == tf.float64:
        values = np.random.uniform(value_range[0], value_range[1], shape).astype(np.float64)
    else:
        raise ValueError(f"Unsupported dtype: {dtype}")
    return tf.constant(values, dtype=dtype)
# ==== BLOCK:HEADER END ====

# ==== BLOCK:CASE_01 START ====
@pytest.mark.parametrize("dtype,shape", [
    (tf.float32, [2, 3]),
    (tf.float64, [4, 4]),  # 参数扩展：float64精度测试
    (tf.float32, [1, 5]),  # 参数扩展：非方阵形状测试
])
def test_basic_gradient_single_tensor(dtype, shape, tf_session):
    """TC-01: 基本梯度计算：单张量ys对单张量xs"""
    # 修复mock路径：使用完整的模块路径
    with patch('tensorflow.python.ops.gradients_impl.gradients_util._GradientsHelper') as mock_helper:
        # Setup mock to return expected gradients
        mock_grad = tf.constant(np.ones(shape), dtype=dtype)
        mock_helper.return_value = [mock_grad]
        
        # Create test tensors
        x = create_tensor(shape, dtype)
        y = tf.matmul(x, tf.transpose(x))  # y = x * x^T
        
        # Call gradients function
        grads = gradients(ys=y, xs=x)
        
        # Verify mock was called with correct arguments
        mock_helper.assert_called_once()
        call_args = mock_helper.call_args
        
        # Check ys argument
        assert call_args[0][0] == y
        
        # Check xs argument
        assert call_args[0][1] == x
        
        # Check other arguments have default values
        assert call_args[0][2] is None  # grad_ys
        assert call_args[1]['name'] == 'gradients'
        assert call_args[1]['colocate_gradients_with_ops'] == False
        assert call_args[1]['gate_gradients'] == False
        assert call_args[1]['aggregation_method'] is None
        assert call_args[1]['stop_gradients'] is None
        assert call_args[1]['unconnected_gradients'].value == 'none'
        
        # Verify return value
        assert isinstance(grads, list)
        assert len(grads) == 1
        assert grads[0] is mock_grad
        
        # Run session to check gradient properties
        with tf_session as sess:
            # Initialize variables
            sess.run(tf.compat.v1.global_variables_initializer())
            
            # Check gradient shape matches input
            assert_gradient_shape_matches(grads[0], x, sess)
            
            # Check gradient dtype matches input
            assert_gradient_dtype_matches(grads[0], x, sess)
            
            # Check gradient values are finite
            assert_gradient_values_finite(grads[0], sess)
# ==== BLOCK:CASE_01 END ====

# ==== BLOCK:CASE_02 START ====
@pytest.mark.parametrize("dtype,shape,ys_type,xs_type", [
    (tf.float32, [3, 2], "list_2_tensors", "list_2_tensors"),
    (tf.float32, [2, 3], "list_3_tensors", "list_2_tensors"),  # 参数扩展：不同长度列表测试
])
def test_gradient_aggregation_multiple_tensors(dtype, shape, ys_type, xs_type, tf_session):
    """TC-02: 列表输入：多ys对多xs的梯度聚合"""
    # 修复mock路径：使用完整的模块路径
    with patch('tensorflow.python.ops.gradients_impl.gradients_util._GradientsHelper') as mock_helper:
        # Determine list sizes based on test parameters
        if xs_type == "list_2_tensors":
            xs_count = 2
        else:
            xs_count = 1
            
        if ys_type == "list_2_tensors":
            ys_count = 2
        elif ys_type == "list_3_tensors":
            ys_count = 3
        else:
            ys_count = 1
        
        # Create mock gradients
        mock_grads = []
        for i in range(xs_count):
            mock_grad = tf.constant(np.ones(shape), dtype=dtype)
            mock_grads.append(mock_grad)
        
        mock_helper.return_value = mock_grads
        
        # Create test tensors
        xs = []
        for i in range(xs_count):
            x = create_tensor(shape, dtype, value_range=(0.5, 1.5))
            xs.append(x)
        
        # Create ys as combinations of xs
        ys = []
        if ys_count >= 1:
            # y1 = x1 + x2 (if available)
            if len(xs) >= 2:
                y1 = tf.add(xs[0], xs[1])
            else:
                y1 = xs[0]
            ys.append(y1)
        
        if ys_count >= 2:
            # y2 = x1 * x2 (if available)
            if len(xs) >= 2:
                y2 = tf.multiply(xs[0], xs[1])
            else:
                y2 = xs[0]
            ys.append(y2)
        
        if ys_count >= 3:
            # y3 = x1 - x2 (if available)
            if len(xs) >= 2:
                y3 = tf.subtract(xs[0], xs[1])
            else:
                y3 = xs[0]
            ys.append(y3)
        
        # Call gradients function
        grads = gradients(ys=ys, xs=xs)
        
        # Verify mock was called with correct arguments
        mock_helper.assert_called_once()
        call_args = mock_helper.call_args
        
        # Check ys argument is a list
        assert isinstance(call_args[0][0], list)
        assert len(call_args[0][0]) == ys_count
        
        # Check xs argument is a list
        assert isinstance(call_args[0][1], list)
        assert len(call_args[0][1]) == xs_count
        
        # Check other arguments have default values
        assert call_args[0][2] is None  # grad_ys
        assert call_args[1]['name'] == 'gradients'
        assert call_args[1]['colocate_gradients_with_ops'] == False
        assert call_args[1]['gate_gradients'] == False
        assert call_args[1]['aggregation_method'] is None
        assert call_args[1]['stop_gradients'] is None
        assert call_args[1]['unconnected_gradients'].value == 'none'
        
        # Verify return value
        assert isinstance(grads, list)
        assert len(grads) == xs_count
        
        # Run session to check gradient properties
        with tf_session as sess:
            # Initialize variables
            sess.run(tf.compat.v1.global_variables_initializer())
            
            # Check each gradient
            for i, (grad, x) in enumerate(zip(grads, xs)):
                # Check gradient shape matches input
                assert_gradient_shape_matches(grad, x, sess)
                
                # Check gradient dtype matches input
                assert_gradient_dtype_matches(grad, x, sess)
                
                # Check gradient values are finite
                assert_gradient_values_finite(grad, sess)
# ==== BLOCK:CASE_02 END ====

# ==== BLOCK:CASE_03 START ====
@pytest.mark.parametrize("dtype,shape,stop_target", [
    (tf.float32, [2, 2], "first_x"),
    (tf.float32, [3, 3], "second_x"),  # 参数扩展：停止不同变量的梯度
])
def test_partial_derivatives_with_stop_gradients(dtype, shape, stop_target, tf_session):
    """TC-03: 偏导数计算：使用stop_gradients参数"""
    # 修复mock路径：使用完整的模块路径
    with patch('tensorflow.python.ops.gradients_impl.gradients_util._GradientsHelper') as mock_helper:
        # Create mock gradients - first gradient should be None if stop_target is "first_x"
        mock_grads = []
        
        # Create two test tensors
        x1 = create_tensor(shape, dtype, value_range=(0.5, 1.5))
        x2 = create_tensor(shape, dtype, value_range=(0.5, 1.5))
        xs = [x1, x2]
        
        # Create y = x1 * x2 + x1
        y = tf.add(tf.matmul(x1, x2), x1)
        
        # Setup mock based on stop_target
        if stop_target == "first_x":
            # First gradient should be None (stopped), second should have value
            mock_grads.append(None)
            mock_grads.append(tf.constant(np.ones(shape), dtype=dtype))
            stop_gradients = [x1]
        else:  # "second_x"
            # First gradient should have value, second should be None (stopped)
            mock_grads.append(tf.constant(np.ones(shape), dtype=dtype))
            mock_grads.append(None)
            stop_gradients = [x2]
        
        mock_helper.return_value = mock_grads
        
        # Call gradients function with stop_gradients
        grads = gradients(ys=y, xs=xs, stop_gradients=stop_gradients)
        
        # Verify mock was called with correct arguments
        mock_helper.assert_called_once()
        call_args = mock_helper.call_args
        
        # Check ys argument
        assert call_args[0][0] == y
        
        # Check xs argument is a list
        assert isinstance(call_args[0][1], list)
        assert len(call_args[0][1]) == 2
        
        # Check stop_gradients argument
        if stop_target == "first_x":
            assert call_args[1]['stop_gradients'] == [x1]
        else:
            assert call_args[1]['stop_gradients'] == [x2]
        
        # Check other arguments have default values
        assert call_args[0][2] is None  # grad_ys
        assert call_args[1]['name'] == 'gradients'
        assert call_args[1]['colocate_gradients_with_ops'] == False
        assert call_args[1]['gate_gradients'] == False
        assert call_args[1]['aggregation_method'] is None
        assert call_args[1]['unconnected_gradients'].value == 'none'
        
        # Verify return value
        assert isinstance(grads, list)
        assert len(grads) == 2
        
        # Check that stopped gradient is None
        if stop_target == "first_x":
            assert grads[0] is None
            assert grads[1] is not None
        else:
            assert grads[0] is not None
            assert grads[1] is None
        
        # Run session to check gradient properties
        with tf_session as sess:
            # Initialize variables
            sess.run(tf.compat.v1.global_variables_initializer())
            
            # Check non-stopped gradient properties
            if stop_target == "first_x":
                grad_to_check = grads[1]
                x_to_check = x2
            else:
                grad_to_check = grads[0]
                x_to_check = x1
            
            if grad_to_check is not None:
                # Check gradient shape matches input
                assert_gradient_shape_matches(grad_to_check, x_to_check, sess)
                
                # Check gradient dtype matches input
                assert_gradient_dtype_matches(grad_to_check, x_to_check, sess)
                
                # Check gradient values are finite
                assert_gradient_values_finite(grad_to_check, sess)
# ==== BLOCK:CASE_03 END ====

# ==== BLOCK:CASE_04 START ====
@pytest.mark.parametrize("dtype,shape,unconnected_strategy", [
    (tf.float32, [2, 2], "NONE"),
    (tf.float32, [2, 2], "ZERO"),  # 参数扩展：未连接时返回零张量
])
def test_unconnected_gradients_handling(dtype, shape, unconnected_strategy, tf_session):
    """TC-04: 未连接梯度处理：none和zero策略"""
    # 修复mock路径：使用完整的模块路径
    with patch('tensorflow.python.ops.gradients_impl.gradients_util._GradientsHelper') as mock_helper:
        # Create independent tensors (no computational graph connection)
        x = create_tensor(shape, dtype, value_range=(0.5, 1.5))
        y = create_tensor(shape, dtype, value_range=(0.5, 1.5))
        
        # Setup mock based on unconnected_strategy
        if unconnected_strategy == "NONE":
            mock_grad = None
        else:  # "ZERO"
            mock_grad = tf.zeros(shape, dtype=dtype)
        
        mock_helper.return_value = [mock_grad]
        
        # Import UnconnectedGradients enum
        from tensorflow.python.ops.unconnected_gradients import UnconnectedGradients
        
        # Call gradients function with unconnected_gradients parameter
        if unconnected_strategy == "NONE":
            grads = gradients(ys=y, xs=x, unconnected_gradients=UnconnectedGradients.NONE)
        else:  # "ZERO"
            grads = gradients(ys=y, xs=x, unconnected_gradients=UnconnectedGradients.ZERO)
        
        # Verify mock was called with correct arguments
        mock_helper.assert_called_once()
        call_args = mock_helper.call_args
        
        # Check ys argument
        assert call_args[0][0] == y
        
        # Check xs argument
        assert call_args[0][1] == x
        
        # Check unconnected_gradients argument
        if unconnected_strategy == "NONE":
            assert call_args[1]['unconnected_gradients'].value == 'none'
        else:
            assert call_args[1]['unconnected_gradients'].value == 'zero'
        
        # Check other arguments have default values
        assert call_args[0][2] is None  # grad_ys
        assert call_args[1]['name'] == 'gradients'
        assert call_args[1]['colocate_gradients_with_ops'] == False
        assert call_args[1]['gate_gradients'] == False
        assert call_args[1]['aggregation_method'] is None
        assert call_args[1]['stop_gradients'] is None
        
        # Verify return value
        assert isinstance(grads, list)
        assert len(grads) == 1
        
        # Check gradient based on strategy
        if unconnected_strategy == "NONE":
            assert grads[0] is None
        else:  # "ZERO"
            assert grads[0] is not None
            
            # Run session to check zero gradient properties
            with tf_session as sess:
                # Initialize variables
                sess.run(tf.compat.v1.global_variables_initializer())
                
                # Check gradient is zero
                grad_values = sess.run(grads[0])
                assert np.all(grad_values == 0), f"Expected zero gradient, got: {grad_values}"
                
                # Check gradient shape matches input
                assert_gradient_shape_matches(grads[0], x, sess)
                
                # Check gradient dtype matches input
                assert_gradient_dtype_matches(grads[0], x, sess)
                
                # Check gradient values are finite (zero is finite)
                assert_gradient_values_finite(grads[0], sess)
# ==== BLOCK:CASE_04 END ====

# ==== BLOCK:CASE_05 START ====
@pytest.mark.parametrize("dtype,shape,grad_ys_type", [
    (tf.float32, [3, 3], "custom_scalar"),
])
def test_custom_initial_gradients(dtype, shape, grad_ys_type, tf_session):
    """TC-05: 自定义初始梯度：通过grad_ys参数"""
    # 修复mock路径：使用完整的模块路径
    with patch('tensorflow.python.ops.gradients_impl.gradients_util._GradientsHelper') as mock_helper:
        # Create test tensors
        x = create_tensor(shape, dtype, value_range=(0.5, 1.5))
        y = tf.matmul(x, tf.transpose(x))  # y = x * x^T
        
        # Create custom grad_ys based on type
        if grad_ys_type == "custom_scalar":
            # Create a scalar gradient (will be broadcast)
            grad_ys = tf.constant(2.0, dtype=dtype)
            # Mock should return gradient scaled by 2.0
            mock_grad = tf.constant(2.0 * np.ones(shape), dtype=dtype)
        else:
            grad_ys = None
            mock_grad = tf.constant(np.ones(shape), dtype=dtype)
        
        mock_helper.return_value = [mock_grad]
        
        # Call gradients function with custom grad_ys
        grads = gradients(ys=y, xs=x, grad_ys=grad_ys)
        
        # Verify mock was called with correct arguments
        mock_helper.assert_called_once()
        call_args = mock_helper.call_args
        
        # Check ys argument
        assert call_args[0][0] == y
        
        # Check xs argument
        assert call_args[0][1] == x
        
        # Check grad_ys argument
        if grad_ys_type == "custom_scalar":
            assert call_args[0][2] == grad_ys
        else:
            assert call_args[0][2] is None
        
        # Check other arguments have default values
        assert call_args[1]['name'] == 'gradients'
        assert call_args[1]['colocate_gradients_with_ops'] == False
        assert call_args[1]['gate_gradients'] == False
        assert call_args[1]['aggregation_method'] is None
        assert call_args[1]['stop_gradients'] is None
        assert call_args[1]['unconnected_gradients'].value == 'none'
        
        # Verify return value
        assert isinstance(grads, list)
        assert len(grads) == 1
        assert grads[0] is mock_grad
        
        # Run session to check gradient properties
        with tf_session as sess:
            # Initialize variables
            sess.run(tf.compat.v1.global_variables_initializer())
            
            # Check gradient shape matches input
            assert_gradient_shape_matches(grads[0], x, sess)
            
            # Check gradient dtype matches input
            assert_gradient_dtype_matches(grads[0], x, sess)
            
            # Check gradient values are finite
            assert_gradient_values_finite(grads[0], sess)
            
            # For custom scalar grad_ys, check values are scaled
            if grad_ys_type == "custom_scalar":
                grad_values = sess.run(grads[0])
                # Check that values are approximately 2.0 (scaled by grad_ys)
                assert np.allclose(grad_values, 2.0, rtol=1e-5), \
                    f"Expected gradient scaled by 2.0, got values: {grad_values}"
# ==== BLOCK:CASE_05 END ====

# ==== BLOCK:FOOTER START ====
# Additional test cases and cleanup
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
# ==== BLOCK:FOOTER END ====