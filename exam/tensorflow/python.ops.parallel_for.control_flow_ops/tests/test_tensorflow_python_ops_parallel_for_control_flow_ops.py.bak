"""
Test cases for tensorflow.python.ops.parallel_for.control_flow_ops
Generated by ATTest-CLI
"""
import math
import numpy as np
import pytest
import tensorflow as tf
from tensorflow.python.ops.parallel_for import control_flow_ops

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# ==== BLOCK:HEADER START ====

# Helper functions and fixtures
def _sequential_for_loop(loop_fn, iters):
    """Sequential implementation for oracle comparison."""
    results = []
    for i in range(iters):
        result = loop_fn(tf.constant(i, dtype=tf.int32))
        results.append(result)
    if not results:
        return None
    # Stack results along first dimension
    return tf.nest.map_structure(lambda *x: tf.stack(x, axis=0), *results)

def _sequential_arithmetic(loop_fn, iters):
    """Sequential arithmetic operations for oracle comparison."""
    return _sequential_for_loop(loop_fn, iters)

def _elementwise_operations(fn, elems):
    """Elementwise operations for oracle comparison."""
    # Unpack first dimension
    if tf.is_tensor(elems):
        batch_size = elems.shape[0]
    else:
        # Assume first element has batch size
        batch_size = elems[0].shape[0]
    
    results = []
    for i in range(batch_size):
        if tf.is_tensor(elems):
            elem = elems[i]
        else:
            elem = tf.nest.map_structure(lambda x: x[i], elems)
        result = fn(elem)
        results.append(result)
    
    if not results:
        return None
    return tf.nest.map_structure(lambda *x: tf.stack(x, axis=0), *results)

def _create_test_tensor(shape, dtype):
    """Create test tensor with given shape and dtype."""
    if len(shape) == 2 and shape[0] == 0 and shape[1] == 0:
        # Handle empty tensor case
        if dtype == tf.float32:
            return tf.constant([], shape=shape, dtype=dtype)
        elif dtype == tf.float64:
            return tf.constant([], shape=shape, dtype=dtype)
        elif dtype == tf.int32:
            return tf.constant([], shape=shape, dtype=dtype)
        elif dtype == tf.int64:
            return tf.constant([], shape=shape, dtype=dtype)
    
    if dtype == tf.float32:
        return tf.constant(np.random.randn(*shape).astype(np.float32), dtype=dtype)
    elif dtype == tf.float64:
        return tf.constant(np.random.randn(*shape).astype(np.float64), dtype=dtype)
    elif dtype == tf.int32:
        return tf.constant(np.random.randint(0, 10, size=shape).astype(np.int32), dtype=dtype)
    elif dtype == tf.int64:
        return tf.constant(np.random.randint(0, 10, size=shape).astype(np.int64), dtype=dtype)
    else:
        raise ValueError(f"Unsupported dtype: {dtype}")

def _is_under_xla_context():
    """Check if under XLA context (simplified)."""
    # Simplified implementation for testing
    return False

# Monkey-patch for testing if needed
import tensorflow.python.ops.parallel_for.control_flow_ops as target_module
if not hasattr(target_module, '_is_under_xla_context'):
    target_module._is_under_xla_context = _is_under_xla_context
# ==== BLOCK:HEADER END ====

# ==== BLOCK:CASE_01 START ====
@pytest.mark.parametrize(
    "iters,dtype,shape,parallel_iterations,test_type",
    [
        # Base case from test plan
        (5, tf.float32, (2, 3), None, "basic_scalar"),
        # Parameter extensions
        (0, tf.float32, (0, 0), None, "zero_iterations"),
        (100, tf.float64, (10, 10), 10, "large_iterations"),
    ]
)
def test_for_loop_basic_functionality(iters, dtype, shape, parallel_iterations, test_type):
    """Test basic functionality of for_loop function.
    
    TC-01: for_loop基础功能
    Priority: High
    Assertion level: weak
    """
    # Create a fixed base tensor that will be scaled by iteration index
    # This ensures consistency between for_loop and sequential implementation
    base_tensor = _create_test_tensor(shape, dtype)
    
    # Define loop function that returns a tensor scaled by iteration index
    def loop_fn(i):
        # Scale the base tensor by (i + 1) to make iterations different
        if dtype in (tf.float32, tf.float64):
            scale = tf.cast(i, dtype) + 1.0
        else:
            scale = tf.cast(i, dtype) + 1
        return scale * base_tensor
    
    # Get output dtypes from loop_fn
    test_tensor = loop_fn(tf.constant(0, dtype=tf.int32))
    loop_fn_dtypes = tf.nest.map_structure(lambda x: x.dtype, test_tensor)
    
    # Call for_loop
    result = control_flow_ops.for_loop(
        loop_fn=loop_fn,
        loop_fn_dtypes=loop_fn_dtypes,
        iters=iters,
        parallel_iterations=parallel_iterations
    )
    
    # Weak assertions
    # 1. shape_match: Check output shape
    if iters > 0:
        expected_shape = (iters,) + shape
        assert result.shape == expected_shape, \
            f"Expected shape {expected_shape}, got {result.shape}"
    else:
        # Zero iterations should return None
        assert result is None, f"Expected None for iters=0, got {result}"
        return  # Early return for zero iterations
    
    # 2. dtype_match: Check dtype
    assert result.dtype == dtype, \
        f"Expected dtype {dtype}, got {result.dtype}"
    
    # 3. finite_values: Check for finite values (no NaN or Inf)
    result_np = result.numpy()
    assert np.all(np.isfinite(result_np)), \
        "Result contains NaN or infinite values"
    
    # 4. basic_structure: Compare with sequential implementation
    sequential_result = _sequential_for_loop(loop_fn, iters)
    
    # Check shape consistency
    assert result.shape == sequential_result.shape, \
        f"Shape mismatch: for_loop {result.shape} vs sequential {sequential_result.shape}"
    
    # Check values with tolerance for floating point
    if dtype in (tf.float32, tf.float64):
        np.testing.assert_allclose(
            result_np, sequential_result.numpy(),
            rtol=1e-6, atol=1e-6,
            err_msg="for_loop result differs from sequential implementation"
        )
    else:
        np.testing.assert_array_equal(
            result_np, sequential_result.numpy(),
            err_msg="for_loop result differs from sequential implementation"
        )
    
    # Additional basic checks
    # Check that all iterations produced different values (for non-zero case)
    if iters > 1 and shape != (0, 0):
        # Check that at least some values are different (not all zeros)
        assert not np.allclose(result_np, 0), \
            "All values are zero, check loop_fn implementation"
        
        # Check that values vary across iterations
        if iters > 2:
            # Compare first and second iteration results
            diff = np.abs(result_np[0] - result_np[1])
            assert np.any(diff > 1e-6), \
                "First two iterations produced identical results"
# ==== BLOCK:CASE_01 END ====

# ==== BLOCK:CASE_02 START ====
@pytest.mark.parametrize(
    "iters,dtype,shape,parallel_iterations,test_type",
    [
        # Base case from test plan
        (10, tf.float64, (3, 4), 4, "arithmetic_ops"),
        # Parameter extension
        (3, tf.int64, (1, 100), 2, "extreme_shape"),
    ]
)
def test_pfor_vectorization_conversion(iters, dtype, shape, parallel_iterations, test_type):
    """Test vectorization conversion of pfor function.
    
    TC-02: pfor向量化转换
    Priority: High
    Assertion level: weak
    Requires mock: True (but using weak assertions only in round 1)
    """
    import unittest.mock as mock
    
    # Create a fixed base tensor that will be scaled by iteration index
    # This ensures consistency between pfor and sequential implementation
    base_tensor = _create_test_tensor(shape, dtype)
    
    # Define a simple arithmetic loop function
    def loop_fn(i):
        # Simple arithmetic operation: multiply by iteration index
        if dtype in (tf.float32, tf.float64):
            scale = tf.cast(i, dtype) + 1.0
        else:
            scale = tf.cast(i, dtype) + 1
        return scale * base_tensor
    
    # For weak assertions in round 1, we don't need to mock internal implementation details
    # Instead, we'll test the actual pfor function with simple operations that should work
    
    # Call pfor with fallback_to_while_loop=True to ensure it works
    # This is a weak assertion test - we just verify the function executes without error
    result = control_flow_ops.pfor(
        loop_fn=loop_fn,
        iters=iters,
        fallback_to_while_loop=True,  # Allow fallback for weak assertions
        parallel_iterations=parallel_iterations
    )
    
    # Weak assertions
    # 1. shape_match: Check output shape
    if iters > 0:
        expected_shape = (iters,) + shape
        assert result.shape == expected_shape, \
            f"Expected shape {expected_shape}, got {result.shape}"
    else:
        # Should not happen with our test parameters, but keep for completeness
        assert result is None or tf.size(result) == 0
        return
    
    # 2. dtype_match: Check dtype
    assert result.dtype == dtype, \
        f"Expected dtype {dtype}, got {result.dtype}"
    
    # 3. finite_values: Check for finite values
    result_np = result.numpy()
    assert np.all(np.isfinite(result_np)), \
        "Result contains NaN or infinite values"
    
    # 4. parallel_execution: Basic check that pfor executed without error
    # For weak assertions, we just verify the function executed
    
    # Compare with sequential implementation (oracle)
    sequential_result = _sequential_arithmetic(loop_fn, iters)
    
    # Check shape consistency
    assert result.shape == sequential_result.shape, \
        f"Shape mismatch: pfor {result.shape} vs sequential {sequential_result.shape}"
    
    # Check values with tolerance
    if dtype in (tf.float32, tf.float64):
        np.testing.assert_allclose(
            result_np, sequential_result.numpy(),
            rtol=1e-6, atol=1e-6,
            err_msg="pfor result differs from sequential implementation"
        )
    else:
        np.testing.assert_array_equal(
            result_np, sequential_result.numpy(),
            err_msg="pfor result differs from sequential implementation"
        )
    
    # Additional weak checks
    # Check that parallel_iterations parameter is respected (if provided)
    if parallel_iterations is not None:
        # For weak assertions, just verify parallel_iterations > 1 for pfor
        assert parallel_iterations > 1, \
            f"parallel_iterations must be >1 for pfor, got {parallel_iterations}"
    
    # Check extreme shape handling
    if test_type == "extreme_shape":
        # For (1, 100) shape, verify dimensions are correct
        assert result.shape[0] == iters, f"Expected {iters} iterations"
        assert result.shape[1:] == shape, f"Expected shape {shape}"
# ==== BLOCK:CASE_02 END ====

# ==== BLOCK:CASE_03 START ====
@pytest.mark.parametrize(
    "iters,dtype,shape,fallback_to_while_loop,test_type",
    [
        # Base case from test plan
        (8, tf.int32, (5, 2), False, "broadcast_operations"),
        # Parameter extension
        (1, tf.float32, (100, 1), True, "single_iteration"),
    ]
)
def test_vectorized_map_broadcast_functionality(iters, dtype, shape, fallback_to_while_loop, test_type):
    """Test broadcast functionality of vectorized_map function.
    
    TC-03: vectorized_map广播功能
    Priority: High
    Assertion level: weak
    """
    # Define function for vectorized_map
    def fn(x):
        # Simple broadcast operation: square and add constant
        if dtype in (tf.float32, tf.float64):
            return tf.square(x) + 2.5
        else:
            return tf.square(x) + 2
    
    # Create input elements
    if test_type == "single_iteration" and iters == 1:
        # For single iteration, create a 2D tensor with batch size 1
        elems = _create_test_tensor((1,) + shape, dtype)
    else:
        elems = _create_test_tensor((iters,) + shape, dtype)
    
    # Call vectorized_map
    result = control_flow_ops.vectorized_map(
        fn=fn,
        elems=elems,
        fallback_to_while_loop=fallback_to_while_loop
    )
    
    # Weak assertions
    # 1. shape_match: Check output shape
    expected_shape = elems.shape
    assert result.shape == expected_shape, \
        f"Expected shape {expected_shape}, got {result.shape}"
    
    # 2. dtype_match: Check dtype
    assert result.dtype == dtype, \
        f"Expected dtype {dtype}, got {result.dtype}"
    
    # 3. broadcast_correctness: Check broadcast behavior
    # For broadcast operations, verify elementwise consistency
    result_np = result.numpy()
    elems_np = elems.numpy()
    
    # Compute expected result manually
    if dtype in (tf.float32, tf.float64):
        expected_np = elems_np ** 2 + 2.5
    else:
        expected_np = elems_np ** 2 + 2
    
    # Check with appropriate tolerance
    if dtype in (tf.float32, tf.float64):
        np.testing.assert_allclose(
            result_np, expected_np,
            rtol=1e-6, atol=1e-6,
            err_msg="vectorized_map result incorrect"
        )
    else:
        np.testing.assert_array_equal(
            result_np, expected_np,
            err_msg="vectorized_map result incorrect"
        )
    
    # 4. elementwise_consistency: Compare with sequential implementation
    sequential_result = _elementwise_operations(fn, elems)
    
    # Check shape consistency
    assert result.shape == sequential_result.shape, \
        f"Shape mismatch: vectorized_map {result.shape} vs sequential {sequential_result.shape}"
    
    # Check values
    if dtype in (tf.float32, tf.float64):
        np.testing.assert_allclose(
            result_np, sequential_result.numpy(),
            rtol=1e-6, atol=1e-6,
            err_msg="vectorized_map differs from sequential implementation"
        )
    else:
        np.testing.assert_array_equal(
            result_np, sequential_result.numpy(),
            err_msg="vectorized_map differs from sequential implementation"
        )
    
    # Additional weak checks
    # Check that single iteration works correctly
    if test_type == "single_iteration":
        assert result.shape[0] == 1, \
            f"Expected batch size 1 for single iteration, got {result.shape[0]}"
        
        # Verify the computation is correct for single element
        single_elem = elems[0].numpy()
        single_result = result[0].numpy()
        
        if dtype in (tf.float32, tf.float64):
            expected_single = single_elem ** 2 + 2.5
            np.testing.assert_allclose(single_result, expected_single, rtol=1e-6)
        else:
            expected_single = single_elem ** 2 + 2
            np.testing.assert_array_equal(single_result, expected_single)
    
    # Check fallback_to_while_loop parameter
    # For weak assertions, we just verify the function works with both True and False
    if fallback_to_while_loop:
        # When fallback is enabled, should work even with unsupported ops
        # (though our simple fn should be supported)
        pass
    else:
        # When fallback is disabled, unsupported ops would raise ValueError
        # Our simple fn should not raise
        pass
    
    # Check that all values are finite
    assert np.all(np.isfinite(result_np)), \
        "Result contains NaN or infinite values"
    
    # Check that output preserves input structure
    # For simple tensor input/output, structure is preserved
    assert isinstance(result, tf.Tensor), \
        f"Expected Tensor output, got {type(result)}"
# ==== BLOCK:CASE_03 END ====

# ==== BLOCK:CASE_04 START ====
@pytest.mark.parametrize(
    "iters,dtype,shape,tensor_type,test_type",
    [
        # Base case from test plan
        (3, tf.float32, (4, 6), "SparseTensor", "composite_tensor"),
        # Parameter extension
        (2, tf.float32, (3, 3), "IndexedSlices", "indexed_slices"),
    ]
)
def test_composite_tensor_support(iters, dtype, shape, tensor_type, test_type):
    """Test CompositeTensor support in vectorized_map.
    
    TC-04: CompositeTensor支持
    Priority: High
    Assertion level: weak
    Requires mock: True (but using weak assertions only in round 2)
    """
    import unittest.mock as mock
    
    # Helper function to create sparse tensor
    def create_sparse_tensor(batch_idx, shape, dtype):
        """Create a sparse tensor with a few non-zero elements."""
        # Create indices for a few non-zero elements
        n_elements = min(3, shape[0] * shape[1])  # At most 3 non-zero elements
        indices = []
        values = []
        
        # Create unique indices for each batch element
        for i in range(n_elements):
            row = i % shape[0]
            col = (i + batch_idx) % shape[1]
            indices.append([row, col])
            # Create values based on batch index and position
            if dtype in (tf.float32, tf.float64):
                values.append(float(batch_idx * 10 + i + 1))
            else:
                values.append(batch_idx * 10 + i + 1)
        
        indices_tensor = tf.constant(indices, dtype=tf.int64)
        values_tensor = tf.constant(values, dtype=dtype)
        dense_shape_tensor = tf.constant(shape, dtype=tf.int64)
        
        return tf.SparseTensor(
            indices=indices_tensor,
            values=values_tensor,
            dense_shape=dense_shape_tensor
        )
    
    # Helper function to create IndexedSlices
    def create_indexed_slices(batch_idx, shape, dtype):
        """Create IndexedSlices tensor."""
        # Create indices
        n_indices = min(2, shape[0])
        indices = tf.constant([(batch_idx + i) % shape[0] for i in range(n_indices)], dtype=tf.int32)
        
        # Create values
        if dtype in (tf.float32, tf.float64):
            values_data = [[float(batch_idx * 10 + i + j + 1) for j in range(shape[1])] 
                          for i in range(n_indices)]
        else:
            values_data = [[batch_idx * 10 + i + j + 1 for j in range(shape[1])] 
                          for i in range(n_indices)]
        
        values = tf.constant(values_data, dtype=dtype)
        dense_shape = tf.constant(shape, dtype=tf.int32)
        
        return tf.IndexedSlices(values=values, indices=indices, dense_shape=dense_shape)
    
    # Define function for vectorized_map
    def fn(x):
        if tensor_type == "SparseTensor":
            # For sparse tensor, convert to dense, apply operation, and convert back
            dense_x = tf.sparse.to_dense(x)
            if dtype in (tf.float32, tf.float64):
                result_dense = dense_x * 2.0 + 1.5
            else:
                result_dense = dense_x * 2 + 1
            # Convert back to sparse (keeping only non-zero elements)
            return tf.sparse.from_dense(result_dense)
        else:  # IndexedSlices
            # For IndexedSlices, apply operation to values
            if dtype in (tf.float32, tf.float64):
                new_values = x.values * 2.0 + 1.5
            else:
                new_values = x.values * 2 + 1
            return tf.IndexedSlices(
                values=new_values,
                indices=x.indices,
                dense_shape=x.dense_shape
            )
    
    # Create batch of composite tensors
    elems = []
    for i in range(iters):
        if tensor_type == "SparseTensor":
            elem = create_sparse_tensor(i, shape, dtype)
        else:  # IndexedSlices
            elem = create_indexed_slices(i, shape, dtype)
        elems.append(elem)
    
    # Mock CompositeTensor and specific tensor type imports if needed
    # For weak assertions, we'll test if the function works without mocking
    # since vectorized_map should support CompositeTensor
    
    # Call vectorized_map
    result = control_flow_ops.vectorized_map(
        fn=fn,
        elems=elems,
        fallback_to_while_loop=True  # Allow fallback for composite tensors
    )
    
    # Weak assertions
    # 1. structure_preserved: Check that output has same structure as input
    if tensor_type == "SparseTensor":
        assert isinstance(result[0], tf.SparseTensor), \
            f"Expected SparseTensor output, got {type(result[0])}"
        
        # Check that all outputs are SparseTensors
        for i in range(iters):
            assert isinstance(result[i], tf.SparseTensor), \
                f"Output {i} is not a SparseTensor"
            
            # Check sparse indices shape
            assert result[i].indices.shape[1] == 2, \
                f"Expected 2D indices, got {result[i].indices.shape}"
    else:  # IndexedSlices
        assert isinstance(result[0], tf.IndexedSlices), \
            f"Expected IndexedSlices output, got {type(result[0])}"
        
        # Check that all outputs are IndexedSlices
        for i in range(iters):
            assert isinstance(result[i], tf.IndexedSlices), \
                f"Output {i} is not an IndexedSlices"
    
    # 2. sparse_indices_correct / values_match: Check basic properties
    for i in range(iters):
        if tensor_type == "SparseTensor":
            # Convert to dense for comparison
            input_dense = tf.sparse.to_dense(elems[i])
            expected_dense = input_dense * 2.0 + (1.5 if dtype in (tf.float32, tf.float64) else 1)
            result_dense = tf.sparse.to_dense(result[i])
            
            # Check shape consistency
            assert result_dense.shape == expected_dense.shape, \
                f"Shape mismatch at iteration {i}: {result_dense.shape} vs {expected_dense.shape}"
            
            # Check values with tolerance
            if dtype in (tf.float32, tf.float64):
                np.testing.assert_allclose(
                    result_dense.numpy(), expected_dense.numpy(),
                    rtol=1e-6, atol=1e-6,
                    err_msg=f"Sparse tensor values incorrect at iteration {i}"
                )
            else:
                np.testing.assert_array_equal(
                    result_dense.numpy(), expected_dense.numpy(),
                    err_msg=f"Sparse tensor values incorrect at iteration {i}"
                )
        else:  # IndexedSlices
            # Check indices
            assert tf.reduce_all(tf.equal(result[i].indices, elems[i].indices)), \
                f"Indices changed at iteration {i}"
            
            # Check values
            expected_values = elems[i].values * 2.0 + (1.5 if dtype in (tf.float32, tf.float64) else 1)
            
            if dtype in (tf.float32, tf.float64):
                np.testing.assert_allclose(
                    result[i].values.numpy(), expected_values.numpy(),
                    rtol=1e-6, atol=1e-6,
                    err_msg=f"IndexedSlices values incorrect at iteration {i}"
                )
            else:
                np.testing.assert_array_equal(
                    result[i].values.numpy(), expected_values.numpy(),
                    err_msg=f"IndexedSlices values incorrect at iteration {i}"
                )
    
    # 3. shape_consistency: Check that dense_shape is preserved
    for i in range(iters):
        if tensor_type == "SparseTensor":
            assert tf.reduce_all(tf.equal(result[i].dense_shape, elems[i].dense_shape)), \
                f"Dense shape changed at iteration {i}"
        else:  # IndexedSlices
            assert tf.reduce_all(tf.equal(result[i].dense_shape, elems[i].dense_shape)), \
                f"Dense shape changed at iteration {i}"
    
    # 4. Additional weak checks
    # Check that we have the right number of outputs
    assert len(result) == iters, \
        f"Expected {iters} outputs, got {len(result)}"
    
    # Check that function executed without error (basic functionality)
    # This is already verified by the fact we got results
    
    # Note: For weak assertions, we don't test gradient_sparse, memory_efficiency,
    # or performance_comparison as those are strong assertions
# ==== BLOCK:CASE_04 END ====

# ==== BLOCK:CASE_05 START ====
# Placeholder for CASE_05: fallback_to_while_loop机制 (deferred)
# This block will be replaced in later iterations
# ==== BLOCK:CASE_05 END ====

# ==== BLOCK:FOOTER START ====
# Additional test cases and cleanup
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
# ==== BLOCK:FOOTER END ====