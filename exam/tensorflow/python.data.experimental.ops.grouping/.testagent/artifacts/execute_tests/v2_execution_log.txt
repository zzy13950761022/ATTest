=== Run Tests ===
F.F                                                                      [100%]
================================== FAILURES ===================================
_ test_bucket_by_sequence_length_basic_bucketing[bucket_boundaries0-bucket_batch_sizes0-None-20] _

bucket_boundaries = [5, 10, 15], bucket_batch_sizes = [2, 2, 2, 2]
padding_values = None, dataset_size = 20

    @pytest.mark.parametrize("bucket_boundaries, bucket_batch_sizes, padding_values, dataset_size", [
        ([5, 10, 15], [2, 2, 2, 2], None, 20),  # Basic case from test plan
    ])
    def test_bucket_by_sequence_length_basic_bucketing(bucket_boundaries, bucket_batch_sizes, padding_values, dataset_size):
        """Test basic bucketing functionality of bucket_by_sequence_length."""
    
        # Create dataset of variable length sequences
        dataset = create_variable_length_sequences(dataset_size, max_length=20)
    
        # Weak assertion 1: returns_callable
        transform_fn = bucket_by_sequence_length(
            element_length_func=element_length_func,
            bucket_boundaries=bucket_boundaries,
            bucket_batch_sizes=bucket_batch_sizes,
            padding_values=padding_values
        )
    
        assert callable(transform_fn), "bucket_by_sequence_length should return a callable function"
    
        # Apply transformation to dataset
        transformed_dataset = transform_fn(dataset)
    
        # Weak assertion 2: dataset_interface
        assert hasattr(transformed_dataset, '__iter__'), "Transformed dataset should have iterator interface"
        assert hasattr(transformed_dataset, 'element_spec'), "Transformed dataset should have element_spec"
    
        # Get all batches
        batches = dataset_to_list(transformed_dataset)
    
        # Weak assertion 3: batch_sizes_correct
        # Check that batch sizes are correct (should be one of the bucket_batch_sizes)
        for batch in batches:
            batch_size = batch.shape[0]
>           assert batch_size in bucket_batch_sizes, f"Batch size {batch_size} not in allowed sizes {bucket_batch_sizes}"
E           AssertionError: Batch size 1 not in allowed sizes [2, 2, 2, 2]
E           assert 1 in [2, 2, 2, 2]

tests\test_tensorflow_python_data_experimental_ops_grouping_g2.py:126: AssertionError
---------------------------- Captured stderr call -----------------------------
2026-01-18 16:55:33.624606: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
------------------------------ Captured log call ------------------------------
WARNING  tensorflow:deprecation.py:337 From D:\Project\TestAgent-CLI-main\exam\tensorflow\python.data.experimental.ops.grouping\tests\test_tensorflow_python_data_experimental_ops_grouping_g2.py:103: bucket_by_sequence_length (from tensorflow.python.data.experimental.ops.grouping) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.bucket_by_sequence_length(...)`.
_ test_bucket_by_sequence_length_parameter_validation[non_increasing_boundaries-ValueError] _

invalid_scenario = 'non_increasing_boundaries'
expected_exception = <class 'ValueError'>

    @pytest.mark.parametrize("invalid_scenario, expected_exception", [
        ("mismatched_batch_sizes", ValueError),  # bucket_batch_sizes length mismatch
        ("non_increasing_boundaries", ValueError),  # Non-increasing bucket boundaries
    ])
    def test_bucket_by_sequence_length_parameter_validation(invalid_scenario, expected_exception):
        """Test parameter validation and exception handling for bucket_by_sequence_length."""
    
        # Create a simple dataset
        dataset = create_variable_length_sequences(10, max_length=10)
    
        if invalid_scenario == "mismatched_batch_sizes":
            # bucket_batch_sizes length doesn't match bucket_boundaries + 1
            bucket_boundaries = [5, 10]
            bucket_batch_sizes = [2, 2]  # Should be length 3 (2 boundaries + 1)
    
            # Weak assertion 1: exception_raised
            with pytest.raises(expected_exception) as exc_info:
                transform_fn = bucket_by_sequence_length(
                    element_length_func=element_length_func,
                    bucket_boundaries=bucket_boundaries,
                    bucket_batch_sizes=bucket_batch_sizes
                )
                _ = transform_fn(dataset)
    
            # Weak assertion 2: exception_type
            assert exc_info.type == expected_exception, f"Expected {expected_exception}, got {exc_info.type}"
    
            # Weak assertion 3: error_message_contains
            error_msg = str(exc_info.value).lower()
            # Check for relevant error indicators
            assert any(keyword in error_msg for keyword in ['length', 'size', 'boundary', 'batch']), \
                f"Error message should mention length/size/boundary/batch, got: {error_msg}"
    
        elif invalid_scenario == "non_increasing_boundaries":
            # bucket_boundaries is not strictly increasing
            bucket_boundaries = [10, 5, 15]  # Not increasing: 10 > 5
            bucket_batch_sizes = [2, 2, 2, 2]
    
            # Weak assertion 1: exception_raised
            with pytest.raises(expected_exception) as exc_info:
                transform_fn = bucket_by_sequence_length(
                    element_length_func=element_length_func,
                    bucket_boundaries=bucket_boundaries,
                    bucket_batch_sizes=bucket_batch_sizes
                )
>               _ = transform_fn(dataset)
E               Failed: DID NOT RAISE <class 'ValueError'>

tests\test_tensorflow_python_data_experimental_ops_grouping_g2.py:195: Failed

---------- coverage: platform win32, python 3.9.25-final-0 -----------
Name                                                                Stmts   Miss Branch BrPart  Cover   Missing
---------------------------------------------------------------------------------------------------------------
tests\test_tensorflow_python_data_experimental_ops_grouping_g2.py      92     38     28      3    51%   41-45, 56-59, 63-70, 78-87, 131-145, 198-207, 229-230
---------------------------------------------------------------------------------------------------------------
TOTAL                                                                  92     38     28      3    51%
Coverage XML written to file coverage.xml

=========================== short test summary info ===========================
FAILED tests\test_tensorflow_python_data_experimental_ops_grouping_g2.py::test_bucket_by_sequence_length_basic_bucketing[bucket_boundaries0-bucket_batch_sizes0-None-20]
FAILED tests\test_tensorflow_python_data_experimental_ops_grouping_g2.py::test_bucket_by_sequence_length_parameter_validation[non_increasing_boundaries-ValueError]
2 failed, 1 passed in 1.63s

Error: exit 1