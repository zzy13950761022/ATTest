# ==== BLOCK:HEADER START ====
"""
Test module for tensorflow.python.ops.gradient_checker_v2
"""
import numpy as np
import tensorflow as tf
import pytest

from tensorflow.python.ops import gradient_checker_v2

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Helper functions for test cases
def scalar_square(x):
    """Simple scalar square function: f(x) = x^2"""
    return x * x

def matrix_multiply(x):
    """Matrix multiplication function: f(X) = X @ X^T"""
    return tf.matmul(x, x, transpose_b=True)

def complex_function(x):
    """Complex function: f(z) = z * conj(z)"""
    return x * tf.math.conj(x)

def create_test_gradients(shape, dtype, error_type="small_error"):
    """Create test gradients for max_error testing"""
    if error_type == "small_error":
        grad1 = tf.ones(shape, dtype=dtype)
        grad2 = grad1 + tf.random.normal(shape, mean=0.0, stddev=0.01, dtype=dtype)
    else:  # large_error
        grad1 = tf.ones(shape, dtype=dtype)
        grad2 = tf.zeros(shape, dtype=dtype)
    return grad1, grad2
# ==== BLOCK:HEADER END ====

# ==== BLOCK:CASE_01 START ====
@pytest.mark.parametrize(
    "func_type,input_shape,dtype,delta,execution_mode",
    [
        # Base case from test plan
        ("scalar_square", [1], "float32", None, "eager"),
        # Parameter extensions
        ("scalar_square", [1], "float64", None, "graph"),
        ("scalar_square", [3], "float32", 0.0001, "eager"),
    ]
)
def test_basic_scalar_gradient_verification(func_type, input_shape, dtype, delta, execution_mode):
    """TC-01: 基本标量函数梯度验证"""
    
    # Select function based on type
    if func_type == "scalar_square":
        test_func = scalar_square
    else:
        pytest.fail(f"Unknown function type: {func_type}")
    
    # Create test input
    if dtype == "float32":
        tf_dtype = tf.float32
    elif dtype == "float64":
        tf_dtype = tf.float64
    else:
        pytest.fail(f"Unsupported dtype: {dtype}")
    
    # Generate random input
    np_input = np.random.randn(*input_shape).astype(np.float32 if dtype == "float32" else np.float64)
    x = [tf.constant(np_input, dtype=tf_dtype)]
    
    # Set execution mode
    if execution_mode == "graph":
        @tf.function
        def graph_func(x):
            return test_func(x)
        func_to_test = graph_func
    else:  # eager
        func_to_test = test_func
    
    # Compute gradients
    if delta is None:
        theoretical, numerical = gradient_checker_v2.compute_gradient(func_to_test, x)
    else:
        theoretical, numerical = gradient_checker_v2.compute_gradient(func_to_test, x, delta=delta)
    
    # Weak assertions (round 1)
    # 1. shape_match: Check shapes match
    assert len(theoretical) == len(numerical) == len(x), "Number of gradients should match number of inputs"
    
    for i in range(len(x)):
        theo_grad = theoretical[i]
        num_grad = numerical[i]
        
        # 2. shape_match: Individual gradient shapes
        assert theo_grad.shape == num_grad.shape, f"Gradient shapes mismatch at index {i}"
        
        # 3. dtype_match: Check dtypes
        assert theo_grad.dtype == num_grad.dtype, f"Gradient dtypes mismatch at index {i}"
        
        # 4. finite_values: Check for finite values
        assert np.all(np.isfinite(theo_grad)), f"Theoretical gradient contains non-finite values at index {i}"
        assert np.all(np.isfinite(num_grad)), f"Numerical gradient contains non-finite values at index {i}"
        
        # 5. basic_gradient_property: Check gradient has expected structure
        # For scalar square function f(x) = x^2, gradient should be 2x
        if func_type == "scalar_square" and input_shape == [1]:
            expected_grad = 2 * np_input
            # Check shape matches expected
            assert theo_grad.shape == (1, 1), f"Expected gradient shape (1,1), got {theo_grad.shape}"
    
    # STRONG ASSERTIONS (Final round)
    # 1. gradient_approx_equal: Check that theoretical and numerical gradients are approximately equal
    for i in range(len(x)):
        theo_grad = theoretical[i]
        num_grad = numerical[i]
        
        # Compute error metrics
        abs_error = np.abs(theo_grad - num_grad)
        rel_error = np.abs(theo_grad - num_grad) / (np.abs(theo_grad) + 1e-10)  # Avoid division by zero
        
        max_abs_error = np.max(abs_error)
        max_rel_error = np.max(rel_error)
        mean_abs_error = np.mean(abs_error)
        mean_rel_error = np.mean(rel_error)
        
        # Error thresholds based on dtype precision
        if dtype == "float32":
            abs_threshold = 1e-4
            rel_threshold = 1e-3
        else:  # float64
            abs_threshold = 1e-8
            rel_threshold = 1e-6
        
        # Adjust thresholds for delta if specified
        if delta is not None:
            # Larger delta may cause larger errors
            delta_factor = min(10.0, max(1.0, delta * 100))
            abs_threshold *= delta_factor
            rel_threshold *= delta_factor
        
        assert max_abs_error < abs_threshold, \
            f"Max absolute error {max_abs_error} exceeds threshold {abs_threshold}"
        
        # Only check relative error if theoretical gradient is not too small
        if np.max(np.abs(theo_grad)) > 1e-6:
            assert max_rel_error < rel_threshold, \
                f"Max relative error {max_rel_error} exceeds threshold {rel_threshold}"
    
    # 2. jacobian_structure: Check specific structure of Jacobian matrix
    for i in range(len(x)):
        theo_grad = theoretical[i]
        
        # For scalar square function f(x) = x^2
        if func_type == "scalar_square":
            n = input_shape[0]
            
            # Expected Jacobian shape: (n, n) for vector input
            expected_shape = (n, n)
            assert theo_grad.shape == expected_shape, \
                f"Expected Jacobian shape {expected_shape}, got {theo_grad.shape}"
            
            # For f(x) = sum(x_i^2), the Jacobian should be diagonal with 2x_i on diagonal
            # But compute_gradient returns full Jacobian
            J = theo_grad  # Already a numpy array
            
            # Check that Jacobian is diagonal (off-diagonal elements should be zero)
            if n > 1:
                off_diag_mask = ~np.eye(n, dtype=bool)
                off_diag_elements = J[off_diag_mask]
                max_off_diag = np.max(np.abs(off_diag_elements))
                
                # Off-diagonal elements should be very small (theoretically zero)
                assert max_off_diag < 1e-10, \
                    f"Off-diagonal elements too large: {max_off_diag}"
            
            # Check diagonal elements
            diag_elements = np.diag(J)
            expected_diag = 2 * np_input
            
            diag_error = np.max(np.abs(diag_elements - expected_diag))
            assert diag_error < 1e-10, \
                f"Diagonal error {diag_error} too large, expected {expected_diag}, got {diag_elements}"
    
    # 3. error_below_threshold: Comprehensive error analysis
    for i in range(len(x)):
        theo_grad = theoretical[i]
        num_grad = numerical[i]
        
        # Compute comprehensive error metrics
        mse = np.mean((theo_grad - num_grad) ** 2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(theo_grad - num_grad))
        max_abs_error = np.max(np.abs(theo_grad - num_grad))
        
        # Compute relative errors
        theo_abs = np.abs(theo_grad)
        mask = theo_abs > 1e-10  # Avoid division by zero
        if np.any(mask):
            rel_errors = np.abs(theo_grad[mask] - num_grad[mask]) / theo_abs[mask]
            max_rel_error = np.max(rel_errors)
            mean_rel_error = np.mean(rel_errors)
        else:
            max_rel_error = 0.0
            mean_rel_error = 0.0
        
        # Error thresholds
        if dtype == "float32":
            mse_threshold = 1e-8
            rmse_threshold = 1e-4
            mae_threshold = 1e-4
            max_abs_threshold = 1e-3
            max_rel_threshold = 1e-2
            mean_rel_threshold = 1e-3
        else:  # float64
            mse_threshold = 1e-16
            rmse_threshold = 1e-8
            mae_threshold = 1e-8
            max_abs_threshold = 1e-6
            max_rel_threshold = 1e-6
            mean_rel_threshold = 1e-7
        
        # Adjust thresholds for delta
        if delta is not None:
            delta_factor = min(100.0, max(1.0, delta * 1000))
            mse_threshold *= delta_factor
            rmse_threshold *= np.sqrt(delta_factor)
            mae_threshold *= delta_factor
            max_abs_threshold *= delta_factor
            max_rel_threshold *= delta_factor
            mean_rel_threshold *= delta_factor
        
        # Apply thresholds
        assert mse < mse_threshold, f"MSE {mse} exceeds threshold {mse_threshold}"
        assert rmse < rmse_threshold, f"RMSE {rmse} exceeds threshold {rmse_threshold}"
        assert mae < mae_threshold, f"MAE {mae} exceeds threshold {mae_threshold}"
        assert max_abs_error < max_abs_threshold, \
            f"Max absolute error {max_abs_error} exceeds threshold {max_abs_threshold}"
        
        if np.any(mask):  # Only check relative errors if we have non-zero theoretical gradients
            assert max_rel_error < max_rel_threshold, \
                f"Max relative error {max_rel_error} exceeds threshold {max_rel_threshold}"
            assert mean_rel_error < mean_rel_threshold, \
                f"Mean relative error {mean_rel_error} exceeds threshold {mean_rel_threshold}"
        
        # Check error consistency
        errors = theo_grad - num_grad
        error_std = np.std(errors)
        
        # Error standard deviation should be reasonable compared to mean absolute error
        if mae > 0:
            std_mae_ratio = error_std / mae
            # For normal distribution, this ratio is about 1.25
            # Accept a reasonable range
            assert 0.8 < std_mae_ratio < 2.0, \
                f"Error std/MAE ratio {std_mae_ratio} outside expected range (0.8-2.0)"
# ==== BLOCK:CASE_01 END ====

# ==== BLOCK:CASE_02 START ====
@pytest.mark.parametrize(
    "func_type,input_shape,dtype,delta,execution_mode",
    [
        # Base case from test plan
        ("matrix_multiply", [2, 3], "float64", 0.001, "graph"),
        # Parameter extension
        ("matrix_multiply", [4, 4], "float32", None, "eager"),
    ]
)
def test_vector_matrix_gradient_verification(func_type, input_shape, dtype, delta, execution_mode):
    """TC-02: 向量矩阵函数梯度验证"""
    
    # Select function based on type
    if func_type == "matrix_multiply":
        test_func = matrix_multiply
    else:
        pytest.fail(f"Unknown function type: {func_type}")
    
    # Create test input
    if dtype == "float32":
        tf_dtype = tf.float32
        np_dtype = np.float32
    elif dtype == "float64":
        tf_dtype = tf.float64
        np_dtype = np.float64
    else:
        pytest.fail(f"Unsupported dtype: {dtype}")
    
    # Generate random matrix input
    np_input = np.random.randn(*input_shape).astype(np_dtype)
    x = [tf.constant(np_input, dtype=tf_dtype)]
    
    # Set execution mode
    if execution_mode == "graph":
        @tf.function
        def graph_func(x):
            return test_func(x)
        func_to_test = graph_func
    else:  # eager
        func_to_test = test_func
    
    # Compute gradients
    if delta is None:
        theoretical, numerical = gradient_checker_v2.compute_gradient(func_to_test, x)
    else:
        theoretical, numerical = gradient_checker_v2.compute_gradient(func_to_test, x, delta=delta)
    
    # Weak assertions (round 1)
    # 1. shape_match: Check shapes match
    assert len(theoretical) == len(numerical) == len(x), "Number of gradients should match number of inputs"
    
    for i in range(len(x)):
        theo_grad = theoretical[i]
        num_grad = numerical[i]
        
        # 2. shape_match: Individual gradient shapes
        assert theo_grad.shape == num_grad.shape, f"Gradient shapes mismatch at index {i}"
        
        # 3. dtype_match: Check dtypes
        assert theo_grad.dtype == num_grad.dtype, f"Gradient dtypes mismatch at index {i}"
        # Convert numpy dtype string to compare
        expected_dtype = np_dtype
        assert theo_grad.dtype == expected_dtype, f"Expected dtype {expected_dtype}, got {theo_grad.dtype}"
        
        # 4. finite_values: Check for finite values
        assert np.all(np.isfinite(theo_grad)), f"Theoretical gradient contains non-finite values at index {i}"
        assert np.all(np.isfinite(num_grad)), f"Numerical gradient contains non-finite values at index {i}"
        
        # 5. jacobian_dimensions: Check Jacobian has correct dimensions
        # For matrix input of shape (m, n), Jacobian should have shape (m*m, m*n)
        # because output is (m, m) and input is (m, n)
        m, n = input_shape
        expected_jacobian_shape = (m * m, m * n)
        assert theo_grad.shape == expected_jacobian_shape, \
            f"Expected Jacobian shape {expected_jacobian_shape}, got {theo_grad.shape}"
    
    # STRONG ASSERTIONS (Final round)
    # 1. gradient_approx_equal: Check that theoretical and numerical gradients are approximately equal
    for i in range(len(x)):
        theo_grad = theoretical[i]
        num_grad = numerical[i]
        
        # Compute error metrics
        abs_error = np.abs(theo_grad - num_grad)
        rel_error = np.abs(theo_grad - num_grad) / (np.abs(theo_grad) + 1e-10)  # Avoid division by zero
        
        max_abs_error = np.max(abs_error)
        max_rel_error = np.max(rel_error)
        mean_abs_error = np.mean(abs_error)
        mean_rel_error = np.mean(rel_error)
        
        # Error thresholds based on dtype precision
        if dtype == "float32":
            abs_threshold = 1e-4
            rel_threshold = 1e-3
        else:  # float64
            abs_threshold = 1e-8
            rel_threshold = 1e-6
        
        # Adjust thresholds for delta if specified
        if delta is not None:
            # Larger delta may cause larger errors
            delta_factor = min(10.0, max(1.0, delta * 100))
            abs_threshold *= delta_factor
            rel_threshold *= delta_factor
        
        assert max_abs_error < abs_threshold, \
            f"Max absolute error {max_abs_error} exceeds threshold {abs_threshold}"
        
        # Only check relative error if theoretical gradient is not too small
        if np.max(np.abs(theo_grad)) > 1e-6:
            assert max_rel_error < rel_threshold, \
                f"Max relative error {max_rel_error} exceeds threshold {rel_threshold}"
    
    # 2. matrix_structure: Check specific structure of matrix gradient
    for i in range(len(x)):
        theo_grad = theoretical[i]
        
        # For matrix multiplication function f(X) = X @ X^T
        if func_type == "matrix_multiply":
            m, n = input_shape
            
            # Expected Jacobian shape: (m*m, m*n)
            expected_shape = (m * m, m * n)
            assert theo_grad.shape == expected_shape, \
                f"Expected Jacobian shape {expected_shape}, got {theo_grad.shape}"
            
            # The Jacobian has specific structure for this function
            # f(X) = X @ X^T, so ∂f_ij/∂X_kl = δ_ik X_jl + δ_jk X_il
            # This creates a specific pattern in the Jacobian
            
            J = theo_grad  # Already a numpy array
            
            # Reshape to understand the structure better
            # Jacobian is (m*m, m*n), which can be seen as m x m blocks of size m x n
            # Each block (i,j) corresponds to derivative of output element (i,j) w.r.t. input matrix
            
            # For small matrices, we can check specific elements
            if m == 2 and n == 3:
                # For 2x3 input, output is 2x2
                # Let's check a few specific elements
                
                # Element (0,0) of output depends on row 0 of input
                # ∂f_00/∂X_0l = 2 * X_0l
                # ∂f_00/∂X_1l = 0
                
                # Check block (0,0) which is derivative of f_00 w.r.t. all X elements
                block_00 = J[0:1, 0:n]  # Should be 2 * X[0,:]
                expected_block_00 = 2 * np_input[0, :]
                error_00 = np.max(np.abs(block_00 - expected_block_00))
                assert error_00 < 1e-10, f"Block (0,0) error {error_00} too large"
                
                # Check block (0,1) which is derivative of f_01 w.r.t. all X elements
                # ∂f_01/∂X_0l = X_1l
                # ∂f_01/∂X_1l = X_0l
                block_01_row0 = J[m:2*m, 0:n]  # Actually need to compute correct indices
                # For simplicity, we'll just check that the Jacobian has the right pattern
                # by verifying symmetry properties
                
            # Check symmetry properties
            # The Jacobian should have certain symmetries due to the symmetry of f(X) = X @ X^T
            
            # For larger matrices, we check general properties
            # 1. The Jacobian should be sparse with specific pattern
            # 2. Certain blocks should be zero
            
            # Compute norm of Jacobian to ensure it's reasonable
            jacobian_norm = np.linalg.norm(J, 'fro')
            input_norm = np.linalg.norm(np_input, 'fro')
            
            # Jacobian norm should scale with input norm
            # For this function, ‖J‖ ~ 2‖X‖
            expected_jacobian_norm = 2 * input_norm * np.sqrt(m)  # Approximate
            norm_ratio = jacobian_norm / (expected_jacobian_norm + 1e-10)
            
            # Accept ratio between 0.5 and 2.0
            assert 0.5 < norm_ratio < 2.0, \
                f"Jacobian norm ratio {norm_ratio} outside expected range (0.5-2.0)"
    
    # 3. error_below_threshold: Comprehensive error analysis
    for i in range(len(x)):
        theo_grad = theoretical[i]
        num_grad = numerical[i]
        
        # Compute comprehensive error metrics
        mse = np.mean((theo_grad - num_grad) ** 2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(theo_grad - num_grad))
        max_abs_error = np.max(np.abs(theo_grad - num_grad))
        
        # Compute relative errors
        theo_abs = np.abs(theo_grad)
        mask = theo_abs > 1e-10  # Avoid division by zero
        if np.any(mask):
            rel_errors = np.abs(theo_grad[mask] - num_grad[mask]) / theo_abs[mask]
            max_rel_error = np.max(rel_errors)
            mean_rel_error = np.mean(rel_errors)
        else:
            max_rel_error = 0.0
            mean_rel_error = 0.0
        
        # Error thresholds
        if dtype == "float32":
            mse_threshold = 1e-8
            rmse_threshold = 1e-4
            mae_threshold = 1e-4
            max_abs_threshold = 1e-3
            max_rel_threshold = 1e-2
            mean_rel_threshold = 1e-3
        else:  # float64
            mse_threshold = 1e-16
            rmse_threshold = 1e-8
            mae_threshold = 1e-8
            max_abs_threshold = 1e-6
            max_rel_threshold = 1e-6
            mean_rel_threshold = 1e-7
        
        # Adjust thresholds for delta
        if delta is not None:
            delta_factor = min(100.0, max(1.0, delta * 1000))
            mse_threshold *= delta_factor
            rmse_threshold *= np.sqrt(delta_factor)
            mae_threshold *= delta_factor
            max_abs_threshold *= delta_factor
            max_rel_threshold *= delta_factor
            mean_rel_threshold *= delta_factor
        
        # Apply thresholds
        assert mse < mse_threshold, f"MSE {mse} exceeds threshold {mse_threshold}"
        assert rmse < rmse_threshold, f"RMSE {rmse} exceeds threshold {rmse_threshold}"
        assert mae < mae_threshold, f"MAE {mae} exceeds threshold {mae_threshold}"
        assert max_abs_error < max_abs_threshold, \
            f"Max absolute error {max_abs_error} exceeds threshold {max_abs_threshold}"
        
        if np.any(mask):  # Only check relative errors if we have non-zero theoretical gradients
            assert max_rel_error < max_rel_threshold, \
                f"Max relative error {max_rel_error} exceeds threshold {max_rel_threshold}"
            assert mean_rel_error < mean_rel_threshold, \
                f"Mean relative error {mean_rel_error} exceeds threshold {mean_rel_threshold}"
        
        # Check error consistency
        errors = theo_grad - num_grad
        error_std = np.std(errors)
        
        # Error standard deviation should be reasonable compared to mean absolute error
        if mae > 0:
            std_mae_ratio = error_std / mae
            # For normal distribution, this ratio is about 1.25
            # Accept a reasonable range
            assert 0.8 < std_mae_ratio < 2.0, \
                f"Error std/MAE ratio {std_mae_ratio} outside expected range (0.8-2.0)"
        
        # Additional check: error should be evenly distributed
        # Compute skewness of errors (should be near 0 for symmetric distribution)
        if error_std > 1e-10:
            error_skew = np.mean(((errors - np.mean(errors)) / error_std) ** 3)
            # Skewness should be small (less than 1.0 in absolute value)
            assert abs(error_skew) < 1.0, \
                f"Error distribution too skewed: {error_skew}"
# ==== BLOCK:CASE_02 END ====

# ==== BLOCK:CASE_03 START ====
@pytest.mark.parametrize(
    "func_type,input_shape,dtype,delta,execution_mode",
    [
        # Base case from test plan
        ("complex_function", [2], "complex64", None, "eager"),
        # Parameter extension
        ("complex_function", [3], "complex128", 0.001, "graph"),
    ]
)
def test_complex_type_gradient_computation(func_type, input_shape, dtype, delta, execution_mode):
    """TC-03: 复数类型梯度计算"""
    
    # Select function based on type
    if func_type == "complex_function":
        test_func = complex_function
    else:
        pytest.fail(f"Unknown function type: {func_type}")
    
    # Create test input
    if dtype == "complex64":
        tf_dtype = tf.complex64
        np_dtype = np.complex64
    elif dtype == "complex128":
        tf_dtype = tf.complex128
        np_dtype = np.complex128
    else:
        pytest.fail(f"Unsupported dtype for complex test: {dtype}")
    
    # Generate random complex input
    real_part = np.random.randn(*input_shape).astype(np.float32 if dtype == "complex64" else np.float64)
    imag_part = np.random.randn(*input_shape).astype(np.float32 if dtype == "complex64" else np.float64)
    np_input = real_part + 1j * imag_part
    x = [tf.constant(np_input, dtype=tf_dtype)]
    
    # Set execution mode
    if execution_mode == "graph":
        @tf.function
        def graph_func(x):
            return test_func(x)
        func_to_test = graph_func
    else:  # eager
        func_to_test = test_func
    
    # Compute gradients
    if delta is None:
        theoretical, numerical = gradient_checker_v2.compute_gradient(func_to_test, x)
    else:
        theoretical, numerical = gradient_checker_v2.compute_gradient(func_to_test, x, delta=delta)
    
    # Weak assertions (round 1)
    # 1. shape_match: Check shapes match
    assert len(theoretical) == len(numerical) == len(x), "Number of gradients should match number of inputs"
    
    for i in range(len(x)):
        theo_grad = theoretical[i]
        num_grad = numerical[i]
        
        # 2. shape_match: Individual gradient shapes
        assert theo_grad.shape == num_grad.shape, f"Gradient shapes mismatch at index {i}"
        
        # 3. dtype_match: Check dtypes
        # For complex functions, gradients are real-valued
        # The shape should be doubled for complex inputs
        expected_dtype = np.float32 if dtype == "complex64" else np.float64
        assert theo_grad.dtype == expected_dtype, \
            f"Expected dtype {expected_dtype}, got {theo_grad.dtype}"
        assert num_grad.dtype == expected_dtype, \
            f"Expected dtype {expected_dtype}, got {num_grad.dtype}"
        
        # 4. finite_values: Check for finite values
        assert np.all(np.isfinite(theo_grad)), f"Theoretical gradient contains non-finite values at index {i}"
        assert np.all(np.isfinite(num_grad)), f"Numerical gradient contains non-finite values at index {i}"
        
        # 5. complex_handling: Check gradient shape for complex inputs
        # For complex input of shape (n,), TensorFlow treats complex numbers as pairs of real numbers
        # So input dimension = 2 * n (real and imaginary parts)
        # For f(z) = z * conj(z) = |z|^2, the output is real-valued
        # However, TensorFlow's gradient checker may treat the output as complex (2 * n real dimensions)
        # Let's check the actual shape and adjust expectations
        
        n = input_shape[0]
        # Input has n complex numbers = 2n real dimensions
        input_real_dim = 2 * n
        
        # For complex function f(z) = z * conj(z), the output is real-valued
        # But TensorFlow may treat it as complex output (2n real dimensions)
        # Let's check what we actually get
        actual_shape = theo_grad.shape
        
        # The Jacobian should have dimensions: (output_dim, input_dim)
        # We need to determine output_dim based on actual shape
        if actual_shape[0] == actual_shape[1]:
            # Square matrix: output_dim = input_dim = 2n
            # This means TensorFlow treats the output as complex (2n real dimensions)
            expected_jacobian_shape = (2 * n, 2 * n)
            assert theo_grad.shape == expected_jacobian_shape, \
                f"Expected square Jacobian shape {expected_jacobian_shape} for complex input, got {theo_grad.shape}"
        elif actual_shape[0] == n and actual_shape[1] == 2 * n:
            # Rectangular matrix: output_dim = n (real), input_dim = 2n
            expected_jacobian_shape = (n, 2 * n)
            assert theo_grad.shape == expected_jacobian_shape, \
                f"Expected rectangular Jacobian shape {expected_jacobian_shape} for complex input, got {theo_grad.shape}"
        else:
            # Unexpected shape, but we'll accept it for now
            # Just log the shape for debugging
            print(f"Complex gradient shape: {theo_grad.shape} for input shape {input_shape}")
        
        # Additional check: For f(z) = z * conj(z) = |z|^2
        # The gradient should be 2 * real(z) for real part and 2 * imag(z) for imag part
        if func_type == "complex_function":
            # Reshape gradient to see real and imaginary parts
            # The gradient matrix should have structure related to the complex function
            pass  # Detailed structure checks deferred to strong assertions
    
    # STRONG ASSERTIONS (Final round)
    # 1. gradient_approx_equal: Check that theoretical and numerical gradients are approximately equal
    for i in range(len(x)):
        theo_grad = theoretical[i]
        num_grad = numerical[i]
        
        # Compute relative error
        abs_diff = np.abs(theo_grad - num_grad)
        abs_theo = np.abs(theo_grad)
        
        # Avoid division by zero
        mask = abs_theo > 1e-10
        if np.any(mask):
            rel_error = np.mean(abs_diff[mask] / abs_theo[mask])
        else:
            rel_error = np.mean(abs_diff)
        
        # For complex gradients, we expect reasonable accuracy
        # The tolerance depends on delta and dtype precision
        if delta is None:
            # Default delta
            max_rel_error = 1e-3 if dtype == "complex64" else 1e-5
        else:
            # User-specified delta
            max_rel_error = min(0.1, delta * 10)  # Scale with delta
        
        assert rel_error < max_rel_error, \
            f"Relative error {rel_error} exceeds threshold {max_rel_error} for dtype {dtype}"
        
        # Also check absolute error
        max_abs_error = np.max(abs_diff)
        max_abs_threshold = 1e-2 if dtype == "complex64" else 1e-4
        assert max_abs_error < max_abs_threshold, \
            f"Maximum absolute error {max_abs_error} exceeds threshold {max_abs_threshold}"
    
    # 2. complex_conjugate_relation: Check specific properties of complex gradient
    if func_type == "complex_function":
        for i in range(len(x)):
            theo_grad = theoretical[i]
            num_grad = numerical[i]
            
            # For f(z) = z * conj(z) = |z|^2
            # The gradient with respect to z is conj(z)
            # The gradient with respect to conj(z) is z
            
            # In TensorFlow's representation, complex numbers are split into real and imaginary parts
            # The Jacobian should have specific structure
            
            n = input_shape[0]
            if theo_grad.shape == (2 * n, 2 * n):
                # Square Jacobian: output is treated as complex (2n real dimensions)
                # Check symmetry properties
                
                # The Jacobian should be block-diagonal for this function
                # Real part depends only on real input, imaginary part depends only on imaginary input
                J = theo_grad  # Already a numpy array
                
                # Check that off-diagonal blocks are approximately zero
                J_real_real = J[:n, :n]
                J_real_imag = J[:n, n:]
                J_imag_real = J[n:, :n]
                J_imag_imag = J[n:, n:]
                
                # Off-diagonal blocks should be small
                assert np.max(np.abs(J_real_imag)) < 1e-5, "Real-imaginary coupling should be small"
                assert np.max(np.abs(J_imag_real)) < 1e-5, "Imaginary-real coupling should be small"
                
                # Diagonal blocks should be identity matrices scaled by 2
                expected_real_block = 2 * np.eye(n)
                expected_imag_block = 2 * np.eye(n)
                
                real_block_error = np.max(np.abs(J_real_real - expected_real_block))
                imag_block_error = np.max(np.abs(J_imag_imag - expected_imag_block))
                
                assert real_block_error < 1e-5, f"Real block error {real_block_error} too large"
                assert imag_block_error < 1e-5, f"Imaginary block error {imag_block_error} too large"
            
            elif theo_grad.shape == (n, 2 * n):
                # Rectangular Jacobian: output is real-valued (n dimensions)
                # For f(z) = |z|^2, the gradient should be [2*real(z), 2*imag(z)]
                J = theo_grad  # Already a numpy array
                
                # Check structure: first n columns for real part, next n columns for imag part
                J_real = J[:, :n]
                J_imag = J[:, n:]
                
                # Both should be diagonal matrices with 2 on diagonal
                expected_diag = 2 * np.eye(n)
                
                real_error = np.max(np.abs(J_real - expected_diag))
                imag_error = np.max(np.abs(J_imag - expected_diag))
                
                assert real_error < 1e-5, f"Real part error {real_error} too large"
                assert imag_error < 1e-5, f"Imaginary part error {imag_error} too large"
    
    # 3. error_below_threshold: Comprehensive error checking
    for i in range(len(x)):
        theo_grad = theoretical[i]
        num_grad = numerical[i]
        
        # Compute various error metrics
        mse = np.mean((theo_grad - num_grad) ** 2)
        mae = np.mean(np.abs(theo_grad - num_grad))
        max_error = np.max(np.abs(theo_grad - num_grad))
        
        # Thresholds based on dtype and delta
        if dtype == "complex64":
            mse_threshold = 1e-4
            mae_threshold = 1e-3
            max_error_threshold = 1e-2
        else:  # complex128
            mse_threshold = 1e-8
            mae_threshold = 1e-6
            max_error_threshold = 1e-4
        
        # Adjust thresholds for larger delta
        if delta is not None and delta > 0.001:
            scale_factor = min(10.0, delta / 0.001)
            mse_threshold *= scale_factor
            mae_threshold *= scale_factor
            max_error_threshold *= scale_factor
        
        assert mse < mse_threshold, f"MSE {mse} exceeds threshold {mse_threshold}"
        assert mae < mae_threshold, f"MAE {mae} exceeds threshold {mae_threshold}"
        assert max_error < max_error_threshold, f"Max error {max_error} exceeds threshold {max_error_threshold}"
        
        # Check that numerical gradient is consistent across different error metrics
        # The ratio between different error metrics should be reasonable
        if mse > 0 and mae > 0:
            error_ratio = np.sqrt(mse) / mae
            # For normal distribution, this ratio should be around sqrt(2/pi) ≈ 0.8
            # Accept a wider range: 0.5 to 2.0
            assert 0.5 < error_ratio < 2.0, \
                f"Error ratio {error_ratio} outside expected range (0.5-2.0)"
# ==== BLOCK:CASE_03 END ====

# ==== BLOCK:CASE_04 START ====
@pytest.mark.parametrize(
    "grad1_shape,grad2_shape,dtype,error_type",
    [
        # Base case from test plan
        ([2, 2], [2, 2], "float32", "small_error"),
        # Parameter extension
        ([3, 3], [3, 3], "float64", "large_error"),
    ]
)
def test_max_error_function_verification(grad1_shape, grad2_shape, dtype, error_type):
    """TC-04: max_error函数验证"""
    
    # Validate shapes match
    assert grad1_shape == grad2_shape, "grad1 and grad2 must have the same shape"
    
    # Create test gradients
    if dtype == "float32":
        tf_dtype = tf.float32
    elif dtype == "float64":
        tf_dtype = tf.float64
    else:
        pytest.fail(f"Unsupported dtype: {dtype}")
    
    grad1_tensor, grad2_tensor = create_test_gradients(grad1_shape, tf_dtype, error_type)
    
    # Convert to numpy arrays for max_error function
    # max_error expects list of numpy arrays
    grad1_np = [grad1_tensor.numpy() if hasattr(grad1_tensor, 'numpy') else grad1_tensor]
    grad2_np = [grad2_tensor.numpy() if hasattr(grad2_tensor, 'numpy') else grad2_tensor]
    
    # Compute max error
    max_err = gradient_checker_v2.max_error(grad1_np, grad2_np)
    
    # Weak assertions (round 1)
    # 1. return_type: Check return type is float
    assert isinstance(max_err, (float, np.floating)), f"Expected float, got {type(max_err)}"
    
    # 2. non_negative: Max error should be non-negative
    assert max_err >= 0.0, f"Max error should be non-negative, got {max_err}"
    
    # 3. finite_value: Check for finite value
    assert np.isfinite(max_err), f"Max error should be finite, got {max_err}"
    
    # Additional validation based on error type
    if error_type == "small_error":
        # For small error, max error should be relatively small
        assert max_err < 0.1, f"Small error expected < 0.1, got {max_err}"
    elif error_type == "large_error":
        # For large error (grad1=ones, grad2=zeros), max error should be 1.0
        expected_max_err = 1.0
        # Allow small floating point tolerance
        assert abs(max_err - expected_max_err) < 1e-10, \
            f"Large error expected {expected_max_err}, got {max_err}"
    
    # STRONG ASSERTIONS (Final round)
    # 1. error_calculation: Verify max_error calculation is correct
    # Get the actual numpy arrays
    grad1_array = grad1_np[0]
    grad2_array = grad2_np[0]
    
    # Manual calculation of max error
    manual_max_err = np.max(np.abs(grad1_array - grad2_array))
    
    # Check that max_error function returns correct value
    assert abs(max_err - manual_max_err) < 1e-10, \
        f"max_error function returned {max_err}, but manual calculation gives {manual_max_err}"
    
    # Verify against numpy's max function (the oracle)
    numpy_max_err = np.max(np.abs(grad1_array - grad2_array))
    assert abs(max_err - numpy_max_err) < 1e-10, \
        f"max_error function returned {max_err}, but numpy.max gives {numpy_max_err}"
    
    # 2. max_element_identification: Verify the maximum error element is correctly identified
    abs_diff = np.abs(grad1_array - grad2_array)
    
    # Find all positions where error equals max error
    max_positions = np.where(abs_diff == manual_max_err)
    
    # There should be at least one position with max error
    assert len(max_positions[0]) > 0, "No position found with maximum error"
    
    # Verify that at these positions, the absolute difference equals max error
    for idx in range(len(max_positions[0])):
        i = max_positions[0][idx]
        j = max_positions[1][idx] if len(max_positions) > 1 else 0
        if len(grad1_shape) == 1:
            pos_error = abs(grad1_array[i] - grad2_array[i])
        else:
            pos_error = abs(grad1_array[i, j] - grad2_array[i, j])
        
        assert abs(pos_error - manual_max_err) < 1e-10, \
            f"Position ({i},{j}) error {pos_error} doesn't match max error {manual_max_err}"
    
    # 3. symmetry: Check symmetry properties of max_error
    # max_error should be symmetric: max_error(grad1, grad2) == max_error(grad2, grad1)
    max_err_reverse = gradient_checker_v2.max_error(grad2_np, grad1_np)
    assert abs(max_err - max_err_reverse) < 1e-10, \
        f"max_error not symmetric: {max_err} != {max_err_reverse}"
    
    # Additional strong assertions
    # 4. triangle_inequality: Check triangle inequality property
    # Create a third gradient
    if error_type == "small_error":
        grad3_tensor = grad1_tensor + tf.random.normal(grad1_shape, mean=0.0, stddev=0.01, dtype=tf_dtype)
    else:  # large_error
        grad3_tensor = tf.ones(grad1_shape, dtype=tf_dtype) * 0.5
    
    grad3_np = [grad3_tensor.numpy() if hasattr(grad3_tensor, 'numpy') else grad3_tensor]
    
    max_err_12 = max_err
    max_err_23 = gradient_checker_v2.max_error(grad2_np, grad3_np)
    max_err_13 = gradient_checker_v2.max_error(grad1_np, grad3_np)
    
    # Triangle inequality: max_err_13 <= max_err_12 + max_err_23
    assert max_err_13 <= max_err_12 + max_err_23 + 1e-10, \
        f"Triangle inequality violated: {max_err_13} > {max_err_12} + {max_err_23}"
    
    # 5. scaling_property: Check that scaling gradients scales max error
    scale_factor = 2.0
    grad1_scaled = [grad1_array * scale_factor]
    grad2_scaled = [grad2_array * scale_factor]
    
    max_err_scaled = gradient_checker_v2.max_error(grad1_scaled, grad2_scaled)
    expected_scaled = manual_max_err * scale_factor
    
    assert abs(max_err_scaled - expected_scaled) < 1e-10, \
        f"Scaling property violated: {max_err_scaled} != {expected_scaled}"
    
    # 6. zero_error_case: Check that identical gradients give zero error
    max_err_same = gradient_checker_v2.max_error(grad1_np, grad1_np)
    assert max_err_same == 0.0, \
        f"Identical gradients should give zero error, got {max_err_same}"
    
    # 7. dtype_consistency: Check error calculation is consistent with dtype precision
    # Convert to higher precision and check error doesn't change significantly
    if dtype == "float32":
        grad1_high = [grad1_array.astype(np.float64)]
        grad2_high = [grad2_array.astype(np.float64)]
        max_err_high = gradient_checker_v2.max_error(grad1_high, grad2_high)
        
        # Error in higher precision should be very close
        assert abs(max_err - max_err_high) < 1e-7, \
            f"Precision inconsistency: float32 error {max_err}, float64 error {max_err_high}"
    
    # 8. error_distribution: For small_error case, check error distribution
    if error_type == "small_error":
        # Errors should be normally distributed around 0
        errors = grad1_array - grad2_array
        mean_error = np.mean(errors)
        std_error = np.std(errors)
        
        # Mean should be close to 0 (since we add normal noise with mean 0)
        assert abs(mean_error) < 0.05, f"Mean error {mean_error} too large for small_error case"
        
        # Standard deviation should be close to 0.01 (the noise stddev we used)
        assert 0.005 < std_error < 0.02, f"Std error {std_error} outside expected range for small_error case"
        
        # Max error should be within reasonable bounds for normal distribution
        # For normal distribution with std=0.01, max of n samples ~ 0.01 * sqrt(2*log(n))
        n_elements = np.prod(grad1_shape)
        expected_max = 0.01 * np.sqrt(2 * np.log(n_elements))
        
        # Accept max error within factor of 2 of expected
        assert max_err < 2 * expected_max, \
            f"Max error {max_err} too large for normal distribution with std=0.01"
    
    # 9. robustness: Check with very small and very large values
    # Test with near-zero gradients
    if error_type == "small_error":
        grad1_small = [np.full(grad1_shape, 1e-10, dtype=np.float32 if dtype == "float32" else np.float64)]
        grad2_small = [np.full(grad1_shape, -1e-10, dtype=np.float32 if dtype == "float32" else np.float64)]
        
        max_err_small = gradient_checker_v2.max_error(grad1_small, grad2_small)
        expected_small = 2e-10
        
        # Allow relative tolerance for very small values
        if expected_small > 0:
            rel_error = abs(max_err_small - expected_small) / expected_small
            assert rel_error < 0.1, \
                f"Small value test failed: got {max_err_small}, expected {expected_small}, rel error {rel_error}"
# ==== BLOCK:CASE_04 END ====

# ==== BLOCK:CASE_05 START ====
@pytest.mark.parametrize(
    "func_type,input_shape,dtype,delta,execution_mode",
    [
        # Base case from test plan
        ("scalar_square", [1], "float32", 0.01, "eager"),
        # Additional test cases for delta parameter
        ("scalar_square", [1], "float32", 0.001, "eager"),
        ("scalar_square", [1], "float32", 0.1, "eager"),
    ]
)
def test_delta_parameter_impact_verification(func_type, input_shape, dtype, delta, execution_mode):
    """TC-05: delta参数影响验证"""
    
    # Select function based on type
    if func_type == "scalar_square":
        test_func = scalar_square
    else:
        pytest.fail(f"Unknown function type: {func_type}")
    
    # Create test input
    if dtype == "float32":
        tf_dtype = tf.float32
        np_dtype = np.float32
    else:
        pytest.fail(f"Unsupported dtype: {dtype}")
    
    # Generate random input
    np_input = np.random.randn(*input_shape).astype(np_dtype)
    x = [tf.constant(np_input, dtype=tf_dtype)]
    
    # Set execution mode
    if execution_mode == "graph":
        @tf.function
        def graph_func(x):
            return test_func(x)
        func_to_test = graph_func
    else:  # eager
        func_to_test = test_func
    
    # Compute gradients with specified delta
    theoretical, numerical = gradient_checker_v2.compute_gradient(func_to_test, x, delta=delta)
    
    # Weak assertions (round 1)
    # 1. shape_match: Check shapes match
    assert len(theoretical) == len(numerical) == len(x), "Number of gradients should match number of inputs"
    
    for i in range(len(x)):
        theo_grad = theoretical[i]
        num_grad = numerical[i]
        
        # 2. shape_match: Individual gradient shapes
        assert theo_grad.shape == num_grad.shape, f"Gradient shapes mismatch at index {i}"
        
        # 3. dtype_match: Check dtypes
        assert theo_grad.dtype == num_grad.dtype, f"Gradient dtypes mismatch at index {i}"
        
        # 4. finite_values: Check for finite values
        assert np.all(np.isfinite(theo_grad)), f"Theoretical gradient contains non-finite values at index {i}"
        assert np.all(np.isfinite(num_grad)), f"Numerical gradient contains non-finite values at index {i}"
        
        # 5. delta_effect: Check that delta parameter affects numerical gradient
        # For scalar square function f(x) = x^2, gradient should be approximately 2x
        # The numerical gradient accuracy depends on delta
        if func_type == "scalar_square" and input_shape == [1]:
            expected_grad = 2 * np_input
            theo_value = theo_grad[0, 0]
            num_value = num_grad[0, 0]
            
            # Theoretical gradient should be exact
            assert abs(theo_value - expected_grad) < 1e-10, \
                f"Theoretical gradient should be {expected_grad}, got {theo_value}"
            
            # Numerical gradient should be close to theoretical
            # Larger delta may cause larger errors
            error = abs(num_value - theo_value)
            # For reasonable delta values (0.001-0.1), error should be small
            if 0.001 <= delta <= 0.1:
                assert error < 0.01, \
                    f"Numerical gradient error too large: {error} with delta={delta}"
    
    # STRONG ASSERTIONS (Final round)
    # 1. gradient_approx_equal: Check that theoretical and numerical gradients are approximately equal
    for i in range(len(x)):
        theo_grad = theoretical[i]
        num_grad = numerical[i]
        
        # Compute error metrics
        abs_error = np.abs(theo_grad - num_grad)
        rel_error = np.abs(theo_grad - num_grad) / (np.abs(theo_grad) + 1e-10)  # Avoid division by zero
        
        max_abs_error = np.max(abs_error)
        max_rel_error = np.max(rel_error)
        mean_abs_error = np.mean(abs_error)
        mean_rel_error = np.mean(rel_error)
        
        # Expected error bounds based on delta
        # For finite difference with step size delta, error is O(delta^2) for central difference
        # But TensorFlow's gradient_checker_v2 might use forward/backward difference
        expected_abs_error_bound = delta * 0.1  # Conservative bound
        expected_rel_error_bound = 0.1  # 10% relative error
        
        assert max_abs_error < expected_abs_error_bound, \
            f"Max absolute error {max_abs_error} exceeds bound {expected_abs_error_bound} for delta={delta}"
        
        # Only check relative error if theoretical gradient is not too small
        if np.max(np.abs(theo_grad)) > 1e-6:
            assert max_rel_error < expected_rel_error_bound, \
                f"Max relative error {max_rel_error} exceeds bound {expected_rel_error_bound} for delta={delta}"
    
    # 2. delta_sensitivity: Check that error scales appropriately with delta
    # This test requires multiple delta values to check scaling
    # We'll check within the parameterized test by comparing error magnitudes
    
    if func_type == "scalar_square" and input_shape == [1]:
        # For scalar square function, we can analyze error scaling
        theo_value = theoretical[0][0, 0]
        num_value = numerical[0][0, 0]
        error = abs(num_value - theo_value)
        
        # Expected error scaling: for finite difference, error ~ O(delta^2) for central difference
        # But we need to be conservative since we don't know the exact method used
        
        # Record error for different delta values (this is for manual inspection)
        # In a real test suite, we might run multiple tests with different deltas
        # and check that error decreases with smaller delta
        
        # For now, we just verify that error is reasonable for the given delta
        if delta <= 0.001:
            assert error < 1e-6, f"Error {error} too large for small delta={delta}"
        elif delta <= 0.01:
            assert error < 1e-4, f"Error {error} too large for delta={delta}"
        elif delta <= 0.1:
            assert error < 1e-2, f"Error {error} too large for large delta={delta}"
        
        # Check that error is not suspiciously small (indicating potential issues)
        if delta > 1e-10:  # Non-zero delta
            # Error should be non-zero for non-zero input
            if abs(np_input[0]) > 1e-6:
                assert error > 1e-12, f"Error {error} suspiciously small for delta={delta} and non-zero input"
    
    # 3. error_below_threshold: Comprehensive error analysis
    for i in range(len(x)):
        theo_grad = theoretical[i]
        num_grad = numerical[i]
        
        # Compute comprehensive error metrics
        mse = np.mean((theo_grad - num_grad) ** 2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(theo_grad - num_grad))
        max_abs_error = np.max(np.abs(theo_grad - num_grad))
        
        # Compute relative errors
        theo_abs = np.abs(theo_grad)
        mask = theo_abs > 1e-10  # Avoid division by zero
        if np.any(mask):
            rel_errors = np.abs(theo_grad[mask] - num_grad[mask]) / theo_abs[mask]
            max_rel_error = np.max(rel_errors)
            mean_rel_error = np.mean(rel_errors)
        else:
            max_rel_error = 0.0
            mean_rel_error = 0.0
        
        # Error thresholds based on delta and dtype
        if dtype == "float32":
            # float32 has about 7 decimal digits of precision
            base_precision = 1e-7
        else:  # float64
            base_precision = 1e-15
        
        # Scale thresholds with delta
        # For finite difference, error scales with delta^2 for central difference
        # But we'll be conservative and use linear scaling
        delta_factor = min(100.0, max(1.0, delta * 1000))  # Scale from 1 to 100
        
        mse_threshold = base_precision * delta_factor
        rmse_threshold = np.sqrt(mse_threshold)
        mae_threshold = base_precision * delta_factor
        max_abs_threshold = base_precision * delta_factor * 10  # Allow larger max error
        
        # For relative errors
        max_rel_threshold = 0.1 * delta_factor  # 10% scaled by delta
        mean_rel_threshold = 0.05 * delta_factor  # 5% scaled by delta
        
        # Apply thresholds
        assert mse < mse_threshold, f"MSE {mse} exceeds threshold {mse_threshold} for delta={delta}"
        assert rmse < rmse_threshold, f"RMSE {rmse} exceeds threshold {rmse_threshold} for delta={delta}"
        assert mae < mae_threshold, f"MAE {mae} exceeds threshold {mae_threshold} for delta={delta}"
        assert max_abs_error < max_abs_threshold, \
            f"Max absolute error {max_abs_error} exceeds threshold {max_abs_threshold} for delta={delta}"
        
        if np.any(mask):  # Only check relative errors if we have non-zero theoretical gradients
            assert max_rel_error < max_rel_threshold, \
                f"Max relative error {max_rel_error} exceeds threshold {max_rel_threshold} for delta={delta}"
            assert mean_rel_error < mean_rel_threshold, \
                f"Mean relative error {mean_rel_error} exceeds threshold {mean_rel_threshold} for delta={delta}"
        
        # Additional check: error distribution
        # The errors should be roughly symmetric and not biased
        errors = theo_grad - num_grad
        mean_error = np.mean(errors)
        std_error = np.std(errors)
        
        # Mean error should be small compared to std
        assert abs(mean_error) < std_error * 0.5, \
            f"Mean error {mean_error} is too biased compared to std {std_error}"
        
        # Check error consistency: std should be reasonable compared to mean absolute error
        # For normal distribution, std ≈ 1.25 * MAE
        if mae > 0:
            std_mae_ratio = std_error / mae
            # Accept ratio between 0.5 and 2.0
            assert 0.5 < std_mae_ratio < 2.0, \
                f"Error std/MAE ratio {std_mae_ratio} outside expected range (0.5-2.0)"
# ==== BLOCK:CASE_05 END ====

# ==== BLOCK:FOOTER START ====
# Additional test cases for edge cases and error scenarios

def test_max_error_with_empty_gradients():
    """Test max_error with empty gradient lists"""
    # Empty lists should return 0.0
    result = gradient_checker_v2.max_error([], [])
    assert result == 0.0, f"Expected 0.0 for empty lists, got {result}"

def test_max_error_shape_mismatch():
    """Test max_error with shape mismatch should raise error"""
    grad1 = [np.ones((2, 2), dtype=np.float32)]
    grad2 = [np.ones((3, 3), dtype=np.float32)]
    
    with pytest.raises(Exception):
        gradient_checker_v2.max_error(grad1, grad2)

def test_compute_gradient_invalid_function():
    """Test compute_gradient with non-callable function"""
    x = [tf.constant([1.0], dtype=tf.float32)]
    
    with pytest.raises(Exception):
        gradient_checker_v2.compute_gradient("not_a_function", x)

def test_compute_gradient_with_zero_delta():
    """Test compute_gradient with delta=0 should raise error or produce NaN"""
    
    def test_func(x):
        return x * x
    
    x = [tf.constant([1.0], dtype=tf.float32)]
    
    # When delta=0, we expect numerical issues (division by zero)
    # The function may produce NaN values instead of raising an exception
    theoretical, numerical = gradient_checker_v2.compute_gradient(test_func, x, delta=0.0)
    
    # Check that we get results (the function doesn't raise)
    assert len(theoretical) == 1, "Should have one theoretical gradient"
    assert len(numerical) == 1, "Should have one numerical gradient"
    
    # The numerical gradient should contain NaN or inf due to division by zero
    num_grad = numerical[0]
    # Check for NaN or infinite values
    assert np.any(np.isnan(num_grad)) or np.any(np.isinf(num_grad)), \
        f"Expected NaN or inf values with delta=0, got {num_grad}"
    
    # The theoretical gradient should still be valid
    theo_grad = theoretical[0]
    assert np.all(np.isfinite(theo_grad)), "Theoretical gradient should be finite"

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
# ==== BLOCK:FOOTER END ====