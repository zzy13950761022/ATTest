# START:HEADER
"""
Test cases for tensorflow.python.ops.confusion_matrix
Generated by TestAgent
"""

import numpy as np
import tensorflow as tf
import pytest

# Import the target function
from tensorflow.python.ops import confusion_matrix

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)


def assert_confusion_matrix_shape(result, expected_num_classes):
    """Assert confusion matrix has correct shape."""
    assert result.shape == (expected_num_classes, expected_num_classes), \
        f"Expected shape ({expected_num_classes}, {expected_num_classes}), got {result.shape}"


def assert_confusion_matrix_dtype(result, expected_dtype):
    """Assert confusion matrix has correct dtype."""
    assert result.dtype == expected_dtype, \
        f"Expected dtype {expected_dtype}, got {result.dtype}"


def assert_confusion_matrix_properties(result):
    """Assert basic properties of confusion matrix."""
    # All values should be non-negative
    assert tf.reduce_all(result >= 0).numpy(), "Confusion matrix contains negative values"
    
    # Sum of all elements should equal number of samples
    # (This is true for unweighted confusion matrices)
    # We'll check this in specific test cases where appropriate
# END:HEADER


# START:CASE_01
@pytest.mark.parametrize(
    "labels,predictions,num_classes,weights,dtype,name,expected_num_classes",
    [
        # Base case from test plan
        (
            [1, 2, 4],
            [2, 2, 4],
            None,
            None,
            tf.int32,
            None,
            5  # max(1,2,4,2,2,4) + 1 = 4 + 1 = 5
        ),
        # Parameter extension: binary classification
        (
            [0, 0, 1, 1],
            [0, 1, 0, 1],
            2,
            None,
            tf.int32,
            None,
            2
        ),
    ]
)
def test_basic_confusion_matrix(labels, predictions, num_classes, weights, dtype, name, expected_num_classes):
    """
    CASE_01: 基本混淆矩阵计算
    验证标准分类场景的正确性，矩阵列表示预测标签，行表示真实标签
    """
    # Convert inputs to tensors
    labels_tensor = tf.constant(labels, dtype=tf.int32)
    predictions_tensor = tf.constant(predictions, dtype=tf.int32)
    
    if weights is not None:
        weights_tensor = tf.constant(weights, dtype=dtype)
    else:
        weights_tensor = None
    
    # Call the function
    result = confusion_matrix.confusion_matrix(
        labels=labels_tensor,
        predictions=predictions_tensor,
        num_classes=num_classes,
        weights=weights_tensor,
        dtype=dtype,
        name=name
    )
    
    # Weak assertions (round 1)
    # 1. Shape assertion
    assert_confusion_matrix_shape(result, expected_num_classes)
    
    # 2. Dtype assertion
    assert_confusion_matrix_dtype(result, dtype)
    
    # 3. Basic property assertion
    assert_confusion_matrix_properties(result)
    
    # Additional weak assertion: sum of matrix equals number of samples for unweighted case
    if weights is None:
        total_samples = len(labels)
        matrix_sum = tf.reduce_sum(result).numpy()
        assert matrix_sum == total_samples, \
            f"Sum of confusion matrix ({matrix_sum}) should equal number of samples ({total_samples})"
# END:CASE_01


# START:CASE_02
@pytest.mark.parametrize(
    "labels,predictions,num_classes,weights,dtype,name,expected_num_classes",
    [
        # Base case from test plan
        (
            [0, 1, 2, 3],
            [1, 1, 3, 3],
            None,
            None,
            tf.int32,
            None,
            4  # max(0,1,2,3,1,1,3,3) + 1 = 3 + 1 = 4
        ),
        # Parameter extension: large label values
        (
            [5, 10, 15],
            [5, 10, 15],
            None,
            None,
            tf.int32,
            None,
            16  # max(5,10,15,5,10,15) + 1 = 15 + 1 = 16
        ),
    ]
)
def test_auto_num_classes_inference(labels, predictions, num_classes, weights, dtype, name, expected_num_classes):
    """
    CASE_02: 自动num_classes推断
    测试未指定类别数时的自动计算，基于预测和标签的最大值
    """
    # Convert inputs to tensors
    labels_tensor = tf.constant(labels, dtype=tf.int32)
    predictions_tensor = tf.constant(predictions, dtype=tf.int32)
    
    if weights is not None:
        weights_tensor = tf.constant(weights, dtype=dtype)
    else:
        weights_tensor = None
    
    # Call the function
    result = confusion_matrix.confusion_matrix(
        labels=labels_tensor,
        predictions=predictions_tensor,
        num_classes=num_classes,
        weights=weights_tensor,
        dtype=dtype,
        name=name
    )
    
    # Weak assertions (round 1)
    # 1. Shape assertion
    assert_confusion_matrix_shape(result, expected_num_classes)
    
    # 2. Dtype assertion
    assert_confusion_matrix_dtype(result, dtype)
    
    # 3. Num classes inferred assertion
    # Verify that the inferred number of classes is correct
    # This is already covered by shape assertion, but we can add explicit check
    inferred_num_classes = result.shape[0]
    assert inferred_num_classes == expected_num_classes, \
        f"Inferred num_classes should be {expected_num_classes}, got {inferred_num_classes}"
    
    # 4. Basic property assertion
    assert_confusion_matrix_properties(result)
    
    # Additional weak assertion: sum of matrix equals number of samples for unweighted case
    if weights is None:
        total_samples = len(labels)
        matrix_sum = tf.reduce_sum(result).numpy()
        assert matrix_sum == total_samples, \
            f"Sum of confusion matrix ({matrix_sum}) should equal number of samples ({total_samples})"
# END:CASE_02


# START:CASE_03
@pytest.mark.parametrize(
    "labels,predictions,num_classes,weights,dtype,name,expected_num_classes",
    [
        # Base case from test plan
        (
            [0, 1, 0],
            [0, 1, 1],
            2,
            [1.0, 2.0, 0.5],
            tf.float32,
            None,
            2
        ),
        # Parameter extension: float weights with higher precision
        (
            [0, 1, 2],
            [0, 1, 2],
            3,
            [0.5, 1.5, 2.5],
            tf.float64,
            None,
            3
        ),
    ]
)
def test_weighted_confusion_matrix(labels, predictions, num_classes, weights, dtype, name, expected_num_classes):
    """
    CASE_03: 带权重计算
    验证权重参数对矩阵计数的影响，权重张量形状需与预测值匹配
    """
    # Convert inputs to tensors
    labels_tensor = tf.constant(labels, dtype=tf.int32)
    predictions_tensor = tf.constant(predictions, dtype=tf.int32)
    weights_tensor = tf.constant(weights, dtype=dtype)
    
    # Call the function
    result = confusion_matrix.confusion_matrix(
        labels=labels_tensor,
        predictions=predictions_tensor,
        num_classes=num_classes,
        weights=weights_tensor,
        dtype=dtype,
        name=name
    )
    
    # Weak assertions (round 1)
    # 1. Shape assertion
    assert_confusion_matrix_shape(result, expected_num_classes)
    
    # 2. Dtype assertion
    assert_confusion_matrix_dtype(result, dtype)
    
    # 3. Weighted property assertion
    # For weighted case, all values should still be non-negative
    assert tf.reduce_all(result >= 0).numpy(), "Weighted confusion matrix contains negative values"
    
    # 4. Weighted sum assertion
    # Sum of weighted confusion matrix should equal sum of weights
    total_weight = tf.reduce_sum(weights_tensor).numpy()
    matrix_sum = tf.reduce_sum(result).numpy()
    
    # Use appropriate tolerance based on dtype
    if dtype in [tf.float32, tf.float64]:
        # For floating point, use relative tolerance
        rtol = 1e-6 if dtype == tf.float32 else 1e-12
        assert abs(matrix_sum - total_weight) < rtol * abs(total_weight), \
            f"Sum of weighted confusion matrix ({matrix_sum}) should equal total weight ({total_weight})"
    else:
        # For integer types, exact equality
        assert matrix_sum == total_weight, \
            f"Sum of weighted confusion matrix ({matrix_sum}) should equal total weight ({total_weight})"
# END:CASE_03


# START:CASE_04
@pytest.mark.parametrize(
    "labels,predictions,num_classes,weights,dtype,name,expected_num_classes,expected_matrix",
    [
        # Base case from test plan: int64 dtype
        (
            [0, 1, 2],
            [0, 1, 2],
            3,
            None,
            tf.int64,
            None,
            3,
            [[1, 0, 0],
             [0, 1, 0],
             [0, 0, 1]]
        ),
        # Parameter extension: float32 dtype with name parameter
        (
            [0, 1],
            [0, 1],
            2,
            None,
            tf.float32,
            "test_confusion_matrix",
            2,
            [[1, 0],
             [0, 1]]
        ),
    ]
)
def test_data_type_conversion(labels, predictions, num_classes, weights, dtype, name, expected_num_classes, expected_matrix):
    """
    CASE_04: 数据类型转换
    测试不同dtype参数的正确性，包括int32、int64、float32等
    """
    # Convert inputs to tensors
    labels_tensor = tf.constant(labels, dtype=tf.int32)
    predictions_tensor = tf.constant(predictions, dtype=tf.int32)
    
    if weights is not None:
        weights_tensor = tf.constant(weights, dtype=dtype)
    else:
        weights_tensor = None
    
    # Call the function
    result = confusion_matrix.confusion_matrix(
        labels=labels_tensor,
        predictions=predictions_tensor,
        num_classes=num_classes,
        weights=weights_tensor,
        dtype=dtype,
        name=name
    )
    
    # Weak assertions (round 2)
    # 1. Shape assertion
    assert_confusion_matrix_shape(result, expected_num_classes)
    
    # 2. Dtype assertion
    assert_confusion_matrix_dtype(result, dtype)
    
    # 3. Type conversion assertion
    # Verify that the dtype conversion worked correctly
    # For integer types, values should be exact
    # For float types, we need to handle floating point precision
    
    # 4. Basic property assertion
    assert_confusion_matrix_properties(result)
    
    # Additional weak assertion: compare with expected matrix
    result_np = result.numpy()
    expected_np = np.array(expected_matrix, dtype=result_np.dtype)
    
    if dtype in [tf.float32, tf.float64]:
        # For floating point, use numpy allclose with appropriate tolerance
        rtol = 1e-6 if dtype == tf.float32 else 1e-12
        assert np.allclose(result_np, expected_np, rtol=rtol), \
            f"Result matrix does not match expected. Got:\n{result_np}\nExpected:\n{expected_np}"
    else:
        # For integer types, exact equality
        assert np.array_equal(result_np, expected_np), \
            f"Result matrix does not match expected. Got:\n{result_np}\nExpected:\n{expected_np}"
    
    # Additional check: sum of matrix equals number of samples for unweighted case
    if weights is None:
        total_samples = len(labels)
        matrix_sum = tf.reduce_sum(result).numpy()
        assert matrix_sum == total_samples, \
            f"Sum of confusion matrix ({matrix_sum}) should equal number of samples ({total_samples})"
# END:CASE_04


# START:CASE_05
@pytest.mark.parametrize(
    "labels,predictions,num_classes,weights,dtype,name,expected_num_classes,expected_matrix",
    [
        # Base case from test plan: empty input with num_classes=0
        (
            [],
            [],
            0,
            None,
            tf.int32,
            None,
            0,
            []  # Empty 0x0 matrix
        ),
        # Parameter extension: single class classification
        (
            [0],
            [0],
            1,
            None,
            tf.int32,
            None,
            1,
            [[1]]
        ),
    ]
)
def test_edge_case_empty_input(labels, predictions, num_classes, weights, dtype, name, expected_num_classes, expected_matrix):
    """
    CASE_05: 边界值处理-空输入
    验证空张量输入（长度为0）和num_classes=0的边界场景
    """
    # Convert inputs to tensors
    labels_tensor = tf.constant(labels, dtype=tf.int32)
    predictions_tensor = tf.constant(predictions, dtype=tf.int32)
    
    if weights is not None:
        weights_tensor = tf.constant(weights, dtype=dtype)
    else:
        weights_tensor = None
    
    # Call the function
    result = confusion_matrix.confusion_matrix(
        labels=labels_tensor,
        predictions=predictions_tensor,
        num_classes=num_classes,
        weights=weights_tensor,
        dtype=dtype,
        name=name
    )
    
    # Weak assertions (round 2)
    # 1. Shape assertion
    if expected_num_classes == 0:
        # For empty matrix, shape should be (0, 0)
        assert result.shape == (0, 0), \
            f"Expected shape (0, 0) for empty input, got {result.shape}"
    else:
        assert_confusion_matrix_shape(result, expected_num_classes)
    
    # 2. Dtype assertion
    assert_confusion_matrix_dtype(result, dtype)
    
    # 3. Empty input assertion
    # For empty input, the matrix should be empty
    if len(labels) == 0 and num_classes == 0:
        assert tf.size(result).numpy() == 0, \
            "Confusion matrix should be empty for empty input with num_classes=0"
    
    # 4. Basic property assertion (skip for empty matrix)
    if expected_num_classes > 0:
        assert_confusion_matrix_properties(result)
    
    # Additional weak assertion: compare with expected matrix
    result_np = result.numpy()
    
    if expected_num_classes == 0:
        # For empty matrix, result should be empty array
        assert result_np.size == 0, \
            f"Expected empty matrix for num_classes=0, got shape {result_np.shape}"
    else:
        expected_np = np.array(expected_matrix, dtype=result_np.dtype)
        
        if dtype in [tf.float32, tf.float64]:
            # For floating point, use numpy allclose with appropriate tolerance
            rtol = 1e-6 if dtype == tf.float32 else 1e-12
            assert np.allclose(result_np, expected_np, rtol=rtol), \
                f"Result matrix does not match expected. Got:\n{result_np}\nExpected:\n{expected_np}"
        else:
            # For integer types, exact equality
            assert np.array_equal(result_np, expected_np), \
                f"Result matrix does not match expected. Got:\n{result_np}\nExpected:\n{expected_np}"
    
    # Additional check: sum of matrix equals number of samples for unweighted case
    if weights is None and expected_num_classes > 0:
        total_samples = len(labels)
        matrix_sum = tf.reduce_sum(result).numpy()
        assert matrix_sum == total_samples, \
            f"Sum of confusion matrix ({matrix_sum}) should equal number of samples ({total_samples})"
# END:CASE_05


# START:CASE_06
def test_dimension_mismatch_error():
    """
    CASE_06: 维度不匹配错误处理
    测试当labels和predictions维度不匹配时是否抛出适当异常
    """
    # Test case 1: Different lengths
    labels = tf.constant([0, 1, 2], dtype=tf.int32)
    predictions = tf.constant([0, 1], dtype=tf.int32)  # Different length
    
    # TensorFlow typically raises InvalidArgumentError for dimension mismatch
    with pytest.raises((tf.errors.InvalidArgumentError, ValueError)) as exc_info:
        confusion_matrix.confusion_matrix(
            labels=labels,
            predictions=predictions,
            num_classes=3
        )
    
    # Verify that an error was raised with appropriate message
    assert exc_info.value is not None, "Expected exception for dimension mismatch"
    error_msg = str(exc_info.value).lower()
    # Check for dimension/shape related keywords in error message
    assert any(keyword in error_msg for keyword in ['dimension', 'shape', 'size', 'length', 'must match']), \
        f"Error message should mention dimension/shape mismatch. Got: {error_msg}"
    
    # Test case 2: Different shapes (2D vs 1D)
    labels_2d = tf.constant([[0, 1], [2, 3]], dtype=tf.int32)
    predictions_1d = tf.constant([0, 1, 2, 3], dtype=tf.int32)
    
    with pytest.raises((tf.errors.InvalidArgumentError, ValueError)) as exc_info2:
        confusion_matrix.confusion_matrix(
            labels=labels_2d,
            predictions=predictions_1d,
            num_classes=4
        )
    
    # Verify that an error was raised
    assert exc_info2.value is not None, "Expected exception for shape mismatch"
    error_msg2 = str(exc_info2.value).lower()
    # Check for dimension/shape related keywords in error message
    assert any(keyword in error_msg2 for keyword in ['dimension', 'shape', 'rank', 'must match']), \
        f"Error message should mention dimension/shape mismatch. Got: {error_msg2}"
    
    # Test case 3: Weights shape mismatch
    labels = tf.constant([0, 1, 2], dtype=tf.int32)
    predictions = tf.constant([0, 1, 2], dtype=tf.int32)
    weights = tf.constant([1.0, 2.0], dtype=tf.float32)  # Wrong length
    
    with pytest.raises((tf.errors.InvalidArgumentError, ValueError)) as exc_info3:
        confusion_matrix.confusion_matrix(
            labels=labels,
            predictions=predictions,
            num_classes=3,
            weights=weights
        )
    
    # Verify that an error was raised for weights shape mismatch
    assert exc_info3.value is not None, "Expected exception for weights shape mismatch"
    error_msg3 = str(exc_info3.value).lower()
    # Check for weights/shape related keywords in error message
    assert any(keyword in error_msg3 for keyword in ['weight', 'shape', 'dimension', 'must match']), \
        f"Error message should mention weights shape mismatch. Got: {error_msg3}"
# END:CASE_06


# START:CASE_07
def test_negative_labels_error():
    """
    CASE_07: 负标签值错误处理
    测试当labels或predictions包含负值时是否抛出适当异常
    """
    # Test case 1: Negative labels
    labels = tf.constant([0, -1, 2], dtype=tf.int32)  # Contains negative value
    predictions = tf.constant([0, 1, 2], dtype=tf.int32)
    
    # TensorFlow typically raises InvalidArgumentError for negative values
    with pytest.raises((tf.errors.InvalidArgumentError, ValueError)) as exc_info:
        confusion_matrix.confusion_matrix(
            labels=labels,
            predictions=predictions,
            num_classes=3
        )
    
    # Verify that an error was raised with appropriate message
    assert exc_info.value is not None, "Expected exception for negative labels"
    error_msg = str(exc_info.value).lower()
    # Check for negative/non-negative related keywords in error message
    assert any(keyword in error_msg for keyword in ['negative', 'non-negative', '>= 0', 'greater than or equal']), \
        f"Error message should mention negative values. Got: {error_msg}"
    
    # Test case 2: Negative predictions
    labels = tf.constant([0, 1, 2], dtype=tf.int32)
    predictions = tf.constant([0, -1, 2], dtype=tf.int32)  # Contains negative value
    
    with pytest.raises((tf.errors.InvalidArgumentError, ValueError)) as exc_info2:
        confusion_matrix.confusion_matrix(
            labels=labels,
            predictions=predictions,
            num_classes=3
        )
    
    # Verify that an error was raised
    assert exc_info2.value is not None, "Expected exception for negative predictions"
    error_msg2 = str(exc_info2.value).lower()
    # Check for negative/non-negative related keywords in error message
    assert any(keyword in error_msg2 for keyword in ['negative', 'non-negative', '>= 0', 'greater than or equal']), \
        f"Error message should mention negative values. Got: {error_msg2}"
    
    # Test case 3: Both negative
    labels = tf.constant([-1, -2, -3], dtype=tf.int32)
    predictions = tf.constant([-1, -2, -3], dtype=tf.int32)
    
    with pytest.raises((tf.errors.InvalidArgumentError, ValueError)) as exc_info3:
        confusion_matrix.confusion_matrix(
            labels=labels,
            predictions=predictions,
            num_classes=5
        )
    
    # Verify that an error was raised
    assert exc_info3.value is not None, "Expected exception for negative values in both inputs"
    error_msg3 = str(exc_info3.value).lower()
    # Check for negative/non-negative related keywords in error message
    assert any(keyword in error_msg3 for keyword in ['negative', 'non-negative', '>= 0', 'greater than or equal']), \
        f"Error message should mention negative values. Got: {error_msg3}"
    
    # Test case 4: Negative weights (weights can be negative, but let's test anyway)
    labels = tf.constant([0, 1, 2], dtype=tf.int32)
    predictions = tf.constant([0, 1, 2], dtype=tf.int32)
    weights = tf.constant([1.0, -2.0, 3.0], dtype=tf.float32)  # Negative weight
    
    # Negative weights should be allowed in confusion matrix
    # This test verifies that negative weights don't cause errors
    try:
        result = confusion_matrix.confusion_matrix(
            labels=labels,
            predictions=predictions,
            num_classes=3,
            weights=weights
        )
        # If no error, verify the matrix can have negative values
        # (since weights are negative, matrix values can be negative)
        assert result is not None, "Confusion matrix should be computed even with negative weights"
    except Exception as e:
        # If an error is raised, it shouldn't be about negative values
        error_msg4 = str(e).lower()
        assert 'negative' not in error_msg4, \
            f"Negative weights should be allowed. Got error: {error_msg4}"
# END:CASE_07


# START:CASE_08
def test_label_exceeds_num_classes_error():
    """
    CASE_08: 标签超出num_classes错误处理
    测试当标签或预测值大于等于num_classes时是否抛出适当异常
    """
    # Test case 1: Label exceeds num_classes
    labels = tf.constant([0, 1, 5], dtype=tf.int32)  # 5 >= 3
    predictions = tf.constant([0, 1, 2], dtype=tf.int32)
    
    with pytest.raises((tf.errors.InvalidArgumentError, ValueError)) as exc_info:
        confusion_matrix.confusion_matrix(
            labels=labels,
            predictions=predictions,
            num_classes=3  # Labels contain 5 which is >= 3
        )
    
    # Verify that an error was raised with appropriate message
    assert exc_info.value is not None, "Expected exception for label exceeding num_classes"
    error_msg = str(exc_info.value).lower()
    # Check for out of bound related keywords in error message
    # Actual error message is "`labels` out of bound"
    assert any(keyword in error_msg for keyword in ['out of bound', 'out of range', 'exceed', '>=', 'less than', 'num_classes']), \
        f"Error message should mention out of bound/range violation. Got: {error_msg}"
    
    # Test case 2: Prediction exceeds num_classes
    labels = tf.constant([0, 1, 2], dtype=tf.int32)
    predictions = tf.constant([0, 1, 4], dtype=tf.int32)  # 4 >= 3
    
    with pytest.raises((tf.errors.InvalidArgumentError, ValueError)) as exc_info2:
        confusion_matrix.confusion_matrix(
            labels=labels,
            predictions=predictions,
            num_classes=3
        )
    
    # Verify that an error was raised
    assert exc_info2.value is not None, "Expected exception for prediction exceeding num_classes"
    error_msg2 = str(exc_info2.value).lower()
    # Check for out of bound related keywords in error message
    assert any(keyword in error_msg2 for keyword in ['out of bound', 'out of range', 'exceed', '>=', 'less than', 'num_classes']), \
        f"Error message should mention out of bound/range violation. Got: {error_msg2}"
    
    # Test case 3: Both exceed num_classes
    labels = tf.constant([3, 4, 5], dtype=tf.int32)
    predictions = tf.constant([3, 4, 5], dtype=tf.int32)
    
    with pytest.raises((tf.errors.InvalidArgumentError, ValueError)) as exc_info3:
        confusion_matrix.confusion_matrix(
            labels=labels,
            predictions=predictions,
            num_classes=3
        )
    
    # Verify that an error was raised
    assert exc_info3.value is not None, "Expected exception for both inputs exceeding num_classes"
    error_msg3 = str(exc_info3.value).lower()
    # Check for out of bound related keywords in error message
    assert any(keyword in error_msg3 for keyword in ['out of bound', 'out of range', 'exceed', '>=', 'less than', 'num_classes']), \
        f"Error message should mention out of bound/range violation. Got: {error_msg3}"
    
    # Test case 4: Edge case - value equals num_classes (should fail since indices are 0-based)
    labels = tf.constant([0, 1, 3], dtype=tf.int32)  # 3 == 3
    predictions = tf.constant([0, 1, 2], dtype=tf.int32)
    
    with pytest.raises((tf.errors.InvalidArgumentError, ValueError)) as exc_info4:
        confusion_matrix.confusion_matrix(
            labels=labels,
            predictions=predictions,
            num_classes=3
        )
    
    # Verify that an error was raised
    assert exc_info4.value is not None, "Expected exception for value equal to num_classes"
    error_msg4 = str(exc_info4.value).lower()
    # Check for num_classes/range related keywords in error message
    assert any(keyword in error_msg4 for keyword in ['num_classes', 'range', 'less than', '>=', 'exceed', 'out of range']), \
        f"Error message should mention num_classes/range violation. Got: {error_msg4}"
    
    # Test case 5: Valid case for comparison (should not raise error)
    labels = tf.constant([0, 1, 2], dtype=tf.int32)  # All values < 3
    predictions = tf.constant([0, 1, 2], dtype=tf.int32)
    
    # This should not raise an error
    result = confusion_matrix.confusion_matrix(
        labels=labels,
        predictions=predictions,
        num_classes=3
    )
    
    # Verify that no error was raised and result is correct
    assert result is not None, "Confusion matrix should be computed for valid inputs"
    assert result.shape == (3, 3), f"Expected shape (3, 3), got {result.shape}"
    
    # Test case 6: When num_classes is None, no range checking should occur
    labels = tf.constant([0, 1, 5], dtype=tf.int32)
    predictions = tf.constant([0, 1, 5], dtype=tf.int32)
    
    # With num_classes=None, larger values should be allowed (auto-inference)
    result2 = confusion_matrix.confusion_matrix(
        labels=labels,
        predictions=predictions,
        num_classes=None  # Will be inferred as max(0,1,5,0,1,5) + 1 = 5 + 1 = 6
    )
    
    # Verify that no error was raised and num_classes was inferred correctly
    assert result2 is not None, "Confusion matrix should be computed with num_classes=None"
    assert result2.shape == (6, 6), f"Expected shape (6, 6) for auto-inferred num_classes, got {result2.shape}"
# END:CASE_08


# START:FOOTER
# FOOTER block - cleanup and additional utilities

# Additional error handling test cases (deferred for later rounds)

def test_weights_shape_mismatch_error():
    """Test that weights shape mismatch raises appropriate error."""
    # This will be implemented in later rounds
    pass





def test_weights_shape_mismatch_error():
    """Test that weights shape mismatch raises appropriate error."""
    # This will be implemented in later rounds
    pass


if __name__ == "__main__":
    # Simple test runner for debugging
    import sys
    pytest.main(sys.argv)
# END:FOOTER