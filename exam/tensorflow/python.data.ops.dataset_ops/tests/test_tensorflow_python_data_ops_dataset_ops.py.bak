"""
Test cases for tensorflow.python.data.ops.dataset_ops module.
Generated by ATTest-CLI.
"""
import math
import pytest
import numpy as np
import tensorflow as tf
from tensorflow.python.data.ops import dataset_ops

# Set random seed for reproducibility
tf.random.set_seed(42)
np.random.seed(42)

# ==== BLOCK:HEADER START ====
# Test class for dataset operations
class TestDatasetOps:
    """Test cases for tensorflow.python.data.ops.dataset_ops module."""
    
    def setup_method(self):
        """Setup method for each test."""
        tf.random.set_seed(42)
        np.random.seed(42)
# ==== BLOCK:HEADER END ====

# ==== BLOCK:CASE_01 START ====
    @pytest.mark.parametrize("data_type,data_shape,dtype,device,batch_size,shuffle", [
        ("tensor", [5, 3], "float32", "cpu", None, False),
        ("tensor", [100, 10], "float64", "cpu", None, False),  # param extension
    ])
    def test_from_tensor_slices(self, data_type, data_shape, dtype, device, batch_size, shuffle):
        """Test creating dataset from tensor slices (CASE_01)."""
        # Create test data based on parameters
        if data_type == "tensor":
            if dtype == "float32":
                data = tf.random.normal(data_shape, dtype=tf.float32)
            elif dtype == "float64":
                data = tf.random.normal(data_shape, dtype=tf.float64)
            else:
                raise ValueError(f"Unsupported dtype: {dtype}")
        else:
            raise ValueError(f"Unsupported data_type: {data_type}")
        
        # Create dataset using from_tensor_slices
        dataset = tf.data.Dataset.from_tensor_slices(data)
        
        # Weak assertions
        # 1. element_spec_exists
        assert hasattr(dataset, 'element_spec'), "Dataset should have element_spec attribute"
        assert dataset.element_spec is not None, "element_spec should not be None"
        
        # 2. cardinality_correct
        expected_cardinality = data_shape[0]  # First dimension is sliced
        # Note: cardinality might be tf.data.INFINITE_CARDINALITY for some datasets
        # but for from_tensor_slices it should be known
        if hasattr(dataset, 'cardinality'):
            cardinality = dataset.cardinality()
            if cardinality != tf.data.INFINITE_CARDINALITY:
                assert cardinality == expected_cardinality, \
                    f"Cardinality mismatch: expected {expected_cardinality}, got {cardinality}"
        
        # 3. iteration_complete
        collected_elements = []
        for element in dataset:
            collected_elements.append(element)
        
        assert len(collected_elements) == expected_cardinality, \
            f"Should iterate through {expected_cardinality} elements, got {len(collected_elements)}"
        
        # 4. shape_preserved
        # Check that each element has the correct shape (original shape without first dimension)
        expected_element_shape = data_shape[1:]  # Remove first dimension
        for i, element in enumerate(collected_elements):
            assert element.shape == tuple(expected_element_shape), \
                f"Element {i} shape mismatch: expected {expected_element_shape}, got {element.shape}"
        
        # Verify values match original data
        for i in range(expected_cardinality):
            expected_slice = data[i]
            actual_slice = collected_elements[i]
            # Use numpy for comparison
            np.testing.assert_array_almost_equal(
                expected_slice.numpy(),
                actual_slice.numpy(),
                decimal=5 if dtype == "float32" else 7,
                err_msg=f"Element {i} value mismatch"
            )
        
        # Strong assertions (enabled in final rounds)
        # 1. exact_values_match - already implemented above with precise comparison
        
        # 2. type_strict - ensure exact type matching
        element_spec = dataset.element_spec
        if dtype == "float32":
            assert element_spec.dtype == tf.float32, \
                f"Element spec dtype should be exactly float32, got {element_spec.dtype}"
        elif dtype == "float64":
            assert element_spec.dtype == tf.float64, \
                f"Element spec dtype should be exactly float64, got {element_spec.dtype}"
        
        # Check that all elements have the exact same dtype
        for i, element in enumerate(collected_elements):
            assert element.dtype == element_spec.dtype, \
                f"Element {i} dtype mismatch: expected {element_spec.dtype}, got {element.dtype}"
        
        # 3. memory_efficient - check that dataset doesn't unnecessarily copy data
        # For from_tensor_slices, the dataset should reference the original tensor
        # We can't directly check memory sharing, but we can check performance characteristics
        
        # Test that multiple iterations work without memory issues
        for iteration in range(3):
            iter_elements = []
            for element in dataset:
                iter_elements.append(element)
            
            assert len(iter_elements) == expected_cardinality, \
                f"Iteration {iteration}: should have {expected_cardinality} elements, got {len(iter_elements)}"
            
            # Verify values are still correct
            for i in range(expected_cardinality):
                expected_slice = data[i]
                actual_slice = iter_elements[i]
                np.testing.assert_array_almost_equal(
                    expected_slice.numpy(),
                    actual_slice.numpy(),
                    decimal=5 if dtype == "float32" else 7,
                    err_msg=f"Iteration {iteration}, element {i} value mismatch"
                )
        
        # Test with large dataset to ensure memory efficiency
        if data_shape[0] >= 100:  # For the param extension case
            # This should not cause memory issues
            large_dataset = tf.data.Dataset.from_tensor_slices(data)
            large_count = 0
            for _ in large_dataset:
                large_count += 1
            
            assert large_count == expected_cardinality, \
                f"Large dataset iteration failed: expected {expected_cardinality}, got {large_count}"
        
        # Additional type strictness: check shape specification
        assert element_spec.shape == tuple(expected_element_shape), \
            f"Element spec shape mismatch: expected {expected_element_shape}, got {element_spec.shape}"
        
        # Check that element spec is a TensorSpec
        assert isinstance(element_spec, tf.TensorSpec), \
            f"Element spec should be TensorSpec, got {type(element_spec)}"
        
        # Verify no unexpected attributes or methods are missing
        required_attrs = ['element_spec', 'cardinality', '__iter__', '__len__']
        for attr in required_attrs:
            assert hasattr(dataset, attr), f"Dataset should have attribute {attr}"
# ==== BLOCK:CASE_01 END ====

# ==== BLOCK:CASE_02 START ====
    @pytest.mark.parametrize("data_type,data_shape,dtype,device,batch_size,shuffle", [
        ("list", [10], "int32", "cpu", None, False),
        ("list", [1], "int64", "cpu", None, False),  # param extension
    ])
    def test_from_python_list(self, data_type, data_shape, dtype, device, batch_size, shuffle):
        """Test creating dataset from Python list (CASE_02)."""
        # Create test data based on parameters
        if data_type == "list":
            if dtype == "int32":
                data = list(range(data_shape[0]))
            elif dtype == "int64":
                data = list(range(data_shape[0]))
            else:
                raise ValueError(f"Unsupported dtype: {dtype}")
        else:
            raise ValueError(f"Unsupported data_type: {data_type}")
        
        # Create dataset using from_tensor_slices
        dataset = tf.data.Dataset.from_tensor_slices(data)
        
        # Weak assertions
        # 1. element_spec_exists
        assert hasattr(dataset, 'element_spec'), "Dataset should have element_spec attribute"
        assert dataset.element_spec is not None, "element_spec should not be None"
        
        # 2. cardinality_correct
        expected_cardinality = data_shape[0]  # Length of the list
        if hasattr(dataset, 'cardinality'):
            cardinality = dataset.cardinality()
            if cardinality != tf.data.INFINITE_CARDINALITY:
                assert cardinality == expected_cardinality, \
                    f"Cardinality mismatch: expected {expected_cardinality}, got {cardinality}"
        
        # 3. iteration_complete
        collected_elements = []
        for element in dataset:
            collected_elements.append(element)
        
        assert len(collected_elements) == expected_cardinality, \
            f"Should iterate through {expected_cardinality} elements, got {len(collected_elements)}"
        
        # 4. values_preserved
        # Verify values match original data
        for i in range(expected_cardinality):
            expected_value = data[i]
            actual_value = collected_elements[i]
            
            # Convert tensor to scalar for comparison
            if hasattr(actual_value, 'numpy'):
                actual_scalar = actual_value.numpy()
            else:
                actual_scalar = actual_value
            
            assert actual_scalar == expected_value, \
                f"Element {i} value mismatch: expected {expected_value}, got {actual_scalar}"
        
        # Additional check: element spec should be scalar
        element_spec = dataset.element_spec
        assert element_spec.shape == tf.TensorShape([]), \
            f"Element spec shape should be scalar, got {element_spec.shape}"
        
        # Check dtype conversion
        if dtype == "int32":
            assert element_spec.dtype == tf.int32, \
                f"Element spec dtype should be int32, got {element_spec.dtype}"
        elif dtype == "int64":
            # Note: Python ints are converted to tf.int64 by default in eager mode
            assert element_spec.dtype in [tf.int64, tf.int32], \
                f"Element spec dtype should be int64 or int32, got {element_spec.dtype}"
        
        # Strong assertions (enabled in final rounds)
        # 1. type_conversion_correct - ensure proper type conversion from Python to TensorFlow
        element_spec = dataset.element_spec
        
        # Verify that all elements have consistent dtype
        for i, element in enumerate(collected_elements):
            if hasattr(element, 'dtype'):
                assert element.dtype == element_spec.dtype, \
                    f"Element {i} dtype mismatch: expected {element_spec.dtype}, got {element.dtype}"
        
        # Test type conversion with different Python types
        if dtype == "int32":
            # Test with negative numbers
            negative_data = [-i for i in range(data_shape[0])]
            negative_dataset = tf.data.Dataset.from_tensor_slices(negative_data)
            negative_elements = list(negative_dataset.as_numpy_iterator())
            
            for i, (actual, expected) in enumerate(zip(negative_elements, negative_data)):
                assert actual == expected, \
                    f"Negative element {i} mismatch: expected {expected}, got {actual}"
                assert negative_dataset.element_spec.dtype == tf.int32, \
                    f"Negative dataset dtype should be int32, got {negative_dataset.element_spec.dtype}"
        
        # 2. performance_acceptable - test iteration performance
        import time
        
        # Time multiple iterations to ensure performance is acceptable
        start_time = time.time()
        for iteration in range(10):  # 10 iterations
            count = 0
            for _ in dataset:
                count += 1
            assert count == expected_cardinality, \
                f"Iteration {iteration}: expected {expected_cardinality} elements, got {count}"
        
        end_time = time.time()
        elapsed = end_time - start_time
        
        # Performance check: 10 iterations should complete quickly
        # This is a soft check - just ensure it doesn't take too long
        max_acceptable_time = 1.0  # 1 second for 10 iterations
        assert elapsed < max_acceptable_time, \
            f"Performance issue: 10 iterations took {elapsed:.2f}s, expected < {max_acceptable_time}s"
        
        # 3. memory_usage - test memory characteristics
        # Create multiple datasets to ensure memory doesn't leak
        datasets = []
        for i in range(5):
            new_dataset = tf.data.Dataset.from_tensor_slices(data)
            datasets.append(new_dataset)
            
            # Iterate through each dataset
            count = 0
            for _ in new_dataset:
                count += 1
            assert count == expected_cardinality, \
                f"Dataset {i}: expected {expected_cardinality} elements, got {count}"
        
        # Test with large list (for param extension case)
        if data_shape[0] >= 10:  # For the regular case
            # Create larger dataset
            large_data = list(range(1000))
            large_dataset = tf.data.Dataset.from_tensor_slices(large_data)
            
            # Iterate once to ensure no memory issues
            large_count = 0
            for _ in large_dataset:
                large_count += 1
            
            assert large_count == 1000, \
                f"Large dataset iteration failed: expected 1000, got {large_count}"
        
        # Test memory efficiency by checking that repeated iterations work
        # without increasing memory usage significantly
        memory_test_dataset = tf.data.Dataset.from_tensor_slices(data)
        
        # Perform many iterations
        for i in range(20):
            iter_count = 0
            for _ in memory_test_dataset:
                iter_count += 1
            
            assert iter_count == expected_cardinality, \
                f"Memory test iteration {i}: expected {expected_cardinality}, got {iter_count}"
        
        # Additional strong assertions
        
        # Test that dataset can be used in tf.data pipeline
        pipeline = dataset.map(lambda x: x * 2).batch(2).prefetch(1)
        pipeline_elements = list(pipeline.as_numpy_iterator())
        
        # Calculate expected batches
        expected_batches = math.ceil(expected_cardinality / 2)
        assert len(pipeline_elements) == expected_batches, \
            f"Pipeline batch count mismatch: expected {expected_batches}, got {len(pipeline_elements)}"
        
        # Verify pipeline values
        flattened = []
        for batch in pipeline_elements:
            for elem in batch:
                flattened.append(elem)
        
        # Check that all original values multiplied by 2 are present
        for i in range(expected_cardinality):
            expected = data[i] * 2
            assert flattened[i] == expected, \
                f"Pipeline element {i} mismatch: expected {expected}, got {flattened[i]}"
        
        # Test error handling for invalid inputs
        # Empty list should work
        empty_dataset = tf.data.Dataset.from_tensor_slices([])
        empty_elements = list(empty_dataset.as_numpy_iterator())
        assert len(empty_elements) == 0, "Empty list should produce empty dataset"
        
        # Nested lists should raise error
        nested_data = [[1, 2], [3, 4]]
        # This might work or raise error depending on TensorFlow version
        # We'll test it but not assert specific behavior
        
        # Test with mixed types (should convert to consistent type)
        mixed_data = [1, 2.0, 3]
        mixed_dataset = tf.data.Dataset.from_tensor_slices(mixed_data)
        mixed_elements = list(mixed_dataset.as_numpy_iterator())
        
        # All elements should have same dtype after conversion
        if len(mixed_elements) > 0:
            first_dtype = mixed_elements[0].dtype if hasattr(mixed_elements[0], 'dtype') else type(mixed_elements[0])
            for i, elem in enumerate(mixed_elements):
                elem_dtype = elem.dtype if hasattr(elem, 'dtype') else type(elem)
                # dtype should be consistent (likely float32 or float64)
                # We'll just verify iteration works without error
# ==== BLOCK:CASE_02 END ====

# ==== BLOCK:CASE_03 START ====
    @pytest.mark.parametrize("data_type,data_shape,dtype,device,batch_size,shuffle,map_func", [
        ("tensor", [8, 2], "float32", "cpu", None, False, "lambda x: x * 2"),
        ("tensor", [5, 2, 2], "float32", "cpu", None, False, "lambda x: tf.reduce_sum(x)"),  # param extension
    ])
    def test_map_operation(self, data_type, data_shape, dtype, device, batch_size, shuffle, map_func):
        """Test map operation on dataset (CASE_03)."""
        # Create test data
        if data_type == "tensor":
            if dtype == "float32":
                data = tf.random.normal(data_shape, dtype=tf.float32)
            else:
                raise ValueError(f"Unsupported dtype: {dtype}")
        else:
            raise ValueError(f"Unsupported data_type: {data_type}")
        
        # Create base dataset
        dataset = tf.data.Dataset.from_tensor_slices(data)
        
        # Parse map function
        if map_func == "lambda x: x * 2":
            def map_fn(x):
                return x * 2
        elif map_func == "lambda x: tf.reduce_sum(x)":
            def map_fn(x):
                return tf.reduce_sum(x)
        else:
            raise ValueError(f"Unsupported map_func: {map_func}")
        
        # Apply map operation
        mapped_dataset = dataset.map(map_fn)
        
        # Weak assertions
        # 1. element_spec_preserved (structure preserved, dtype might change)
        assert hasattr(mapped_dataset, 'element_spec'), "Mapped dataset should have element_spec"
        assert mapped_dataset.element_spec is not None, "element_spec should not be None"
        
        # 2. cardinality_unchanged
        expected_cardinality = data_shape[0]
        
        # Collect elements from both datasets
        original_elements = list(dataset.as_numpy_iterator())
        mapped_elements = list(mapped_dataset.as_numpy_iterator())
        
        assert len(mapped_elements) == len(original_elements), \
            f"Cardinality changed: original {len(original_elements)}, mapped {len(mapped_elements)}"
        
        assert len(mapped_elements) == expected_cardinality, \
            f"Expected {expected_cardinality} elements, got {len(mapped_elements)}"
        
        # 3. map_applied
        # Verify map function was applied correctly
        for i, (orig, mapped) in enumerate(zip(original_elements, mapped_elements)):
            if map_func == "lambda x: x * 2":
                expected = orig * 2
                np.testing.assert_array_almost_equal(
                    mapped,
                    expected,
                    decimal=5,
                    err_msg=f"Map function not applied correctly to element {i}"
                )
            elif map_func == "lambda x: tf.reduce_sum(x)":
                expected = np.sum(orig)
                np.testing.assert_array_almost_equal(
                    mapped,
                    expected,
                    decimal=5,
                    err_msg=f"Reduce sum not applied correctly to element {i}"
                )
        
        # 4. shape_unchanged (for multiplication) or changed (for reduction)
        if map_func == "lambda x: x * 2":
            # Shape should be unchanged
            expected_element_shape = data_shape[1:]
            for i, element in enumerate(mapped_elements):
                assert element.shape == tuple(expected_element_shape), \
                    f"Element {i} shape mismatch: expected {expected_element_shape}, got {element.shape}"
        elif map_func == "lambda x: tf.reduce_sum(x)":
            # Shape should be scalar after reduction
            for i, element in enumerate(mapped_elements):
                assert element.shape == (), \
                    f"Element {i} should be scalar after reduction, got shape {element.shape}"
        
        # Check that iteration order is preserved
        for i in range(len(original_elements)):
            # Verify mapping is consistent
            if map_func == "lambda x: x * 2":
                assert np.allclose(mapped_elements[i], original_elements[i] * 2), \
                    f"Mapping not consistent for element {i}"
            elif map_func == "lambda x: tf.reduce_sum(x)":
                assert np.allclose(mapped_elements[i], np.sum(original_elements[i])), \
                    f"Reduction not consistent for element {i}"
        
        # Strong assertions (enabled in final rounds)
        # 1. function_composition - test chaining multiple map operations
        if map_func == "lambda x: x * 2":
            # Chain multiple map operations
            def add_one(x):
                return x + 1.0
            
            def square(x):
                return x * x
            
            # Compose: multiply by 2, then add 1, then square
            composed_dataset = dataset.map(map_fn).map(add_one).map(square)
            composed_elements = list(composed_dataset.as_numpy_iterator())
            
            assert len(composed_elements) == expected_cardinality, \
                f"Composed dataset cardinality mismatch: expected {expected_cardinality}, got {len(composed_elements)}"
            
            # Verify composition correctness
            for i, orig in enumerate(original_elements):
                expected = ((orig * 2) + 1.0) ** 2
                actual = composed_elements[i]
                np.testing.assert_array_almost_equal(
                    actual,
                    expected,
                    decimal=5,
                    err_msg=f"Function composition incorrect for element {i}"
                )
        
        # 2. parallel_execution - test map with num_parallel_calls parameter
        if map_func == "lambda x: x * 2":
            # Test parallel map execution
            for num_parallel_calls in [1, 2, tf.data.AUTOTUNE]:
                parallel_dataset = dataset.map(map_fn, num_parallel_calls=num_parallel_calls)
                parallel_elements = list(parallel_dataset.as_numpy_iterator())
                
                assert len(parallel_elements) == expected_cardinality, \
                    f"Parallel map (num_parallel_calls={num_parallel_calls}) cardinality mismatch: " \
                    f"expected {expected_cardinality}, got {len(parallel_elements)}"
                
                # Verify correctness
                for i, (orig, parallel) in enumerate(zip(original_elements, parallel_elements)):
                    expected = orig * 2
                    np.testing.assert_array_almost_equal(
                        parallel,
                        expected,
                        decimal=5,
                        err_msg=f"Parallel map (num_parallel_calls={num_parallel_calls}) incorrect for element {i}"
                    )
        
        # 3. error_handling - test map with error conditions
        # Test with function that raises an error
        def error_func(x):
            if x[0] > 0.5:  # Arbitrary condition
                raise ValueError("Test error")
            return x
        
        # This should work without error until we try to iterate
        error_dataset = dataset.map(error_func)
        
        # Iteration might raise error or skip problematic elements depending on TensorFlow version
        # We'll test that we can create the dataset without immediate error
        assert hasattr(error_dataset, 'element_spec'), "Error dataset should have element_spec"
        
        # Test with invalid function (not callable)
        with pytest.raises(TypeError, match="must be callable"):
            invalid_dataset = dataset.map("not a function")
            # Try to iterate to trigger error
            list(invalid_dataset.as_numpy_iterator())
        
        # Test map with None function (should raise error)
        with pytest.raises(TypeError, match="must be callable"):
            none_dataset = dataset.map(None)
            list(none_dataset.as_numpy_iterator())
        
        # Test map preserves element spec dtype
        mapped_spec = mapped_dataset.element_spec
        if map_func == "lambda x: x * 2":
            # Multiplication preserves dtype
            assert mapped_spec.dtype == tf.float32, \
                f"Map should preserve float32 dtype, got {mapped_spec.dtype}"
        elif map_func == "lambda x: tf.reduce_sum(x)":
            # Reduction produces scalar, dtype should still be float32
            assert mapped_spec.dtype == tf.float32, \
                f"Reduce sum should produce float32 scalar, got {mapped_spec.dtype}"
        
        # Additional strong assertions
        
        # Test map with deterministic=False (non-deterministic execution)
        if map_func == "lambda x: x * 2":
            nondet_dataset = dataset.map(map_fn, deterministic=False)
            nondet_elements = list(nondet_dataset.as_numpy_iterator())
            
            assert len(nondet_elements) == expected_cardinality, \
                f"Non-deterministic map cardinality mismatch: expected {expected_cardinality}, got {len(nondet_elements)}"
            
            # Should still produce correct results
            for i, (orig, nondet) in enumerate(zip(original_elements, nondet_elements)):
                expected = orig * 2
                np.testing.assert_array_almost_equal(
                    nondet,
                    expected,
                    decimal=5,
                    err_msg=f"Non-deterministic map incorrect for element {i}"
                )
        
        # Test map in pipeline with other operations
        pipeline = dataset.map(map_fn).batch(2).prefetch(1)
        pipeline_elements = list(pipeline.as_numpy_iterator())
        
        # Calculate expected batches
        expected_batches = math.ceil(expected_cardinality / 2)
        assert len(pipeline_elements) == expected_batches, \
            f"Map pipeline batch count mismatch: expected {expected_batches}, got {len(pipeline_elements)}"
        
        # Verify pipeline values
        if map_func == "lambda x: x * 2":
            # Flatten batches and verify
            flattened = []
            for batch in pipeline_elements:
                for elem in batch:
                    flattened.append(elem)
            
            for i, orig in enumerate(original_elements):
                expected = orig * 2
                np.testing.assert_array_almost_equal(
                    flattened[i],
                    expected,
                    decimal=5,
                    err_msg=f"Map pipeline element {i} mismatch"
                )
        
        # Test map with stateful function (should work but be careful)
        counter = [0]  # Use list to make it mutable
        
        def stateful_func(x):
            counter[0] += 1
            return x * counter[0]
        
        # Note: Stateful functions in map are not recommended but should work
        stateful_dataset = dataset.map(stateful_func)
        stateful_elements = list(stateful_dataset.as_numpy_iterator())
        
        # Counter should have been incremented for each element
        assert counter[0] == expected_cardinality, \
            f"Stateful function should be called {expected_cardinality} times, got {counter[0]}"
        
        # Test map with function that returns different dtype
        if map_func == "lambda x: x * 2":
            def cast_func(x):
                return tf.cast(x, tf.float64)
            
            cast_dataset = dataset.map(cast_func)
            cast_elements = list(cast_dataset.as_numpy_iterator())
            
            assert len(cast_elements) == expected_cardinality, \
                f"Cast map cardinality mismatch: expected {expected_cardinality}, got {len(cast_elements)}"
            
            # Check dtype changed
            cast_spec = cast_dataset.element_spec
            assert cast_spec.dtype == tf.float64, \
                f"Cast map should produce float64, got {cast_spec.dtype}"
            
            # Verify values
            for i, (orig, cast) in enumerate(zip(original_elements, cast_elements)):
                expected = orig.astype(np.float64)
                np.testing.assert_array_almost_equal(
                    cast,
                    expected,
                    decimal=5,
                    err_msg=f"Cast map element {i} mismatch"
                )
# ==== BLOCK:CASE_03 END ====

# ==== BLOCK:CASE_04 START ====
    @pytest.mark.parametrize("data_type,data_shape,dtype,device,batch_size,shuffle,map_func", [
        ("tensor", [10, 3], "float32", "cpu", 3, False, None),
        ("tensor", [7, 4], "float32", "cpu", 2, False, None),  # param extension
    ])
    def test_batch_operation(self, data_type, data_shape, dtype, device, batch_size, shuffle, map_func):
        """Test batch operation on dataset (CASE_04)."""
        # Create test data
        if data_type == "tensor":
            if dtype == "float32":
                data = tf.random.normal(data_shape, dtype=tf.float32)
            else:
                raise ValueError(f"Unsupported dtype: {dtype}")
        else:
            raise ValueError(f"Unsupported data_type: {data_type}")
        
        # Create base dataset
        dataset = tf.data.Dataset.from_tensor_slices(data)
        
        # Apply batch operation
        batched_dataset = dataset.batch(batch_size)
        
        # Weak assertions
        # 1. batch_shape_correct
        expected_num_batches = math.ceil(data_shape[0] / batch_size)
        expected_last_batch_size = data_shape[0] % batch_size if data_shape[0] % batch_size != 0 else batch_size
        
        batched_elements = list(batched_dataset.as_numpy_iterator())
        
        assert len(batched_elements) == expected_num_batches, \
            f"Expected {expected_num_batches} batches, got {len(batched_elements)}"
        
        # Check batch shapes
        for i, batch in enumerate(batched_elements):
            if i < len(batched_elements) - 1:
                # Full batches
                assert batch.shape[0] == batch_size, \
                    f"Batch {i} should have size {batch_size}, got {batch.shape[0]}"
            else:
                # Last batch (might be smaller)
                assert batch.shape[0] == expected_last_batch_size, \
                    f"Last batch should have size {expected_last_batch_size}, got {batch.shape[0]}"
            
            # Check element shape within batch
            expected_element_shape = (batch.shape[0],) + tuple(data_shape[1:])
            assert batch.shape == expected_element_shape, \
                f"Batch {i} shape mismatch: expected {expected_element_shape}, got {batch.shape}"
        
        # 2. total_elements_preserved
        total_elements_in_batches = sum(batch.shape[0] for batch in batched_elements)
        assert total_elements_in_batches == data_shape[0], \
            f"Total elements mismatch: expected {data_shape[0]}, got {total_elements_in_batches}"
        
        # 3. last_batch_handled
        last_batch = batched_elements[-1]
        if data_shape[0] % batch_size == 0:
            # Last batch should be full
            assert last_batch.shape[0] == batch_size, \
                f"Last batch should be full when divisible: expected {batch_size}, got {last_batch.shape[0]}"
        else:
            # Last batch should be smaller
            assert last_batch.shape[0] == data_shape[0] % batch_size, \
                f"Last batch size incorrect: expected {data_shape[0] % batch_size}, got {last_batch.shape[0]}"
        
        # 4. element_spec_updated
        assert hasattr(batched_dataset, 'element_spec'), "Batched dataset should have element_spec"
        batched_spec = batched_dataset.element_spec
        
        # Element spec should reflect batched shape
        assert batched_spec.shape[0] is None, "First dimension should be unknown (variable batch size)"
        assert batched_spec.shape[1:] == tuple(data_shape[1:]), \
            f"Element spec shape mismatch: expected {tuple(data_shape[1:])}, got {batched_spec.shape[1:]}"
        
        # Verify values are preserved
        # Flatten batches and compare with original data
        reconstructed = []
        for batch in batched_elements:
            for j in range(batch.shape[0]):
                reconstructed.append(batch[j])
        
        original_elements = list(dataset.as_numpy_iterator())
        
        assert len(reconstructed) == len(original_elements), \
            f"Reconstructed length mismatch: {len(reconstructed)} vs {len(original_elements)}"
        
        for i in range(len(original_elements)):
            np.testing.assert_array_almost_equal(
                reconstructed[i],
                original_elements[i],
                decimal=5,
                err_msg=f"Element {i} value changed after batching"
            )
        
        # Test that iteration order is preserved
        flat_original = np.concatenate([elem[np.newaxis, ...] for elem in original_elements], axis=0)
        flat_batched = np.concatenate(batched_elements, axis=0)
        
        np.testing.assert_array_almost_equal(
            flat_original,
            flat_batched,
            decimal=5,
            err_msg="Batching changed the order or values of elements"
        )
# ==== BLOCK:CASE_04 END ====

# ==== BLOCK:CASE_05 START ====
    @pytest.mark.parametrize("data_type,data_shape,dtype,device,batch_size,shuffle", [
        ("empty", [0, 3], "float32", "cpu", None, False),
    ])
    def test_empty_dataset(self, data_type, data_shape, dtype, device, batch_size, shuffle):
        """Test empty dataset and edge cases (CASE_05)."""
        # Create empty test data based on parameters
        if data_type == "empty":
            if dtype == "float32":
                # Create empty tensor with specified shape
                data = tf.zeros(data_shape, dtype=tf.float32)
            else:
                raise ValueError(f"Unsupported dtype: {dtype}")
        else:
            raise ValueError(f"Unsupported data_type: {data_type}")
        
        # Create dataset using from_tensor_slices
        dataset = tf.data.Dataset.from_tensor_slices(data)
        
        # Weak assertions
        # 1. empty_iteration
        collected_elements = []
        for element in dataset:
            collected_elements.append(element)
        
        assert len(collected_elements) == 0, \
            f"Empty dataset should have 0 elements, got {len(collected_elements)}"
        
        # Verify iteration completes without errors
        # The loop above should complete normally
        
        # 2. element_spec_valid
        assert hasattr(dataset, 'element_spec'), "Dataset should have element_spec attribute"
        assert dataset.element_spec is not None, "element_spec should not be None"
        
        # Check element spec shape matches expected (original shape without first dimension)
        expected_element_shape = data_shape[1:]  # Remove first dimension
        element_spec = dataset.element_spec
        
        # For empty dataset, element spec should still have correct shape
        assert element_spec.shape == tuple(expected_element_shape), \
            f"Element spec shape mismatch: expected {expected_element_shape}, got {element_spec.shape}"
        
        # Check dtype
        if dtype == "float32":
            assert element_spec.dtype == tf.float32, \
                f"Element spec dtype should be float32, got {element_spec.dtype}"
        
        # 3. no_crash
        # Test various operations on empty dataset that should not crash
        
        # Test map operation on empty dataset
        def identity_fn(x):
            return x
        
        mapped_dataset = dataset.map(identity_fn)
        mapped_elements = list(mapped_dataset.as_numpy_iterator())
        assert len(mapped_elements) == 0, "Map on empty dataset should produce empty dataset"
        
        # Test batch operation on empty dataset
        if batch_size is not None:
            batched_dataset = dataset.batch(batch_size)
            batched_elements = list(batched_dataset.as_numpy_iterator())
            assert len(batched_elements) == 0, "Batch on empty dataset should produce empty dataset"
        
        # Test cardinality
        if hasattr(dataset, 'cardinality'):
            cardinality = dataset.cardinality()
            # Empty dataset should have cardinality 0
            assert cardinality == 0, f"Empty dataset cardinality should be 0, got {cardinality}"
        
        # Test that dataset can be converted to list
        dataset_list = list(dataset.as_numpy_iterator())
        assert len(dataset_list) == 0, "Empty dataset should convert to empty list"
        
        # Test that take operation works
        taken_dataset = dataset.take(5)
        taken_elements = list(taken_dataset.as_numpy_iterator())
        assert len(taken_elements) == 0, "Take from empty dataset should be empty"
        
        # Test that skip operation works
        skipped_dataset = dataset.skip(5)
        skipped_elements = list(skipped_dataset.as_numpy_iterator())
        assert len(skipped_elements) == 0, "Skip from empty dataset should be empty"
        
        # Test that filter operation works
        def always_true(x):
            return True
        
        filtered_dataset = dataset.filter(always_true)
        filtered_elements = list(filtered_dataset.as_numpy_iterator())
        assert len(filtered_elements) == 0, "Filter on empty dataset should be empty"
        
        # Test that shuffle operation works
        if shuffle:
            shuffled_dataset = dataset.shuffle(buffer_size=10)
            shuffled_elements = list(shuffled_dataset.as_numpy_iterator())
            assert len(shuffled_elements) == 0, "Shuffle on empty dataset should be empty"
        
        # Test that repeat operation works
        repeated_dataset = dataset.repeat(3)
        repeated_elements = list(repeated_dataset.as_numpy_iterator())
        assert len(repeated_elements) == 0, "Repeat on empty dataset should be empty"
        
        # Test that prefetch operation works
        prefetched_dataset = dataset.prefetch(buffer_size=1)
        prefetched_elements = list(prefetched_dataset.as_numpy_iterator())
        assert len(prefetched_elements) == 0, "Prefetch on empty dataset should be empty"
        
        # Additional edge case: single element empty shape
        # Test with shape [1, 0] - one element that is empty
        if data_shape == [0, 3]:
            # This is already tested above
            pass
        
        # Verify that empty dataset can be used in tf.data pipeline
        # without causing shape inference errors
        pipeline = dataset.map(lambda x: x).batch(1).prefetch(1)
        pipeline_elements = list(pipeline.as_numpy_iterator())
        assert len(pipeline_elements) == 0, "Pipeline with empty dataset should be empty"
# ==== BLOCK:CASE_05 END ====

# ==== BLOCK:CASE_06 START ====
    @pytest.mark.parametrize("data_type,data_shape,dtype,device,batch_size,shuffle,seed", [
        ("tensor", [10, 2], "float32", "cpu", None, True, 42),
        ("list", [20], "int32", "cpu", None, True, 123),
    ])
    def test_shuffle_operation(self, data_type, data_shape, dtype, device, batch_size, shuffle, seed):
        """Test shuffle operation on dataset (CASE_06)."""
        # Set random seed for reproducibility
        tf.random.set_seed(seed)
        np.random.seed(seed)
        
        # Create test data based on parameters
        if data_type == "tensor":
            if dtype == "float32":
                # Create 2D tensor with sequential values
                total_elements = data_shape[0] * data_shape[1]
                flat_data = list(range(total_elements))
                data = tf.constant(flat_data, dtype=tf.float32)
                # Reshape to specified shape
                data = tf.reshape(data, data_shape)
            else:
                raise ValueError(f"Unsupported dtype: {dtype}")
        elif data_type == "list":
            if dtype == "int32":
                data = list(range(data_shape[0]))
            else:
                raise ValueError(f"Unsupported dtype: {dtype}")
        else:
            raise ValueError(f"Unsupported data_type: {data_type}")
        
        # Create base dataset
        if data_type == "tensor":
            dataset = tf.data.Dataset.from_tensor_slices(data)
        else:  # list
            dataset = tf.data.Dataset.from_tensor_slices(data)
        
        # Apply shuffle operation
        buffer_size = min(5, data_shape[0])  # Use smaller buffer for testing
        shuffled_dataset = dataset.shuffle(buffer_size=buffer_size, seed=seed)
        
        # Weak assertions for shuffle operation
        # 1. element_spec_preserved
        assert hasattr(shuffled_dataset, 'element_spec'), "Shuffled dataset should have element_spec"
        original_spec = dataset.element_spec
        shuffled_spec = shuffled_dataset.element_spec
        
        assert shuffled_spec == original_spec, \
            f"Element spec changed after shuffle: original {original_spec}, shuffled {shuffled_spec}"
        
        # 2. cardinality_unchanged
        expected_cardinality = data_shape[0]
        
        # Collect elements from both datasets
        original_elements = list(dataset.as_numpy_iterator())
        shuffled_elements = list(shuffled_dataset.as_numpy_iterator())
        
        assert len(shuffled_elements) == len(original_elements), \
            f"Cardinality changed: original {len(original_elements)}, shuffled {len(shuffled_elements)}"
        
        assert len(shuffled_elements) == expected_cardinality, \
            f"Expected {expected_cardinality} elements, got {len(shuffled_elements)}"
        
        # 3. all_elements_present (no data loss)
        # Convert to sets for comparison (need to handle tensors properly)
        if data_type == "list":
            original_set = set(original_elements)
            shuffled_set = set(shuffled_elements)
            assert original_set == shuffled_set, "Shuffle lost some elements"
        else:
            # For tensors, compare as numpy arrays
            original_values = [elem for elem in original_elements]
            shuffled_values = [elem for elem in shuffled_elements]
            
            # Sort both lists for comparison
            original_sorted = sorted(original_values, key=lambda x: tuple(x.flatten()))
            shuffled_sorted = sorted(shuffled_values, key=lambda x: tuple(x.flatten()))
            
            for orig, shuffled in zip(original_sorted, shuffled_sorted):
                np.testing.assert_array_equal(orig, shuffled, 
                    err_msg="Shuffle changed element values")
        
        # 4. order_changed (shuffle actually shuffled)
        # This is probabilistic, but with fixed seed should be deterministic
        if len(original_elements) > 1:  # Can't shuffle single element
            # Check if order changed
            order_changed = False
            for orig, shuffled in zip(original_elements, shuffled_elements):
                if data_type == "list":
                    if orig != shuffled:
                        order_changed = True
                        break
                else:
                    if not np.array_equal(orig, shuffled):
                        order_changed = True
                        break
            
            # With buffer_size < dataset size, shuffle should change order
            if buffer_size < len(original_elements):
                assert order_changed, "Shuffle should change order with buffer_size < dataset size"
            
            # Test reproducibility with same seed
            # Create another shuffled dataset with same seed
            tf.random.set_seed(seed)
            np.random.seed(seed)
            shuffled_dataset2 = dataset.shuffle(buffer_size=buffer_size, seed=seed)
            shuffled_elements2 = list(shuffled_dataset2.as_numpy_iterator())
            
            # Should get same shuffled order
            for elem1, elem2 in zip(shuffled_elements, shuffled_elements2):
                if data_type == "list":
                    assert elem1 == elem2, "Shuffle not reproducible with same seed"
                else:
                    np.testing.assert_array_equal(elem1, elem2,
                        err_msg="Shuffle not reproducible with same seed")
        
        # Test with different buffer sizes
        if data_shape[0] > 1:
            # Test with buffer_size = 1 (minimal shuffle)
            small_buffer_dataset = dataset.shuffle(buffer_size=1, seed=seed)
            small_buffer_elements = list(small_buffer_dataset.as_numpy_iterator())
            
            # With buffer_size=1, order should be same as original
            # (though implementation might still shuffle within buffer)
            # We'll just verify no data loss
            if data_type == "list":
                assert set(small_buffer_elements) == set(original_elements), \
                    "Shuffle with buffer_size=1 lost elements"
            else:
                small_buffer_values = [elem for elem in small_buffer_elements]
                original_values = [elem for elem in original_elements]
                small_buffer_sorted = sorted(small_buffer_values, key=lambda x: tuple(x.flatten()))
                original_sorted = sorted(original_values, key=lambda x: tuple(x.flatten()))
                
                for buf, orig in zip(small_buffer_sorted, original_sorted):
                    np.testing.assert_array_equal(buf, orig,
                        err_msg="Shuffle with buffer_size=1 changed element values")
            
            # Test with buffer_size >= dataset size (perfect shuffle)
            large_buffer_dataset = dataset.shuffle(buffer_size=data_shape[0] + 10, seed=seed)
            large_buffer_elements = list(large_buffer_dataset.as_numpy_iterator())
            
            # Verify no data loss
            if data_type == "list":
                assert set(large_buffer_elements) == set(original_elements), \
                    "Shuffle with large buffer lost elements"
            else:
                large_buffer_values = [elem for elem in large_buffer_elements]
                large_buffer_sorted = sorted(large_buffer_values, key=lambda x: tuple(x.flatten()))
                
                for buf, orig in zip(large_buffer_sorted, original_sorted):
                    np.testing.assert_array_equal(buf, orig,
                        err_msg="Shuffle with large buffer changed element values")
        
        # Test shuffle with reshuffle_each_iteration=False
        # Create dataset that shouldn't reshuffle each iteration
        no_reshuffle_dataset = dataset.shuffle(
            buffer_size=buffer_size, 
            seed=seed,
            reshuffle_each_iteration=False
        )
        
        # First iteration
        iter1_elements = list(no_reshuffle_dataset.as_numpy_iterator())
        
        # Second iteration should produce same order
        iter2_elements = list(no_reshuffle_dataset.as_numpy_iterator())
        
        assert len(iter1_elements) == len(iter2_elements), \
            "Iteration length changed"
        
        for elem1, elem2 in zip(iter1_elements, iter2_elements):
            if data_type == "list":
                assert elem1 == elem2, "Dataset reshuffled when reshuffle_each_iteration=False"
            else:
                np.testing.assert_array_equal(elem1, elem2,
                    err_msg="Dataset reshuffled when reshuffle_each_iteration=False")
        
        # Test shuffle in pipeline
        pipeline = dataset.shuffle(buffer_size=buffer_size).batch(2).prefetch(1)
        pipeline_elements = list(pipeline.as_numpy_iterator())
        
        # Verify pipeline produces correct number of batches
        expected_batches = math.ceil(data_shape[0] / 2)
        assert len(pipeline_elements) == expected_batches, \
            f"Pipeline batch count mismatch: expected {expected_batches}, got {len(pipeline_elements)}"
        
        # Verify all elements present in pipeline (flatten batches)
        total_elements = sum(batch.shape[0] for batch in pipeline_elements)
        assert total_elements == data_shape[0], \
            f"Pipeline lost elements: expected {data_shape[0]}, got {total_elements}"
# ==== BLOCK:CASE_06 END ====

# ==== BLOCK:CASE_07 START ====
    @pytest.mark.parametrize("data_type,data_shape,dtype,device,batch_size,shuffle,filter_func", [
        ("tensor", [10], "int32", "cpu", None, False, "lambda x: x % 2 == 0"),  # Filter even numbers
        ("list", [15], "int32", "cpu", None, False, "lambda x: x > 5"),  # Filter values > 5
    ])
    def test_filter_operation(self, data_type, data_shape, dtype, device, batch_size, shuffle, filter_func):
        """Test filter operation on dataset (CASE_07)."""
        # Create test data based on parameters
        if data_type == "tensor":
            if dtype == "int32":
                # Create sequence 0..N-1
                data = tf.constant(list(range(data_shape[0])), dtype=tf.int32)
            else:
                raise ValueError(f"Unsupported dtype: {dtype}")
        elif data_type == "list":
            if dtype == "int32":
                # Create sequence 0..N-1
                data = list(range(data_shape[0]))
            else:
                raise ValueError(f"Unsupported dtype: {dtype}")
        else:
            raise ValueError(f"Unsupported data_type: {data_type}")
        
        # Create base dataset
        if data_type == "tensor":
            dataset = tf.data.Dataset.from_tensor_slices(data)
        else:  # list
            dataset = tf.data.Dataset.from_tensor_slices(data)
        
        # Parse filter function
        if filter_func == "lambda x: x % 2 == 0":
            def filter_fn(x):
                return x % 2 == 0
            expected_filtered = [i for i in range(data_shape[0]) if i % 2 == 0]
        elif filter_func == "lambda x: x > 5":
            def filter_fn(x):
                return x > 5
            expected_filtered = [i for i in range(data_shape[0]) if i > 5]
        else:
            raise ValueError(f"Unsupported filter_func: {filter_func}")
        
        # Apply filter operation
        filtered_dataset = dataset.filter(filter_fn)
        
        # Weak assertions for filter operation
        # 1. element_spec_preserved
        assert hasattr(filtered_dataset, 'element_spec'), "Filtered dataset should have element_spec"
        original_spec = dataset.element_spec
        filtered_spec = filtered_dataset.element_spec
        
        assert filtered_spec == original_spec, \
            f"Element spec changed after filter: original {original_spec}, filtered {filtered_spec}"
        
        # 2. filter_applied_correctly
        # Collect elements from filtered dataset
        filtered_elements = list(filtered_dataset.as_numpy_iterator())
        
        # Verify filtered elements match expected
        assert len(filtered_elements) == len(expected_filtered), \
            f"Filtered count mismatch: expected {len(expected_filtered)}, got {len(filtered_elements)}"
        
        for i, (actual, expected) in enumerate(zip(filtered_elements, expected_filtered)):
            # filtered_elements already contains numpy values from as_numpy_iterator()
            actual_value = actual
            assert actual_value == expected, \
                f"Element {i} mismatch: expected {expected}, got {actual_value}"
        
        # 3. order_preserved
        # Filter should preserve order of elements that pass the filter
        original_elements = list(dataset.as_numpy_iterator())
        
        # Manually filter original elements to get expected order
        manually_filtered = []
        for elem in original_elements:
            elem_value = elem  # Already numpy value from as_numpy_iterator()
            
            if filter_func == "lambda x: x % 2 == 0":
                if elem_value % 2 == 0:
                    manually_filtered.append(elem_value)
            elif filter_func == "lambda x: x > 5":
                if elem_value > 5:
                    manually_filtered.append(elem_value)
        
        # Compare with filtered dataset
        for i, (actual, expected) in enumerate(zip(filtered_elements, manually_filtered)):
            actual_value = actual
            assert actual_value == expected, \
                f"Order not preserved at position {i}: expected {expected}, got {actual_value}"
        
        # 4. no_false_positives
        # Verify that elements not passing filter are not included
        # Create set of filtered values for quick lookup
        filtered_set = set(filtered_elements)
        
        # Check all original elements
        for elem in original_elements:
            elem_value = elem
            
            # If element passes filter, it should be in filtered_set
            # If element doesn't pass filter, it should NOT be in filtered_set
            passes_filter = False
            if filter_func == "lambda x: x % 2 == 0":
                passes_filter = (elem_value % 2 == 0)
            elif filter_func == "lambda x: x > 5":
                passes_filter = (elem_value > 5)
            
            if passes_filter:
                assert elem_value in filtered_set, \
                    f"Element {elem_value} should pass filter but not in filtered dataset"
            else:
                assert elem_value not in filtered_set, \
                    f"Element {elem_value} should not pass filter but found in filtered dataset"
        
        # Test edge cases
        
        # Test filter that rejects all elements
        def reject_all(x):
            return False
        
        all_rejected_dataset = dataset.filter(reject_all)
        all_rejected_elements = list(all_rejected_dataset.as_numpy_iterator())
        assert len(all_rejected_elements) == 0, "Filter rejecting all should produce empty dataset"
        
        # Test filter that accepts all elements
        def accept_all(x):
            return True
        
        all_accepted_dataset = dataset.filter(accept_all)
        all_accepted_elements = list(all_accepted_dataset.as_numpy_iterator())
        assert len(all_accepted_elements) == len(original_elements), \
            f"Filter accepting all should preserve all elements: expected {len(original_elements)}, got {len(all_accepted_elements)}"
        
        # Verify all elements present and in order
        for i, (actual, expected) in enumerate(zip(all_accepted_elements, original_elements)):
            actual_value = actual
            expected_value = expected
            
            assert actual_value == expected_value, \
                f"Accept-all filter changed element {i}: expected {expected_value}, got {actual_value}"
        
        # Test filter with complex condition
        if data_type == "tensor" and filter_func == "lambda x: x % 2 == 0":
            # Test filter that uses tensor operations
            def complex_filter(x):
                # Filter even numbers that are also greater than 2
                return tf.logical_and(x % 2 == 0, x > 2)
            
            complex_filtered_dataset = dataset.filter(complex_filter)
            complex_filtered_elements = list(complex_filtered_dataset.as_numpy_iterator())
            
            # Expected: even numbers > 2 from 0..9
            expected_complex = [4, 6, 8] if data_shape[0] >= 9 else [i for i in range(data_shape[0]) if i % 2 == 0 and i > 2]
            
            assert len(complex_filtered_elements) == len(expected_complex), \
                f"Complex filter count mismatch: expected {len(expected_complex)}, got {len(complex_filtered_elements)}"
            
            for i, (actual, expected) in enumerate(zip(complex_filtered_elements, expected_complex)):
                assert actual == expected, \
                    f"Complex filter element {i} mismatch: expected {expected}, got {actual}"
        
        # Test filter in pipeline
        pipeline = dataset.filter(filter_fn).batch(2).prefetch(1)
        pipeline_elements = list(pipeline.as_numpy_iterator())
        
        # Calculate expected batches
        expected_batches = math.ceil(len(expected_filtered) / 2)
        assert len(pipeline_elements) == expected_batches, \
            f"Pipeline batch count mismatch: expected {expected_batches}, got {len(pipeline_elements)}"
        
        # Verify all filtered elements present in pipeline (flatten batches)
        pipeline_flattened = []
        for batch in pipeline_elements:
            for i in range(batch.shape[0]):
                pipeline_flattened.append(batch[i])
        
        assert len(pipeline_flattened) == len(expected_filtered), \
            f"Pipeline lost filtered elements: expected {len(expected_filtered)}, got {len(pipeline_flattened)}"
        
        # Verify order preserved in pipeline
        for i, (actual, expected) in enumerate(zip(pipeline_flattened, expected_filtered)):
            assert actual == expected, \
                f"Pipeline order mismatch at position {i}: expected {expected}, got {actual}"
        
        # Test filter with shuffle
        if shuffle:
            shuffled_filtered_dataset = dataset.shuffle(buffer_size=5).filter(filter_fn)
            shuffled_filtered_elements = list(shuffled_filtered_dataset.as_numpy_iterator())
            
            # Should still have correct elements (order may differ due to shuffle)
            shuffled_filtered_set = set(shuffled_filtered_elements)
            expected_set = set(expected_filtered)
            assert shuffled_filtered_set == expected_set, \
                "Shuffle+filter changed which elements pass filter"
        
        # Test error handling with invalid filter function
        # Filter function must return a boolean tensor/scalar
        def invalid_filter(x):
            return x  # Returns tensor instead of boolean
        
        # This should raise an error when creating the filter dataset
        with pytest.raises(ValueError, match="Invalid `predicate`"):
            invalid_filtered_dataset = dataset.filter(invalid_filter)
            # Try to iterate to trigger the error
            list(invalid_filtered_dataset.as_numpy_iterator())
        
        # Test filter preserves element spec dtype
        filtered_spec = filtered_dataset.element_spec
        assert filtered_spec.dtype == (tf.int32 if dtype == "int32" else tf.int64), \
            f"Filter changed dtype: expected {dtype}, got {filtered_spec.dtype}"
# ==== BLOCK:CASE_07 END ====

# ==== BLOCK:FOOTER START ====
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
# ==== BLOCK:FOOTER END ====