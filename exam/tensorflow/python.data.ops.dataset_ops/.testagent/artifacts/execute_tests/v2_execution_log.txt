=== Run Tests ===
.........F.FF                                                            [100%]
================================== FAILURES ===================================
_ TestDatasetOps.test_shuffle_operation[tensor-data_shape0-float32-cpu-None-True-42] _

self = <test_tensorflow_python_data_ops_dataset_ops.TestDatasetOps object at 0x00000147DDEE9EE0>
data_type = 'tensor', data_shape = [10, 2], dtype = 'float32', device = 'cpu'
batch_size = None, shuffle = True, seed = 42

    @pytest.mark.parametrize("data_type,data_shape,dtype,device,batch_size,shuffle,seed", [
        ("tensor", [10, 2], "float32", "cpu", None, True, 42),
        ("list", [20], "int32", "cpu", None, True, 123),
    ])
    def test_shuffle_operation(self, data_type, data_shape, dtype, device, batch_size, shuffle, seed):
        """Test shuffle operation on dataset (CASE_06)."""
        # Set random seed for reproducibility
        tf.random.set_seed(seed)
        np.random.seed(seed)
    
        # Create test data based on parameters
        if data_type == "tensor":
            if dtype == "float32":
                data = tf.constant(list(range(data_shape[0])), dtype=tf.float32)
                # Reshape to 2D if needed
                if len(data_shape) > 1:
>                   data = tf.reshape(data, data_shape)

tests\test_tensorflow_python_data_ops_dataset_ops.py:500: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
D:\Coding\Anaconda\envs\testagent-experiment\lib\site-packages\tensorflow\python\util\traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

op_name = b'Reshape', num_outputs = 1
inputs = [<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([10,  2])>]
attrs = ('T', 1, 'Tshape', 3)
ctx = <tensorflow.python.eager.context.Context object at 0x00000147D6AD9D90>
name = None

    def quick_execute(op_name, num_outputs, inputs, attrs, ctx, name=None):
      """Execute a TensorFlow operation.
    
      Args:
        op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to
          execute.
        num_outputs: The number of outputs of the operation to fetch. (Explicitly
          provided instead of being inferred for performance reasons).
        inputs: A list of inputs to the operation. Each entry should be a Tensor, or
          a value which can be passed to the Tensor constructor to create one.
        attrs: A tuple with alternating string attr names and attr values for this
          operation.
        ctx: The value of context.context().
        name: Customized name for the operation.
    
      Returns:
        List of output Tensor objects. The list is empty if there are no outputs
    
      Raises:
        An exception on error.
      """
      device_name = ctx.device_name
      # pylint: disable=protected-access
      try:
        ctx.ensure_initialized()
>       tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
                                            inputs, attrs, num_outputs)
E                                           tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 10 values, but the requested shape has 20 [Op:Reshape]

D:\Coding\Anaconda\envs\testagent-experiment\lib\site-packages\tensorflow\python\eager\execute.py:54: InvalidArgumentError
_ TestDatasetOps.test_filter_operation[tensor-data_shape0-int32-cpu-None-False-lambda x: x % 2 == 0] _

self = <test_tensorflow_python_data_ops_dataset_ops.TestDatasetOps object at 0x00000147DDEF24C0>
data_type = 'tensor', data_shape = [10], dtype = 'int32', device = 'cpu'
batch_size = None, shuffle = False, filter_func = 'lambda x: x % 2 == 0'

    @pytest.mark.parametrize("data_type,data_shape,dtype,device,batch_size,shuffle,filter_func", [
        ("tensor", [10], "int32", "cpu", None, False, "lambda x: x % 2 == 0"),  # Filter even numbers
        ("list", [15], "int32", "cpu", None, False, "lambda x: x > 5"),  # Filter values > 5
    ])
    def test_filter_operation(self, data_type, data_shape, dtype, device, batch_size, shuffle, filter_func):
        """Test filter operation on dataset (CASE_07)."""
        # Create test data based on parameters
        if data_type == "tensor":
            if dtype == "int32":
                # Create sequence 0..N-1
                data = tf.constant(list(range(data_shape[0])), dtype=tf.int32)
            else:
                raise ValueError(f"Unsupported dtype: {dtype}")
        elif data_type == "list":
            if dtype == "int32":
                # Create sequence 0..N-1
                data = list(range(data_shape[0]))
            else:
                raise ValueError(f"Unsupported dtype: {dtype}")
        else:
            raise ValueError(f"Unsupported data_type: {data_type}")
    
        # Create base dataset
        if data_type == "tensor":
            dataset = tf.data.Dataset.from_tensor_slices(data)
        else:  # list
            dataset = tf.data.Dataset.from_tensor_slices(data)
    
        # Parse filter function
        if filter_func == "lambda x: x % 2 == 0":
            def filter_fn(x):
                return x % 2 == 0
            expected_filtered = [i for i in range(data_shape[0]) if i % 2 == 0]
        elif filter_func == "lambda x: x > 5":
            def filter_fn(x):
                return x > 5
            expected_filtered = [i for i in range(data_shape[0]) if i > 5]
        else:
            raise ValueError(f"Unsupported filter_func: {filter_func}")
    
        # Apply filter operation
        filtered_dataset = dataset.filter(filter_fn)
    
        # Weak assertions for filter operation
        # 1. element_spec_preserved
        assert hasattr(filtered_dataset, 'element_spec'), "Filtered dataset should have element_spec"
        original_spec = dataset.element_spec
        filtered_spec = filtered_dataset.element_spec
    
        assert filtered_spec == original_spec, \
            f"Element spec changed after filter: original {original_spec}, filtered {filtered_spec}"
    
        # 2. filter_applied_correctly
        # Collect elements from filtered dataset
        filtered_elements = list(filtered_dataset.as_numpy_iterator())
    
        # Verify filtered elements match expected
        assert len(filtered_elements) == len(expected_filtered), \
            f"Filtered count mismatch: expected {len(expected_filtered)}, got {len(filtered_elements)}"
    
        for i, (actual, expected) in enumerate(zip(filtered_elements, expected_filtered)):
            if data_type == "tensor":
>               actual_value = actual.numpy()
E               AttributeError: 'numpy.int32' object has no attribute 'numpy'

tests\test_tensorflow_python_data_ops_dataset_ops.py:736: AttributeError
_ TestDatasetOps.test_filter_operation[list-data_shape1-int32-cpu-None-False-lambda x: x > 5] _

self = <test_tensorflow_python_data_ops_dataset_ops.TestDatasetOps object at 0x00000147DDEF27C0>
data_type = 'list', data_shape = [15], dtype = 'int32', device = 'cpu'
batch_size = None, shuffle = False, filter_func = 'lambda x: x > 5'

    @pytest.mark.parametrize("data_type,data_shape,dtype,device,batch_size,shuffle,filter_func", [
        ("tensor", [10], "int32", "cpu", None, False, "lambda x: x % 2 == 0"),  # Filter even numbers
        ("list", [15], "int32", "cpu", None, False, "lambda x: x > 5"),  # Filter values > 5
    ])
    def test_filter_operation(self, data_type, data_shape, dtype, device, batch_size, shuffle, filter_func):
        """Test filter operation on dataset (CASE_07)."""
        # Create test data based on parameters
        if data_type == "tensor":
            if dtype == "int32":
                # Create sequence 0..N-1
                data = tf.constant(list(range(data_shape[0])), dtype=tf.int32)
            else:
                raise ValueError(f"Unsupported dtype: {dtype}")
        elif data_type == "list":
            if dtype == "int32":
                # Create sequence 0..N-1
                data = list(range(data_shape[0]))
            else:
                raise ValueError(f"Unsupported dtype: {dtype}")
        else:
            raise ValueError(f"Unsupported data_type: {data_type}")
    
        # Create base dataset
        if data_type == "tensor":
            dataset = tf.data.Dataset.from_tensor_slices(data)
        else:  # list
            dataset = tf.data.Dataset.from_tensor_slices(data)
    
        # Parse filter function
        if filter_func == "lambda x: x % 2 == 0":
            def filter_fn(x):
                return x % 2 == 0
            expected_filtered = [i for i in range(data_shape[0]) if i % 2 == 0]
        elif filter_func == "lambda x: x > 5":
            def filter_fn(x):
                return x > 5
            expected_filtered = [i for i in range(data_shape[0]) if i > 5]
        else:
            raise ValueError(f"Unsupported filter_func: {filter_func}")
    
        # Apply filter operation
        filtered_dataset = dataset.filter(filter_fn)
    
        # Weak assertions for filter operation
        # 1. element_spec_preserved
        assert hasattr(filtered_dataset, 'element_spec'), "Filtered dataset should have element_spec"
        original_spec = dataset.element_spec
        filtered_spec = filtered_dataset.element_spec
    
        assert filtered_spec == original_spec, \
            f"Element spec changed after filter: original {original_spec}, filtered {filtered_spec}"
    
        # 2. filter_applied_correctly
        # Collect elements from filtered dataset
        filtered_elements = list(filtered_dataset.as_numpy_iterator())
    
        # Verify filtered elements match expected
        assert len(filtered_elements) == len(expected_filtered), \
            f"Filtered count mismatch: expected {len(expected_filtered)}, got {len(filtered_elements)}"
    
        for i, (actual, expected) in enumerate(zip(filtered_elements, expected_filtered)):
            if data_type == "tensor":
                actual_value = actual.numpy()
            else:
                actual_value = actual
    
            assert actual_value == expected, \
                f"Element {i} mismatch: expected {expected}, got {actual_value}"
    
        # 3. order_preserved
        # Filter should preserve order of elements that pass the filter
        original_elements = list(dataset.as_numpy_iterator())
    
        # Manually filter original elements to get expected order
        manually_filtered = []
        for elem in original_elements:
            if data_type == "tensor":
                elem_value = elem.numpy()
            else:
                elem_value = elem
    
            if filter_func == "lambda x: x % 2 == 0":
                if elem_value % 2 == 0:
                    manually_filtered.append(elem_value)
            elif filter_func == "lambda x: x > 5":
                if elem_value > 5:
                    manually_filtered.append(elem_value)
    
        # Compare with filtered dataset
        for i, (actual, expected) in enumerate(zip(filtered_elements, manually_filtered)):
            if data_type == "tensor":
                actual_value = actual.numpy()
            else:
                actual_value = actual
    
            assert actual_value == expected, \
                f"Order not preserved at position {i}: expected {expected}, got {actual_value}"
    
        # 4. no_false_positives
        # Verify that elements not passing filter are not included
        # Create set of filtered values for quick lookup
        filtered_set = set()
        for elem in filtered_elements:
            if data_type == "tensor":
                filtered_set.add(elem.numpy())
            else:
                filtered_set.add(elem)
    
        # Check all original elements
        for elem in original_elements:
            if data_type == "tensor":
                elem_value = elem.numpy()
            else:
                elem_value = elem
    
            # If element passes filter, it should be in filtered_set
            # If element doesn't pass filter, it should NOT be in filtered_set
            passes_filter = False
            if filter_func == "lambda x: x % 2 == 0":
                passes_filter = (elem_value % 2 == 0)
            elif filter_func == "lambda x: x > 5":
                passes_filter = (elem_value > 5)
    
            if passes_filter:
                assert elem_value in filtered_set, \
                    f"Element {elem_value} should pass filter but not in filtered dataset"
            else:
                assert elem_value not in filtered_set, \
                    f"Element {elem_value} should not pass filter but found in filtered dataset"
    
        # Test edge cases
    
        # Test filter that rejects all elements
        def reject_all(x):
            return False
    
        all_rejected_dataset = dataset.filter(reject_all)
        all_rejected_elements = list(all_rejected_dataset.as_numpy_iterator())
        assert len(all_rejected_elements) == 0, "Filter rejecting all should produce empty dataset"
    
        # Test filter that accepts all elements
        def accept_all(x):
            return True
    
        all_accepted_dataset = dataset.filter(accept_all)
        all_accepted_elements = list(all_accepted_dataset.as_numpy_iterator())
        assert len(all_accepted_elements) == len(original_elements), \
            f"Filter accepting all should preserve all elements: expected {len(original_elements)}, got {len(all_accepted_elements)}"
    
        # Verify all elements present and in order
        for i, (actual, expected) in enumerate(zip(all_accepted_elements, original_elements)):
            if data_type == "tensor":
                actual_value = actual.numpy()
                expected_value = expected.numpy()
            else:
                actual_value = actual
                expected_value = expected
    
            assert actual_value == expected_value, \
                f"Accept-all filter changed element {i}: expected {expected_value}, got {actual_value}"
    
        # Test filter with complex condition
        if data_type == "tensor" and filter_func == "lambda x: x % 2 == 0":
            # Test filter that uses tensor operations
            def complex_filter(x):
                # Filter even numbers that are also greater than 2
                return tf.logical_and(x % 2 == 0, x > 2)
    
            complex_filtered_dataset = dataset.filter(complex_filter)
            complex_filtered_elements = list(complex_filtered_dataset.as_numpy_iterator())
    
            # Expected: even numbers > 2 from 0..9
            expected_complex = [4, 6, 8] if data_shape[0] >= 9 else [i for i in range(data_shape[0]) if i % 2 == 0 and i > 2]
    
            assert len(complex_filtered_elements) == len(expected_complex), \
                f"Complex filter count mismatch: expected {len(expected_complex)}, got {len(complex_filtered_elements)}"
    
            for i, (actual, expected) in enumerate(zip(complex_filtered_elements, expected_complex)):
                assert actual.numpy() == expected, \
                    f"Complex filter element {i} mismatch: expected {expected}, got {actual.numpy()}"
    
        # Test filter in pipeline
        pipeline = dataset.filter(filter_fn).batch(2).prefetch(1)
        pipeline_elements = list(pipeline.as_numpy_iterator())
    
        # Calculate expected batches
        expected_batches = math.ceil(len(expected_filtered) / 2)
        assert len(pipeline_elements) == expected_batches, \
            f"Pipeline batch count mismatch: expected {expected_batches}, got {len(pipeline_elements)}"
    
        # Verify all filtered elements present in pipeline (flatten batches)
        pipeline_flattened = []
        for batch in pipeline_elements:
            if data_type == "tensor":
                for i in range(batch.shape[0]):
                    pipeline_flattened.append(batch[i].numpy())
            else:
                for elem in batch:
                    pipeline_flattened.append(elem)
    
        assert len(pipeline_flattened) == len(expected_filtered), \
            f"Pipeline lost filtered elements: expected {len(expected_filtered)}, got {len(pipeline_flattened)}"
    
        # Verify order preserved in pipeline
        for i, (actual, expected) in enumerate(zip(pipeline_flattened, expected_filtered)):
            assert actual == expected, \
                f"Pipeline order mismatch at position {i}: expected {expected}, got {actual}"
    
        # Test filter with shuffle
        if shuffle:
            shuffled_filtered_dataset = dataset.shuffle(buffer_size=5).filter(filter_fn)
            shuffled_filtered_elements = list(shuffled_filtered_dataset.as_numpy_iterator())
    
            # Should still have correct elements (order may differ due to shuffle)
            shuffled_filtered_set = set()
            for elem in shuffled_filtered_elements:
                if data_type == "tensor":
                    shuffled_filtered_set.add(elem.numpy())
                else:
                    shuffled_filtered_set.add(elem)
    
            expected_set = set(expected_filtered)
            assert shuffled_filtered_set == expected_set, \
                "Shuffle+filter changed which elements pass filter"
    
        # Test error handling with invalid filter function
        # Filter function must return a boolean tensor/scalar
        def invalid_filter(x):
            return x  # Returns tensor instead of boolean
    
        # This should raise an error when iterating
>       invalid_filtered_dataset = dataset.filter(invalid_filter)

tests\test_tensorflow_python_data_ops_dataset_ops.py:905: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
D:\Coding\Anaconda\envs\testagent-experiment\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py:2214: in filter
    return FilterDataset(self, predicate, name=name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <FilterDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>
input_dataset = <TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>
predicate = <function TestDatasetOps.test_filter_operation.<locals>.invalid_filter at 0x00000147DF0A85E0>
use_legacy_function = False, name = None

    def __init__(self,
                 input_dataset,
                 predicate,
                 use_legacy_function=False,
                 name=None):
      """See `Dataset.filter()` for details."""
      self._input_dataset = input_dataset
      wrapped_func = structured_function.StructuredFunctionWrapper(
          predicate,
          self._transformation_name(),
          dataset=input_dataset,
          use_legacy_function=use_legacy_function)
      if not wrapped_func.output_structure.is_compatible_with(
          tensor_spec.TensorSpec([], dtypes.bool)):
>       raise ValueError(f"Invalid `predicate`. `predicate` must return a "
                         f"`tf.bool` scalar tensor, but its return type is "
                         f"{wrapped_func.output_structure}.")
E       ValueError: Invalid `predicate`. `predicate` must return a `tf.bool` scalar tensor, but its return type is TensorSpec(shape=(), dtype=tf.int32, name=None).

D:\Coding\Anaconda\envs\testagent-experiment\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py:5442: ValueError

---------- coverage: platform win32, python 3.9.25-final-0 -----------
Name                                                   Stmts   Miss Branch BrPart  Cover   Missing
--------------------------------------------------------------------------------------------------
tests\test_tensorflow_python_data_ops_dataset_ops.py     435     93    214     59    74%   40-42, 56->63, 58->63, 104-106, 118->125, 120->125, 142, 156->exit, 174-176, 184, 187-189, 223->214, 239->246, 251->246, 268-270, 313, 373-375, 384, 405->413, 414, 422-424, 427->433, 448, 456-458, 499->512, 502, 507-509, 513, 551-559, 564->597, 567->578, 569->567, 573-575, 578->583, 593, 597->636, 609-615, 627-631, 655, 686, 692-694, 705, 709, 712, 751, 756-757, 758->749, 765, 778, 785, 793, 794->797, 808, 816, 826-827, 838-852, 868-869, 884-896, 902, 906-916, 922
--------------------------------------------------------------------------------------------------
TOTAL                                                    435     93    214     59    74%
Coverage XML written to file coverage.xml

=========================== short test summary info ===========================
FAILED tests\test_tensorflow_python_data_ops_dataset_ops.py::TestDatasetOps::test_shuffle_operation[tensor-data_shape0-float32-cpu-None-True-42]
FAILED tests\test_tensorflow_python_data_ops_dataset_ops.py::TestDatasetOps::test_filter_operation[tensor-data_shape0-int32-cpu-None-False-lambda x: x % 2 == 0]
FAILED tests\test_tensorflow_python_data_ops_dataset_ops.py::TestDatasetOps::test_filter_operation[list-data_shape1-int32-cpu-None-False-lambda x: x > 5]
3 failed, 10 passed in 2.02s

Error: exit 1